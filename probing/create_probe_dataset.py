from thinker.main import make, Env
from thinker.actor_net import DRCNet
from torch.utils.data.dataset import Dataset
import torch
from thinker import util
from typing import Callable, NamedTuple
import os

def make_current_board_feature_detector(feature_idx: int, mode: str) -> Callable:
    """Create feature detector functions to extract discrete features from mini-sokoban boards. Boards must be (7,8,8) arrays

    Args:
        feature_idx (int): index of feature of interest (see sokoban.cpp)
        mode (str): either "adj" (to count number of adjacent features) or "num" (to count total number of features on board)

    Returns:
        Callable: feature detector function, takes in a board state and returns a count for the number of times the relevant feature appears
    """
    assert mode in ["adj", "num"], "Please enter a valid mode - either ADJ or NUM"
    if mode == "adj":
        def feature_detector(board: torch.tensor) -> int:
            h, w = board.shape[1:]
            x, y = ((board[4,:,:]==1) + (board[5,:,:]==1)).nonzero()[0,:]
            adj_coords = [(xp, yp) for xp, yp in [(x+1,y), (x-1,y), (x,y+1), (x,y-1)] if xp>-1 and xp<h and yp>-1 and yp<w]
            n_hits = 0
            for (xp,yp) in adj_coords:
                if board[feature_idx, xp, yp] == 1:
                    n_hits += 1
            return n_hits
    else:
        def feature_detector(board: torch.tensor) -> int:
            return torch.sum((board[feature_idx,:,:]==1).int()).item()
    return feature_detector

def make_future_feature_detector(feature_name: str, steps_ahead: int) -> Callable:
    """Create function that adds a feature to each transition (i.e. a dictionary of features) corresponding to the feature with name feature_name in steps_ahead steps

    Args:
        feature_name (str): feature to track steps_ahead into the future
        steps_ahead (int): number of steps ahead into the future to look for this features

    Returns:
        Callable: feature detector function, takes in a list of transitions for a single episode and adds an entry for feature_name in steps_ahead steps
    """
    future_feature_name = f"{feature_name}_plus_{steps_ahead}"
    def feature_detector(episode_entry: list) -> list:
        assert feature_name in episode_entry[0].keys(), f"Error: This feature detector has been set up to track {feature_name} which is not contained in the episode entry - please re-create it using one of the following features: {episode_entry[0].keys()}"
        episode_length = len(episode_entry)
        for step, step_entry in enumerate(episode_entry):
            step_entry[future_feature_name] = episode_entry[step+steps_ahead][feature_name] if step < episode_length-steps_ahead-1 else 42
        return episode_entry
    return feature_detector

@torch.no_grad
def create_probing_data(drc_net: DRCNet, env: Env, flags: NamedTuple, num_episodes: int, current_board_feature_fncs: list, future_feature_fncs: list) -> list:
    """Generate a list where each entry is a dictionary of features corresponding to a single transition

    Args:
        drc_net (DRCNet): Trained DRC network used to generate transitions
        env (Env): Sokoban environment
        flags (NamedTuple): flag object
        num_episodes (int): number of episodes to run to generate the transitions
        current_board_feature_fncs (list): list of tuples of the form (feature_name, feature_fnc), where each feature_fnc extracts a discrete feature from the current state of the Sokoban board; this feature is then added to the episode entry (dictionary) with the key feature_name
        future_feature_fncs (list): list of functions where each function adds a feature to the current transition corresponding to the value taken by some other feature in a future transition

    Returns:
        list: returns probing_data, a list of dictionaries where each dictionary contains features for a single transition generated by the DRC agent
    """
    rnn_state = drc_net.initial_state(batch_size=1, device=env.device)
    state = env.reset() 
    env_out = util.init_env_out(state, flags, dim_actions=1, tuple_action=False)
    actor_out, rnn_state = drc_net(env_out, rnn_state)

    episode_length = 0
    board_num = 0
    probing_data = []
    episode_entry = []
    while(board_num < num_episodes):
        if episode_length > 0:
            step_entry["reward"] = reward.item()
            episode_entry.append(step_entry)
        state, reward, done, info = env.step(actor_out.action)
        env_out = util.create_env_out(actor_out.action, state, reward, done, info, flags)
        actor_out, rnn_state = drc_net(env_out, rnn_state)

        step_entry = {feature:fnc(state["real_states"][0]) for feature, fnc in current_board_feature_fncs}
        step_entry["action"] = actor_out.action.item()
        step_entry["board_state"] = state["real_states"][0] # tensor of size (channels, board_height, board_width)
        step_entry["hidden_states"] = drc_net.hidden_state[0] # tensor of size (ticks+1, layers*64, representation_heigh, representation_width)
        step_entry["board_num"] = board_num
        episode_length += 1

        if done:
            for fnc in future_feature_fncs:
                episode_entry = fnc(episode_entry)
            for step, step_entry in enumerate(episode_entry):
                step_entry["steps_remaining"] = episode_length - step - 1 
            probing_data += episode_entry
            episode_length = 0
            board_num += 1

    return probing_data

class ProbingDataset(Dataset):
    def __init__(self, data: list):
        self.data = data
    def __len__(self) -> int:
        return len(self.data)
    def __getitem__(self, index: int) -> dict:
        return self.data[index]
    def get_feature_range(self, feature: str) -> tuple[int, int]:
        assert feature in self.data[0].keys(), f"Please enter a feature in dataset: {self.data[0].keys()}"
        min_feature_value, max_feature_value = self.data[0][feature], self.data[0][feature]
        for entry in self.data:
            if entry[feature] > max_feature_value:
                max_feature_value = entry[feature]
            elif entry[feature] < min_feature_value:
                min_feature_value = entry[feature]
        return (min_feature_value, max_feature_value)

if __name__=="__main__":

    mini = True
    gpu = False
    pct_train = 0.6
    num_episodes = 10

    adj_wall_detector = make_current_board_feature_detector(feature_idx=0, mode="adj")
    adj_boxnotontar_detector = make_current_board_feature_detector(feature_idx=2, mode="adj")
    adj_boxontar_detector = make_current_board_feature_detector(feature_idx=3, mode="adj")
    adj_tar_detector = make_current_board_feature_detector(feature_idx=6, mode="adj")
    num_boxnotontar_detector = make_current_board_feature_detector(feature_idx=2, mode="num")
    current_board_feature_fncs = [
        ("adj_walls", adj_wall_detector),
        ("adj_boxnotontar", adj_boxnotontar_detector),
        ("adj_boxontar", adj_boxontar_detector),
        ("adj_tar_detector", adj_tar_detector),
        ("num_boxnotontar_detector", num_boxnotontar_detector)
    ]

    future_feature_fncs = [make_future_feature_detector(feature_name="action",steps_ahead=t) for t in range(1,6)] + [make_future_feature_detector(feature_name="reward",steps_ahead=t) for t in range(1,6)]

    env = make("Sokoban-v0",env_n=1,gpu=gpu,wrapper_type=1,has_model=False,train_model=False,parallel=False,save_flags=False,mini=mini)
    flags = util.create_setting(args=[], save_flags=False, wrapper_type=1) 
    flags.mini = mini
    drc_net = DRCNet(
        obs_space=env.observation_space,
        action_space=env.action_space,
        flags=flags,
        record_state=True,
    )
    ckp_path = "../drc_mini"
    ckp_path = os.path.join(util.full_path(ckp_path), "ckp_actor_realstep49500192.tar")
    ckp = torch.load(ckp_path, env.device)
    drc_net.load_state_dict(ckp["actor_net_state_dict"], strict=False)
    drc_net.to(env.device)

    probing_data = create_probing_data(drc_net=drc_net,
                                       env=env,
                                       flags=flags,
                                       num_episodes=num_episodes,
                                       current_board_feature_fncs=current_board_feature_fncs,
                                       future_feature_fncs=future_feature_fncs)

    final_train_board = int(num_episodes * pct_train)
    final_val_board = final_train_board + int(num_episodes * 0.5 * (1 - pct_train))
    probing_train_data = [entry for entry in probing_data if entry["board_num"] <= final_train_board]
    probing_val_data = [entry for entry in probing_data if entry["board_num"] > final_train_board and entry["board_num"] <= final_val_board]
    probing_test_data = [entry for entry in probing_data if entry["board_num"] > final_val_board]
    torch.save(ProbingDataset(probing_train_data), "./data/train_data.pt")
    torch.save(ProbingDataset(probing_val_data), "./data/val_data.pt")
    torch.save(ProbingDataset(probing_test_data), "./data/test_data.pt")
