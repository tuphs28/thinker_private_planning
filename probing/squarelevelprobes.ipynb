{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/mlmi/dissertation/working_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-26 08:17:26,528\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from typing import Callable, Optional\n",
    "from probe_model import DRCProbe\n",
    "from create_probe_dataset import ProbingDataset, ProbingDatasetCleaned\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import thinker\n",
    "import thinker.viz_utils as viz\n",
    "import thinker.util as util\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from celluloid import Camera\n",
    "from thinker.actor_net import DRCNet\n",
    "import os\n",
    "import random\n",
    "from numpy.random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe Single Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = torch.load(\"./data/train_data_random.pt\")\n",
    "test_data = torch.load(\"./data/test_data_random.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniProbe(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(32,2, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = MiniProbe()\n",
    "optimiser = torch.optim.Adam(probe.parameters(), lr=3e-4)\n",
    "loss_fnc = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4053,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ===== xenc ======\n",
      "0.49141630901287553\n",
      "0.5536480686695279\n",
      "0.5429184549356223\n",
      "0.5472103004291845\n",
      "0.5536480686695279\n",
      "0.5450643776824035\n",
      " ===== layer0 ======\n",
      "0.6180257510729614\n",
      "0.7253218884120172\n",
      "0.7167381974248928\n",
      "0.721030042918455\n",
      "0.7253218884120172\n",
      "0.7296137339055794\n",
      " ===== layer1 ======\n",
      "0.5515021459227468\n",
      "0.7167381974248928\n",
      "0.7103004291845494\n",
      "0.7081545064377682\n",
      "0.7124463519313304\n",
      "0.7124463519313304\n",
      " ===== layer2 ======\n",
      "0.572961373390558\n",
      "0.723175965665236\n",
      "0.721030042918455\n",
      "0.7124463519313304\n",
      "0.7081545064377682\n",
      "0.7060085836909872\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "feature = \"tar_next\"\n",
    "alt_feature = \"justtar_loc\"\n",
    "pct_keep = 1\n",
    "for name, c in [(\"xenc\", 192), (\"layer0\", 32), (\"layer1\", 96), (\"layer2\", 160)]:\n",
    "\n",
    "    probe = MiniProbe()\n",
    "    optimiser = torch.optim.Adam(probe.parameters(), lr=3e-4)\n",
    "    loss_fnc = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(f\" ===== {name} ======\")\n",
    "    sup_data = []\n",
    "    for trans in val_data.data:\n",
    "        if len(trans[alt_feature]) > 1:\n",
    "            next_loc = trans[feature][0]\n",
    "            #next_loc = trans[feature]\n",
    "            other_loc = random.choice([t_loc for t_loc in trans[alt_feature] if t_loc!=next_loc])\n",
    "            next_loc_x = next_loc % 8\n",
    "            next_loc_y = (next_loc - next_loc_x) // 8\n",
    "            other_loc_x = other_loc % 8\n",
    "            other_loc_y = (other_loc - other_loc_x) // 8\n",
    "            next_tar_vec = trans[\"hidden_states\"][-1,c:c+32,next_loc_y, next_loc_x]\n",
    "            other_tar_vec = trans[\"hidden_states\"][-1,c:c+32,other_loc_y, other_loc_x]\n",
    "            diff_tar_vec = next_tar_vec - other_tar_vec\n",
    "            if uniform(0,1) < pct_keep:\n",
    "                sup_data.append((next_tar_vec, 1))\n",
    "                sup_data.append((other_tar_vec, 0))\n",
    "    train_dataset = ProbingDatasetCleaned(sup_data)\n",
    "\n",
    "    sup_data = []\n",
    "    for trans in test_data.data:\n",
    "        if len(trans[alt_feature]) > 1:\n",
    "            next_loc = trans[feature][0]\n",
    "            #next_loc = trans[feature]\n",
    "            other_loc = random.choice([t_loc for t_loc in trans[alt_feature] if t_loc!=next_loc])\n",
    "            next_loc_x = next_loc % 8\n",
    "            next_loc_y = (next_loc - next_loc_x) // 8\n",
    "            other_loc_x = other_loc % 8\n",
    "            other_loc_y = (other_loc - other_loc_x) // 8\n",
    "            next_tar_vec = trans[\"hidden_states\"][-1,c:c+32,next_loc_y, next_loc_x]\n",
    "            other_tar_vec = trans[\"hidden_states\"][-1,c:c+32,other_loc_y, other_loc_x]\n",
    "            diff_tar_vec = next_tar_vec - other_tar_vec\n",
    "            if uniform(0,1) < pct_keep:\n",
    "                sup_data.append((next_tar_vec, 1))\n",
    "                sup_data.append((other_tar_vec, 0))\n",
    "    test_dataset = ProbingDatasetCleaned(sup_data)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=16)\n",
    "    test_loader = DataLoader(test_dataset, shuffle=True, batch_size=16)\n",
    "\n",
    "    probe = MiniProbe()\n",
    "    optimiser = torch.optim.Adam(probe.parameters(), lr=3e-4)\n",
    "    loss_fnc = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(300):\n",
    "        for vecs, labs in train_loader:\n",
    "            optimiser.zero_grad()\n",
    "            logits = probe(vecs)\n",
    "            loss = loss_fnc(logits, labs)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                running_acc = 0\n",
    "                for vecs, labs in test_loader:\n",
    "                    logits = probe(vecs)\n",
    "                    running_acc += torch.sum(logits.argmax(dim=1) == labs).item()\n",
    "                print(running_acc / len(test_dataset.data))\n",
    "    results[name] = {\"test acc\": round(running_acc / len(test_dataset.data), 3), \"weights\": probe.fc.weight.tolist()}\n",
    "pd.DataFrame(results).to_csv(f\"./squarelevelproberesults/{feature}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2914,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"next_start_move_box_loc\"\n",
    "results_df = pd.read_csv(f\"./squarelevelproberesults/{feature}.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2915,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(height=[float(c) for c in results_df.iloc[0,:].tolist()], x=range(4))\n",
    "plt.xticks(range(4), [c[0] for c in [(\"xenc\", 192), (\"layer0\", 32), (\"layer1\", 96), (\"layer2\", 160)]])\n",
    "plt.ylim(0.4,1)\n",
    "plt.ylabel(\"test acc\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intervene On Single Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinker.actor_net import sample\n",
    "class ActPatchDRCNet:\n",
    "\n",
    "    def __init__(self, drc_net):\n",
    "        self.drc_net = drc_net\n",
    "\n",
    "    def forward_normal(self, env_out, rnn_state):\n",
    "        return self.drc_net(env_out, rnn_state)\n",
    "    \n",
    "    def forward_patch(self, env_out, rnn_state, greedy=True, activ_type=None, patch_dict={}, activ_ticks=[], activs=None):\n",
    "        \n",
    "        activ_layers = list(patch_dict.keys())\n",
    "\n",
    "        done = env_out.done\n",
    "        T, B = done.shape\n",
    "        x = self.drc_net.normalize(env_out.real_states.float())\n",
    "        x = torch.flatten(x, 0, 1)\n",
    "        x_enc = self.drc_net.encoder(x)\n",
    "        core_input = x_enc.view(*((T, B) + x_enc.shape[1:]))\n",
    "\n",
    "        record_state=self.drc_net.record_state\n",
    "\n",
    "        assert len(core_input.shape) == 5\n",
    "        core_output_list = []\n",
    "        reset = done.float()\n",
    "        if self.drc_net.record_state: \n",
    "            self.drc_net.core.hidden_state = []\n",
    "            self.drc_net.core.hidden_state.append(torch.concat(rnn_state, dim=1)) \n",
    "        for n, (x_single, reset_single) in enumerate(\n",
    "            zip(core_input.unbind(), reset.unbind())\n",
    "        ):\n",
    "            for t in range(self.drc_net.core.tran_t):\n",
    "                if t > 0:\n",
    "                    reset_single = torch.zeros_like(reset_single)\n",
    "                reset_single = reset_single.view(-1)\n",
    "                if t in activ_ticks and (0 in activ_layers or 1 in activ_layers or 2 in activ_layers):\n",
    "                    #print(f\"----- patching activations for tick {t} ---- \")\n",
    "                    output, rnn_state = self.forward_single_patch(\n",
    "                        x=x_single,\n",
    "                        core_state=rnn_state,\n",
    "                        reset=reset_single,\n",
    "                        activ_type=activ_type, \n",
    "                        patch_dict=patch_dict,\n",
    "                        activs=activs[:,t,:,:,:]\n",
    "                    )  # output shape: 1, B, core_output_size\n",
    "                else:\n",
    "                    output, rnn_state = self.drc_net.core.forward_single(\n",
    "                        x_single, rnn_state, reset_single, reset_single\n",
    "                    )        \n",
    "                if self.drc_net.record_state: self.drc_net.core.hidden_state.append(torch.concat(rnn_state, dim=1))      \n",
    "            core_output_list.append(output)\n",
    "        core_output = torch.cat(core_output_list)\n",
    "        if self.drc_net.record_state: \n",
    "           self.drc_net.core.hidden_state = torch.stack(self.drc_net.core.hidden_state, dim=1)\n",
    "\n",
    "        core_output = torch.flatten(core_output, 0, 1)\n",
    "\n",
    "        if activ_type == \"xenc\" and 3 in activ_layers and 2 in activ_ticks:\n",
    "            #print(f\"--- Patching Layer 3 ---\")\n",
    "            #### activ_channels = [192+ c patch_dict[3]]\n",
    "            patch_channels = patch_dict[3]\n",
    "            x_enc[:,patch_channels,:,:] = activs[:,-1,[192+c for c in patch_channels],:,:]\n",
    "\n",
    "        core_output = torch.cat([x_enc, core_output], dim=1)\n",
    "\n",
    "        core_output = torch.flatten(core_output, 1)\n",
    "        final_out = torch.nn.functional.relu(self.drc_net.final_layer(core_output))\n",
    "        pri_logits = self.drc_net.policy(final_out)\n",
    "        pri_logits = pri_logits.view(T*B, self.drc_net.dim_actions, self.drc_net.num_actions)\n",
    "        pri_probs = torch.nn.functional.softmax(pri_logits.view(-1), dim=0)\n",
    "        pri = sample(pri_logits, greedy=greedy, dim=-1)\n",
    "        pri = pri.view(T, B, self.drc_net.dim_actions) \n",
    "        pri_env = pri[-1, :, 0] if not self.drc_net.tuple_action else pri[-1]   \n",
    "        action = pri_env\n",
    "        return action, pri_probs, pri_logits.view(-1), rnn_state\n",
    "    \n",
    "    def forward_single_patch(self, x, core_state, reset, activ_type=None, patch_dict={}, activs=None):\n",
    "        reset = reset.float()\n",
    "\n",
    "        activ_layers = list(patch_dict.keys())\n",
    "\n",
    "        b, c, h, w = x.shape\n",
    "        layer_n = 2\n",
    "        out = core_state[(self.drc_net.core.num_layers - 1) * layer_n] * (1 - reset).view(\n",
    "            b, 1, 1, 1\n",
    "        )  # h_cur on last layer\n",
    "\n",
    "        core_out = []\n",
    "        new_core_state = []\n",
    "        for n, cell in enumerate(self.drc_net.core.layers):\n",
    "            cell_input = torch.concat([x, out], dim=1)\n",
    "            h_cur = core_state[n * layer_n + 0] * (1 - reset.view(b, 1, 1, 1))\n",
    "            c_cur = core_state[n * layer_n + 1] * (1 - reset.view(b, 1, 1, 1))\n",
    "        \n",
    "            if n in activ_layers and activ_type is not None:\n",
    "                #print(f\"--- Patching Layer {n} ---\")\n",
    "                patch_channels = patch_dict[n]\n",
    "                if activ_type == \"xenc\":\n",
    "                    patch_activs = activs[:,[192+c for c in patch_channels],:,:].detach().clone()\n",
    "                elif activ_type == \"hidden\":\n",
    "                    patch_activs = activs[:,[64*n+c for c in patch_channels],:,:].detach().clone()\n",
    "                elif activ_type == \"cell\":\n",
    "                    patch_activs = activs[:,[64*n+32+c for c in patch_channels],:,:].detach().clone()\n",
    "\n",
    "                if activ_type == \"xenc\" and n in activ_layers:\n",
    "                    #print(f\"patching channels {patch_channels} in xenc\")\n",
    "                    cell_input[:,patch_channels,:,:] = patch_activs\n",
    "\n",
    "                #print(\"out, x, cell_input:\", out.sum(), x.sum(), cell_input.sum())\n",
    "\n",
    "                h_next, c_next = self.forward_cell_patch(\n",
    "                    convlstm_cell=cell,\n",
    "                    input=cell_input,\n",
    "                    h_cur=h_cur,\n",
    "                    c_cur=c_cur,\n",
    "                    activ_type=activ_type,\n",
    "                    patch_channels=patch_channels, \n",
    "                    patch_activs=patch_activs\n",
    "                )\n",
    "            else:\n",
    "                #print(f\"--- NOT patching layer {n} ---\")\n",
    "                h_next, c_next, concat_k, concat_v = cell(\n",
    "                    cell_input, h_cur, c_cur, None, None, None\n",
    "                )\n",
    "            if self.drc_net.core.grad_scale < 1 and h_next.requires_grad:\n",
    "                h_next.register_hook(lambda grad: grad * self.drc_net.core.grad_scale)\n",
    "                c_next.register_hook(lambda grad: grad * self.drc_net.core.grad_scale)\n",
    "            new_core_state.append(h_next)\n",
    "            new_core_state.append(c_next)\n",
    "            out = h_next\n",
    "\n",
    "        core_state = tuple(new_core_state)\n",
    "        core_out = out.unsqueeze(0)\n",
    "        return core_out, core_state\n",
    "    \n",
    "    def forward_cell_patch(self, convlstm_cell, input, h_cur, c_cur, activ_type=None, patch_channels=[], patch_activs=None):\n",
    "        B = input.shape[0]\n",
    "        #print(\"h_cur, c_cur, input\", h_cur.sum(), c_cur.sum(), input.sum())\n",
    "        combined = torch.cat([input, h_cur], dim=1)  # concatenate along channel axis\n",
    "        if convlstm_cell.pool_inject:\n",
    "            combined = torch.cat(\n",
    "                [combined, convlstm_cell.proj_max_mean(h_cur)], dim=1\n",
    "            )  # concatenate along channel axis\n",
    "        if convlstm_cell.linear:\n",
    "            combined_conv = convlstm_cell.main(combined[:, :, 0, 0]).unsqueeze(-1).unsqueeze(-1)\n",
    "        else:\n",
    "            combined_conv = convlstm_cell.main(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g, cc_a = torch.split(combined_conv, convlstm_cell.embed_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        c_next = f * c_cur + i * g\n",
    "        #print(\"c_next:\", c_next.sum())\n",
    "        if activ_type==\"cell\":\n",
    "            #print(f\"patching channels {patch_channels} in cell\")\n",
    "            c_next[:,patch_channels,:,:] = patch_activs\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        if activ_type==\"hidden\":\n",
    "            #print(f\"patching channels {patch_channels} in hidden\")\n",
    "            h_next[:,patch_channels,:,:] = patch_activs\n",
    "        #print(h_next.sum())\n",
    "\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3647,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing env 0 with device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Init. environment with obs space \u001b[91mBox(0, 1, (7, 8, 8), uint8)\u001b[0m and action space \u001b[91mDiscrete(5)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sokoban-v0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_n = 1\n",
    "gpu = False\n",
    "mini_sokoban = True \n",
    "mini_unqtar = False\n",
    "mini_unqbox = False\n",
    "\n",
    "env = thinker.make(\n",
    "    \"Sokoban-v0\", \n",
    "    env_n=env_n, \n",
    "    gpu=gpu,\n",
    "    wrapper_type=1, \n",
    "    has_model=False, \n",
    "    train_model=False, \n",
    "    parallel=False, \n",
    "    save_flags=False,\n",
    "    mini=mini_sokoban,\n",
    "    mini_unqtar=mini_unqtar,\n",
    "    mini_unqbox=mini_unqbox         \n",
    "    ) \n",
    "flags = util.create_setting(args=[], save_flags=False, wrapper_type=1) \n",
    "flags.mini = mini_sokoban\n",
    "flags.mini_unqbtar = mini_unqtar\n",
    "flags.mini_unqbox = mini_unqbox\n",
    "drc_net = DRCNet(\n",
    "    obs_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    flags=flags,\n",
    "    record_state=True,\n",
    "    )\n",
    "drc_net.to(env.device)\n",
    "\n",
    "ckp_path = \"../drc_mini\"\n",
    "ckp_path = os.path.join(util.full_path(ckp_path), \"ckp_actor_realstep249000192.tar\")\n",
    "ckp = torch.load(ckp_path, env.device)\n",
    "drc_net.load_state_dict(ckp[\"actor_net_state_dict\"], strict=False)\n",
    "rnn_state = drc_net.initial_state(batch_size=env_n, device=env.device)\n",
    "\n",
    "# run the trained drc again\n",
    "state = env.reset() \n",
    "env_out = util.init_env_out(state, flags, dim_actions=1, tuple_action=False) # this converts the state to EnvOut object that can be processed by actor\n",
    "#actor_out, rnn_state = drc_net(env_out, rnn_state, greedy=True)\n",
    "#state, reward, done, info = env.step(actor_out.action)\n",
    "#env_out = util.create_env_out(actor_out.action, state, reward, done, info, flags)\n",
    "#actor_out, new_rnn_state = drc_net(env_out, rnn_state, greedy=True)\n",
    "viz.plot_mini_sokoban(state[\"real_states\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interv(layer, state, hidden_states):\n",
    "    #weight = torch.tensor([[-0.48737913370132446, 2.0204238891601562, -0.20710276067256927, -0.1559930294752121, -1.4778072834014893, 1.0226231813430786, -0.011008787900209427, 0.26454436779022217, 0.5662028789520264, 0.826072633266449, 0.7518832087516785, 0.4854520261287689, -0.545423686504364, -0.1605425626039505, -2.30305552482605, 0.10711660981178284, -0.20925910770893097, -0.4371994435787201, -0.09227673709392548, 0.2594224214553833, 0.09071329236030579, -0.10446739196777344, -0.008032598532736301, -0.6721022725105286, 0.14969028532505035, -0.5917406678199768, 0.7830581068992615, -0.018811164423823357, 0.2390570491552353, 0.2955656945705414, -1.1725964546203613, 0.9227076768875122], [0.6500030755996704, -1.9482715129852295, 0.21371221542358398, -0.007815630175173283, 1.2696133852005005, -0.7610629200935364, 0.0742838978767395, -0.525004506111145, -0.3204478919506073, -0.8447422981262207, -0.5190479755401611, -0.7739930152893066, 0.6830257177352905, 0.15014739334583282, 2.5916659832000732, -0.32344695925712585, 0.09805583208799362, 0.6653913259506226, 0.010809963569045067, -0.0951007530093193, 0.04532609134912491, -0.021794166415929794, 0.18280953168869019, 0.7764242887496948, -0.0533781498670578, 0.40867361426353455, -0.7241909503936768, 0.18064184486865997, -0.30017533898353577, -0.36463192105293274, 1.2368476390838623, -0.9249445199966431]])\n",
    "    weight = torch.tensor([[1.7741026878356934, 0.23829807341098785, -0.4765743911266327, -0.3090250790119171, -0.33118438720703125, -2.8092799186706543, 0.16634029150009155, -1.807407021522522, -0.13387714326381683, -0.34786245226860046, 0.657535195350647, -0.5856123566627502, 0.4517926871776581, -0.9095766544342041, -0.10291017591953278, -0.09858949482440948, -0.35263383388519287, -1.615708351135254, -0.7799985408782959, 1.6272573471069336, -0.012647507712244987, 0.02794782631099224, 0.0159939993172884, 1.9977424144744873, -0.2700195610523224, -0.38910531997680664, -0.1412203162908554, -0.09455776959657669, 0.39089444279670715, -0.37262558937072754, 0.6665008664131165, -0.7619373798370361], [-2.0696938037872314, -0.38802820444107056, 0.5280684232711792, 0.23703162372112274, 0.3948691189289093, 2.719857692718506, -0.12987162172794342, 1.7273046970367432, 0.014804677106440067, 0.12781506776809692, -0.47968780994415283, 0.5010322332382202, -0.5012488961219788, 0.7640618681907654, 0.2018471658229828, 0.030183838680386543, 0.4156574606895447, 1.6790746450424194, 0.6045333743095398, -1.8027441501617432, -0.30582278966903687, -0.2585926949977875, -0.027122506871819496, -1.7820113897323608, 0.23489412665367126, 0.4310240149497986, 0.10243157297372818, -0.05739375576376915, -0.1980225145816803, 0.543531596660614, -0.8056098222732544, 0.7186568379402161]])\n",
    "    channels = list(range(32))\n",
    "    if layer == 0:\n",
    "        c = 32\n",
    "    elif layer == 1:\n",
    "        c = 96\n",
    "    elif layer == 2:\n",
    "        c = 160\n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    map = torch.zeros(size=(2,8,8))\n",
    "    for i in range(len(channels)):\n",
    "        map[0] += hidden_states[-1,c+channels[i],:,:] * weight[0,i]\n",
    "        map[1] += hidden_states[-1,c+channels[i],:,:] * weight[1,i]\n",
    "    if state.shape[0] != state.shape[1]:\n",
    "        state = state.permute(1,2,0)\n",
    "    dim_z = 7\n",
    "    mini_board = np.zeros(state.shape[:-1])\n",
    "    for i in range(1,1+dim_z):\n",
    "        mini_board[(state[:,:,i-1] == 1)] = i\n",
    "    mini_board = np.flip(mini_board, axis=0)\n",
    "    cmap = colors.ListedColormap(['black', \"white\", \"aqua\", \"gold\", \"green\",\"magenta\", \"khaki\"])\n",
    "    bounds= [i+0.5 for i in range(1+dim_z)]\n",
    "    norm = colors.BoundaryNorm(bounds, cmap.N)\n",
    "    axs[0].imshow(mini_board, interpolation='nearest', origin='lower', cmap=cmap, norm=norm)\n",
    "    axs[0].axes.get_yaxis().set_visible(False)\n",
    "    axs[0].axes.get_xaxis().set_visible(False)\n",
    "    axs[1].imshow(map.argmax(dim=0), vmin=-1,vmax=1)\n",
    "    axs[1].axes.get_yaxis().set_visible(False)\n",
    "    axs[1].axes.get_xaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0091, 0.0057, 0.0047, 0.9738, 0.0067], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer = 2\n",
    "alpha = 50\n",
    "if layer == 0:\n",
    "    c1, c2 = 32,64\n",
    "elif layer == 1:\n",
    "    c1, c2 = 96,128\n",
    "elif layer == 2:\n",
    "    c1, c2 = 160,192\n",
    "\n",
    "patch_net = ActPatchDRCNet(drc_net)\n",
    "patch_dict = {layer: list(range(32))}\n",
    "actor_out, new_rnn_state = drc_net(env_out, rnn_state, greedy=True)\n",
    "activs = drc_net.hidden_state.detach().clone()\n",
    "#activs[:,:,160:192,6,1] += 5*torch.tensor(results[\"layer2\"][\"weights\"][0])#\n",
    "#activs[:,:,160:192,6,2] += 50*torch.tensor(results[\"layer2\"][\"weights\"][0])\n",
    "#activs[:,:,c1:c2,6,1] += alpha*torch.tensor(results[f\"layer{layer}\"][\"weights\"][0])\n",
    "#activs[:,:,c1:c2,5,0] += alpha*torch.tensor(results[f\"layer{layer}\"][\"weights\"][0])\n",
    "#activs[:,:,c1:c2,4,5] += alpha*torch.tensor(results[f\"layer{layer}\"][\"weights\"][1])\n",
    "#activs[:,:,c1:c2,4,7] -= alpha*torch.tensor(results[f\"layer{layer}\"][\"weights\"][1])\n",
    "#activs[:,:,c1:c2,5,5] += alpha*torch.tensor(results[f\"layer{layer}\"][\"weights\"][1])\n",
    "\n",
    "patch_action, patch_action_probs, patch_logits, rnn_state = patch_net.forward_patch(env_out, rnn_state, activ_type=\"cell\", activ_ticks=[0,1,2],\n",
    "                                                                    patch_dict=patch_dict, activs=activs)\n",
    "state, reward, done, info = env.step(patch_action)\n",
    "env_out = util.create_env_out(patch_action, state, reward, done, info, flags)\n",
    "#viz.plot_mini_sokoban(state[\"real_states\"][0])\n",
    "plot_interv(2, state[\"real_states\"][0], activs[0])\n",
    "print(patch_action_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
