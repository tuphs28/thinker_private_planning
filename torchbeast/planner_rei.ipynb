{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d1c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchbeast.core import file_writer\n",
    "from torchbeast.core import prof\n",
    "from torchbeast.core import vtrace\n",
    "from torchbeast.atari_wrappers import *\n",
    "from torchbeast.transformer_rnn import *\n",
    "from torchbeast.train import *\n",
    "from torchbeast.model import Model\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "# Update to original core funct\n",
    "\n",
    "def create_buffers(flags, obs_shape, num_actions, num_rewards) -> Buffers:\n",
    "    T = flags.unroll_length\n",
    "    specs = dict(\n",
    "        frame=dict(size=(T + 1, *obs_shape), dtype=torch.float32),\n",
    "        reward=dict(size=(T + 1, num_rewards), dtype=torch.float32),\n",
    "        done=dict(size=(T + 1,), dtype=torch.bool),\n",
    "        truncated_done=dict(size=(T + 1,), dtype=torch.bool),\n",
    "        episode_return=dict(size=(T + 1, num_rewards), dtype=torch.float32),\n",
    "        episode_step=dict(size=(T + 1,), dtype=torch.int32),\n",
    "        policy_logits=dict(size=(T + 1, num_actions), dtype=torch.float32),\n",
    "        im_policy_logits=dict(size=(T + 1, num_actions), dtype=torch.float32),        \n",
    "        reset_policy_logits=dict(size=(T + 1, 2), dtype=torch.float32),\n",
    "        baseline=dict(size=(T + 1, num_rewards), dtype=torch.float32),\n",
    "        last_action=dict(size=(T + 1, 3 if not flags.flex_t else 4), dtype=torch.int64),\n",
    "        action=dict(size=(T + 1,), dtype=torch.int64),\n",
    "        im_action=dict(size=(T + 1,), dtype=torch.int64),        \n",
    "        reset_action=dict(size=(T + 1,), dtype=torch.int64), \n",
    "        reg_loss=dict(size=(T + 1,), dtype=torch.float32),  \n",
    "        cur_t=dict(size=(T + 1,), dtype=torch.int64),             \n",
    "        max_rollout_depth=dict(size=(T + 1,), dtype=torch.float32),  \n",
    "    )\n",
    "    if flags.flex_t:\n",
    "        specs.update(dict(\n",
    "            term_policy_logits=dict(size=(T + 1, 2), dtype=torch.float32),\n",
    "            term_action=dict(size=(T + 1,), dtype=torch.int64),)\n",
    "                     )\n",
    "    \n",
    "    buffers: Buffers = {key: [] for key in specs}\n",
    "    for _ in range(flags.num_buffers):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "    return buffers  \n",
    "\n",
    "def act(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "    full_queue: mp.SimpleQueue,\n",
    "    actor_net: torch.nn.Module,\n",
    "    model: torch.nn.Module,\n",
    "    buffers: Buffers,\n",
    "    initial_agent_state_buffers,\n",
    "):\n",
    "    try:\n",
    "        logging.info(\"Actor %i started.\", actor_index)\n",
    "        timings = prof.Timings()  # Keep track of how fast things are.\n",
    "\n",
    "        gym_env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "                               model=model, flags=flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()\n",
    "        agent_state = actor_net.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = actor_net(env_output, agent_state)\n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break\n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:           \n",
    "                if key in buffers: buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                if key in buffers: buffers[key][index][0, ...] = agent_output[key]                    \n",
    "            for i, tensor in enumerate(agent_state):\n",
    "                initial_agent_state_buffers[index][i][...] = tensor\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.unroll_length):\n",
    "                timings.reset()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = actor_net(env_output, agent_state)                    \n",
    "\n",
    "                timings.time(\"actor_net\")\n",
    "                \n",
    "                action = [agent_output['action'], agent_output['im_action'], agent_output['reset_action']]\n",
    "                if 'term_action' in agent_output: action.append(agent_output['term_action'])\n",
    "                action = torch.cat(action, dim=-1)\n",
    "                env_output = env.step(action.unsqueeze(0))\n",
    "\n",
    "                if flags.trun_bs:\n",
    "                    if env_output['truncated_done']: \n",
    "                        env_output['reward'] = env_output['reward'] + flags.im_discounting * agent_output['baseline']\n",
    "\n",
    "                timings.time(\"step\")\n",
    "\n",
    "                for key in env_output:\n",
    "                    if key in buffers:\n",
    "                        buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    if key in buffers:\n",
    "                        buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "\n",
    "                timings.time(\"write\")\n",
    "            full_queue.put(index)\n",
    "\n",
    "        if actor_index == 0:\n",
    "            logging.info(\"Actor %i: %s\", actor_index, timings.summary())\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        print()\n",
    "        raise e\n",
    "\n",
    "def compute_policy_gradient_loss(logits_ls, actions_ls, advantages_ls, masks_ls):\n",
    "    loss = 0.\n",
    "    for logits, actions, advantages, masks in zip(logits_ls, actions_ls, advantages_ls, masks_ls):\n",
    "        cross_entropy = F.nll_loss(\n",
    "            F.log_softmax(torch.flatten(logits, 0, 1), dim=-1),\n",
    "            target=torch.flatten(actions, 0, 1),\n",
    "            reduction=\"none\",)\n",
    "        cross_entropy = cross_entropy.view_as(advantages)\n",
    "        adv_cross_entropy = cross_entropy * advantages.detach()\n",
    "        loss = loss + torch.sum(adv_cross_entropy * (1-masks))\n",
    "    return loss  \n",
    "\n",
    "def compute_entropy_loss(logits_ls, masks_ls, c_ls):\n",
    "    \"\"\"Return the entropy loss, i.e., the negative entropy of the policy.\"\"\"\n",
    "    loss = 0.\n",
    "    for logits, masks, c in zip(logits_ls, masks_ls, c_ls):\n",
    "        policy = F.softmax(logits, dim=-1)\n",
    "        log_policy = F.log_softmax(logits, dim=-1)\n",
    "        ent = torch.sum(policy * log_policy, dim=-1) #* (1-masks)\n",
    "        loss = loss + torch.sum(ent) * c \n",
    "    return loss\n",
    "\n",
    "def action_log_probs(policy_logits, actions):\n",
    "    return -F.nll_loss(\n",
    "        F.log_softmax(torch.flatten(policy_logits, 0, -2), dim=-1),\n",
    "        torch.flatten(actions),\n",
    "        reduction=\"none\",\n",
    "    ).view_as(actions) \n",
    "  \n",
    "def from_logits(\n",
    "    behavior_logits_ls, target_logits_ls, actions_ls, masks_ls,\n",
    "    discounts, rewards, values, bootstrap_value, clip_rho_threshold=1.0,\n",
    "    clip_pg_rho_threshold=1.0, lamb=1.0,):\n",
    "    \"\"\"V-trace for softmax policies.\"\"\"\n",
    "    \n",
    "    log_rhos = 0.\n",
    "    \n",
    "    for behavior_logits, target_logits, actions, masks in zip(behavior_logits_ls, \n",
    "             target_logits_ls, actions_ls, masks_ls):\n",
    "        behavior_log_probs = action_log_probs(behavior_logits, actions)        \n",
    "        target_log_probs = action_log_probs(target_logits, actions)\n",
    "        log_rho = target_log_probs - behavior_log_probs\n",
    "        log_rhos = log_rhos + log_rho * (1-masks)\n",
    "    \n",
    "    vtrace_returns = vtrace.from_importance_weights(\n",
    "        log_rhos=log_rhos,\n",
    "        discounts=discounts,\n",
    "        rewards=rewards,\n",
    "        values=values,\n",
    "        bootstrap_value=bootstrap_value,\n",
    "        clip_rho_threshold=clip_rho_threshold,\n",
    "        clip_pg_rho_threshold=clip_pg_rho_threshold,\n",
    "        lamb=lamb\n",
    "    )\n",
    "    return vtrace.VTraceFromLogitsReturns(\n",
    "        log_rhos=log_rhos,\n",
    "        behavior_action_log_probs=None,\n",
    "        target_action_log_probs=None,\n",
    "        **vtrace_returns._asdict(),\n",
    "    )  \n",
    "  \n",
    "def learn(\n",
    "    flags,\n",
    "    actor_model,\n",
    "    model,\n",
    "    batch,\n",
    "    initial_agent_state,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    real_step,\n",
    "    lock=threading.Lock(),  # noqa: B008\n",
    "):\n",
    "    \"\"\"Performs a learning (optimization) step.\"\"\"\n",
    "    with lock:                \n",
    "        learner_outputs, unused_state = model(batch, initial_agent_state)\n",
    "        #learner_outputs[\"im_policy_logits\"].register_hook(lambda grad: grad / (flags.rec_t - 1))\n",
    "        #learner_outputs[\"reset_policy_logits\"].register_hook(lambda grad: grad / (flags.rec_t - 1))\n",
    "        #learner_outputs[\"baseline\"].register_hook(lambda grad: grad / (flags.rec_t - 1))\n",
    "        \n",
    "        # Take final value function slice for bootstrapping.\n",
    "        bootstrap_value = learner_outputs[\"baseline\"][-1]        \n",
    "\n",
    "        # Move from obs[t] -> action[t] to action[t] -> obs[t].\n",
    "        batch = {key: tensor[1:] for key, tensor in batch.items()}\n",
    "        learner_outputs = {key: tensor[:-1] for key, tensor in learner_outputs.items()}\n",
    "        \n",
    "        T, B = batch[\"done\"].shape\n",
    "\n",
    "        rewards = batch[\"reward\"]\n",
    "        if flags.reward_clipping > 0:\n",
    "            clipped_rewards = torch.clamp(rewards, -flags.reward_clipping, flags.reward_clipping)\n",
    "        else:\n",
    "            clipped_rewards = rewards\n",
    "        \n",
    "        # compute advantage w.r.t real rewards\n",
    "        \n",
    "        discounts = (~batch[\"done\"]).float() * flags.im_discounting        \n",
    "        behavior_logits_ls = [batch[\"policy_logits\"], batch[\"im_policy_logits\"], batch[\"reset_policy_logits\"]]\n",
    "        target_logits_ls = [learner_outputs[\"policy_logits\"], learner_outputs[\"im_policy_logits\"], learner_outputs[\"reset_policy_logits\"]]\n",
    "        actions_ls = [batch[\"action\"], batch[\"im_action\"], batch[\"reset_action\"]]        \n",
    "        im_mask = (batch[\"cur_t\"] == 0).float()\n",
    "        real_mask = 1 - im_mask\n",
    "        masks_ls = [real_mask, im_mask, im_mask]                \n",
    "           \n",
    "        if flags.flex_t:\n",
    "            behavior_logits_ls.append(batch[\"term_policy_logits\"])\n",
    "            target_logits_ls.append(learner_outputs[\"term_policy_logits\"])\n",
    "            actions_ls.append(batch[\"term_action\"])\n",
    "            masks_ls.append(im_mask)\n",
    "\n",
    "        vtrace_returns = from_logits(\n",
    "            behavior_logits_ls, target_logits_ls, actions_ls, masks_ls,\n",
    "            discounts=discounts,\n",
    "            rewards=clipped_rewards[:, :, 0],\n",
    "            values=learner_outputs[\"baseline\"][:, :, 0],\n",
    "            bootstrap_value=bootstrap_value[:, 0],\n",
    "            lamb=flags.lamb\n",
    "        )\n",
    "        \n",
    "        advantages_ls = [vtrace_returns.pg_advantages, vtrace_returns.pg_advantages, vtrace_returns.pg_advantages]\n",
    "        pg_loss = compute_policy_gradient_loss(target_logits_ls, actions_ls, advantages_ls, masks_ls)         \n",
    "        baseline_loss = flags.baseline_cost * compute_baseline_loss(\n",
    "            vtrace_returns.vs - learner_outputs[\"baseline\"][:, :, 0])        \n",
    "       \n",
    "        # compute advantage w.r.t imagainary rewards\n",
    "        \n",
    "        cs_ls = [flags.entropy_cost, flags.im_entropy_cost, flags.reset_entropy_cost, flags.term_entropy_cost]\n",
    "        entropy_loss = compute_entropy_loss(target_logits_ls, masks_ls, cs_ls)        \n",
    "\n",
    "        if flags.reward_type == 1:\n",
    "            if flags.reward_carry:                \n",
    "                discounts = (~batch[\"done\"]).float() * flags.im_discounting \n",
    "            else:\n",
    "                discounts = (~(batch[\"cur_t\"] == 0)).float() * flags.im_discounting        \n",
    "            behavior_logits_ls = [batch[\"im_policy_logits\"], batch[\"reset_policy_logits\"]]\n",
    "            target_logits_ls = [learner_outputs[\"im_policy_logits\"], learner_outputs[\"reset_policy_logits\"]]\n",
    "            actions_ls = [batch[\"im_action\"], batch[\"reset_action\"]] \n",
    "            im_mask = (batch[\"cur_t\"] == 0).float()\n",
    "            masks_ls = [im_mask, im_mask]  \n",
    "            \n",
    "            if flags.flex_t:\n",
    "                behavior_logits_ls.append(batch[\"term_policy_logits\"])\n",
    "                target_logits_ls.append(learner_outputs[\"term_policy_logits\"])\n",
    "                actions_ls.append(batch[\"term_action\"])\n",
    "                masks_ls.append(im_mask)\n",
    "            \n",
    "            vtrace_returns = from_logits(\n",
    "                behavior_logits_ls, target_logits_ls, actions_ls, masks_ls,\n",
    "                discounts=discounts,\n",
    "                rewards=clipped_rewards[:, :, 1],\n",
    "                values=learner_outputs[\"baseline\"][:, :, 1],\n",
    "                bootstrap_value=bootstrap_value[:, 1],\n",
    "                lamb=flags.lamb\n",
    "            )\n",
    "            advantages_ls = [vtrace_returns.pg_advantages, vtrace_returns.pg_advantages]\n",
    "            im_pg_loss = compute_policy_gradient_loss(target_logits_ls, actions_ls, advantages_ls, masks_ls)   \n",
    "            im_baseline_loss = flags.baseline_cost * compute_baseline_loss(\n",
    "                vtrace_returns.vs - learner_outputs[\"baseline\"][:, :, 1])     \n",
    "\n",
    "        reg_loss = flags.reg_cost * torch.sum(learner_outputs[\"reg_loss\"])\n",
    "        total_loss = pg_loss + baseline_loss + entropy_loss + reg_loss\n",
    "        \n",
    "        if flags.reward_type == 1:\n",
    "            total_loss = total_loss + flags.im_cost * (im_pg_loss + im_baseline_loss)\n",
    "        \n",
    "        episode_returns = batch[\"episode_return\"][batch[\"done\"]][:, 0]  \n",
    "        max_rollout_depth = (batch[\"max_rollout_depth\"][batch[\"cur_t\"] == 0]).detach().cpu().numpy()\n",
    "        max_rollout_depth = np.average(max_rollout_depth) if len (max_rollout_depth) > 0 else 0.        \n",
    "        stats = {\n",
    "            \"episode_returns\": tuple(episode_returns.detach().cpu().numpy()),\n",
    "            \"mean_episode_return\": torch.mean(episode_returns).item(),\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"pg_loss\": pg_loss.item(),\n",
    "            \"baseline_loss\": baseline_loss.item(),\n",
    "            \"entropy_loss\": entropy_loss.item(),\n",
    "            \"reg_loss\": reg_loss.item(),\n",
    "            \"max_rollout_depth\": max_rollout_depth,\n",
    "            \"real_step\": torch.sum(batch[\"cur_t\"]==0).item(),\n",
    "            \"mean_plan_step\": T * B / torch.sum(batch[\"cur_t\"]==0).item(),\n",
    "        }\n",
    "        \n",
    "        if flags.reward_type == 1:            \n",
    "            im_episode_returns = batch[\"episode_return\"][batch[\"cur_t\"] == 0][:, 1]\n",
    "            stats[\"im_episode_returns\"] = tuple(im_episode_returns.detach().cpu().numpy())\n",
    "            stats[\"im_pg_loss\"] = im_pg_loss.item()\n",
    "            stats[\"im_baseline_loss\"] = im_baseline_loss.item()   \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimize_params = optimizer.param_groups[0]['params']\n",
    "        if flags.grad_norm_clipping > 0:\n",
    "            total_norm = nn.utils.clip_grad_norm_(optimize_params, flags.grad_norm_clipping)\n",
    "        else:\n",
    "            total_norm = 0.\n",
    "            parameters = [p for p in optimize_params if p.grad is not None and p.requires_grad]\n",
    "            for p in parameters:\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "        stats[\"total_norm\"] = total_norm\n",
    "        \n",
    "        optimizer.step()\n",
    "        if not flags.flex_t:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            scheduler.last_epoch = real_step - 1  # scheduler does not support setting epoch directly\n",
    "            scheduler.step() \n",
    "\n",
    "        actor_model.load_state_dict(model.state_dict())\n",
    "        return stats  \n",
    "\n",
    "# Wrap the environment with a model\n",
    "\n",
    "def _format_frame(frame, bsz=None):\n",
    "    if type(frame) == np.ndarray:\n",
    "        frame = torch.from_numpy(frame).float()\n",
    "    if bsz is not None:\n",
    "        return frame.view((1,) + frame.shape)\n",
    "    else:\n",
    "        return frame.view((1, 1) + frame.shape)\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, gym_env):\n",
    "        self.gym_env = gym_env\n",
    "        self.episode_return = None\n",
    "        self.episode_step = None\n",
    "\n",
    "    def initial(self):\n",
    "        initial_reward = torch.zeros(1, 1)\n",
    "        # This supports only single-tensor actions ATM.\n",
    "        initial_last_action = torch.zeros(1, 1, dtype=torch.int64)\n",
    "        self.episode_return = torch.zeros(1, 1, 1)\n",
    "        self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n",
    "        initial_done = torch.ones(1, 1, dtype=torch.bool)\n",
    "        initial_frame = _format_frame(self.gym_env.reset())\n",
    "        return dict(\n",
    "            frame=initial_frame,\n",
    "            reward=initial_reward,\n",
    "            done=initial_done,\n",
    "            truncated_done=torch.tensor(0).view(1, 1).bool(),\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=torch.tensor(0).view(1, 1),\n",
    "            last_action=initial_last_action,\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        frame, reward, done, unused_info = self.gym_env.step(action[0,0].cpu().detach().numpy())     \n",
    "        self.episode_step += 1\n",
    "        self.episode_return = self.episode_return + torch.tensor(reward).unsqueeze(0).unsqueeze(0)\n",
    "        episode_step = self.episode_step\n",
    "        episode_return = self.episode_return.clone()\n",
    "        if done:\n",
    "            frame = self.gym_env.reset()\n",
    "            self.episode_return = torch.zeros(1, 1, 1)\n",
    "            self.episode_step = torch.zeros(1, 1, dtype=torch.int32)        \n",
    "        frame = _format_frame(frame)\n",
    "        reward = torch.tensor(reward).view(1, 1, -1)\n",
    "        done = torch.tensor(done).view(1, 1)\n",
    "        truncated_done = 'TimeLimit.truncated' in unused_info and unused_info['TimeLimit.truncated']\n",
    "        truncated_done = torch.tensor(truncated_done).view(1, 1)\n",
    "        cur_t = torch.tensor(unused_info[\"cur_t\"]).view(1, 1)\n",
    "        if cur_t == 0 and self.episode_return.shape[2] > 1:\n",
    "            self.episode_return[:, :, 1] = 0.\n",
    "        if 'max_rollout_depth' in unused_info:\n",
    "            max_rollout_depth = torch.tensor(unused_info[\"max_rollout_depth\"]).view(1, 1)\n",
    "        else:\n",
    "            max_rollout_depth = torch.tensor(0.).view(1, 1)\n",
    "        \n",
    "        return dict(\n",
    "            frame=frame,\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            truncated_done=truncated_done,          \n",
    "            episode_return=episode_return,\n",
    "            episode_step=episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=action,\n",
    "            max_rollout_depth=max_rollout_depth\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        self.gym_env.close()\n",
    "\n",
    "    def clone_state(self):\n",
    "        state = self.gym_env.clone_state()\n",
    "        state[\"env_episode_return\"] = self.episode_return.clone()\n",
    "        state[\"env_episode_step\"] = self.episode_step.clone()\n",
    "        return state\n",
    "        \n",
    "    def restore_state(self, state):\n",
    "        self.episode_return = state[\"env_episode_return\"].clone()\n",
    "        self.episode_step = state[\"env_episode_step\"].clone()\n",
    "        self.gym_env.restore_state(state)\n",
    "        \n",
    "class Vec_Environment:\n",
    "    # deprecated\n",
    "    def __init__(self, gym_env, bsz):\n",
    "        self.gym_env = gym_env\n",
    "        self.bsz = bsz\n",
    "        self.episode_return = torch.zeros(1, self.bsz)\n",
    "        self.episode_step = torch.zeros(1, self.bsz)        \n",
    "\n",
    "    def initial(self):\n",
    "        initial_reward = torch.zeros(1, self.bsz, 1)\n",
    "        # This supports only single-tensor actions ATM.\n",
    "        initial_last_action = torch.zeros(1, self.bsz, dtype=torch.int64)\n",
    "        self.episode_return = torch.zeros(1, self.bsz)\n",
    "        self.episode_step = torch.zeros(1, self.bsz, dtype=torch.int32)\n",
    "        initial_done = torch.ones(1, self.bsz, dtype=torch.uint8)\n",
    "        initial_frame = _format_frame(self.gym_env.reset(), self.bsz)\n",
    "        cur_t = torch.zeros(1, self.bsz)\n",
    "        \n",
    "        return dict(\n",
    "            frame=initial_frame,\n",
    "            reward=initial_reward,\n",
    "            done=initial_done,\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=initial_last_action,\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        frame, reward, done, unused_info = self.gym_env.step(action.detach().cpu().numpy())   \n",
    "        \n",
    "        self.episode_step += 1\n",
    "        self.episode_return += torch.Tensor(reward).unsqueeze(0)\n",
    "        \n",
    "        done = torch.tensor(done).view(1, self.bsz)\n",
    "        truncated_done = ['TimeLimit.truncated' in x and x['TimeLimit.truncated'] for x in unused_info]\n",
    "        truncated_done = torch.tensor(truncated_done).view(1, self.bsz)\n",
    "        \n",
    "        self.episode_return = (~done).float().unsqueeze(-1) * self.episode_return\n",
    "        self.episode_step = (~done).float() * self.episode_step\n",
    "        \n",
    "        frame = _format_frame(frame, self.bsz)\n",
    "        reward = torch.tensor(reward).view(1, self.bsz).float()\n",
    "        \n",
    "        cur_t = [x[\"cur_t\"] for x in unused_info]  \n",
    "        cur_t = torch.tensor(cur_t).view(1, self.bsz)\n",
    "        \n",
    "        return dict(\n",
    "            frame=frame,\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            truncated_done=truncated_done,\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=action.unsqueeze(0),\n",
    "        )\n",
    "    \n",
    "    def clone_state(self):        \n",
    "        state = {}\n",
    "        state[\"env_episode_return\"] = self.episode_return.clone()\n",
    "        state[\"env_episode_step\"] = self.episode_step.clone()\n",
    "        for n, k in enumerate(self.gym_env.envs): \n",
    "            state[\"env_%d\"%n] = k.clone_state()\n",
    "        return state\n",
    "        \n",
    "    def restore_state(self, state):\n",
    "        self.episode_return = state[\"env_episode_return\"].clone()\n",
    "        self.episode_step = state[\"env_episode_step\"].clone()\n",
    "        for n, k in enumerate(self.gym_env.envs): \n",
    "            k.restore_state(state[\"env_%d\"%n])\n",
    "\n",
    "    def close(self):\n",
    "        self.gym_env.close()  \n",
    "\n",
    "class Actor_net(nn.Module):    \n",
    "    def __init__(self, obs_shape, num_actions, flags):\n",
    "\n",
    "        super(Actor_net, self).__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions  \n",
    "        \n",
    "        self.tran_t = flags.tran_t                   # number of recurrence of RNN        \n",
    "        self.tran_mem_n = flags.tran_mem_n           # size of memory for the attn modules\n",
    "        self.tran_layer_n = flags.tran_layer_n       # number of layers\n",
    "        self.tran_lstm = flags.tran_lstm             # to use lstm or not\n",
    "        self.tran_lstm_no_attn = flags.tran_lstm_no_attn  # to use attention in lstm or not\n",
    "        self.attn_mask_b = flags.tran_attn_b         # atention bias for current position\n",
    "        self.tran_norm_first = flags.tran_norm_first # to use norm first in transformer (not on LSTM)\n",
    "        self.tran_ff_n = flags.tran_ff_n             # number of dim of ff in transformer (not on LSTM)        \n",
    "        self.tran_skip = flags.tran_skip             # whether to add skip connection\n",
    "        self.conv_out = flags.tran_dim               # size of transformer / LSTM embedding dim        \n",
    "        self.no_mem = flags.no_mem                   # whether to earse real memory at the end of planning stage\n",
    "        self.num_rewards = flags.num_rewards         # dim of rewards (1 for vanilla; 2 for planning rewards)\n",
    "        self.flex_t = flags.flex_t                   # whether to output the terminate action\n",
    "        self.flex_t_term_b = flags.flex_t_term_b     # bias added to the logit of terminating\n",
    "        \n",
    "        self.conv_out_hw = 1   \n",
    "        self.d_model = self.conv_out\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=self.obs_shape[0], out_channels=self.conv_out//2, kernel_size=1, stride=1)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv_out//2, out_channels=self.conv_out, kernel_size=1, stride=1)        \n",
    "        self.frame_conv = torch.nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU())\n",
    "        self.env_input_size = self.conv_out\n",
    "        d_in = self.env_input_size + self.d_model \n",
    "        \n",
    "        if self.tran_lstm:\n",
    "            self.core = ConvAttnLSTM(h=self.conv_out_hw, w=self.conv_out_hw,\n",
    "                                 input_dim=d_in-self.d_model, hidden_dim=self.d_model,\n",
    "                                 kernel_size=1, num_layers=self.tran_layer_n,\n",
    "                                 num_heads=8, mem_n=self.tran_mem_n, attn=not self.tran_lstm_no_attn,\n",
    "                                 attn_mask_b=self.attn_mask_b)\n",
    "        else:            \n",
    "            self.core = ConvTransformerRNN(d_in=d_in,\n",
    "                                       h=self.conv_out_hw, w=self.conv_out_hw, d_model=self.d_model, \n",
    "                                       num_heads=8, dim_feedforward=self.tran_ff_n, \n",
    "                                       mem_n=self.tran_mem_n, norm_first=self.tran_norm_first,\n",
    "                                       num_layers=self.tran_layer_n, rpos=True, conv=False)   \n",
    "                         \n",
    "        \n",
    "        if self.tran_skip:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * (self.d_model + self.env_input_size)\n",
    "        else:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * self.d_model\n",
    "                \n",
    "        self.fc = nn.Linear(rnn_out_size, 256)        \n",
    "        \n",
    "        self.im_policy = nn.Linear(256, self.num_actions)        \n",
    "        self.policy = nn.Linear(256, self.num_actions)        \n",
    "        self.baseline = nn.Linear(256, self.num_rewards)        \n",
    "        self.reset = nn.Linear(256, 2)        \n",
    "        \n",
    "        if self.flex_t: self.term = nn.Linear(256, 2)        \n",
    "        \n",
    "        print(\"actor size: \", sum(p.numel() for p in self.parameters()))\n",
    "        #for k, v in self.named_parameters(): print(k, v.numel())   \n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        state = self.core.init_state(batch_size) + (torch.zeros(1, batch_size, \n",
    "               self.env_input_size, self.conv_out_hw, self.conv_out_hw),)\n",
    "        return state\n",
    "\n",
    "    def forward(self, obs, core_state=(), debug=False):\n",
    "        # one-step forward for the actor\n",
    "        # input / done shape x: T x B x C x 1 x 1 / B x C x 1 x 1\n",
    "        # only supports T = 1 at the moment; all output does not have T dim.        \n",
    "        \n",
    "        x = obs[\"frame\"]\n",
    "        done = obs[\"done\"]\n",
    "        \n",
    "        if len(x.shape) == 4: x = x.unsqueeze(0)\n",
    "        if len(done.shape) == 1: done = done.unsqueeze(0)  \n",
    "            \n",
    "        T, B, *_ = x.shape\n",
    "        x = torch.flatten(x, 0, 1)  # Merge time and batch.  \n",
    "        env_input = self.frame_conv(x)                \n",
    "        core_input = env_input.view(T, B, -1, self.conv_out_hw, self.conv_out_hw)\n",
    "        core_output_list = []\n",
    "        notdone = ~(done.bool())\n",
    "        \n",
    "        for n, (input, nd) in enumerate(zip(core_input.unbind(), notdone.unbind())):       \n",
    "            if self.no_mem and obs[\"cur_t\"][n, 0] == 0:\n",
    "                core_state = self.initial_state(B)\n",
    "                core_state = tuple(v.to(x.device) for v in core_state)\n",
    "                \n",
    "            # Input shape: B, self.conv_out + self.num_actions + 1, H, W\n",
    "            for t in range(self.tran_t):                \n",
    "                if t > 0: nd = torch.ones(B).to(x.device).bool()                    \n",
    "                nd = nd.view(-1)      \n",
    "                output, core_state = self.core(input, core_state, nd, nd) # output shape: 1, B, core_output_size \n",
    "                \n",
    "            last_input = input   \n",
    "            core_output_list.append(output)\n",
    "                                   \n",
    "        core_output = torch.cat(core_output_list)  \n",
    "        if self.tran_skip: core_output = torch.concat([core_output, core_input], dim=-3)\n",
    "        core_output = torch.flatten(core_output, 0, 1)        \n",
    "        core_output = F.relu(self.fc(torch.flatten(core_output, start_dim=1)))   \n",
    "        \n",
    "        policy_logits = self.policy(core_output)\n",
    "        im_policy_logits = self.im_policy(core_output)\n",
    "        reset_policy_logits = self.reset(core_output)\n",
    "        if self.flex_t: \n",
    "            term_policy_logits = self.term(core_output)            \n",
    "            term_policy_logits[:, 1] += self.flex_t_term_b\n",
    "        \n",
    "        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        im_action = torch.multinomial(F.softmax(im_policy_logits, dim=1), num_samples=1)\n",
    "        reset_action = torch.multinomial(F.softmax(reset_policy_logits, dim=1), num_samples=1)\n",
    "        if self.flex_t: term_action = torch.multinomial(F.softmax(term_policy_logits, dim=1), num_samples=1)\n",
    "                \n",
    "        baseline = self.baseline(core_output)\n",
    "                   \n",
    "        reg_loss = (1e-3 * torch.sum(policy_logits**2, dim=-1) / 2 + \n",
    "                    1e-5 * torch.sum(core_output**2, dim=-1) / 2)\n",
    "        reg_loss = reg_loss.view(T, B)\n",
    "        \n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        im_policy_logits = im_policy_logits.view(T, B, self.num_actions)\n",
    "        reset_policy_logits = reset_policy_logits.view(T, B, 2)\n",
    "        if self.flex_t: term_policy_logits = term_policy_logits.view(T, B, 2)\n",
    "            \n",
    "        \n",
    "        action = action.view(T, B)      \n",
    "        im_action = im_action.view(T, B)      \n",
    "        reset_action = reset_action.view(T, B)             \n",
    "        if self.flex_t: term_action = term_action.view(T, B)\n",
    "        baseline = baseline.view(T, B, self.num_rewards)\n",
    "        \n",
    "        ret_dict = dict(policy_logits=policy_logits,                         \n",
    "                        im_policy_logits=im_policy_logits,                         \n",
    "                        reset_policy_logits=reset_policy_logits,     \n",
    "                        action=action,     \n",
    "                        im_action=im_action,\n",
    "                        reset_action=reset_action,\n",
    "                        baseline=baseline, \n",
    "                        reg_loss=reg_loss, )\n",
    "        \n",
    "        if self.flex_t: ret_dict.update(dict(term_policy_logits=term_policy_logits,    \n",
    "                                             term_action=term_action))\n",
    "        return (ret_dict, core_state) \n",
    "    \n",
    "        \n",
    "class ModelWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, model, flags):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        \n",
    "        self.env = env\n",
    "        self.model = model                \n",
    "        self.rec_t = flags.rec_t        \n",
    "        self.flex_t = flags.flex_t \n",
    "        self.flex_t_cost = flags.flex_t_cost         \n",
    "        self.discounting = flags.discounting\n",
    "        self.stat_type = flags.stat_type    \n",
    "        self.reward_type = flags.reward_type    \n",
    "        self.no_mem = flags.no_mem\n",
    "        self.perfect_model = flags.perfect_model\n",
    "        self.reset_m = flags.reset_m\n",
    "        self.tree_carry = flags.tree_carry\n",
    "        self.tree_vb = flags.tree_vb\n",
    "        self.thres_carry = flags.thres_carry        \n",
    "        self.thres_discounting = flags.thres_discounting\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.root_node = None\n",
    "        \n",
    "        # 2 for tree stat\n",
    "        assert self.stat_type in [2]         \n",
    "        if self.stat_type == 2:\n",
    "            if not self.flex_t:\n",
    "                obs_n = 9 + num_actions * 10 + self.rec_t   \n",
    "            else:\n",
    "                obs_n = 11 + num_actions * 10                \n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "          low=-np.inf, high=np.inf, shape=(obs_n, 1, 1), dtype=float)\n",
    "        self.model.train(False)        \n",
    "        \n",
    "        self.max_rollout_depth = 0.\n",
    "        self.thres = None\n",
    "        self.root_max_q = None\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        x = self.env.reset()\n",
    "        self.cur_t = 0        \n",
    "        out = self.use_model(x, 0., 0, self.cur_t, reset=1., term=0., done=False)\n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q\n",
    "        return out.unsqueeze(-1).unsqueeze(-1)\n",
    "    \n",
    "    def step(self, action):  \n",
    "        if not self.flex_t:\n",
    "            re_action, im_action, reset = action\n",
    "            term = None\n",
    "        else:\n",
    "            re_action, im_action, reset, term = action\n",
    "        info = {}\n",
    "        info[\"max_rollout_depth\"] = self.max_rollout_depth\n",
    "        if (not self.flex_t and self.cur_t < self.rec_t - 1) or (\n",
    "            self.flex_t and self.cur_t < self.rec_t - 1 and not term):\n",
    "          self.cur_t += 1\n",
    "          out = self.use_model(None, None, im_action, self.cur_t, reset=reset, term=term, done=False)          \n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([0.])\n",
    "          else:\n",
    "            flex_t_cost = 0. if not self.flex_t else self.flex_t_cost\n",
    "            r = np.array([0., (self.root_max_q - self.last_root_max_q - flex_t_cost).item()], dtype=np.float32)\n",
    "          done = False\n",
    "          info['cur_t'] = self.cur_t   \n",
    "        else:\n",
    "          self.cur_t = 0\n",
    "          if self.perfect_model: self.env.restore_state(self.root_node.encoded)\n",
    "          x, r, done, info_ = self.env.step(re_action)                    \n",
    "          out = self.use_model(x, r, re_action, self.cur_t, reset=1., term=term, done=done) \n",
    "          info.update(info_)\n",
    "          info['cur_t'] = self.cur_t\n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([r])\n",
    "          else:\n",
    "            r = np.array([r, 0.], dtype=np.float32)   \n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q   \n",
    "        \n",
    "        return out.unsqueeze(-1).unsqueeze(-1), r, done, info        \n",
    "        \n",
    "    def use_model(self, x, r, a, cur_t, reset, term, done=False):\n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:\n",
    "                self.rollout_depth = 0.\n",
    "                self.unexpand_rollout_depth = 0.\n",
    "                self.pass_unexpand = False\n",
    "                self.max_rollout_depth = 0.\n",
    "                \n",
    "                if self.root_max_q is not None:\n",
    "                    self.thres = (self.root_max_q - r) / self.discounting\n",
    "                if done:\n",
    "                    self.thres = None\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    re_action = 0\n",
    "                    re_reward = torch.tensor([0.], dtype=torch.float32)                \n",
    "                else:\n",
    "                    re_action = a                \n",
    "                    re_reward = torch.tensor([r], dtype=torch.float32)                \n",
    "                \n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = self.x_ = x_tensor\n",
    "                a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)                \n",
    "                _, vs, logits, encodeds = self.model(x_tensor, a_tensor.unsqueeze(0), one_hot=True) \n",
    "                \n",
    "                if self.perfect_model: \n",
    "                    encoded = self.clone_state()\n",
    "                else:\n",
    "                    encoded=encodeds[-1]\n",
    "                \n",
    "                if (not self.tree_carry or self.root_node is None or \n",
    "                    not self.root_node.children[a].expanded() or done):\n",
    "                \n",
    "                    self.root_node = Node(parent=None, action=re_action, logit=None, \n",
    "                                          num_actions=self.num_actions,\n",
    "                                          discounting=self.discounting,\n",
    "                                          rec_t=self.rec_t)\n",
    "                    self.root_node.expand(r=torch.tensor([0.], dtype=torch.float32), \n",
    "                                          v=vs[-1, 0].unsqueeze(-1), logits=logits[-1, 0],\n",
    "                                          encoded=encoded)\n",
    "                else:\n",
    "                    self.root_node = self.root_node.children[a]\n",
    "                    self.root_node.expand(r=torch.tensor([0.], dtype=torch.float32), \n",
    "                                          v=vs[-1, 0].unsqueeze(-1), logits=logits[-1, 0],\n",
    "                                          encoded=encoded, override=True)\n",
    "                    self.parent = None\n",
    "                \n",
    "                if self.thres is not None:\n",
    "                    self.thres = self.thres_discounting * self.thres + (1 - self.thres_discounting) * vs[-1, 0].item()\n",
    "                \n",
    "                self.root_node.visit()\n",
    "                self.cur_node = self.root_node\n",
    "                \n",
    "            else:\n",
    "                self.rollout_depth += 1                    \n",
    "                self.max_rollout_depth = max(self.max_rollout_depth, self.rollout_depth)\n",
    "                next_node = self.cur_node.children[a]\n",
    "                \n",
    "                if not next_node.expanded():\n",
    "                    self.pass_unexpand = True\n",
    "                    a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions) \n",
    "                    if not self.perfect_model:\n",
    "                        rs, vs, logits, encodeds = self.model.forward_encoded(self.cur_node.encoded, \n",
    "                            a_tensor.unsqueeze(0), one_hot=True)\n",
    "                        next_node.expand(r=rs[-1, 0].unsqueeze(-1), v=vs[-1, 0].unsqueeze(-1), \n",
    "                                     logits=logits[-1, 0], encoded=encodeds[-1])\n",
    "                    else:                        \n",
    "                        if \"done\" not in self.cur_node.encoded:                            \n",
    "                            self.env.restore_state(self.cur_node.encoded)                        \n",
    "                            x, r, done, info = self.env.step(a)                        \n",
    "                            encoded = self.env.clone_state()\n",
    "                            if done: encoded[\"done\"] = True                        \n",
    "                            x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                            self.x_ = x_tensor\n",
    "                            a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions) \n",
    "                            _, vs, logits, _ = self.model(x_tensor, a_tensor.unsqueeze(0), one_hot=True)                        \n",
    "\n",
    "                            if done:\n",
    "                                v = torch.tensor([0.], dtype=torch.float32)\n",
    "                            else:\n",
    "                                v = vs[-1, 0].unsqueeze(-1)\n",
    "\n",
    "                            next_node.expand(r=torch.tensor([r], dtype=torch.float32), \n",
    "                                             v=v, \n",
    "                                             logits=logits[-1, 0], \n",
    "                                             encoded=encoded)\n",
    "                        else:\n",
    "                            logits = torch.concat([x.logit for x in self.cur_node.children])  \n",
    "                            next_node.expand(r=torch.tensor([0.], dtype=torch.float32), \n",
    "                                             v=torch.tensor([0.], dtype=torch.float32),\n",
    "                                             logits=logits, \n",
    "                                             encoded=self.cur_node.encoded)                            \n",
    "                            \n",
    "                next_node.visit()\n",
    "                self.cur_node = next_node\n",
    "            \n",
    "            if self.pass_unexpand:                 \n",
    "                self.unexpand_rollout_depth += 1    \n",
    "                if self.reset_m >= 0 and self.unexpand_rollout_depth > self.reset_m:\n",
    "                    reset = True\n",
    "            \n",
    "            root_node_stat = self.root_node.stat()\n",
    "            cur_node_stat = self.cur_node.stat()                        \n",
    "            reset = torch.tensor([reset], dtype=torch.float32)            \n",
    "            depc = torch.tensor([self.discounting ** (self.rollout_depth-1)])\n",
    "            \n",
    "            root_trail_r = self.root_node.trail_r / self.discounting\n",
    "            root_rollout_q = self.root_node.rollout_q / self.discounting\n",
    "            if self.tree_vb != 0:\n",
    "                rollout_qs = [x + (self.tree_vb if n == 0 else 0.) for n, x in enumerate(self.root_node.rollout_qs)]\n",
    "            else:\n",
    "                rollout_qs = self.root_node.rollout_qs\n",
    "            root_max_q = torch.max(torch.concat(rollout_qs)).unsqueeze(-1) / self.discounting\n",
    "            if self.thres_carry and self.thres is not None:\n",
    "                root_max_q = torch.max(root_max_q, self.thres)\n",
    "                \n",
    "            ret_list = [root_node_stat, cur_node_stat, root_trail_r, root_rollout_q, root_max_q, reset, depc]\n",
    "            if not self.flex_t:\n",
    "                time = F.one_hot(torch.tensor(cur_t).long(), self.rec_t)\n",
    "                ret_list.extend([time])            \n",
    "            else:\n",
    "                time = torch.tensor([self.discounting ** (self.cur_t)])\n",
    "                term = torch.tensor([term], dtype=torch.float32)            \n",
    "                ret_list.extend([term, time])    \n",
    "                \n",
    "            out = torch.concat(ret_list, dim=-1)            \n",
    "            self.last_node = self.cur_node     \n",
    "            \n",
    "            self.root_max_q = root_max_q\n",
    "            self.ret_dict = {\"v0\": self.root_node.ret_dict[\"v\"].unsqueeze(0),\n",
    "                             \"q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_mean\"].unsqueeze(0),\n",
    "                             \"max_q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_max\"].unsqueeze(0),\n",
    "                             \"n_s_a\": self.root_node.ret_dict[\"child_rollout_ns\"].unsqueeze(0),\n",
    "                             \"logit0\": self.root_node.ret_dict[\"child_logits\"].unsqueeze(0),\n",
    "                             \"logit\": self.cur_node.ret_dict[\"child_logits\"].unsqueeze(0),\n",
    "                             \"reset\": reset,\n",
    "                             \"term\": term}\n",
    "            \n",
    "            if self.thres is not None:\n",
    "                self.ret_dict[\"thres\"] = self.thres\n",
    "            \n",
    "            if reset:\n",
    "                self.rollout_depth = 0\n",
    "                self.unexpand_rollout_depth = 0.\n",
    "                self.cur_node = self.root_node\n",
    "                self.cur_node.visit()\n",
    "                self.pass_unexpand = False\n",
    "                \n",
    "            return out\n",
    "                \n",
    "class Node:\n",
    "    def __init__(self, parent, action, logit, num_actions, discounting, rec_t):        \n",
    "        \n",
    "        self.action = F.one_hot(torch.tensor(action).long(), num_actions) # shape (1, num_actions)        \n",
    "        self.r = torch.tensor([0.], dtype=torch.float32)    \n",
    "        self.v = torch.tensor([0.], dtype=torch.float32)            \n",
    "        self.logit = logit # shape (1,)        \n",
    "        \n",
    "        self.rollout_qs = []  # list of tensors of shape (1,)\n",
    "        self.rollout_n = torch.tensor([0.], dtype=torch.float32)    \n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.encoded = None \n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.discounting = discounting\n",
    "        self.rec_t = rec_t        \n",
    "        \n",
    "        self.visited = False\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def expand(self, r, v, logits, encoded, override=False):\n",
    "        \"\"\"\n",
    "        First time arriving a node and so we expand it\n",
    "        r, v: tensor of shape (1,)\n",
    "        logits: tensor of shape (num_actions,)\n",
    "        \"\"\"\n",
    "        if not override: assert not self.expanded()\n",
    "        if override:\n",
    "            self.rollout_qs = [x - self.r + r for x in self.rollout_qs]\n",
    "            self.rollout_qs[0] = v * self.discounting\n",
    "        self.r = r\n",
    "        self.v = v\n",
    "        self.encoded = encoded\n",
    "        for a in range(self.num_actions):\n",
    "            if not override:\n",
    "                child = self.children.append(Node(self, a, logits[[a]], \n",
    "                   self.num_actions, self.discounting, self.rec_t))\n",
    "            else:\n",
    "                self.children[a].logit = logits[[a]]        \n",
    "            \n",
    "    def visit(self):\n",
    "        self.trail_r = torch.tensor([0.], dtype=torch.float32)    \n",
    "        self.trail_discount = 1.\n",
    "        self.propagate(self.r, self.v, not self.visited)        \n",
    "        self.visited = True\n",
    "        \n",
    "    def propagate(self, r, v, new_rollout):\n",
    "        self.trail_r = self.trail_r + self.trail_discount * r\n",
    "        self.trail_discount = self.trail_discount * self.discounting\n",
    "        self.rollout_q = self.trail_r + self.trail_discount * v\n",
    "        if new_rollout:\n",
    "            self.rollout_qs.append(self.rollout_q)\n",
    "            self.rollout_n = self.rollout_n + 1\n",
    "        if self.parent is not None: self.parent.propagate(r, v, new_rollout)\n",
    "            \n",
    "    def stat(self):\n",
    "        assert self.expanded()\n",
    "        self.child_logits = torch.concat([x.logit for x in self.children])        \n",
    "        child_rollout_qs_mean = []\n",
    "        child_rollout_qs_max = []\n",
    "        for x in self.children:\n",
    "            if len(x.rollout_qs) > 0:                \n",
    "                q_mean = torch.mean(torch.cat(x.rollout_qs), dim=-1, keepdim=True)\n",
    "                q_max = torch.max(torch.cat(x.rollout_qs), dim=-1, keepdim=True)[0]\n",
    "            else:\n",
    "                q_mean = torch.tensor([0.], dtype=torch.float32)    \n",
    "                q_max = torch.tensor([0.], dtype=torch.float32)    \n",
    "            child_rollout_qs_mean.append(q_mean)\n",
    "            child_rollout_qs_max.append(q_max)\n",
    "        self.child_rollout_qs_mean = torch.concat(child_rollout_qs_mean)\n",
    "        self.child_rollout_qs_max = torch.concat(child_rollout_qs_max)\n",
    "        self.child_rollout_ns = torch.concat([x.rollout_n for x in self.children]) / self.rec_t       \n",
    "            \n",
    "        ret_list = [\"action\", \"r\", \"v\", \"child_logits\", \"child_rollout_qs_mean\",\n",
    "                    \"child_rollout_qs_max\", \"child_rollout_ns\"]\n",
    "        self.ret_dict = {x: getattr(self, x) for x in ret_list}\n",
    "        out = torch.concat(list(self.ret_dict.values()))        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae5f1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_parser():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n",
    "\n",
    "    parser.add_argument(\"--env\", type=str, default=\"Sokoban-v0\",\n",
    "                        help=\"Gym environment.\")\n",
    "    parser.add_argument(\"--env_disable_noop\", action=\"store_true\",\n",
    "                        help=\"Disable noop in environment or not. (sokoban only)\")\n",
    "\n",
    "    parser.add_argument(\"--xpid\", default=None,\n",
    "                        help=\"Experiment id (default: None).\")\n",
    "\n",
    "    parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n",
    "                        help=\"Disable saving checkpoint.\")\n",
    "    parser.add_argument(\"--load_checkpoint\", default=\"\",\n",
    "                        help=\"Load checkpoint directory.\")    \n",
    "    parser.add_argument(\"--savedir\", default=\"~/RS/thinker/logs/torchbeast\",\n",
    "                        help=\"Root dir where experiment data will be saved.\")\n",
    "\n",
    "    # Training settings.        \n",
    "    parser.add_argument(\"--num_actors\", default=48, type=int, metavar=\"N\",\n",
    "                        help=\"Number of actors (default: 48).\")\n",
    "    parser.add_argument(\"--total_steps\", default=500000000, type=int, metavar=\"T\",\n",
    "                        help=\"Total environment steps to train for.\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, metavar=\"B\",\n",
    "                        help=\"Learner batch size.\")\n",
    "    parser.add_argument(\"--unroll_length\", default=100, type=int, metavar=\"T\",\n",
    "                        help=\"The unroll length (time dimension).\")\n",
    "    parser.add_argument(\"--num_buffers\", default=None, type=int,\n",
    "                        metavar=\"N\", help=\"Number of shared-memory buffers.\")\n",
    "    parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=1, type=int,\n",
    "                        metavar=\"N\", help=\"Number learner threads.\")\n",
    "    parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n",
    "                        help=\"Disable CUDA.\")\n",
    "\n",
    "    # Architecture settings\n",
    "    parser.add_argument(\"--tran_dim\", default=64, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer hidden dim.\")\n",
    "    parser.add_argument(\"--tran_mem_n\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer memory.\")\n",
    "    parser.add_argument(\"--tran_layer_n\", default=3, type=int, metavar=\"N\",\n",
    "                        help=\"Number of transformer layer.\")\n",
    "    parser.add_argument(\"--tran_t\", default=1, type=int, metavar=\"T\",\n",
    "                        help=\"Number of recurrent step for transformer.\")\n",
    "    parser.add_argument(\"--tran_ff_n\", default=256, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer ff .\")\n",
    "    parser.add_argument(\"--tran_skip\", action=\"store_true\",\n",
    "                        help=\"Whether to enable skip conn.\")\n",
    "    parser.add_argument(\"--tran_norm_first\", action=\"store_true\",\n",
    "                        help=\"Whether to use norm first in transformer.\")\n",
    "    parser.add_argument(\"--tran_rpos\", action=\"store_true\",\n",
    "                        help=\"Whether to use relative position in transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm\", action=\"store_true\",\n",
    "                        help=\"Whether to use LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm_no_attn\", action=\"store_true\",\n",
    "                        help=\"Whether to disable attention in LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_attn_b\", default=5.,\n",
    "                        type=float, help=\"Bias attention for current position.\")    \n",
    "    parser.add_argument(\"--tran_erasep\", action=\"store_true\",\n",
    "                        help=\"Whether to erase past memories if not planning.\")\n",
    "    \n",
    "    # Loss settings.\n",
    "    parser.add_argument(\"--entropy_cost\", default=0.0001,\n",
    "                        type=float, help=\"Entropy cost/multiplier.\")\n",
    "    parser.add_argument(\"--im_entropy_cost\", default=0.0001,\n",
    "                        type=float, help=\"Imagainary Entropy cost/multiplier.\")   \n",
    "    parser.add_argument(\"--reset_entropy_cost\", default=0.0001,\n",
    "                        type=float, help=\"Reset Entropy cost/multiplier.\")       \n",
    "    parser.add_argument(\"--baseline_cost\", default=0.5,\n",
    "                        type=float, help=\"Baseline cost/multiplier.\")\n",
    "    parser.add_argument(\"--reg_cost\", default=0.1,\n",
    "                        type=float, help=\"Reg cost/multiplier.\")\n",
    "    parser.add_argument(\"--im_cost\", default=1,\n",
    "                        type=float, help=\"Imaginary reward cost/multiplier.\")    \n",
    "    parser.add_argument(\"--discounting\", default=0.99,\n",
    "                        type=float, help=\"Discounting factor.\")\n",
    "    parser.add_argument(\"--lamb\", default=1.,\n",
    "                        type=float, help=\"Lambda when computing trace.\")\n",
    "    parser.add_argument(\"--reward_clipping\", default=10, type=int, \n",
    "                        metavar=\"N\", help=\"Reward clipping.\")\n",
    "    parser.add_argument(\"--trun_bs\", action=\"store_true\",\n",
    "                        help=\"Whether to add baseline as reward when truncated.\")\n",
    "    \n",
    "    # Model settings\n",
    "    parser.add_argument(\"--reward_type\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"Reward type\")   \n",
    "    parser.add_argument(\"--reset_m\", default=-1, type=int, metavar=\"N\",\n",
    "                        help=\"Auto reset after passing m node since an unexpanded noded\")    \n",
    "    parser.add_argument(\"--model_type_nn\", default=0,\n",
    "                        type=float, help=\"Model type.\")     \n",
    "    parser.add_argument(\"--perfect_model\", action=\"store_true\",\n",
    "                        help=\"Whether to use perfect model.\")    \n",
    "    parser.add_argument(\"--rec_t\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Number of planning steps.\")\n",
    "    parser.add_argument(\"--flex_t\", action=\"store_true\",\n",
    "                        help=\"Whether to enable flexible planning steps.\") \n",
    "    parser.add_argument(\"--flex_t_cost\", default=-1e-6,\n",
    "                        type=float, help=\"Cost of planning step (only enabled when flex_t == True).\")\n",
    "    parser.add_argument(\"--stat_type\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"Staistic type (0: raw; 1: root node stat; 2. root & current node stat).\")    \n",
    "    parser.add_argument(\"--no_mem\", action=\"store_true\",\n",
    "                        help=\"Whether to erase all memories after each real action.\")   \n",
    "    parser.add_argument(\"--tree_carry\", action=\"store_true\",\n",
    "                        help=\"Whether to carry over the tree.\")   \n",
    "    parser.add_argument(\"--tree_vb\", default=0., type=float,\n",
    "                        help=\"Adjustment to initial max-Q.\")    \n",
    "    parser.add_argument(\"--thres_carry\", action=\"store_true\",\n",
    "                        help=\"Whether to carry threshold over.\")   \n",
    "    parser.add_argument(\"--reward_carry\", action=\"store_true\",\n",
    "                        help=\"Whether to carry planning reward over.\")      \n",
    "    parser.add_argument(\"--thres_discounting\", default=0.99,\n",
    "                        type=float, help=\"Threshold discounting factor.\")    \n",
    "    \n",
    "\n",
    "    # Optimizer settings.\n",
    "    parser.add_argument(\"--learning_rate\", default=0.00005,\n",
    "                        type=float, metavar=\"LR\", help=\"Learning rate.\")\n",
    "    parser.add_argument(\"--disable_adam\", action=\"store_true\",\n",
    "                        help=\"Use Aadm optimizer or not.\")\n",
    "    parser.add_argument(\"--alpha\", default=0.99, type=float,\n",
    "                        help=\"RMSProp smoothing constant.\")\n",
    "    parser.add_argument(\"--momentum\", default=0, type=float,\n",
    "                        help=\"RMSProp momentum.\")\n",
    "    parser.add_argument(\"--epsilon\", default=0.01, type=float,\n",
    "                        help=\"RMSProp epsilon.\")\n",
    "    parser.add_argument(\"--grad_norm_clipping\", default=0.0, type=float,\n",
    "                        help=\"Global gradient norm clip.\")\n",
    "    # yapf: enable\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.load_checkpoint = \"\"\n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.num_actors = 1\n",
    "flags.batch_size = 32\n",
    "flags.unroll_length = 50\n",
    "flags.learning_rate = 0.0001\n",
    "flags.grad_norm_clipping = 60\n",
    "\n",
    "flags.entropy_cost = 0.00001\n",
    "flags.im_entropy_cost = 0.00001\n",
    "flags.reset_entropy_cost = 0.00001\n",
    "flags.term_entropy_cost = 0.00001\n",
    "flags.reg_cost = 0.01\n",
    "flags.im_cost = 1\n",
    "flags.discounting = 0.97\n",
    "flags.lamb = 1.\n",
    "\n",
    "flags.trun_bs = False\n",
    "flags.total_steps = 500000000\n",
    "flags.disable_adam = False\n",
    "\n",
    "flags.tran_t = 1\n",
    "flags.tran_mem_n = 5\n",
    "flags.tran_layer_n = 3\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = False\n",
    "flags.tran_attn_b = 5\n",
    "flags.tran_norm_first = False\n",
    "flags.tran_ff_n = 256\n",
    "flags.tran_skip = False\n",
    "flags.tran_erasep = False\n",
    "flags.tran_dim = 64\n",
    "flags.tran_rpos = True\n",
    "\n",
    "flags.no_mem = True\n",
    "flags.rec_t = 5\n",
    "flags.model_type_nn = 0\n",
    "flags.perfect_model = True\n",
    "flags.stat_type = 2\n",
    "flags.reward_type = 1\n",
    "\n",
    "flags.reset_m = -1\n",
    "flags.tree_carry = True\n",
    "flags.thres_carry = True\n",
    "flags.reward_carry = True\n",
    "flags.thres_discounting = 0.97\n",
    "flags._c = True\n",
    "flags.flex_t_cost = 1e-6\n",
    "flags.flex_t_term_b = -5.\n",
    "\n",
    "flags.savedir = \"~/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flags.reward_type == 0:\n",
    "    flags.num_rewards = num_rewards = 1\n",
    "else:\n",
    "    flags.num_rewards = num_rewards = 2\n",
    "flags.im_discounting = flags.discounting ** (1/flags.rec_t)    \n",
    "    \n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "\n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags))\n",
    "obs_shape = env.gym_env.observation_space.shape\n",
    "\n",
    "mp.set_sharing_strategy('file_system')\n",
    "\n",
    "if flags.load_checkpoint:\n",
    "    flags.savedir = os.path.split(os.path.split(flags.load_checkpoint)[0])[0]\n",
    "    flags.xpid = os.path.split(os.path.split(flags.load_checkpoint)[0])[-1]    \n",
    "else:\n",
    "    if flags.xpid is None:\n",
    "        flags.xpid = \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "plogger = file_writer.FileWriter(\n",
    "    xpid=flags.xpid, xp_args=flags.__dict__, rootdir=flags.savedir\n",
    ")\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available():\n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser(\"%s/%s/%s\" % (flags.savedir, flags.xpid, \"model.tar\"))\n",
    ")\n",
    "\n",
    "if flags.num_buffers is None:  # Set sensible default for num_buffers.\n",
    "    flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n",
    "if flags.num_actors >= flags.num_buffers:\n",
    "    raise ValueError(\"num_buffers should be larger than num_actors\")\n",
    "if flags.num_buffers < flags.batch_size:\n",
    "    raise ValueError(\"num_buffers should be larger than batch_size\")\n",
    "\n",
    "T = flags.unroll_length\n",
    "B = flags.batch_size\n",
    "\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags)\n",
    "buffers = create_buffers(flags, obs_shape, num_actions, num_rewards)\n",
    "\n",
    "if flags.load_checkpoint:\n",
    "    train_checkpoint = torch.load(flags.load_checkpoint)\n",
    "    actor_net.load_state_dict(train_checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "actor_net.share_memory()\n",
    "\n",
    "# Add initial RNN state.\n",
    "initial_agent_state_buffers = []\n",
    "for _ in range(flags.num_buffers):\n",
    "    state = actor_net.initial_state(batch_size=1)\n",
    "    for t in state:\n",
    "        t.share_memory_()\n",
    "    initial_agent_state_buffers.append(state)\n",
    "\n",
    "actor_processes = []\n",
    "ctx = mp.get_context()\n",
    "free_queue = ctx.SimpleQueue()\n",
    "full_queue = ctx.SimpleQueue()\n",
    "\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=act, args=(flags, i, free_queue, full_queue,\n",
    "            actor_net, model, buffers, initial_agent_state_buffers,),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)\n",
    "\n",
    "learner_net = Actor_net(obs_shape, num_actions, flags)\n",
    "if flags.load_checkpoint:\n",
    "    learner_net.load_state_dict(train_checkpoint[\"model_state_dict\"])\n",
    "learner_net = learner_net.to(device=flags.device)  \n",
    "\n",
    "if not flags.disable_adam:\n",
    "    print(\"Using Adam...\")        \n",
    "    optimizer = torch.optim.Adam(learner_net.parameters(),lr=flags.learning_rate)\n",
    "else:\n",
    "    print(\"Using RMS Prop...\")\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        learner_net.actor.parameters(),\n",
    "        lr=flags.learning_rate,\n",
    "        momentum=flags.momentum,\n",
    "        eps=flags.epsilon,\n",
    "        alpha=flags.alpha,)\n",
    "    \n",
    "if flags.load_checkpoint:\n",
    "    optimizer.load_state_dict(train_checkpoint[\"optimizer_state_dict\"])    \n",
    "    \n",
    "print(\"All parameters: \")\n",
    "for k, v in learner_net.named_parameters(): print(k, v.numel())    \n",
    "\n",
    "if not flags.flex_t:\n",
    "    lr_lambda = lambda epoch: 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n",
    "else:\n",
    "    lr_lambda = lambda epoch: 1 - min(epoch, flags.total_steps) / flags.total_steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "if flags.load_checkpoint:\n",
    "    scheduler.load_state_dict(train_checkpoint[\"scheduler_state_dict\"])\n",
    "    \n",
    "logger = logging.getLogger(\"logfile\")\n",
    "stat_keys = [\"mean_episode_return\", \"episode_returns\", \"total_loss\",\n",
    "    \"pg_loss\", \"baseline_loss\", \"entropy_loss\", \"max_rollout_depth\"]\n",
    "if flags.reward_type == 1:\n",
    "    stat_keys.extend([\"im_pg_loss\", \"im_baseline_loss\"])\n",
    "\n",
    "logger.info(\"# Step\\t%s\", \"\\t\".join(stat_keys))\n",
    "\n",
    "step, real_step, stats, last_returns, last_im_returns, tot_eps = 0, 0, {}, deque(maxlen=400), deque(maxlen=40000), 0\n",
    "if flags.load_checkpoint:\n",
    "    if \"step\" in train_checkpoint.keys():    \n",
    "        step = train_checkpoint[\"step\"]\n",
    "        real_step = train_checkpoint[\"real_step\"]\n",
    "    else:\n",
    "        # legacy support\n",
    "        step = train_checkpoint[\"scheduler_state_dict\"][\"_step_count\"] * T * B\n",
    "    \n",
    "def batch_and_learn(i, lock=threading.Lock()):\n",
    "    \"\"\"Thread target for the learning process.\"\"\"\n",
    "    #nonlocal step, stats, last_returns, tot_eps\n",
    "    global step, real_step, stats, last_returns, last_im_returns, tot_eps\n",
    "    timings = prof.Timings()\n",
    "    while step < flags.total_steps:\n",
    "        timings.reset()\n",
    "        batch, agent_state = get_batch(flags, free_queue, full_queue, buffers,\n",
    "            initial_agent_state_buffers, timings,)\n",
    "        stats = learn(flags, actor_net, learner_net, batch, \n",
    "                      agent_state, optimizer, scheduler, real_step)\n",
    "        last_returns.extend(stats[\"episode_returns\"])\n",
    "        if \"im_episode_returns\" in stats:\n",
    "            last_im_returns.extend(stats[\"im_episode_returns\"])\n",
    "        tot_eps = tot_eps + len(stats[\"episode_returns\"])\n",
    "        timings.time(\"learn\")\n",
    "        with lock:\n",
    "            to_log = dict(step=step, real_step=real_step)\n",
    "            to_log.update({k: stats[k] for k in stat_keys})            \n",
    "            to_log.update({\"rmean_im_episode_return\": np.average(last_im_returns) if len(last_im_returns) > 0 else 0.,\n",
    "                           \"rmean_episode_return\": np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "                           \"episode\": tot_eps})\n",
    "            plogger.log(to_log)\n",
    "            step += T * B\n",
    "            real_step += stats[\"real_step\"]\n",
    "\n",
    "    if i == 0:\n",
    "        logging.info(\"Batch and learn: %s\", timings.summary())\n",
    "\n",
    "for m in range(flags.num_buffers):\n",
    "    free_queue.put(m)\n",
    "\n",
    "threads = []\n",
    "for i in range(flags.num_learner_threads):\n",
    "    thread = threading.Thread(\n",
    "        target=batch_and_learn, name=\"batch-and-learn-%d\" % i, args=(i,)\n",
    "    )\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "def checkpoint():\n",
    "    if flags.disable_checkpoint:\n",
    "        return\n",
    "    logging.info(\"Saving checkpoint to %s\", checkpointpath)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": actor_net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"step\": step,\n",
    "            \"real_step\": real_step,\n",
    "            \"flags\": vars(flags),\n",
    "        },\n",
    "        checkpointpath,\n",
    "    )\n",
    "\n",
    "timer = timeit.default_timer\n",
    "try:\n",
    "    last_checkpoint_time = timer()\n",
    "    while real_step < flags.total_steps:\n",
    "        start_step = step\n",
    "        start_time = timer()\n",
    "        time.sleep(5)\n",
    "\n",
    "        if timer() - last_checkpoint_time > 10 * 60:  # Save every 10 min.\n",
    "            checkpoint()\n",
    "            last_checkpoint_time = timer()\n",
    "\n",
    "        sps = (step - start_step) / (timer() - start_time)\n",
    "        if stats.get(\"episode_returns\", None):\n",
    "            mean_return = (\n",
    "                \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n",
    "            )\n",
    "        else:\n",
    "            mean_return = \"\"\n",
    "        total_loss = stats.get(\"total_loss\", float(\"inf\"))\n",
    "\n",
    "        print_str =  \"Steps %i (%i) @ %.1f SPS. Eps %i. Return %f (%f). Loss %.2f\" % (real_step, step, sps, tot_eps, \n",
    "            np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "            np.average(last_im_returns) if len(last_im_returns) > 0 else 0.,\n",
    "            total_loss)\n",
    "\n",
    "        for s in [\"mean_plan_step\", \"max_rollout_depth\", \"pg_loss\", \"baseline_loss\", \"im_pg_loss\", \n",
    "                  \"im_baseline_loss\", \"entropy_loss\", \"reg_loss\", \"total_norm\"]:\n",
    "            if s in stats: print_str += \" %s %.2f\" % (s, stats[s])\n",
    "\n",
    "        logging.info(print_str)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    for thread in threads:\n",
    "        thread.join()        \n",
    "    # Try joining actors then quit.\n",
    "else:\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    logging.info(\"Learning finished after %d steps.\", step)\n",
    "finally:\n",
    "    for _ in range(flags.num_actors):\n",
    "        free_queue.put(None)\n",
    "    for actor in actor_processes:\n",
    "        actor.join(timeout=1)\n",
    "\n",
    "checkpoint()\n",
    "plogger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b3eb280-7716-44b4-8057-8f688442b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.last_epoch = 250000000\n",
    "scheduler.step = ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59d67083-0b2e-46b3-b5cf-0a24d571dc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5e-05]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler.get_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a726743",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = flags.total_steps + 1\n",
    "for thread in threads:\n",
    "     thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(flags.num_actors):\n",
    "    free_queue.put(None)\n",
    "for actor in actor_processes:\n",
    "    actor.join(timeout=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b71ad",
   "metadata": {},
   "source": [
    "<font size=\"5\">Agent Debug and Visualize</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d487f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "\n",
    "def plot_obs(x, ax=None, title=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    ax.imshow(torch.swapaxes(torch.swapaxes(x[0].cpu(),0,2),0,1), interpolation='nearest', aspect=\"auto\")\n",
    "    if title is not None: ax.set_title(title)\n",
    "    \n",
    "def plot_qn_sa(q_s_a, n_s_a, max_q_s_a=None, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    xs = np.arange(len(q_s_a))\n",
    "\n",
    "    ax.bar(xs - 0.3, q_s_a.cpu(), color = 'g', width = 0.3, label=\"q_s_a\")    \n",
    "    ax_n = ax.twinx()\n",
    "    if max_q_s_a is not None:\n",
    "        ax.bar(xs, max_q_s_a.cpu(), color = 'r', width = 0.3, label=\"max_q_s_a\")        \n",
    "    ax_n.bar(xs + (0.3 if max_q_s_a is not None else 0.), \n",
    "             n_s_a.cpu(), bottom=0, color = 'b', width = 0.3, label=\"n_s_a\")\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(q_s_a))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))    \n",
    "    ax.legend(loc=\"upper left\")   \n",
    "    ax_n.legend(loc=\"upper right\") \n",
    "    ax.set_title(\"q_s_a and n_s_a\")\n",
    "    \n",
    "def plot_policies(model_logit, actor_logit, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    model_prob = torch.softmax(model_logit, dim=-1).detach().cpu().numpy()\n",
    "    prob = torch.softmax(actor_logit, dim=-1).detach().cpu().numpy()\n",
    "    ax.set_title(\"Real policy prob\")\n",
    "    xs = np.arange(len(model_prob))\n",
    "    ax.bar(xs - 0.1, model_prob, color = 'g', width = 0.1, label=\"model policy prob\")\n",
    "    ax.bar(xs, prob, color = 'r', width = 0.1, label=\"agent policy prob\")\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(model_prob))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))\n",
    "    ax.set_ylim(0, 1)        \n",
    "    ax.legend()       \n",
    "        \n",
    "def plot_im_policies(im_policy_logits, reset_policy_logits, im_action, reset_action, \n",
    "                     one_hot=True, reset_ind=0, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "        \n",
    "    rec_t, num_actions = im_policy_logits.shape\n",
    "    num_actions += 1\n",
    "    rec_t -= 1\n",
    "        \n",
    "    im_prob = torch.softmax(im_policy_logits, dim=-1).detach().cpu().numpy()\n",
    "    reset_prob = torch.softmax(reset_policy_logits, dim=-1)[:,[reset_ind]].detach().cpu().numpy()\n",
    "    im_reset_prob = np.concatenate([im_prob, reset_prob], axis=-1)\n",
    "    \n",
    "    if not one_hot: im_action = F.one_hot(im_action, num_actions - 1)\n",
    "    im_action = im_action.detach().cpu().numpy()\n",
    "    reset_action = reset_action.unsqueeze(-1).detach().cpu().numpy()    \n",
    "    im_reset_action = np.concatenate([im_action, reset_action], axis=-1)\n",
    "    \n",
    "    im_reset_prob = im_reset_prob[:-1]\n",
    "    im_reset_action = im_reset_action[:-1]\n",
    "    \n",
    "    xs = np.arange(rec_t)\n",
    "    labels = ['noop', 'up', 'down', 'left', 'right', 'reset']\n",
    "    for i in range(num_actions):        \n",
    "        c = ax.bar(xs + 0.8 * (i / num_actions), im_reset_prob[:,i], width = 0.8 / (num_actions), label=labels[i])  \n",
    "        color = c.patches[0].get_facecolor()\n",
    "        color = color[:3] + (color[3] * 0.5,)\n",
    "        ax.bar(xs + 0.8 * (i / num_actions), im_reset_action[:,i], width = 0.8 / (num_actions), color=color)\n",
    "        \n",
    "    \n",
    "    #xs = np.arange(num_actions)\n",
    "    #for i in range(rec_t):\n",
    "    #    ax.bar(xs + 0.8 * (i / rec_t), im_reset_action[i], width = 0.8 / (rec_t), color=\"#cccccc\")\n",
    "    #    ax.bar(xs + 0.8 * (i / rec_t), im_reset_prob[i], width = 0.8 / (rec_t))        \n",
    "    #ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(num_actions)))\n",
    "    #ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right', 'reset'))    \n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)   \n",
    "    ax.set_title(\"Imagainary policy prob\")\n",
    "    \n",
    "def print_im_actions(im_dict, print_stat=False):\n",
    "\n",
    "    lookup_dict = {0:\"Noop\",\n",
    "                   1:\"Up\",\n",
    "                   2:\"Down\",\n",
    "                   3:\"Left\",\n",
    "                   4:\"Right\"}\n",
    "\n",
    "    print_strs = []\n",
    "    n, s = 1, \"\"\n",
    "    reset = False\n",
    "    for im, reset in zip(im_dict[\"im_action\"][:-1], im_dict[\"reset_action\"][:-1]):\n",
    "        s += lookup_dict[im.item()] + \", \"\n",
    "        if reset:        \n",
    "            s += \"Reset\"\n",
    "            print_strs.append(\"%d: %s\" %(n, s))\n",
    "            s = \"\"\n",
    "            n += 1\n",
    "    if not reset: print_strs.append(\"%d: %s\" %(n, s[:-2]))\n",
    "    if print_stat: \n",
    "        for s in print_strs: print(s) \n",
    "    return print_strs\n",
    "\n",
    "def plot_im_actions(im_dict, ax=None):    \n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    print_strs = print_im_actions(im_dict, print_stat=False)\n",
    "    for n, s in enumerate(print_strs):  \n",
    "        txt = ax.text(0, 0.8 - 0.1 * n, s, size='x-large')        \n",
    "        txt.set_clip_on(True) \n",
    "    ax.set_xlim(0,1)    \n",
    "        \n",
    "def plot_base_policies(logits, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    prob = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    rec_t, num_actions = logits.shape\n",
    "    xs = np.arange(rec_t)\n",
    "    labels = ['noop', 'up', 'down', 'left', 'right']\n",
    "    for i in range(num_actions):        \n",
    "        c = ax.bar(xs + 0.8 * (i / num_actions), prob[:,i], width = 0.8 / (num_actions), label=labels[i])  \n",
    "        color = c.patches[0].get_facecolor()\n",
    "        color = color[:3] + (color[3] * 0.5,)\n",
    "        ax.bar(xs + 0.8 * (i / num_actions), prob[:,i], width = 0.8 / (num_actions), color=color)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)   \n",
    "    ax.set_title(\"Model policy prob\")\n",
    "        \n",
    "def plot_attn(attn_output):\n",
    "    plt.figure()\n",
    "    ln = flags.tran_layer_n\n",
    "    fig, axarr = plt.subplots(ln,8, figsize=(12,4)) \n",
    "    attn_output_ = torch.concat(attn_output, dim=-2)\n",
    "    for k in range(ln):\n",
    "        for i in range(8):\n",
    "            out = attn_output_[k, i]\n",
    "            out = out.detach().cpu().numpy()\n",
    "            axarr[k, i].imshow(out, interpolation='nearest', vmin=0, vmax=1)   \n",
    "            axarr[k, i].set_xticks([])\n",
    "            axarr[k, i].set_yticks([])\n",
    "    fig.suptitle('Attention')\n",
    "    fig.text(0.5, 0.04, 'Attention Head', ha='center', va='center')\n",
    "    fig.text(0.12, 0.5, 'Layer', ha='center', va='center', rotation='vertical')\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 1\n",
    "\n",
    "name = \"alstm_3_1_rec_n_60_rei_aug2_clip_mem1_imcost_1_perfect_treecarry_threscarry\"\n",
    "#checkpoint = torch.load(\"/home/sc/RS/thinker/logs/planner_logs/%s/model.tar\" % name)\n",
    "checkpoint = torch.load(\"D:/data/thinker/logs/planner_logs/%s/model.tar\" % name)\n",
    "\n",
    "flags_ = checkpoint[\"flags\"]\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])  \n",
    "for k, v in flags_.items(): setattr(flags, k, v)\n",
    "\n",
    "if flags.reward_type == 0:\n",
    "    flags.num_rewards = num_rewards = 1\n",
    "else:\n",
    "    flags.num_rewards = num_rewards = 2\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "model_checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(model_checkpoint[\"model_state_dict\"])   \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags))\n",
    "obs_shape = env.gym_env.observation_space.shape\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available():\n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags).to(flags.device)\n",
    "actor_net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "print(\"flags:\", flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb82bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = True\n",
    "eps_n = 1000\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "model.train(False)\n",
    "env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True), model=model, flags=flags)\n",
    "env = Environment(env)\n",
    "\n",
    "core_state = actor_net.initial_state(bsz)\n",
    "core_state = tuple(v.to(flags.device) for v in core_state)\n",
    "returns = []\n",
    "obs = env.initial()\n",
    "\n",
    "t = 0\n",
    "im_list = [\"im_policy_logits\", \"reset_policy_logits\", \"im_action\", \"reset_action\"]\n",
    "im_dict = {k: [] for k in im_list}\n",
    "model_logits, attn_output = [], []\n",
    "\n",
    "while(True):\n",
    "    if len(returns) > (0 if debug else eps_n): break    \n",
    "    with torch.no_grad():\n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}                             \n",
    "        actor_out, core_state = actor_net(obs, core_state, debug=False)\n",
    "        action = torch.cat([actor_out['action'][0].unsqueeze(-1), \n",
    "                            actor_out['im_action'][0].unsqueeze(-1), \n",
    "                            actor_out['reset_action'][0].unsqueeze(-1)], dim=-1)\n",
    "        if len(im_dict['reset_action']) > 0:\n",
    "            im_dict['reset_action'][-1] = env.gym_env.ret_dict['reset'].to(flags.device)\n",
    "        for k in im_list: \n",
    "            im_dict[k].append(actor_out[k][0,0].unsqueeze(0))        \n",
    "        model_logits.append(env.gym_env.ret_dict[\"logit\"])\n",
    "        attn_output.append(torch.cat([x.attn_output_weights.unsqueeze(0).unsqueeze(-2) for x in actor_net.core.layers])[:, :, 0, :])\n",
    "        if debug and obs[\"cur_t\"][0,0] == (flags.rec_t - 1): \n",
    "            plot_attn(attn_output)\n",
    "            for k in im_list: im_dict[k] = torch.concat(im_dict[k], dim=0)            \n",
    "            fig, axs = plt.subplots(1, 5, figsize=(30,6))              \n",
    "            ret_dict = env.gym_env.ret_dict\n",
    "            title = \"step: %d; values: %.4f\" % (t, ret_dict[\"v0\"][0].cpu())\n",
    "            if \"thres\" in ret_dict: title += \" thres: %.4f\" % ret_dict[\"thres\"][0].cpu()\n",
    "            if flags.reward_type == 1: title += \" im_return: %.4f\" % obs['episode_return'][..., 1]                        \n",
    "            plot_obs(env.gym_env.x/255, axs[0], title=title)          \n",
    "            max_q_s_a = ret_dict[\"max_q_s_a\"][0] if \"max_q_s_a\" in ret_dict else None                        \n",
    "            plot_qn_sa(ret_dict[\"q_s_a\"][0], ret_dict[\"n_s_a\"][0], max_q_s_a, ax=axs[3])             \n",
    "            plot_policies(ret_dict[\"logit0\"][0], actor_out[\"policy_logits\"][0,0], ax=axs[4])    \n",
    "            plot_base_policies(torch.concat(model_logits), ax=axs[1])  \n",
    "            plot_im_policies(**im_dict, one_hot=False, reset_ind=1, ax=axs[2])              \n",
    "            plt.show()                        \n",
    "            print_im_actions(im_dict, print_stat=True)            \n",
    "            im_dict = {k: [] for k in im_list}          \n",
    "            model_logits, attn_output = [], []            \n",
    "        obs = env.step(action.unsqueeze(0))        \n",
    "        if torch.any(obs['done']):\n",
    "            new_rets = obs['episode_return'][obs['done']][:,0].numpy()\n",
    "            returns.extend(new_rets)\n",
    "            print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599b3d8f-5a0f-42eb-9b8f-cb79cf20c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "f, axarr = plt.subplots(1, 3, figsize=(6,6)) \n",
    "for k in range(3):\n",
    "    out = actor_net.core.layers[k].pos_b.detach().cpu().numpy()\n",
    "    ax = axarr[k]\n",
    "    ax.imshow(out, interpolation='nearest')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    im = ax.imshow(out)\n",
    "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "f.tight_layout()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = True\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "\n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "                model=model, rec_t=flags.rec_t, flags=flags))\n",
    "obs = env.initial()\n",
    "\n",
    "obs_shape, num_actions = env.gym_env.observation_space.shape, env.gym_env.action_space.n   \n",
    "num_actions = env.gym_env.action_space.n\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags)\n",
    "core_state = actor_net.initial_state(1)\n",
    "\n",
    "for t in range(10):\n",
    "  actor_out, core_state = actor_net(obs, core_state=core_state)\n",
    "  action = torch.cat([actor_out['action'], actor_out['im_action'], actor_out['reset_action']], dim=-1)\n",
    "  obs = env.step(action.unsqueeze(0))\n",
    "  print(t, obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 4    \n",
    "root = Node(parent=None, action=2, logit=torch.tensor([0.3]), num_actions=num_actions, discounting=0.97)\n",
    "root.expand(r=torch.tensor([3.]), v=torch.tensor([2.]), logits=torch.tensor([0.3,0.4,0.5,0.6]), encoded=None)\n",
    "root.visit()\n",
    "cur_node = root\n",
    "print(1, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[3]\n",
    "cur_node.expand(r=torch.tensor([1.]), v=torch.tensor([4.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "\n",
    "print(2, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[2]\n",
    "cur_node.expand(r=torch.tensor([100.]), v=torch.tensor([0.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "print(3, root.rollout_qs, root.stat())\n",
    "\n",
    "root.visit()\n",
    "cur_node = root\n",
    "print(4, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[3]\n",
    "#cur_node.expand(r=torch.tensor([1.]), v=torch.tensor([3.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "\n",
    "print(5, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[1]\n",
    "cur_node.expand(r=torch.tensor([-10.]), v=torch.tensor([0.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "print(6, root.rollout_qs, root.stat())\n",
    "\n",
    "flags.rec_t = 20\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n     \n",
    "    \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags))\n",
    "_ = env.initial()\n",
    "\n",
    "act_seqs = [[2, 1, 1], [2, 2, 1], [2, 3, 1], [2, 4, 1], [2, 1, 0], [2, 2, 1]]\n",
    "#act_seqs = [[2, 4, 0], [2, 1, 1], [2, 4, 0], [2, 2, 1], [2, 2, 1]]\n",
    "\n",
    "for a in act_seqs:\n",
    "    obs = env.step(torch.tensor(a).unsqueeze(0).unsqueeze(0))\n",
    "    x = obs['frame'][0, 0, :, 0, 0]\n",
    "    r = obs['reward'][0, 0, :]\n",
    "    print(\"================================================\")\n",
    "    print(\"a\", a[1:])\n",
    "    print(\"x\", x)\n",
    "    print(\"r\", r)\n",
    "    print(\"rollout_qs\", torch.concat(env.gym_env.root_node.rollout_qs) / flags.discounting)\n",
    "    print(\"root stat\", env.gym_env.root_node.ret_dict)\n",
    "    print(\"last stat\", env.gym_env.last_node.ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e62d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class ModelWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, model, flags):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.env = env\n",
    "        self.model = model                \n",
    "        self.rec_t = flags.rec_t        \n",
    "        self.discounting = flags.discounting\n",
    "        self.stat_type = flags.stat_type    \n",
    "        self.reward_type = flags.reward_type    \n",
    "        self.no_mem = flags.no_mem\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        # 0 for the most basic; \n",
    "        # 1 for augmented input in root stat; \n",
    "        # 2 for tree stat\n",
    "        if self.stat_type == 0:\n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 5 + num_actions * 4 + self.rec_t\n",
    "        elif self.stat_type == 1:            \n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 7 + num_actions * 7 + self.rec_t\n",
    "        elif self.stat_type == 1.5:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 6 + num_actions * 10 + self.rec_t             \n",
    "        elif self.stat_type == 2:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 9 + num_actions * 10 + self.rec_t         \n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "          low=-np.inf, high=np.inf, shape=(obs_n, 1, 1), dtype=float)\n",
    "        self.model.train(False)        \n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        x = self.env.reset()\n",
    "        self.cur_t = 0        \n",
    "        out = self.use_model_raw(x, 0., 0, self.cur_t, 1.)\n",
    "        out_ = self.use_model_tree(x, 0., 0, self.cur_t, 1.)\n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q\n",
    "        self.max_rollout_depth = 0.\n",
    "        return out, out_\n",
    "    \n",
    "    def step(self, action):  \n",
    "        re_action, im_action, reset = action\n",
    "        info = {}\n",
    "        info[\"max_rollout_depth\"] = self.max_rollout_depth\n",
    "        if self.cur_t < self.rec_t - 1:\n",
    "          self.cur_t += 1\n",
    "          out = self.use_model_raw(None, None, im_action, self.cur_t, reset)      \n",
    "          out_ = self.use_model_tree(None, None, im_action, self.cur_t, reset)      \n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([0.])\n",
    "          else:\n",
    "            r = np.array([0., (self.root_max_q - self.last_root_max_q).item()], dtype=np.float32)\n",
    "          done = False\n",
    "          info['cur_t'] = self.cur_t   \n",
    "        else:\n",
    "          self.cur_t = 0\n",
    "          x, r, done, info_ = self.env.step(re_action)                    \n",
    "          out = self.use_model_raw(x, r, re_action, self.cur_t, 1.) \n",
    "          out_ = self.use_model_tree(x, r, re_action, self.cur_t, 1.) \n",
    "          info.update(info_)\n",
    "          info['cur_t'] = self.cur_t\n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([r])\n",
    "          else:\n",
    "            r = np.array([r, 0.], dtype=np.float32)   \n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q        \n",
    "        return out, out_, r, done, info        \n",
    "        \n",
    "    def use_model_raw(self, x, r, a, cur_t, reset):\n",
    "        # input: \n",
    "        # r: reward - [,]; x: frame - [C, H, W]; a: action - [,]\n",
    "        # cur_t: int; reset at cur_t == 0  \n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:                \n",
    "                self.rollout_depth_ = 0.\n",
    "                if self.no_mem:\n",
    "                    self.re_action = F.one_hot(torch.zeros(1, dtype=torch.long), self.num_actions)   \n",
    "                else:\n",
    "                    self.re_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)                   \n",
    "                \n",
    "                x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = x\n",
    "                _, vs, logits, encodeds = self.model(x, self.re_action.unsqueeze(0), one_hot=True)                \n",
    "                self.encoded = encodeds[-1]    \n",
    "                self.encoded_reset = self.encoded.clone()\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    self.re_reward = torch.tensor([[0.]], dtype=torch.float32)                \n",
    "                else:\n",
    "                    self.re_reward = torch.tensor([[r]], dtype=torch.float32)                \n",
    "                    \n",
    "                self.v0 = vs[-1].unsqueeze(-1).clone()\n",
    "                self.logit0 = logits[-1].clone()\n",
    "                \n",
    "                self.im_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.im_reset = torch.tensor([[1.]], dtype=torch.float32)\n",
    "                self.im_reward = torch.zeros(1, 1, dtype=torch.float32)                                \n",
    "                self.v = vs[-1].unsqueeze(-1)\n",
    "                self.logit = logits[-1]\n",
    "                self.rollout_first_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                self.rollout_return = torch.zeros(1, 1, dtype=torch.float32)                \n",
    "                self.q_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.n_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)                \n",
    "            else:\n",
    "                self.rollout_depth_ += 1                \n",
    "                \n",
    "                self.im_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)   \n",
    "                rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "                   self.im_action.unsqueeze(0), one_hot=True)\n",
    "                self.encoded = encodeds[-1]        \n",
    "                \n",
    "                self.im_reward = rs[-1].unsqueeze(-1)\n",
    "                self.v = vs[-1].unsqueeze(-1)    \n",
    "                self.logit = logits[-1]     \n",
    "                \n",
    "                if self.im_reset: \n",
    "                    # last action's reset is true; re-initialize everything                    \n",
    "                    self.rollout_first_action = self.im_action.clone()\n",
    "                    self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                    self.rollout_depth_ = 1                      \n",
    "                    \n",
    "                self.rollout_return_wo_v += (self.discounting ** (self.rollout_depth_-1)) * self.im_reward\n",
    "                self.rollout_return = self.rollout_return_wo_v + (\n",
    "                    self.discounting ** (self.rollout_depth_)) * self.v   \n",
    "                    \n",
    "                self.im_reset = torch.tensor([[reset]], dtype=torch.float32)\n",
    "                if self.im_reset:                    \n",
    "                    rollout_first_action_label = torch.argmax(self.rollout_first_action, dim=1)                    \n",
    "                    q = self.q_s_a[:, rollout_first_action_label]\n",
    "                    n = self.n_s_a[:, rollout_first_action_label]                    \n",
    "                    ret = self.rollout_return[:, 0]\n",
    "                    self.n_s_a[:, rollout_first_action_label] += 1                    \n",
    "                    self.q_s_a[:, rollout_first_action_label] = (n * q) / (n + 1) + ret / (n + 1)\n",
    "                    \n",
    "        time = F.one_hot(torch.tensor([cur_t]).long(), self.rec_t)\n",
    "        depc = torch.tensor([[self.discounting ** (self.rollout_depth_-1)]])\n",
    "        ret_dict = {\"re_action\": self.re_action,\n",
    "                    \"re_reward\": self.re_reward,\n",
    "                    \"v0\": self.v0,\n",
    "                    \"logit0\": self.logit0,\n",
    "                    \"im_action\": self.im_action,\n",
    "                    \"im_reset\": self.im_reset,\n",
    "                    \"im_reward\": self.im_reward,\n",
    "                    \"v\": self.v,\n",
    "                    \"logit\": self.logit,\n",
    "                    \"rollout_first_action\": self.rollout_first_action,\n",
    "                    \"rollout_return\": self.rollout_return,\n",
    "                    \"n_s_a\": self.n_s_a / self.rec_t,\n",
    "                    \"q_s_a\": self.q_s_a,\n",
    "                    \"time\": time,\n",
    "                    \"depc\": depc}        \n",
    "        self.ret_dict = ret_dict\n",
    "        if self.stat_type == 1:\n",
    "            out = torch.concat(list(ret_dict.values()), dim=-1)   \n",
    "        else:\n",
    "            core_inputs = [\"re_action\", \"re_reward\", \"v0\", \"logit0\", \"im_action\",\n",
    "                           \"im_reset\", \"im_reward\", \"v\", \"logit\", \"time\"]\n",
    "            out = torch.concat([ret_dict[v] for v in core_inputs], dim=-1)   \n",
    "        self.encoded = reset * self.encoded_reset + (1 - reset) * self.encoded\n",
    "        return out[0]      \n",
    "    \n",
    "    def use_model_tree(self, x, r, a, cur_t, reset):\n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:\n",
    "                self.rollout_depth = 0.\n",
    "                self.max_rollout_depth = 0.\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    re_action = 0\n",
    "                    re_reward = torch.tensor([0.], dtype=torch.float32)                \n",
    "                else:\n",
    "                    re_action = a                \n",
    "                    re_reward = torch.tensor([r], dtype=torch.float32)                \n",
    "                \n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = x_tensor\n",
    "                a_tensor = F.one_hot(torch.tensor(re_action, dtype=torch.long).unsqueeze(0), self.num_actions)\n",
    "                \n",
    "                _, vs, logits, encodeds = self.model(x_tensor, a_tensor.unsqueeze(0), one_hot=True)\n",
    "                \n",
    "                self.root_node = Node(parent=None, action=re_action, logit=None, \n",
    "                                      num_actions=self.num_actions,\n",
    "                                      discounting=self.discounting,\n",
    "                                      rec_t=self.rec_t)\n",
    "                self.root_node.expand(r=re_reward, v=vs[-1, 0].unsqueeze(-1), logits=logits[-1, 0],\n",
    "                                      encoded=encodeds[-1])\n",
    "                self.root_node.visit()\n",
    "                self.cur_node = self.root_node\n",
    "                self.rollout_first_action_ = torch.zeros(self.num_actions)\n",
    "            else:\n",
    "                if self.rollout_depth == 0:\n",
    "                    self.rollout_first_action_ = F.one_hot(torch.tensor(a, dtype=torch.long), self.num_actions)\n",
    "                self.rollout_depth += 1    \n",
    "                self.max_rollout_depth = max(self.max_rollout_depth, self.rollout_depth)\n",
    "                next_node = self.cur_node.children[a]\n",
    "                if not next_node.expanded():\n",
    "                    a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions) \n",
    "                    rs, vs, logits, encodeds = self.model.forward_encoded(self.cur_node.encoded, \n",
    "                        a_tensor.unsqueeze(0), one_hot=True)\n",
    "                    next_node.expand(r=rs[-1, 0].unsqueeze(-1), v=vs[-1, 0].unsqueeze(-1), \n",
    "                                     logits=logits[-1, 0], encoded=encodeds[-1])\n",
    "                next_node.visit()\n",
    "                self.cur_node = next_node\n",
    "            \n",
    "            root_node_stat = self.root_node.stat()\n",
    "            cur_node_stat = self.cur_node.stat()                        \n",
    "            reset = torch.tensor([reset], dtype=torch.float32)\n",
    "            time = F.one_hot(torch.tensor(cur_t).long(), self.rec_t)\n",
    "            depc = torch.tensor([self.discounting ** (self.rollout_depth-1)])\n",
    "            \n",
    "            root_trail_r = self.root_node.trail_r / self.discounting\n",
    "            root_rollout_q = self.root_node.rollout_q / self.discounting\n",
    "            root_max_q = torch.max(torch.concat(self.root_node.rollout_qs)).unsqueeze(-1) / self.discounting\n",
    "            \n",
    "            ret_list = [root_node_stat, cur_node_stat, reset, time, depc,]\n",
    "            if self.stat_type >= 2: ret_list.extend([root_trail_r, root_rollout_q, root_max_q])            \n",
    "                \n",
    "            if self.stat_type == 1:\n",
    "                ret_list = [self.root_node.ret_dict[\"action\"],\n",
    "                            self.root_node.ret_dict[\"r\"],\n",
    "                            self.root_node.ret_dict[\"v\"],\n",
    "                            self.root_node.ret_dict[\"child_logits\"],\n",
    "                            self.cur_node.ret_dict[\"action\"],\n",
    "                            reset,\n",
    "                            self.cur_node.ret_dict[\"r\"],\n",
    "                            self.cur_node.ret_dict[\"v\"],\n",
    "                            self.cur_node.ret_dict[\"child_logits\"],\n",
    "                            self.rollout_first_action_,\n",
    "                            self.root_node.rollout_q / self.discounting,\n",
    "                            self.root_node.ret_dict[\"child_rollout_ns\"],\n",
    "                            self.root_node.ret_dict[\"child_rollout_qs_mean\"],\n",
    "                            time,\n",
    "                            depc                            \n",
    "                            ]                 \n",
    "                \n",
    "                \n",
    "            out = torch.concat(ret_list, dim=-1)            \n",
    "            self.last_node = self.cur_node     \n",
    "            \n",
    "            self.root_max_q = root_max_q\n",
    "            self.ret_dict = {\"v0\": self.root_node.ret_dict[\"v\"].unsqueeze(0),\n",
    "                             \"q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_mean\"].unsqueeze(0),\n",
    "                             \"max_q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_max\"].unsqueeze(0),\n",
    "                             \"n_s_a\": self.root_node.ret_dict[\"child_rollout_ns\"].unsqueeze(0),\n",
    "                             \"logit0\": self.root_node.ret_dict[\"child_logits\"].unsqueeze(0),}\n",
    "            \n",
    "            if reset:\n",
    "                self.rollout_depth = 0\n",
    "                self.cur_node = self.root_node\n",
    "                self.cur_node.visit()\n",
    "                \n",
    "            return out\n",
    "        \n",
    "flags.stat_type = 1\n",
    "flags.rec_t = 10\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n         \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags)\n",
    "\n",
    "_ = env.reset()\n",
    "\n",
    "def debug_out(out, num_actions, rec_t):\n",
    "    struct = [[\"a0\", num_actions], \n",
    "        [\"r0\", 1], \n",
    "        [\"v0\", 1], \n",
    "        [\"pi0\", num_actions],\n",
    "        [\"a\", num_actions],\n",
    "        [\"reset\", 1],\n",
    "        [\"r\", 1],\n",
    "        [\"v\", 1],\n",
    "        [\"pi\",num_actions],\n",
    "        [\"first_a\", num_actions],\n",
    "        [\"rollout_v\", 1],\n",
    "        [\"ns\", num_actions],\n",
    "        [\"qs\", num_actions],\n",
    "        [\"t\", rec_t],\n",
    "        [\"depc\", 1]]\n",
    "    m = 0\n",
    "    for key, n in struct:\n",
    "        print(key, out[m:m+n])\n",
    "        m = m + n\n",
    "\n",
    "out, out_, r, done, info = env.step([1, 1, 0])\n",
    "out, out_, r, done, info = env.step([1, 2, 0])\n",
    "out, out_, r, done, info = env.step([1, 3, 1])\n",
    "out, out_, r, done, info = env.step([1, 1, 0])\n",
    "out, out_, r, done, info = env.step([1, 2, 1])\n",
    "out, out_, r, done, info = env.step([1, 1, 1])\n",
    "\n",
    "print(\"error:\", torch.sum((out - out_)**2))\n",
    "print(\"old\")\n",
    "debug_out(out, 5, 10)\n",
    "print(\"new\")\n",
    "debug_out(out_, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.perfect_model = True\n",
    "flags.stat_type = 2\n",
    "flags.reward_type = 1\n",
    "flags.rec_t = 5\n",
    "flags.reset_m = -1\n",
    "flags.tree_carry = True\n",
    "flags.tree_vb = 0.5\n",
    "flags.thres_carry = False\n",
    "flags.thres_discounting = 0.9\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n         \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags)\n",
    "env = Environment(env)\n",
    "obs = env.initial()\n",
    "plot_obs(env.gym_env.x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e543ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 3\n",
    "a_seqs = ([a,a,0],[a,a,0],[a,a,0],[a,a,0],[a,a,0])\n",
    "\n",
    "#a_seqs = ([2,4,0],[2,2,0],[2,2,0],[2,2,0],[2,3,0])\n",
    "\n",
    "for n, a in enumerate(a_seqs):\n",
    "    print(\"=========%d========\"%n)\n",
    "    a_tensor = torch.tensor([[a]]).long()\n",
    "    print(\"a_tensor\", a_tensor)\n",
    "    obs = env.step(a_tensor)\n",
    "    print(\"eps_step\", obs['episode_step'])\n",
    "    print(\"eps_return\", obs['episode_return'])\n",
    "    print(\"reward\", obs['reward'])\n",
    "    print(\"cur_t\", obs['cur_t'])\n",
    "    print(\"thres\", env.gym_env.thres)\n",
    "    print(\"root_node qs\", [x / 0.97 for x in env.gym_env.root_node.rollout_qs])\n",
    "    print(\"child_node qs\", env.gym_env.root_node.children[1].rollout_qs)\n",
    "    plot_obs(env.gym_env.x_/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2df76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gym_env.root_node.expanded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_seqs = [[0,0,0]]\n",
    "\n",
    "for n, a in enumerate(a_seqs):\n",
    "    print(\"=========%d========\"%n)\n",
    "    a_tensor = torch.tensor([[a]]).long()\n",
    "    print(\"a_tensor\", a_tensor)\n",
    "    obs = env.step(a_tensor)\n",
    "    print(\"eps_step\", obs['episode_step'])\n",
    "    print(\"eps_return\", obs['episode_return'])\n",
    "    print(\"reward\", obs['reward'])\n",
    "    print(\"cur_t\", obs['cur_t'])\n",
    "    plot_obs(env.gym_env.x_/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343eadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gym_env.root_node.encoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
