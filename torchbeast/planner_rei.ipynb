{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core import file_writer\n",
    "from torchbeast.core import prof\n",
    "from torchbeast.core import vtrace\n",
    "from torchbeast.core.environment import Environment, Vec_Environment\n",
    "from torchbeast.atari_wrappers import *\n",
    "from torchbeast.transformer_rnn import *\n",
    "from torchbeast.train import *\n",
    "from torchbeast.model import Model\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to original core funct\n",
    "\n",
    "def create_buffers(flags, obs_shape, num_actions) -> Buffers:\n",
    "    T = flags.unroll_length\n",
    "    specs = dict(\n",
    "        frame=dict(size=(T + 1, *obs_shape), dtype=torch.float32),\n",
    "        reward=dict(size=(T + 1,), dtype=torch.float32),\n",
    "        done=dict(size=(T + 1,), dtype=torch.bool),\n",
    "        truncated_done=dict(size=(T + 1,), dtype=torch.bool),\n",
    "        episode_return=dict(size=(T + 1,), dtype=torch.float32),\n",
    "        episode_step=dict(size=(T + 1,), dtype=torch.int32),\n",
    "        policy_logits=dict(size=(T + 1, num_actions), dtype=torch.float32),\n",
    "        im_policy_logits=dict(size=(T + 1, num_actions), dtype=torch.float32),\n",
    "        reset_policy_logits=dict(size=(T + 1, 2), dtype=torch.float32),\n",
    "        baseline=dict(size=(T + 1,), dtype=torch.float32),\n",
    "        last_action=dict(size=(T + 1, 3), dtype=torch.int64),\n",
    "        action=dict(size=(T + 1,), dtype=torch.int64),\n",
    "        im_action=dict(size=(T + 1,), dtype=torch.int64),\n",
    "        reset_action=dict(size=(T + 1,), dtype=torch.int64),\n",
    "        cur_t=dict(size=(T + 1,), dtype=torch.int64),\n",
    "        reg_loss=dict(size=(T + 1,), dtype=torch.float32),        \n",
    "    )\n",
    "    buffers: Buffers = {key: [] for key in specs}\n",
    "    for _ in range(flags.num_buffers):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "    return buffers  \n",
    "\n",
    "def act(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "    full_queue: mp.SimpleQueue,\n",
    "    actor_net: torch.nn.Module,\n",
    "    model: torch.nn.Module,\n",
    "    buffers: Buffers,\n",
    "    initial_agent_state_buffers,\n",
    "):\n",
    "    try:\n",
    "        logging.info(\"Actor %i started.\", actor_index)\n",
    "        timings = prof.Timings()  # Keep track of how fast things are.\n",
    "\n",
    "        gym_env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "                               model=model, rec_t=flags.rec_t)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()\n",
    "        agent_state = actor_net.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = actor_net(env_output, agent_state)\n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break\n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:                \n",
    "                if key in buffers: buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                if key in buffers: buffers[key][index][0, ...] = agent_output[key]                    \n",
    "            for i, tensor in enumerate(agent_state):\n",
    "                initial_agent_state_buffers[index][i][...] = tensor\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.unroll_length):\n",
    "                timings.reset()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = actor_net(env_output, agent_state)                    \n",
    "\n",
    "                timings.time(\"actor_net\")\n",
    "                \n",
    "                action = torch.cat([agent_output['action'], agent_output['im_action'], agent_output['reset_action']], dim=-1)\n",
    "                env_output = env.step(action.unsqueeze(0))\n",
    "\n",
    "                if flags.trun_bs:\n",
    "                    if env_output['truncated_done']: \n",
    "                        env_output['reward'] = env_output['reward'] + flags.discounting * agent_output['baseline']\n",
    "\n",
    "                timings.time(\"step\")\n",
    "\n",
    "                for key in env_output:\n",
    "                    if key in buffers:\n",
    "                        buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    if key in buffers:\n",
    "                        buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "\n",
    "                timings.time(\"write\")\n",
    "            full_queue.put(index)\n",
    "\n",
    "        if actor_index == 0:\n",
    "            logging.info(\"Actor %i: %s\", actor_index, timings.summary())\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        print()\n",
    "        raise e\n",
    "\n",
    "def compute_policy_gradient_loss(logits, im_logits, reset_logits,\n",
    "    actions, im_actions, reset_actions, cur_t, advantages):\n",
    "    re_cross_entropy = F.nll_loss(\n",
    "        F.log_softmax(torch.flatten(logits, 0, 1), dim=-1),\n",
    "        target=torch.flatten(actions, 0, 1),\n",
    "        reduction=\"none\",)\n",
    "    im_cross_entropy = F.nll_loss(\n",
    "        F.log_softmax(torch.flatten(im_logits, 0, 1), dim=-1),\n",
    "        target=torch.flatten(im_actions, 0, 1),\n",
    "        reduction=\"none\",) \n",
    "    reset_cross_entropy = F.nll_loss(\n",
    "        F.log_softmax(torch.flatten(reset_logits, 0, 1), dim=-1),\n",
    "        target=torch.flatten(reset_actions, 0, 1),\n",
    "        reduction=\"none\",)    \n",
    "    re_mask = (cur_t == 0).view_as(re_cross_entropy)\n",
    "    cross_entropy = torch.where(re_mask, re_cross_entropy, im_cross_entropy+reset_cross_entropy)\n",
    "    cross_entropy = cross_entropy.view_as(advantages)\n",
    "    return torch.sum(cross_entropy * advantages.detach())  \n",
    "\n",
    "def action_log_probs(policy_logits, actions):\n",
    "    return -F.nll_loss(\n",
    "        F.log_softmax(torch.flatten(policy_logits, 0, -2), dim=-1),\n",
    "        torch.flatten(actions),\n",
    "        reduction=\"none\",\n",
    "    ).view_as(actions) \n",
    "  \n",
    "def from_logits(\n",
    "    behavior_policy_logits,\n",
    "    behavior_im_policy_logits,\n",
    "    behavior_reset_policy_logits,\n",
    "    target_policy_logits,\n",
    "    target_im_policy_logits,\n",
    "    target_reset_policy_logits,            \n",
    "    actions,\n",
    "    im_actions,\n",
    "    reset_actions,\n",
    "    cur_t,     \n",
    "    discounts,\n",
    "    rewards,\n",
    "    values,\n",
    "    bootstrap_value,\n",
    "    clip_rho_threshold=1.0,\n",
    "    clip_pg_rho_threshold=1.0,\n",
    "    lamb=1.0,\n",
    "):\n",
    "    \"\"\"V-trace for softmax policies.\"\"\"\n",
    "    \n",
    "    target_action_log_probs = action_log_probs(target_policy_logits, actions)\n",
    "    target_im_reset_action_log_probs = (action_log_probs(target_im_policy_logits, im_actions) +\n",
    "        action_log_probs(target_reset_policy_logits, reset_actions))\n",
    "    \n",
    "    behavior_action_log_probs = action_log_probs(behavior_policy_logits, actions)\n",
    "    behavior_im_reset_action_log_probs = (action_log_probs(behavior_im_policy_logits, im_actions) +\n",
    "        action_log_probs(behavior_reset_policy_logits, reset_actions))    \n",
    "    \n",
    "    log_rhos = torch.where(cur_t==0, target_action_log_probs - behavior_action_log_probs,\n",
    "        target_im_reset_action_log_probs - behavior_im_reset_action_log_probs)    \n",
    "    \n",
    "    vtrace_returns = vtrace.from_importance_weights(\n",
    "        log_rhos=log_rhos,\n",
    "        discounts=discounts,\n",
    "        rewards=rewards,\n",
    "        values=values,\n",
    "        bootstrap_value=bootstrap_value,\n",
    "        clip_rho_threshold=clip_rho_threshold,\n",
    "        clip_pg_rho_threshold=clip_pg_rho_threshold,\n",
    "        lamb=lamb\n",
    "    )\n",
    "    return vtrace.VTraceFromLogitsReturns(\n",
    "        log_rhos=log_rhos,\n",
    "        behavior_action_log_probs=behavior_action_log_probs,\n",
    "        target_action_log_probs=target_action_log_probs,\n",
    "        **vtrace_returns._asdict(),\n",
    "    )  \n",
    "  \n",
    "def learn(\n",
    "    flags,\n",
    "    actor_model,\n",
    "    model,\n",
    "    batch,\n",
    "    initial_agent_state,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    lock=threading.Lock(),  # noqa: B008\n",
    "):\n",
    "    \"\"\"Performs a learning (optimization) step.\"\"\"\n",
    "    with lock:\n",
    "        learner_outputs, unused_state = model(batch, initial_agent_state)\n",
    "\n",
    "        # Take final value function slice for bootstrapping.\n",
    "        bootstrap_value = learner_outputs[\"baseline\"][-1]\n",
    "\n",
    "        # Move from obs[t] -> action[t] to action[t] -> obs[t].\n",
    "        batch = {key: tensor[1:] for key, tensor in batch.items()}\n",
    "        learner_outputs = {key: tensor[:-1] for key, tensor in learner_outputs.items()}\n",
    "\n",
    "        rewards = batch[\"reward\"]\n",
    "        if flags.reward_clipping > 0:\n",
    "            clipped_rewards = torch.clamp(rewards, -flags.reward_clipping, flags.reward_clipping)\n",
    "        else:\n",
    "            clipped_rewards = rewards\n",
    "\n",
    "        discounts = (~batch[\"done\"]).float() * flags.discounting\n",
    "\n",
    "        vtrace_returns = from_logits(\n",
    "            behavior_policy_logits=batch[\"policy_logits\"],\n",
    "            behavior_im_policy_logits=batch[\"im_policy_logits\"],\n",
    "            behavior_reset_policy_logits=batch[\"reset_policy_logits\"],\n",
    "            target_policy_logits=learner_outputs[\"policy_logits\"],\n",
    "            target_im_policy_logits=learner_outputs[\"im_policy_logits\"],\n",
    "            target_reset_policy_logits=learner_outputs[\"reset_policy_logits\"],            \n",
    "            actions=batch[\"action\"],\n",
    "            im_actions=batch[\"im_action\"],\n",
    "            reset_actions=batch[\"reset_action\"],\n",
    "            cur_t=batch[\"cur_t\"],          \n",
    "            discounts=discounts,\n",
    "            rewards=clipped_rewards,\n",
    "            values=learner_outputs[\"baseline\"],\n",
    "            bootstrap_value=bootstrap_value,\n",
    "            lamb=flags.lamb\n",
    "        )\n",
    "\n",
    "        pg_loss = compute_policy_gradient_loss(\n",
    "            learner_outputs[\"policy_logits\"],\n",
    "            learner_outputs[\"im_policy_logits\"],\n",
    "            learner_outputs[\"reset_policy_logits\"],                \n",
    "            batch[\"action\"],\n",
    "            batch[\"im_action\"],\n",
    "            batch[\"reset_action\"],\n",
    "            batch[\"cur_t\"],      \n",
    "            vtrace_returns.pg_advantages,\n",
    "        )\n",
    "        baseline_loss = flags.baseline_cost * compute_baseline_loss(\n",
    "            vtrace_returns.vs - learner_outputs[\"baseline\"]\n",
    "        )\n",
    "        re_entropy_loss = compute_entropy_loss(learner_outputs[\"policy_logits\"])\n",
    "        im_entropy_loss = compute_entropy_loss(learner_outputs[\"im_policy_logits\"])                                       \n",
    "        reset_entropy_loss = compute_entropy_loss(learner_outputs[\"reset_policy_logits\"]) \n",
    "        entropy_loss = flags.entropy_cost * (re_entropy_loss)\n",
    "        im_entropy_loss = flags.im_entropy_cost * (im_entropy_loss + reset_entropy_loss)\n",
    "        reg_loss = flags.reg_cost * torch.sum(learner_outputs[\"reg_loss\"])\n",
    "        total_loss = pg_loss + baseline_loss + entropy_loss + reg_loss\n",
    "\n",
    "        episode_returns = batch[\"episode_return\"][batch[\"done\"]]\n",
    "        stats = {\n",
    "            \"episode_returns\": tuple(episode_returns.cpu().numpy()),\n",
    "            \"mean_episode_return\": torch.mean(episode_returns).item(),\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"pg_loss\": pg_loss.item(),\n",
    "            \"baseline_loss\": baseline_loss.item(),\n",
    "            \"entropy_loss\": entropy_loss.item(),\n",
    "            \"im_entropy_loss\": im_entropy_loss.item(),\n",
    "            \"reg_loss\": reg_loss.item()\n",
    "        }\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        if flags.grad_norm_clipping > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), flags.grad_norm_clipping)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        actor_model.load_state_dict(model.state_dict())\n",
    "        return stats  \n",
    "\n",
    "# Wrap the environment with a model\n",
    "\n",
    "def _format_frame(frame, bsz=None):\n",
    "    #frame = torch.from_numpy(frame)\n",
    "    if bsz is not None:\n",
    "        return frame.view((1,) + frame.shape)\n",
    "    else:\n",
    "        return frame.view((1, 1) + frame.shape)\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, gym_env):\n",
    "        self.gym_env = gym_env\n",
    "        self.episode_return = None\n",
    "        self.episode_step = None\n",
    "\n",
    "    def initial(self):\n",
    "        initial_reward = torch.zeros(1, 1)\n",
    "        # This supports only single-tensor actions ATM.\n",
    "        initial_last_action = torch.zeros(1, 1, dtype=torch.int64)\n",
    "        self.episode_return = torch.zeros(1, 1)\n",
    "        self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n",
    "        initial_done = torch.ones(1, 1, dtype=torch.bool)\n",
    "        initial_frame = _format_frame(self.gym_env.reset())\n",
    "        return dict(\n",
    "            frame=initial_frame,\n",
    "            reward=initial_reward,\n",
    "            done=initial_done,\n",
    "            truncated_done=torch.tensor(0).view(1, 1).bool(),\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=torch.tensor(0).view(1, 1),\n",
    "            last_action=initial_last_action,\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        frame, reward, done, unused_info = self.gym_env.step(action[0,0].cpu().detach().numpy())     \n",
    "        self.episode_step += 1\n",
    "        self.episode_return += reward\n",
    "        episode_step = self.episode_step\n",
    "        episode_return = self.episode_return\n",
    "        if done:\n",
    "            frame = self.gym_env.reset()\n",
    "            self.episode_return = torch.zeros(1, 1)\n",
    "            self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n",
    "        frame = _format_frame(frame)\n",
    "        reward = torch.tensor(reward).view(1, 1)\n",
    "        done = torch.tensor(done).view(1, 1)\n",
    "        truncated_done = 'TimeLimit.truncated' in unused_info and unused_info['TimeLimit.truncated']\n",
    "        truncated_done = torch.tensor(truncated_done).view(1, 1)\n",
    "        cur_t = torch.tensor(unused_info[\"cur_t\"]).view(1, 1)\n",
    "        return dict(\n",
    "            frame=frame,\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            truncated_done=truncated_done,          \n",
    "            episode_return=episode_return,\n",
    "            episode_step=episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=action,\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        self.gym_env.close()\n",
    "\n",
    "    def clone_state(self):\n",
    "        state = [self.episode_return.clone(), self.episode_step.clone()]\n",
    "        state.append(self.gym_env.clone_state())\n",
    "        return state\n",
    "        \n",
    "    def restore_state(self, state):\n",
    "        self.episode_return = state[0].clone()\n",
    "        self.episode_step = state[1].clone()\n",
    "        self.gym_env.restore_state(state[2])\n",
    "\n",
    "class ModelWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, model, rec_t):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.env = env\n",
    "        self.model = model                \n",
    "        self.rec_t = rec_t\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "          low=-np.inf, high=np.inf,\n",
    "          shape=(5 + num_actions * 3 + self.rec_t, 1, 1), dtype=float)\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        x = self.env.reset()\n",
    "        self.cur_t = 0        \n",
    "        out = self.use_model(x, 0., 0, self.cur_t, 1.)        \n",
    "        return out.unsqueeze(-1).unsqueeze(-1)\n",
    "    \n",
    "    def step(self, action):  \n",
    "        re_action, im_action, reset = action\n",
    "        if self.cur_t < self.rec_t - 1:\n",
    "          self.cur_t += 1\n",
    "          out = self.use_model(None, None, im_action, self.cur_t, reset)          \n",
    "          r = 0.\n",
    "          done = False\n",
    "          info = {'cur_t': self.cur_t}  \n",
    "          self.encoded = reset * self.encoded_reset + (1 - reset) * self.encoded\n",
    "        else:\n",
    "          self.cur_t = 0\n",
    "          x, r, done, info = self.env.step(re_action)          \n",
    "          out = self.use_model(x, r, re_action, self.cur_t, 1.)          \n",
    "          info['cur_t'] = self.cur_t\n",
    "        return out.unsqueeze(-1).unsqueeze(-1), r, done, info\n",
    "        \n",
    "    def use_model(self, x, r, a, cur_t, reset):\n",
    "        # input: \n",
    "        # r: reward - [,]; x: frame - [C, H, W]; a: action - [,]\n",
    "        # cur_t: int; reset at cur_t == 0  \n",
    "        a = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)           \n",
    "        reset = torch.tensor([[reset]], dtype=torch.float32)\n",
    "        if cur_t == 0:\n",
    "          x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "          r = torch.tensor(r, dtype=torch.float32).unsqueeze(0)    \n",
    "          with torch.no_grad():\n",
    "              rs, vs, logits, encodeds = self.model(x, a.unsqueeze(0), one_hot=True)\n",
    "          self.encoded_reset = encodeds[0].clone()\n",
    "        else:\n",
    "          with torch.no_grad():\n",
    "              rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "                   a.unsqueeze(0), one_hot=True)\n",
    "          \n",
    "        self.encoded = encodeds[-1]        \n",
    "        r = (r if cur_t == 0 else rs[-1]).unsqueeze(-1)\n",
    "        v = vs[-1].unsqueeze(-1)\n",
    "        logit = logits[-1]    \n",
    "        \n",
    "        if cur_t == 0:\n",
    "          self.r0 = r.clone()\n",
    "          self.v0 = v.clone()\n",
    "          self.logit0 = logit.clone()        \n",
    "        time = F.one_hot(torch.tensor([cur_t], device=a.device).long(), self.rec_t)\n",
    "        out = torch.concat([reset, a, r, v, logit, self.r0, self.v0, self.logit0, time], dim=-1)   \n",
    "        return out[0]\n",
    "        \n",
    "class Actor_net(nn.Module):    \n",
    "    def __init__(self, obs_shape, num_actions, flags):\n",
    "\n",
    "        super(Actor_net, self).__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions  \n",
    "        \n",
    "        self.tran_t = flags.tran_t                   # number of recurrence of RNN\n",
    "        self.tran_mem_n = flags.tran_mem_n           # size of memory for the attn modules\n",
    "        self.tran_layer_n = flags.tran_layer_n       # number of layers\n",
    "        self.tran_lstm = flags.tran_lstm             # to use lstm or not\n",
    "        self.tran_lstm_no_attn = flags.tran_lstm_no_attn  # to use attention in lstm or not\n",
    "        self.tran_norm_first = flags.tran_norm_first # to use norm first in transformer (not on LSTM)\n",
    "        self.tran_ff_n = flags.tran_ff_n             # number of dim of ff in transformer (not on LSTM)        \n",
    "        self.tran_skip = flags.tran_skip             # whether to add skip connection\n",
    "        self.conv_out = flags.tran_dim               # size of transformer / LSTM embedding dim\n",
    "        \n",
    "        self.conv_out_hw = 1   \n",
    "        self.d_model = self.conv_out\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=self.obs_shape[0], out_channels=self.conv_out//2, kernel_size=1, stride=1)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv_out//2, out_channels=self.conv_out, kernel_size=1, stride=1)        \n",
    "        self.frame_conv = torch.nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU())\n",
    "        self.env_input_size = self.conv_out\n",
    "        d_in = self.env_input_size + self.d_model \n",
    "        \n",
    "        if self.tran_lstm:\n",
    "            self.core = ConvAttnLSTM(h=self.conv_out_hw, w=self.conv_out_hw,\n",
    "                                 input_dim=d_in-self.d_model, hidden_dim=self.d_model,\n",
    "                                 kernel_size=1, num_layers=self.tran_layer_n,\n",
    "                                 num_heads=8, mem_n=self.tran_mem_n, attn=not self.tran_lstm_no_attn)\n",
    "        else:            \n",
    "            self.core = ConvTransformerRNN(d_in=d_in,\n",
    "                                       h=self.conv_out_hw, w=self.conv_out_hw, d_model=self.d_model, \n",
    "                                       num_heads=8, dim_feedforward=self.tran_ff_n, \n",
    "                                       mem_n=self.tran_mem_n, norm_first=self.tran_norm_first,\n",
    "                                       num_layers=self.tran_layer_n, rpos=True, conv=False)   \n",
    "                         \n",
    "        \n",
    "        if self.tran_skip:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * (self.d_model + self.env_input_size)\n",
    "        else:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * self.d_model\n",
    "                \n",
    "        self.fc = nn.Linear(rnn_out_size, 256)        \n",
    "        \n",
    "        self.im_policy = nn.Linear(256, self.num_actions)        \n",
    "        self.policy = nn.Linear(256, self.num_actions)        \n",
    "        self.baseline = nn.Linear(256, 1)        \n",
    "        self.reset = nn.Linear(256, 2)        \n",
    "        \n",
    "        print(\"actor size: \", sum(p.numel() for p in self.parameters()))\n",
    "        #for k, v in self.named_parameters(): print(k, v.numel())   \n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        state = self.core.init_state(batch_size) + (torch.zeros(1, batch_size, \n",
    "               self.env_input_size, self.conv_out_hw, self.conv_out_hw),)\n",
    "        return state\n",
    "\n",
    "    def forward(self, obs, core_state=(), debug=False):\n",
    "        # one-step forward for the actor\n",
    "        # input / done shape x: T x B x C x 1 x 1 / B x C x 1 x 1\n",
    "        # only supports T = 1 at the moment; all output does not have T dim.        \n",
    "        \n",
    "        x = obs[\"frame\"]\n",
    "        done = obs[\"done\"]\n",
    "        \n",
    "        if len(x.shape) == 4: x = x.unsqueeze(0)\n",
    "        if len(done.shape) == 1: done = done.unsqueeze(0)  \n",
    "            \n",
    "        T, B, *_ = x.shape\n",
    "        x = torch.flatten(x, 0, 1)  # Merge time and batch.  \n",
    "        env_input = self.frame_conv(x)                \n",
    "        core_input = env_input.view(T, B, -1, self.conv_out_hw, self.conv_out_hw)\n",
    "        core_output_list = []\n",
    "        notdone = ~(done.bool())\n",
    "        \n",
    "        for n, (input, nd) in enumerate(zip(core_input.unbind(), notdone.unbind())):                \n",
    "            # Input shape: B, self.conv_out + self.num_actions + 1, H, W\n",
    "            for t in range(self.tran_t):                \n",
    "                if t > 0: nd = torch.ones(B).to(x.device).bool()                    \n",
    "                nd = nd.view(-1)      \n",
    "                output, core_state = self.core(input, core_state, nd, nd) # output shape: 1, B, core_output_size \n",
    "                \n",
    "            last_input = input   \n",
    "            core_output_list.append(output)\n",
    "                                   \n",
    "        core_output = torch.cat(core_output_list)  \n",
    "        if self.tran_skip: core_output = torch.concat([core_output, core_input], dim=-3)\n",
    "        core_output = torch.flatten(core_output, 0, 1)        \n",
    "        core_output = F.relu(self.fc(torch.flatten(core_output, start_dim=1)))   \n",
    "        \n",
    "        policy_logits = self.policy(core_output)\n",
    "        im_policy_logits = self.im_policy(core_output)\n",
    "        reset_policy_logits = self.reset(core_output)\n",
    "        \n",
    "        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        im_action = torch.multinomial(F.softmax(im_policy_logits, dim=1), num_samples=1)\n",
    "        reset_action = torch.multinomial(F.softmax(reset_policy_logits, dim=1), num_samples=1)\n",
    "                \n",
    "        baseline = self.baseline(core_output)\n",
    "                   \n",
    "        reg_loss = (1e-3 * torch.sum(policy_logits**2, dim=-1) / 2 + \n",
    "                    1e-3 * torch.sum(im_policy_logits**2, dim=-1) / 2 +\n",
    "                    1e-5 * torch.sum(core_output**2, dim=-1) / 2)\n",
    "        reg_loss = reg_loss.view(T, B)\n",
    "        \n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        im_policy_logits = im_policy_logits.view(T, B, self.num_actions)\n",
    "        reset_policy_logits = reset_policy_logits.view(T, B, 2)\n",
    "        \n",
    "        action = action.view(T, B)      \n",
    "        im_action = im_action.view(T, B)      \n",
    "        reset_action = reset_action.view(T, B)             \n",
    "        baseline = baseline.view(T, B)\n",
    "        \n",
    "        ret_dict = dict(policy_logits=policy_logits,                         \n",
    "                        im_policy_logits=im_policy_logits,                         \n",
    "                        reset_policy_logits=reset_policy_logits,     \n",
    "                        action=action,     \n",
    "                        im_action=im_action,\n",
    "                        reset_action=reset_action,\n",
    "                        baseline=baseline, \n",
    "                        reg_loss=reg_loss, )\n",
    "        \n",
    "        return (ret_dict, core_state)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_parser():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n",
    "\n",
    "    parser.add_argument(\"--env\", type=str, default=\"Sokoban-v0\",\n",
    "                        help=\"Gym environment.\")\n",
    "    parser.add_argument(\"--env_disable_noop\", action=\"store_true\",\n",
    "                        help=\"Disable noop in environment or not. (sokoban only)\")\n",
    "\n",
    "    parser.add_argument(\"--xpid\", default=None,\n",
    "                        help=\"Experiment id (default: None).\")\n",
    "\n",
    "    # Training settings.\n",
    "    parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n",
    "                        help=\"Disable saving checkpoint.\")\n",
    "    parser.add_argument(\"--savedir\", default=\"~/RS/thinker/logs/torchbeast\",\n",
    "                        help=\"Root dir where experiment data will be saved.\")\n",
    "    parser.add_argument(\"--num_actors\", default=48, type=int, metavar=\"N\",\n",
    "                        help=\"Number of actors (default: 48).\")\n",
    "    parser.add_argument(\"--total_steps\", default=100000000, type=int, metavar=\"T\",\n",
    "                        help=\"Total environment steps to train for.\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, metavar=\"B\",\n",
    "                        help=\"Learner batch size.\")\n",
    "    parser.add_argument(\"--unroll_length\", default=20, type=int, metavar=\"T\",\n",
    "                        help=\"The unroll length (time dimension).\")\n",
    "    parser.add_argument(\"--num_buffers\", default=None, type=int,\n",
    "                        metavar=\"N\", help=\"Number of shared-memory buffers.\")\n",
    "    parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=1, type=int,\n",
    "                        metavar=\"N\", help=\"Number learner threads.\")\n",
    "    parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n",
    "                        help=\"Disable CUDA.\")\n",
    "\n",
    "    # Architecture settings\n",
    "    parser.add_argument(\"--tran_dim\", default=64, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer hidden dim.\")\n",
    "    parser.add_argument(\"--tran_mem_n\", default=16, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer memory.\")\n",
    "    parser.add_argument(\"--tran_layer_n\", default=3, type=int, metavar=\"N\",\n",
    "                        help=\"Number of transformer layer.\")\n",
    "    parser.add_argument(\"--tran_t\", default=1, type=int, metavar=\"T\",\n",
    "                        help=\"Number of recurrent step for transformer.\")\n",
    "    parser.add_argument(\"--tran_ff_n\", default=256, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer ff .\")\n",
    "    parser.add_argument(\"--tran_skip\", action=\"store_true\",\n",
    "                        help=\"Whether to enable skip conn.\")\n",
    "    parser.add_argument(\"--tran_norm_first\", action=\"store_true\",\n",
    "                        help=\"Whether to use norm first in transformer.\")\n",
    "    parser.add_argument(\"--tran_rpos\", action=\"store_true\",\n",
    "                        help=\"Whether to use relative position in transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm\", action=\"store_true\",\n",
    "                        help=\"Whether to use LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm_no_attn\", action=\"store_true\",\n",
    "                        help=\"Whether to disable attention in LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_erasep\", action=\"store_true\",\n",
    "                        help=\"Whether to erase past memories if not planning.\")\n",
    "\n",
    "    parser.add_argument(\"--rec_t\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Number of planning steps.\")\n",
    "\n",
    "    # Loss settings.\n",
    "    parser.add_argument(\"--entropy_cost\", default=0.01,\n",
    "                        type=float, help=\"Entropy cost/multiplier.\")\n",
    "    parser.add_argument(\"--im_entropy_cost\", default=0.01,\n",
    "                        type=float, help=\"Imagainary Entropy cost/multiplier.\")    \n",
    "    parser.add_argument(\"--baseline_cost\", default=0.5,\n",
    "                        type=float, help=\"Baseline cost/multiplier.\")\n",
    "    parser.add_argument(\"--reg_cost\", default=1,\n",
    "                        type=float, help=\"Reg cost/multiplier.\")\n",
    "    parser.add_argument(\"--discounting\", default=0.97,\n",
    "                        type=float, help=\"Discounting factor.\")\n",
    "    parser.add_argument(\"--lamb\", default=0.97,\n",
    "                        type=float, help=\"Lambda when computing trace.\")\n",
    "    parser.add_argument(\"--reward_clipping\", default=10, type=int, \n",
    "                        metavar=\"N\", help=\"Reward clipping.\")\n",
    "    parser.add_argument(\"--trun_bs\", action=\"store_true\",\n",
    "                        help=\"Whether to add baseline as reward when truncated.\")\n",
    "\n",
    "    # Optimizer settings.\n",
    "    parser.add_argument(\"--learning_rate\", default=0.0004,\n",
    "                        type=float, metavar=\"LR\", help=\"Learning rate.\")\n",
    "    parser.add_argument(\"--disable_adam\", action=\"store_true\",\n",
    "                        help=\"Use Aadm optimizer or not.\")\n",
    "    parser.add_argument(\"--alpha\", default=0.99, type=float,\n",
    "                        help=\"RMSProp smoothing constant.\")\n",
    "    parser.add_argument(\"--momentum\", default=0, type=float,\n",
    "                        help=\"RMSProp momentum.\")\n",
    "    parser.add_argument(\"--epsilon\", default=0.01, type=float,\n",
    "                        help=\"RMSProp epsilon.\")\n",
    "    parser.add_argument(\"--grad_norm_clipping\", default=0.0, type=float,\n",
    "                        help=\"Global gradient norm clip.\")\n",
    "    # yapf: enable\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.num_actors = 1\n",
    "flags.batch_size = 32\n",
    "flags.unroll_length = 20\n",
    "flags.learning_rate = 0.0002\n",
    "flags.entropy_cost = 0.001\n",
    "flags.im_entropy_cost = 0.00001\n",
    "flags.reg_cost = 0.1\n",
    "flags.discounting = 0.99\n",
    "flags.lamb = 1.\n",
    "\n",
    "flags.trun_bs = False\n",
    "flags.total_steps = 500000000\n",
    "flags.disable_adam = False\n",
    "\n",
    "flags.tran_t = 1\n",
    "flags.tran_mem_n = 16\n",
    "flags.tran_layer_n = 3\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = True\n",
    "flags.tran_norm_first = False\n",
    "flags.tran_ff_n = 256\n",
    "flags.tran_skip = True\n",
    "flags.tran_erasep = False\n",
    "flags.tran_dim = 64\n",
    "flags.tran_rpos = True\n",
    "flags.rec_t = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499a960",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "\n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "                               model=model, rec_t=flags.rec_t))\n",
    "\n",
    "obs_shape = env.gym_env.observation_space.shape\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "\n",
    "mp.set_sharing_strategy('file_system')\n",
    "\n",
    "if flags.xpid is None:\n",
    "    flags.xpid = \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "plogger = file_writer.FileWriter(\n",
    "    xpid=flags.xpid, xp_args=flags.__dict__, rootdir=flags.savedir\n",
    ")\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available():\n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser(\"%s/%s/%s\" % (flags.savedir, flags.xpid, \"model.tar\"))\n",
    ")\n",
    "\n",
    "if flags.num_buffers is None:  # Set sensible default for num_buffers.\n",
    "    flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n",
    "if flags.num_actors >= flags.num_buffers:\n",
    "    raise ValueError(\"num_buffers should be larger than num_actors\")\n",
    "if flags.num_buffers < flags.batch_size:\n",
    "    raise ValueError(\"num_buffers should be larger than batch_size\")\n",
    "\n",
    "T = flags.unroll_length\n",
    "B = flags.batch_size\n",
    "\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags)\n",
    "buffers = create_buffers(flags, obs_shape, num_actions)\n",
    "\n",
    "actor_net.share_memory()\n",
    "\n",
    "# Add initial RNN state.\n",
    "initial_agent_state_buffers = []\n",
    "for _ in range(flags.num_buffers):\n",
    "    state = actor_net.initial_state(batch_size=1)\n",
    "    for t in state:\n",
    "        t.share_memory_()\n",
    "    initial_agent_state_buffers.append(state)\n",
    "\n",
    "actor_processes = []\n",
    "ctx = mp.get_context()\n",
    "free_queue = ctx.SimpleQueue()\n",
    "full_queue = ctx.SimpleQueue()\n",
    "\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=act, args=(flags, i, free_queue, full_queue,\n",
    "            actor_net, model, buffers, initial_agent_state_buffers,),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)\n",
    "\n",
    "learner_net = Actor_net(obs_shape, num_actions, flags).to(device=flags.device)\n",
    "\n",
    "if not flags.disable_adam:\n",
    "    print(\"Using Adam...\")        \n",
    "    optimizer = torch.optim.Adam(learner_net.parameters(),lr=flags.learning_rate)\n",
    "else:\n",
    "    print(\"Using RMS Prop...\")\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        learner_net.actor.parameters(),\n",
    "        lr=flags.learning_rate,\n",
    "        momentum=flags.momentum,\n",
    "        eps=flags.epsilon,\n",
    "        alpha=flags.alpha,)\n",
    "print(\"All parameters: \")\n",
    "for k, v in learner_net.named_parameters(): print(k, v.numel())    \n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "logger = logging.getLogger(\"logfile\")\n",
    "stat_keys = [\"mean_episode_return\", \"episode_returns\", \"total_loss\",\n",
    "    \"pg_loss\", \"baseline_loss\", \"entropy_loss\", \"im_entropy_loss\"]\n",
    "logger.info(\"# Step\\t%s\", \"\\t\".join(stat_keys))\n",
    "\n",
    "step, stats, last_returns, tot_eps = 0, {}, deque(maxlen=400), 0\n",
    "\n",
    "def batch_and_learn(i, lock=threading.Lock()):\n",
    "    \"\"\"Thread target for the learning process.\"\"\"\n",
    "    #nonlocal step, stats, last_returns, tot_eps\n",
    "    global step, stats, last_returns, tot_eps\n",
    "    timings = prof.Timings()\n",
    "    while step < flags.total_steps:\n",
    "        timings.reset()\n",
    "        batch, agent_state = get_batch(flags, free_queue, full_queue, buffers,\n",
    "            initial_agent_state_buffers, timings,)\n",
    "        stats = learn(flags, actor_net, learner_net, batch, agent_state, optimizer, \n",
    "            scheduler)\n",
    "        last_returns.extend(stats[\"episode_returns\"])\n",
    "        tot_eps = tot_eps + len(stats[\"episode_returns\"])\n",
    "        timings.time(\"learn\")\n",
    "        with lock:\n",
    "            to_log = dict(step=step)\n",
    "            to_log.update({k: stats[k] for k in stat_keys})\n",
    "            to_log.update({\"trail_mean_episode_return\": np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "                           \"episode\": tot_eps})\n",
    "            plogger.log(to_log)\n",
    "            step += T * B\n",
    "\n",
    "    if i == 0:\n",
    "        logging.info(\"Batch and learn: %s\", timings.summary())\n",
    "\n",
    "for m in range(flags.num_buffers):\n",
    "    free_queue.put(m)\n",
    "\n",
    "threads = []\n",
    "for i in range(flags.num_learner_threads):\n",
    "    thread = threading.Thread(\n",
    "        target=batch_and_learn, name=\"batch-and-learn-%d\" % i, args=(i,)\n",
    "    )\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "def checkpoint():\n",
    "    if flags.disable_checkpoint:\n",
    "        return\n",
    "    logging.info(\"Saving checkpoint to %s\", checkpointpath)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": actor_net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"flags\": vars(flags),\n",
    "        },\n",
    "        checkpointpath,\n",
    "    )\n",
    "\n",
    "timer = timeit.default_timer\n",
    "try:\n",
    "    last_checkpoint_time = timer()\n",
    "    while step < flags.total_steps:\n",
    "        start_step = step\n",
    "        start_time = timer()\n",
    "        time.sleep(5)\n",
    "\n",
    "        if timer() - last_checkpoint_time > 10 * 60:  # Save every 10 min.\n",
    "            checkpoint()\n",
    "            last_checkpoint_time = timer()\n",
    "\n",
    "        sps = (step - start_step) / (timer() - start_time)\n",
    "        if stats.get(\"episode_returns\", None):\n",
    "            mean_return = (\n",
    "                \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n",
    "            )\n",
    "        else:\n",
    "            mean_return = \"\"\n",
    "        total_loss = stats.get(\"total_loss\", float(\"inf\"))\n",
    "\n",
    "        print_str =  \"Steps %i @ %.1f SPS. Eps %i. L400 Return %f. Loss %f\" % (step, sps, tot_eps, \n",
    "            np.average(last_returns) if len(last_returns) > 0 else 0., total_loss)\n",
    "\n",
    "        for s in [\"pg_loss\", \"baseline_loss\", \"entropy_loss\", \"im_entropy_loss\", \"reg_loss\"]:\n",
    "            if s in stats:\n",
    "                print_str += \" %s %f\" % (s, stats[s])\n",
    "\n",
    "        logging.info(print_str)\n",
    "except KeyboardInterrupt:\n",
    "    for thread in threads:\n",
    "        thread.join()        \n",
    "    # Try joining actors then quit.\n",
    "else:\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    logging.info(\"Learning finished after %d steps.\", step)\n",
    "finally:\n",
    "    for _ in range(flags.num_actors):\n",
    "        free_queue.put(None)\n",
    "    for actor in actor_processes:\n",
    "        actor.join(timeout=1)\n",
    "\n",
    "checkpoint()\n",
    "plogger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a726743",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = flags.total_steps + 1\n",
    "for thread in threads:\n",
    "     thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(flags.num_actors):\n",
    "    free_queue.put(None)\n",
    "for actor in actor_processes:\n",
    "    actor.join(timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3532f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect trained model\n",
    "bsz = 4\n",
    "eps_n = 100\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "env = gym.vector.SyncVectorEnv([lambda: ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True),\n",
    "                                model=model, rec_t=flags.rec_t)] * bsz)\n",
    "env = Vec_Environment(env, bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_shape = env.gym_env.observation_space.shape[1:]\n",
    "\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags).to(flags.device)\n",
    "checkpoint = torch.load(\"../tmp/latest/model.tar\")\n",
    "actor_net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "core_state = actor_net.initial_state(bsz)\n",
    "core_state = tuple(v.to(flags.device) for v in core_state)\n",
    "\n",
    "returns = []\n",
    "obs = env.initial()\n",
    "\n",
    "#while(len(returns) <= eps_n):\n",
    "for t in range(5):\n",
    "    #print(\"t: %d\" %t)    \n",
    "    with torch.no_grad():\n",
    "        cur_returns = obs['episode_return']            \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}    \n",
    "        obs[\"frame\"] = obs[\"frame\"].float()\n",
    "        actor_out, core_state = actor_net(obs, core_state)\n",
    "        action = torch.concat([actor_out[\"action\"], actor_out[\"im_action\"], actor_out[\"reset_action\"]], dim=0).T\n",
    "        obs = env.step(action)\n",
    "        print(\"===============cur_t:%d===============\" % (t%5))\n",
    "        print(\"p_sel: \\n\", F.softmax(actor_out[\"policy_logits\"], dim=-1).cpu().detach().numpy())\n",
    "        print(\"action: \\n\", actor_out[\"action\"].cpu().detach().numpy())\n",
    "        print(\"im_p_sel: \\n\", F.softmax(actor_out[\"im_policy_logits\"], dim=-1).cpu().detach().numpy())\n",
    "        print(\"im_action: \\n\", actor_out[\"im_action\"].cpu().detach().numpy())\n",
    "        print(\"reset_p_sel: \\n\", F.softmax(actor_out[\"reset_policy_logits\"], dim=-1).cpu().detach().numpy())\n",
    "        print(\"reset_action: \\n\", actor_out[\"reset_action\"].cpu().detach().numpy())\n",
    "        \n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "\n",
    "returns = returns[:eps_n]            \n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "      np.average(returns), np.std(returns) / np.sqrt(len(returns))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b2c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = True\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "\n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "                               model=model, rec_t=flags.rec_t))\n",
    "obs = env.initial()\n",
    "\n",
    "obs_shape, num_actions = env.gym_env.observation_space.shape, env.gym_env.action_space.n   \n",
    "num_actions = env.gym_env.action_space.n\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags)\n",
    "core_state = actor_net.initial_state(1)\n",
    "\n",
    "for t in range(10):\n",
    "  actor_out, core_state = actor_net(obs, core_state=core_state)\n",
    "  action = torch.cat([actor_out['action'], actor_out['im_action'], actor_out['reset_action']], dim=-1)\n",
    "  obs = env.step(action.unsqueeze(0))\n",
    "  print(t, obs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
