{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5b1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core import file_writer\n",
    "from torchbeast.core import prof\n",
    "from torchbeast.core import vtrace\n",
    "from torchbeast.atari_wrappers import *\n",
    "from torchbeast.transformer_rnn import *\n",
    "from torchbeast.train import *\n",
    "from torchbeast.model import Model\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d1c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to original core funct\n",
    "\n",
    "def create_buffers(flags, obs_shape, num_actions, num_rewards) -> Buffers:\n",
    "    T = flags.unroll_length\n",
    "    specs = dict(\n",
    "        frame=dict(size=(T + 1, *obs_shape), dtype=torch.float32),\n",
    "        reward=dict(size=(T + 1, num_rewards), dtype=torch.float32),\n",
    "        done=dict(size=(T + 1,), dtype=torch.bool),\n",
    "        truncated_done=dict(size=(T + 1,), dtype=torch.bool),\n",
    "        episode_return=dict(size=(T + 1, num_rewards), dtype=torch.float32),\n",
    "        episode_step=dict(size=(T + 1,), dtype=torch.int32),\n",
    "        policy_logits=dict(size=(T + 1, num_actions), dtype=torch.float32),\n",
    "        im_policy_logits=dict(size=(T + 1, num_actions), dtype=torch.float32),\n",
    "        reset_policy_logits=dict(size=(T + 1, 2), dtype=torch.float32),\n",
    "        baseline=dict(size=(T + 1, num_rewards), dtype=torch.float32),\n",
    "        last_action=dict(size=(T + 1, 3), dtype=torch.int64),\n",
    "        action=dict(size=(T + 1,), dtype=torch.int64),\n",
    "        im_action=dict(size=(T + 1,), dtype=torch.int64),\n",
    "        reset_action=dict(size=(T + 1,), dtype=torch.int64),        \n",
    "        reg_loss=dict(size=(T + 1,), dtype=torch.float32),  \n",
    "        cur_t=dict(size=(T + 1,), dtype=torch.int64),             \n",
    "        max_rollout_depth=dict(size=(T + 1,), dtype=torch.float32),  \n",
    "    )\n",
    "    buffers: Buffers = {key: [] for key in specs}\n",
    "    for _ in range(flags.num_buffers):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "    return buffers  \n",
    "\n",
    "def act(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "    full_queue: mp.SimpleQueue,\n",
    "    actor_net: torch.nn.Module,\n",
    "    model: torch.nn.Module,\n",
    "    buffers: Buffers,\n",
    "    initial_agent_state_buffers,\n",
    "):\n",
    "    try:\n",
    "        logging.info(\"Actor %i started.\", actor_index)\n",
    "        timings = prof.Timings()  # Keep track of how fast things are.\n",
    "\n",
    "        gym_env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "                               model=model, flags=flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()\n",
    "        agent_state = actor_net.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = actor_net(env_output, agent_state)\n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break\n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:           \n",
    "                if key in buffers: buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                if key in buffers: buffers[key][index][0, ...] = agent_output[key]                    \n",
    "            for i, tensor in enumerate(agent_state):\n",
    "                initial_agent_state_buffers[index][i][...] = tensor\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.unroll_length):\n",
    "                timings.reset()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = actor_net(env_output, agent_state)                    \n",
    "\n",
    "                timings.time(\"actor_net\")\n",
    "                \n",
    "                action = torch.cat([agent_output['action'], agent_output['im_action'], agent_output['reset_action']], dim=-1)\n",
    "                env_output = env.step(action.unsqueeze(0))\n",
    "\n",
    "                if flags.trun_bs:\n",
    "                    if env_output['truncated_done']: \n",
    "                        env_output['reward'] = env_output['reward'] + flags.im_discounting * agent_output['baseline']\n",
    "\n",
    "                timings.time(\"step\")\n",
    "\n",
    "                for key in env_output:\n",
    "                    if key in buffers:\n",
    "                        #print(key, env_output[key].shape, env_output[key])\n",
    "                        buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    if key in buffers:\n",
    "                        buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "\n",
    "                timings.time(\"write\")\n",
    "            full_queue.put(index)\n",
    "\n",
    "        if actor_index == 0:\n",
    "            logging.info(\"Actor %i: %s\", actor_index, timings.summary())\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        print()\n",
    "        raise e\n",
    "\n",
    "def compute_policy_gradient_loss(logits_ls, actions_ls, advantages_ls, masks_ls):\n",
    "    loss = 0.\n",
    "    for logits, actions, advantages, masks in zip(logits_ls, actions_ls, advantages_ls, masks_ls):\n",
    "        cross_entropy = F.nll_loss(\n",
    "            F.log_softmax(torch.flatten(logits, 0, 1), dim=-1),\n",
    "            target=torch.flatten(actions, 0, 1),\n",
    "            reduction=\"none\",)\n",
    "        cross_entropy = cross_entropy.view_as(advantages)\n",
    "        adv_cross_entropy = cross_entropy * advantages.detach()\n",
    "        loss = loss + torch.sum(adv_cross_entropy * (1-masks))\n",
    "    return loss  \n",
    "\n",
    "def compute_entropy_loss(logits_ls, masks_ls, c_ls):\n",
    "    \"\"\"Return the entropy loss, i.e., the negative entropy of the policy.\"\"\"\n",
    "    loss = 0.\n",
    "    for logits, masks, c in zip(logits_ls, masks_ls, c_ls):\n",
    "        policy = F.softmax(logits, dim=-1)\n",
    "        log_policy = F.log_softmax(logits, dim=-1)\n",
    "        ent = torch.sum(policy * log_policy, dim=-1) #* (1-masks)\n",
    "        loss = loss + torch.sum(ent) * c \n",
    "    return loss\n",
    "\n",
    "def action_log_probs(policy_logits, actions):\n",
    "    return -F.nll_loss(\n",
    "        F.log_softmax(torch.flatten(policy_logits, 0, -2), dim=-1),\n",
    "        torch.flatten(actions),\n",
    "        reduction=\"none\",\n",
    "    ).view_as(actions) \n",
    "  \n",
    "def from_logits(\n",
    "    behavior_logits_ls, target_logits_ls, actions_ls, masks_ls,\n",
    "    discounts, rewards, values, bootstrap_value, clip_rho_threshold=1.0,\n",
    "    clip_pg_rho_threshold=1.0, lamb=1.0,):\n",
    "    \"\"\"V-trace for softmax policies.\"\"\"\n",
    "    \n",
    "    log_rhos = 0.\n",
    "    \n",
    "    for behavior_logits, target_logits, actions, masks in zip(behavior_logits_ls, \n",
    "             target_logits_ls, actions_ls, masks_ls):\n",
    "        behavior_log_probs = action_log_probs(behavior_logits, actions)        \n",
    "        target_log_probs = action_log_probs(target_logits, actions)\n",
    "        log_rho = target_log_probs - behavior_log_probs\n",
    "        log_rhos = log_rhos + log_rho * (1-masks)\n",
    "    \n",
    "    vtrace_returns = vtrace.from_importance_weights(\n",
    "        log_rhos=log_rhos,\n",
    "        discounts=discounts,\n",
    "        rewards=rewards,\n",
    "        values=values,\n",
    "        bootstrap_value=bootstrap_value,\n",
    "        clip_rho_threshold=clip_rho_threshold,\n",
    "        clip_pg_rho_threshold=clip_pg_rho_threshold,\n",
    "        lamb=lamb\n",
    "    )\n",
    "    return vtrace.VTraceFromLogitsReturns(\n",
    "        log_rhos=log_rhos,\n",
    "        behavior_action_log_probs=None,\n",
    "        target_action_log_probs=None,\n",
    "        **vtrace_returns._asdict(),\n",
    "    )  \n",
    "  \n",
    "def learn(\n",
    "    flags,\n",
    "    actor_model,\n",
    "    model,\n",
    "    batch,\n",
    "    initial_agent_state,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    lock=threading.Lock(),  # noqa: B008\n",
    "):\n",
    "    \"\"\"Performs a learning (optimization) step.\"\"\"\n",
    "    with lock:        \n",
    "        learner_outputs, unused_state = model(batch, initial_agent_state)\n",
    "        #learner_outputs[\"im_policy_logits\"].register_hook(lambda grad: grad / (flags.rec_t - 1))\n",
    "        #learner_outputs[\"reset_policy_logits\"].register_hook(lambda grad: grad / (flags.rec_t - 1))\n",
    "        #learner_outputs[\"baseline\"].register_hook(lambda grad: grad / (flags.rec_t - 1))\n",
    "        \n",
    "        # Take final value function slice for bootstrapping.\n",
    "        bootstrap_value = learner_outputs[\"baseline\"][-1]        \n",
    "\n",
    "        # Move from obs[t] -> action[t] to action[t] -> obs[t].\n",
    "        batch = {key: tensor[1:] for key, tensor in batch.items()}\n",
    "        learner_outputs = {key: tensor[:-1] for key, tensor in learner_outputs.items()}\n",
    "\n",
    "        rewards = batch[\"reward\"]\n",
    "        if flags.reward_clipping > 0:\n",
    "            clipped_rewards = torch.clamp(rewards, -flags.reward_clipping, flags.reward_clipping)\n",
    "        else:\n",
    "            clipped_rewards = rewards\n",
    "        \n",
    "        # compute advantage w.r.t real rewards\n",
    "        \n",
    "        discounts = (~batch[\"done\"]).float() * flags.im_discounting        \n",
    "        behavior_logits_ls = [batch[\"policy_logits\"], batch[\"im_policy_logits\"], batch[\"reset_policy_logits\"]]\n",
    "        target_logits_ls = [learner_outputs[\"policy_logits\"], learner_outputs[\"im_policy_logits\"], learner_outputs[\"reset_policy_logits\"]]\n",
    "        actions_ls = [batch[\"action\"], batch[\"im_action\"], batch[\"reset_action\"]]        \n",
    "        im_mask = (batch[\"cur_t\"] == 0).float()\n",
    "        real_mask = 1 - im_mask\n",
    "        masks_ls = [real_mask, im_mask, im_mask]        \n",
    "\n",
    "        vtrace_returns = from_logits(\n",
    "            behavior_logits_ls, target_logits_ls, actions_ls, masks_ls,\n",
    "            discounts=discounts,\n",
    "            rewards=clipped_rewards[:, :, 0],\n",
    "            values=learner_outputs[\"baseline\"][:, :, 0],\n",
    "            bootstrap_value=bootstrap_value[:, 0],\n",
    "            lamb=flags.lamb\n",
    "        )\n",
    "        \n",
    "        advantages_ls = [vtrace_returns.pg_advantages, vtrace_returns.pg_advantages, vtrace_returns.pg_advantages]\n",
    "        pg_loss = compute_policy_gradient_loss(target_logits_ls, actions_ls, advantages_ls, masks_ls)         \n",
    "        baseline_loss = flags.baseline_cost * compute_baseline_loss(\n",
    "            vtrace_returns.vs - learner_outputs[\"baseline\"][:, :, 0])        \n",
    "       \n",
    "        # compute advantage w.r.t imagainary rewards\n",
    "        \n",
    "        cs_ls = [flags.entropy_cost, flags.im_entropy_cost, flags.reset_entropy_cost]\n",
    "        entropy_loss = compute_entropy_loss(target_logits_ls, masks_ls, cs_ls)        \n",
    "\n",
    "        if flags.reward_type == 1:\n",
    "            discounts = (~(batch[\"cur_t\"] == 0)).float() * flags.im_discounting        \n",
    "            behavior_logits_ls = [batch[\"im_policy_logits\"], batch[\"reset_policy_logits\"]]\n",
    "            target_logits_ls = [learner_outputs[\"im_policy_logits\"], learner_outputs[\"reset_policy_logits\"]]\n",
    "            actions_ls = [batch[\"im_action\"], batch[\"reset_action\"]] \n",
    "            im_mask = (batch[\"cur_t\"] == 0).float()\n",
    "            masks_ls = [im_mask, im_mask]  \n",
    "            \n",
    "            vtrace_returns = from_logits(\n",
    "                behavior_logits_ls, target_logits_ls, actions_ls, masks_ls,\n",
    "                discounts=discounts,\n",
    "                rewards=clipped_rewards[:, :, 1],\n",
    "                values=learner_outputs[\"baseline\"][:, :, 1],\n",
    "                bootstrap_value=bootstrap_value[:, 1],\n",
    "                lamb=flags.lamb\n",
    "            )\n",
    "            advantages_ls = [vtrace_returns.pg_advantages, vtrace_returns.pg_advantages]\n",
    "            im_pg_loss = compute_policy_gradient_loss(target_logits_ls, actions_ls, advantages_ls, masks_ls)   \n",
    "            im_baseline_loss = flags.baseline_cost * compute_baseline_loss(\n",
    "                vtrace_returns.vs - learner_outputs[\"baseline\"][:, :, 1])     \n",
    "\n",
    "        reg_loss = flags.reg_cost * torch.sum(learner_outputs[\"reg_loss\"])\n",
    "        total_loss = pg_loss + baseline_loss + entropy_loss + reg_loss\n",
    "        \n",
    "        if flags.reward_type == 1:\n",
    "            total_loss = total_loss + flags.im_cost * (im_pg_loss + im_baseline_loss)\n",
    "        \n",
    "        episode_returns = batch[\"episode_return\"][batch[\"done\"]][:, 0]  \n",
    "        max_rollout_depth = (batch[\"max_rollout_depth\"][batch[\"cur_t\"] == 0]).detach().cpu().numpy()\n",
    "        max_rollout_depth = np.average(max_rollout_depth) if len (max_rollout_depth) > 0 else 0.        \n",
    "        stats = {\n",
    "            \"episode_returns\": tuple(episode_returns.detach().cpu().numpy()),\n",
    "            \"mean_episode_return\": torch.mean(episode_returns).item(),\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"pg_loss\": pg_loss.item(),\n",
    "            \"baseline_loss\": baseline_loss.item(),\n",
    "            \"entropy_loss\": entropy_loss.item(),\n",
    "            \"reg_loss\": reg_loss.item(),\n",
    "            \"max_rollout_depth\": max_rollout_depth\n",
    "        }\n",
    "        \n",
    "        if flags.reward_type == 1:            \n",
    "            im_episode_returns = batch[\"episode_return\"][batch[\"cur_t\"] == 0][:, 1]\n",
    "            stats[\"im_episode_returns\"] = tuple(im_episode_returns.detach().cpu().numpy())\n",
    "            stats[\"im_pg_loss\"] = im_pg_loss.item()\n",
    "            stats[\"im_baseline_loss\"] = im_baseline_loss.item()   \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimize_params = optimizer.param_groups[0]['params']\n",
    "        if flags.grad_norm_clipping > 0:\n",
    "            total_norm = nn.utils.clip_grad_norm_(optimize_params, flags.grad_norm_clipping)\n",
    "        else:\n",
    "            total_norm = 0.\n",
    "            parameters = [p for p in optimize_params if p.grad is not None and p.requires_grad]\n",
    "            for p in parameters:\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "        stats[\"total_norm\"] = total_norm\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        actor_model.load_state_dict(model.state_dict())\n",
    "        return stats  \n",
    "\n",
    "# Wrap the environment with a model\n",
    "\n",
    "def _format_frame(frame, bsz=None):\n",
    "    if type(frame) == np.ndarray:\n",
    "        frame = torch.from_numpy(frame).float()\n",
    "    if bsz is not None:\n",
    "        return frame.view((1,) + frame.shape)\n",
    "    else:\n",
    "        return frame.view((1, 1) + frame.shape)\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, gym_env):\n",
    "        self.gym_env = gym_env\n",
    "        self.episode_return = None\n",
    "        self.episode_step = None\n",
    "\n",
    "    def initial(self):\n",
    "        initial_reward = torch.zeros(1, 1)\n",
    "        # This supports only single-tensor actions ATM.\n",
    "        initial_last_action = torch.zeros(1, 1, dtype=torch.int64)\n",
    "        self.episode_return = torch.zeros(1, 1, 1)\n",
    "        self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n",
    "        initial_done = torch.ones(1, 1, dtype=torch.bool)\n",
    "        initial_frame = _format_frame(self.gym_env.reset())\n",
    "        return dict(\n",
    "            frame=initial_frame,\n",
    "            reward=initial_reward,\n",
    "            done=initial_done,\n",
    "            truncated_done=torch.tensor(0).view(1, 1).bool(),\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=torch.tensor(0).view(1, 1),\n",
    "            last_action=initial_last_action,\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        frame, reward, done, unused_info = self.gym_env.step(action[0,0].cpu().detach().numpy())     \n",
    "        self.episode_step += 1\n",
    "        self.episode_return = self.episode_return + torch.tensor(reward).unsqueeze(0).unsqueeze(0)\n",
    "        episode_step = self.episode_step\n",
    "        episode_return = self.episode_return.clone()\n",
    "        if done:\n",
    "            frame = self.gym_env.reset()\n",
    "            self.episode_return = torch.zeros(1, 1, 1)\n",
    "            self.episode_step = torch.zeros(1, 1, dtype=torch.int32)        \n",
    "        frame = _format_frame(frame)\n",
    "        reward = torch.tensor(reward).view(1, 1, -1)\n",
    "        done = torch.tensor(done).view(1, 1)\n",
    "        truncated_done = 'TimeLimit.truncated' in unused_info and unused_info['TimeLimit.truncated']\n",
    "        truncated_done = torch.tensor(truncated_done).view(1, 1)\n",
    "        cur_t = torch.tensor(unused_info[\"cur_t\"]).view(1, 1)\n",
    "        if cur_t == 0 and self.episode_return.shape[2] > 1:\n",
    "            self.episode_return[:, :, 1] = 0.\n",
    "        if 'max_rollout_depth' in unused_info:\n",
    "            max_rollout_depth = torch.tensor(unused_info[\"max_rollout_depth\"]).view(1, 1)\n",
    "        else:\n",
    "            max_rollout_depth = torch.tensor(0.).view(1, 1)\n",
    "        \n",
    "        return dict(\n",
    "            frame=frame,\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            truncated_done=truncated_done,          \n",
    "            episode_return=episode_return,\n",
    "            episode_step=episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=action,\n",
    "            max_rollout_depth=max_rollout_depth\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        self.gym_env.close()\n",
    "\n",
    "    def clone_state(self):\n",
    "        state = self.gym_env.clone_state()\n",
    "        state[\"env_episode_return\"] = self.episode_return.clone()\n",
    "        state[\"env_episode_step\"] = self.episode_step.clone()\n",
    "        return state\n",
    "        \n",
    "    def restore_state(self, state):\n",
    "        self.episode_return = state[\"env_episode_return\"].clone()\n",
    "        self.episode_step = state[\"env_episode_step\"].clone()\n",
    "        self.gym_env.restore_state(state)\n",
    "        \n",
    "class Vec_Environment:\n",
    "    # deprecated\n",
    "    def __init__(self, gym_env, bsz):\n",
    "        self.gym_env = gym_env\n",
    "        self.bsz = bsz\n",
    "        self.episode_return = torch.zeros(1, self.bsz)\n",
    "        self.episode_step = torch.zeros(1, self.bsz)        \n",
    "\n",
    "    def initial(self):\n",
    "        initial_reward = torch.zeros(1, self.bsz, 1)\n",
    "        # This supports only single-tensor actions ATM.\n",
    "        initial_last_action = torch.zeros(1, self.bsz, dtype=torch.int64)\n",
    "        self.episode_return = torch.zeros(1, self.bsz)\n",
    "        self.episode_step = torch.zeros(1, self.bsz, dtype=torch.int32)\n",
    "        initial_done = torch.ones(1, self.bsz, dtype=torch.uint8)\n",
    "        initial_frame = _format_frame(self.gym_env.reset(), self.bsz)\n",
    "        cur_t = torch.zeros(1, self.bsz)\n",
    "        \n",
    "        return dict(\n",
    "            frame=initial_frame,\n",
    "            reward=initial_reward,\n",
    "            done=initial_done,\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=initial_last_action,\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        frame, reward, done, unused_info = self.gym_env.step(action.detach().cpu().numpy())   \n",
    "        \n",
    "        self.episode_step += 1\n",
    "        self.episode_return += torch.Tensor(reward).unsqueeze(0)\n",
    "        \n",
    "        done = torch.tensor(done).view(1, self.bsz)\n",
    "        truncated_done = ['TimeLimit.truncated' in x and x['TimeLimit.truncated'] for x in unused_info]\n",
    "        truncated_done = torch.tensor(truncated_done).view(1, self.bsz)\n",
    "        \n",
    "        self.episode_return = (~done).float().unsqueeze(-1) * self.episode_return\n",
    "        self.episode_step = (~done).float() * self.episode_step\n",
    "        \n",
    "        frame = _format_frame(frame, self.bsz)\n",
    "        reward = torch.tensor(reward).view(1, self.bsz).float()\n",
    "        \n",
    "        cur_t = [x[\"cur_t\"] for x in unused_info]  \n",
    "        cur_t = torch.tensor(cur_t).view(1, self.bsz)\n",
    "        \n",
    "        return dict(\n",
    "            frame=frame,\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            truncated_done=truncated_done,\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=action.unsqueeze(0),\n",
    "        )\n",
    "    \n",
    "    def clone_state(self):        \n",
    "        state = {}\n",
    "        state[\"env_episode_return\"] = self.episode_return.clone()\n",
    "        state[\"env_episode_step\"] = self.episode_step.clone()\n",
    "        for n, k in enumerate(self.gym_env.envs): \n",
    "            state[\"env_%d\"%n] = k.clone_state()\n",
    "        return state\n",
    "        \n",
    "    def restore_state(self, state):\n",
    "        self.episode_return = state[\"env_episode_return\"].clone()\n",
    "        self.episode_step = state[\"env_episode_step\"].clone()\n",
    "        for n, k in enumerate(self.gym_env.envs): \n",
    "            k.restore_state(state[\"env_%d\"%n])\n",
    "\n",
    "    def close(self):\n",
    "        self.gym_env.close()  \n",
    "\n",
    "class Actor_net(nn.Module):    \n",
    "    def __init__(self, obs_shape, num_actions, flags):\n",
    "\n",
    "        super(Actor_net, self).__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions  \n",
    "        \n",
    "        self.tran_t = flags.tran_t                   # number of recurrence of RNN\n",
    "        self.tran_mem_n = flags.tran_mem_n           # size of memory for the attn modules\n",
    "        self.tran_layer_n = flags.tran_layer_n       # number of layers\n",
    "        self.tran_lstm = flags.tran_lstm             # to use lstm or not\n",
    "        self.tran_lstm_no_attn = flags.tran_lstm_no_attn  # to use attention in lstm or not\n",
    "        self.tran_norm_first = flags.tran_norm_first # to use norm first in transformer (not on LSTM)\n",
    "        self.tran_ff_n = flags.tran_ff_n             # number of dim of ff in transformer (not on LSTM)        \n",
    "        self.tran_skip = flags.tran_skip             # whether to add skip connection\n",
    "        self.conv_out = flags.tran_dim               # size of transformer / LSTM embedding dim\n",
    "        self.no_mem = flags.no_mem\n",
    "        self.num_rewards = flags.num_rewards\n",
    "        \n",
    "        self.conv_out_hw = 1   \n",
    "        self.d_model = self.conv_out\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=self.obs_shape[0], out_channels=self.conv_out//2, kernel_size=1, stride=1)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv_out//2, out_channels=self.conv_out, kernel_size=1, stride=1)        \n",
    "        self.frame_conv = torch.nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU())\n",
    "        self.env_input_size = self.conv_out\n",
    "        d_in = self.env_input_size + self.d_model \n",
    "        \n",
    "        if self.tran_lstm:\n",
    "            self.core = ConvAttnLSTM(h=self.conv_out_hw, w=self.conv_out_hw,\n",
    "                                 input_dim=d_in-self.d_model, hidden_dim=self.d_model,\n",
    "                                 kernel_size=1, num_layers=self.tran_layer_n,\n",
    "                                 num_heads=8, mem_n=self.tran_mem_n, attn=not self.tran_lstm_no_attn)\n",
    "        else:            \n",
    "            self.core = ConvTransformerRNN(d_in=d_in,\n",
    "                                       h=self.conv_out_hw, w=self.conv_out_hw, d_model=self.d_model, \n",
    "                                       num_heads=8, dim_feedforward=self.tran_ff_n, \n",
    "                                       mem_n=self.tran_mem_n, norm_first=self.tran_norm_first,\n",
    "                                       num_layers=self.tran_layer_n, rpos=True, conv=False)   \n",
    "                         \n",
    "        \n",
    "        if self.tran_skip:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * (self.d_model + self.env_input_size)\n",
    "        else:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * self.d_model\n",
    "                \n",
    "        self.fc = nn.Linear(rnn_out_size, 256)        \n",
    "        \n",
    "        self.im_policy = nn.Linear(256, self.num_actions)        \n",
    "        self.policy = nn.Linear(256, self.num_actions)        \n",
    "        self.baseline = nn.Linear(256, self.num_rewards)        \n",
    "        self.reset = nn.Linear(256, 2)        \n",
    "        \n",
    "        print(\"actor size: \", sum(p.numel() for p in self.parameters()))\n",
    "        #for k, v in self.named_parameters(): print(k, v.numel())   \n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        state = self.core.init_state(batch_size) + (torch.zeros(1, batch_size, \n",
    "               self.env_input_size, self.conv_out_hw, self.conv_out_hw),)\n",
    "        return state\n",
    "\n",
    "    def forward(self, obs, core_state=(), debug=False):\n",
    "        # one-step forward for the actor\n",
    "        # input / done shape x: T x B x C x 1 x 1 / B x C x 1 x 1\n",
    "        # only supports T = 1 at the moment; all output does not have T dim.        \n",
    "        \n",
    "        x = obs[\"frame\"]\n",
    "        done = obs[\"done\"]\n",
    "        \n",
    "        if len(x.shape) == 4: x = x.unsqueeze(0)\n",
    "        if len(done.shape) == 1: done = done.unsqueeze(0)  \n",
    "            \n",
    "        T, B, *_ = x.shape\n",
    "        x = torch.flatten(x, 0, 1)  # Merge time and batch.  \n",
    "        env_input = self.frame_conv(x)                \n",
    "        core_input = env_input.view(T, B, -1, self.conv_out_hw, self.conv_out_hw)\n",
    "        core_output_list = []\n",
    "        notdone = ~(done.bool())\n",
    "        \n",
    "        for n, (input, nd) in enumerate(zip(core_input.unbind(), notdone.unbind())):       \n",
    "            if self.no_mem and obs[\"cur_t\"][n, 0] == 0:\n",
    "                core_state = self.initial_state(B)\n",
    "                core_state = tuple(v.to(x.device) for v in core_state)\n",
    "                \n",
    "            # Input shape: B, self.conv_out + self.num_actions + 1, H, W\n",
    "            for t in range(self.tran_t):                \n",
    "                if t > 0: nd = torch.ones(B).to(x.device).bool()                    \n",
    "                nd = nd.view(-1)      \n",
    "                output, core_state = self.core(input, core_state, nd, nd) # output shape: 1, B, core_output_size \n",
    "                \n",
    "            last_input = input   \n",
    "            core_output_list.append(output)\n",
    "                                   \n",
    "        core_output = torch.cat(core_output_list)  \n",
    "        if self.tran_skip: core_output = torch.concat([core_output, core_input], dim=-3)\n",
    "        core_output = torch.flatten(core_output, 0, 1)        \n",
    "        core_output = F.relu(self.fc(torch.flatten(core_output, start_dim=1)))   \n",
    "        \n",
    "        policy_logits = self.policy(core_output)\n",
    "        im_policy_logits = self.im_policy(core_output)\n",
    "        reset_policy_logits = self.reset(core_output)\n",
    "        \n",
    "        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        im_action = torch.multinomial(F.softmax(im_policy_logits, dim=1), num_samples=1)\n",
    "        reset_action = torch.multinomial(F.softmax(reset_policy_logits, dim=1), num_samples=1)\n",
    "                \n",
    "        baseline = self.baseline(core_output)\n",
    "                   \n",
    "        reg_loss = (1e-3 * torch.sum(policy_logits**2, dim=-1) / 2 + \n",
    "                    1e-5 * torch.sum(core_output**2, dim=-1) / 2)\n",
    "        reg_loss = reg_loss.view(T, B)\n",
    "        \n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        im_policy_logits = im_policy_logits.view(T, B, self.num_actions)\n",
    "        reset_policy_logits = reset_policy_logits.view(T, B, 2)\n",
    "        \n",
    "        action = action.view(T, B)      \n",
    "        im_action = im_action.view(T, B)      \n",
    "        reset_action = reset_action.view(T, B)             \n",
    "        baseline = baseline.view(T, B, self.num_rewards)\n",
    "        \n",
    "        ret_dict = dict(policy_logits=policy_logits,                         \n",
    "                        im_policy_logits=im_policy_logits,                         \n",
    "                        reset_policy_logits=reset_policy_logits,     \n",
    "                        action=action,     \n",
    "                        im_action=im_action,\n",
    "                        reset_action=reset_action,\n",
    "                        baseline=baseline, \n",
    "                        reg_loss=reg_loss, )\n",
    "        \n",
    "        return (ret_dict, core_state) \n",
    "    \n",
    "        \n",
    "        \n",
    "class ModelWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, model, flags):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.env = env\n",
    "        self.model = model                \n",
    "        self.rec_t = flags.rec_t        \n",
    "        self.discounting = flags.discounting\n",
    "        self.stat_type = flags.stat_type    \n",
    "        self.reward_type = flags.reward_type    \n",
    "        self.no_mem = flags.no_mem\n",
    "        self.perfect_model = flags.perfect_model\n",
    "        \n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        # 0 for the most basic; \n",
    "        # 1 for augmented input in root stat; \n",
    "        # 2 for tree stat\n",
    "        if self.stat_type == 0:\n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 5 + num_actions * 4 + self.rec_t\n",
    "        elif self.stat_type == 1:            \n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 7 + num_actions * 7 + self.rec_t\n",
    "        elif self.stat_type == 1.5:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 6 + num_actions * 10 + self.rec_t             \n",
    "        elif self.stat_type == 2:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 9 + num_actions * 10 + self.rec_t         \n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "          low=-np.inf, high=np.inf, shape=(obs_n, 1, 1), dtype=float)\n",
    "        self.model.train(False)        \n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        x = self.env.reset()\n",
    "        self.cur_t = 0        \n",
    "        out = self.use_model(x, 0., 0, self.cur_t, 1.)\n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q\n",
    "        self.max_rollout_depth = 0.\n",
    "        return out.unsqueeze(-1).unsqueeze(-1)\n",
    "    \n",
    "    def step(self, action):  \n",
    "        re_action, im_action, reset = action\n",
    "        info = {}\n",
    "        info[\"max_rollout_depth\"] = self.max_rollout_depth\n",
    "        if self.cur_t < self.rec_t - 1:\n",
    "          self.cur_t += 1\n",
    "          out = self.use_model(None, None, im_action, self.cur_t, reset)          \n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([0.])\n",
    "          else:\n",
    "            r = np.array([0., (self.root_max_q - self.last_root_max_q).item()], dtype=np.float32)\n",
    "          done = False\n",
    "          info['cur_t'] = self.cur_t   \n",
    "        else:\n",
    "          self.cur_t = 0\n",
    "          if self.perfect_model:\n",
    "                self.env.restore_state(self.root_node.encoded)\n",
    "          x, r, done, info_ = self.env.step(re_action)                    \n",
    "          out = self.use_model(x, r, re_action, self.cur_t, 1.) \n",
    "          info.update(info_)\n",
    "          info['cur_t'] = self.cur_t\n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([r])\n",
    "          else:\n",
    "            r = np.array([r, 0.], dtype=np.float32)   \n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q        \n",
    "        return out.unsqueeze(-1).unsqueeze(-1), r, done, info        \n",
    "        \n",
    "    def use_model_raw(self, x, r, a, cur_t, reset):\n",
    "        # input: \n",
    "        # r: reward - [,]; x: frame - [C, H, W]; a: action - [,]\n",
    "        # cur_t: int; reset at cur_t == 0  \n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:\n",
    "                self.rollout_depth = 0.\n",
    "                if self.no_mem:\n",
    "                    self.re_action = F.one_hot(torch.zeros(1, dtype=torch.long), self.num_actions)   \n",
    "                else:\n",
    "                    self.re_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)                   \n",
    "                \n",
    "                x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = x\n",
    "                _, vs, logits, encodeds = self.model(x, self.re_action.unsqueeze(0), one_hot=True)                \n",
    "                self.encoded = encodeds[-1]    \n",
    "                self.encoded_reset = self.encoded.clone()\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    self.re_reward = torch.tensor([[0.]], dtype=torch.float32)                \n",
    "                else:\n",
    "                    self.re_reward = torch.tensor([[r]], dtype=torch.float32)                \n",
    "                    \n",
    "                self.v0 = vs[-1].unsqueeze(-1).clone()\n",
    "                self.logit0 = logits[-1].clone()\n",
    "                \n",
    "                self.im_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.im_reset = torch.tensor([[1.]], dtype=torch.float32)\n",
    "                self.im_reward = torch.zeros(1, 1, dtype=torch.float32)                                \n",
    "                self.v = vs[-1].unsqueeze(-1)\n",
    "                self.logit = logits[-1]\n",
    "                self.rollout_first_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                self.rollout_return = torch.zeros(1, 1, dtype=torch.float32)                \n",
    "                self.q_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.n_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)                \n",
    "            else:\n",
    "                self.rollout_depth += 1                \n",
    "                \n",
    "                self.im_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)   \n",
    "                rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "                   self.im_action.unsqueeze(0), one_hot=True)\n",
    "                self.encoded = encodeds[-1]        \n",
    "                \n",
    "                self.im_reward = rs[-1].unsqueeze(-1)\n",
    "                self.v = vs[-1].unsqueeze(-1)    \n",
    "                self.logit = logits[-1]     \n",
    "                \n",
    "                if self.im_reset: \n",
    "                    # last action's reset is true; re-initialize everything                    \n",
    "                    self.rollout_first_action = self.im_action.clone()\n",
    "                    self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                    self.rollout_depth = 1                      \n",
    "                    \n",
    "                self.rollout_return_wo_v += (self.discounting ** (self.rollout_depth-1)) * self.im_reward\n",
    "                self.rollout_return = self.rollout_return_wo_v + (\n",
    "                    self.discounting ** (self.rollout_depth)) * self.v                    \n",
    "                    \n",
    "                self.im_reset = torch.tensor([[reset]], dtype=torch.float32)\n",
    "                if self.im_reset:                    \n",
    "                    rollout_first_action_label = torch.argmax(self.rollout_first_action, dim=1)                    \n",
    "                    q = self.q_s_a[:, rollout_first_action_label]\n",
    "                    n = self.n_s_a[:, rollout_first_action_label]                    \n",
    "                    ret = self.rollout_return[:, 0]\n",
    "                    self.n_s_a[:, rollout_first_action_label] += 1                    \n",
    "                    self.q_s_a[:, rollout_first_action_label] = (n * q) / (n + 1) + ret / (n + 1)\n",
    "                    \n",
    "        time = F.one_hot(torch.tensor([cur_t]).long(), self.rec_t)\n",
    "        depc = torch.tensor([[self.discounting ** (self.rollout_depth-1)]])\n",
    "        ret_dict = {\"re_action\": self.re_action,\n",
    "                    \"re_reward\": self.re_reward,\n",
    "                    \"v0\": self.v0,\n",
    "                    \"logit0\": self.logit0,\n",
    "                    \"im_action\": self.im_action,\n",
    "                    \"im_reset\": self.im_reset,\n",
    "                    \"im_reward\": self.im_reward,\n",
    "                    \"v\": self.v,\n",
    "                    \"logit\": self.logit,\n",
    "                    \"rollout_first_action\": self.rollout_first_action,\n",
    "                    \"rollout_return\": self.rollout_return,\n",
    "                    \"n_s_a\": self.n_s_a,\n",
    "                    \"q_s_a\": self.q_s_a,\n",
    "                    \"time\": time,\n",
    "                    \"depc\": depc}        \n",
    "        self.ret_dict = ret_dict\n",
    "        if self.stat_type == 1:\n",
    "            out = torch.concat(list(ret_dict.values()), dim=-1)   \n",
    "        else:\n",
    "            core_inputs = [\"re_action\", \"re_reward\", \"v0\", \"logit0\", \"im_action\",\n",
    "                           \"im_reset\", \"im_reward\", \"v\", \"logit\", \"time\"]\n",
    "            out = torch.concat([ret_dict[v] for v in core_inputs], dim=-1)   \n",
    "        self.encoded = reset * self.encoded_reset + (1 - reset) * self.encoded\n",
    "        return out[0]      \n",
    "    \n",
    "    def use_model_tree(self, x, r, a, cur_t, reset):\n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:\n",
    "                self.rollout_depth = 0.\n",
    "                self.max_rollout_depth = 0.\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    re_action = 0\n",
    "                    re_reward = torch.tensor([0.], dtype=torch.float32)                \n",
    "                else:\n",
    "                    re_action = a                \n",
    "                    re_reward = torch.tensor([r], dtype=torch.float32)                \n",
    "                \n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = self.x_ = x_tensor\n",
    "                a_tensor = F.one_hot(torch.tensor(re_action, dtype=torch.long).unsqueeze(0), self.num_actions)                \n",
    "                _, vs, logits, encodeds = self.model(x_tensor, a_tensor.unsqueeze(0), one_hot=True) \n",
    "                \n",
    "                if self.perfect_model: \n",
    "                    encoded = self.clone_state()\n",
    "                else:\n",
    "                    encoded=encodeds[-1]\n",
    "                \n",
    "                self.root_node = Node(parent=None, action=re_action, logit=None, \n",
    "                                      num_actions=self.num_actions,\n",
    "                                      discounting=self.discounting,\n",
    "                                      rec_t=self.rec_t)\n",
    "                self.root_node.expand(r=re_reward, v=vs[-1, 0].unsqueeze(-1), logits=logits[-1, 0],\n",
    "                                      encoded=encoded)\n",
    "                self.root_node.visit()\n",
    "                self.cur_node = self.root_node\n",
    "            else:\n",
    "                self.rollout_depth += 1    \n",
    "                self.max_rollout_depth = max(self.max_rollout_depth, self.rollout_depth)\n",
    "                next_node = self.cur_node.children[a]\n",
    "                if not next_node.expanded():\n",
    "                    a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions) \n",
    "                    if not self.perfect_model:\n",
    "                        rs, vs, logits, encodeds = self.model.forward_encoded(self.cur_node.encoded, \n",
    "                            a_tensor.unsqueeze(0), one_hot=True)\n",
    "                        next_node.expand(r=rs[-1, 0].unsqueeze(-1), v=vs[-1, 0].unsqueeze(-1), \n",
    "                                     logits=logits[-1, 0], encoded=encodeds[-1])\n",
    "                    else:\n",
    "                        self.env.restore_state(self.cur_node.encoded)\n",
    "                        x, r, done, info = self.env.step(a)\n",
    "                        if done: \n",
    "                            encoded = self.cur_node.encoded\n",
    "                        else:\n",
    "                            encoded = self.env.clone_state()\n",
    "                        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                        self.x_ = x_tensor\n",
    "                        a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions) \n",
    "                        _, vs, logits, _ = self.model(x_tensor, a_tensor.unsqueeze(0), one_hot=True)                        \n",
    "                        next_node.expand(r=torch.tensor([r], dtype=torch.float32), \n",
    "                                         v=vs[-1, 0].unsqueeze(-1), \n",
    "                                         logits=logits[-1, 0], \n",
    "                                         encoded=encoded)\n",
    "                next_node.visit()\n",
    "                self.cur_node = next_node\n",
    "            \n",
    "            root_node_stat = self.root_node.stat()\n",
    "            cur_node_stat = self.cur_node.stat()                        \n",
    "            reset = torch.tensor([reset], dtype=torch.float32)\n",
    "            time = F.one_hot(torch.tensor(cur_t).long(), self.rec_t)\n",
    "            depc = torch.tensor([self.discounting ** (self.rollout_depth-1)])\n",
    "            \n",
    "            root_trail_r = self.root_node.trail_r / self.discounting\n",
    "            root_rollout_q = self.root_node.rollout_q / self.discounting\n",
    "            root_max_q = torch.max(torch.concat(self.root_node.rollout_qs)).unsqueeze(-1) / self.discounting\n",
    "            \n",
    "            ret_list = [root_node_stat, cur_node_stat, reset, time, depc,]\n",
    "            if self.stat_type >= 2: ret_list.extend([root_trail_r, root_rollout_q, root_max_q])            \n",
    "            out = torch.concat(ret_list, dim=-1)            \n",
    "            self.last_node = self.cur_node     \n",
    "            \n",
    "            self.root_max_q = root_max_q\n",
    "            self.ret_dict = {\"v0\": self.root_node.ret_dict[\"v\"].unsqueeze(0),\n",
    "                             \"q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_mean\"].unsqueeze(0),\n",
    "                             \"max_q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_max\"].unsqueeze(0),\n",
    "                             \"n_s_a\": self.root_node.ret_dict[\"child_rollout_ns\"].unsqueeze(0),\n",
    "                             \"logit0\": self.root_node.ret_dict[\"child_logits\"].unsqueeze(0),}\n",
    "            \n",
    "            if reset:\n",
    "                self.rollout_depth = 0\n",
    "                self.cur_node = self.root_node\n",
    "                self.cur_node.visit()\n",
    "                \n",
    "            return out\n",
    "                \n",
    "class Node:\n",
    "    def __init__(self, parent, action, logit, num_actions, discounting, rec_t):        \n",
    "        \n",
    "        self.action = F.one_hot(torch.tensor(action).long(), num_actions) # shape (1, num_actions)        \n",
    "        self.r = torch.tensor([0.], dtype=torch.float32)    \n",
    "        self.v = torch.tensor([0.], dtype=torch.float32)            \n",
    "        self.logit = logit # shape (1,)        \n",
    "        \n",
    "        self.rollout_qs = []  # list of tensors of shape (1,)\n",
    "        self.rollout_n = torch.tensor([0.], dtype=torch.float32)    \n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.encoded = None \n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.discounting = discounting\n",
    "        self.rec_t = rec_t        \n",
    "        \n",
    "        self.visited = False\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def expand(self, r, v, logits, encoded):\n",
    "        \"\"\"\n",
    "        First time arriving a node and so we expand it\n",
    "        r, v: tensor of shape (1,)\n",
    "        logits: tensor of shape (num_actions,)\n",
    "        \"\"\"\n",
    "        assert not self.expanded()\n",
    "        self.r = r\n",
    "        self.v = v\n",
    "        self.encoded = encoded\n",
    "        for a in range(self.num_actions):\n",
    "            child = self.children.append(Node(self, a, logits[[a]], \n",
    "               self.num_actions, self.discounting, self.rec_t))\n",
    "            \n",
    "    def visit(self):\n",
    "        self.trail_r = torch.tensor([0.], dtype=torch.float32)    \n",
    "        self.trail_discount = 1.\n",
    "        self.propagate(self.r, self.v, not self.visited)        \n",
    "        self.visited = True\n",
    "        \n",
    "    def propagate(self, r, v, new_rollout):\n",
    "        self.trail_r = self.trail_r + self.trail_discount * r\n",
    "        self.trail_discount = self.trail_discount * self.discounting\n",
    "        self.rollout_q = self.trail_r + self.trail_discount * v\n",
    "        if new_rollout:\n",
    "            self.rollout_qs.append(self.rollout_q)\n",
    "            self.rollout_n = self.rollout_n + 1\n",
    "        if self.parent is not None: self.parent.propagate(r, v, new_rollout)\n",
    "            \n",
    "    def stat(self):\n",
    "        assert self.expanded\n",
    "\n",
    "        self.child_logits = torch.concat([x.logit for x in self.children])        \n",
    "        child_rollout_qs_mean = []\n",
    "        child_rollout_qs_max = []\n",
    "        for x in self.children:\n",
    "            if len(x.rollout_qs) > 0:                \n",
    "                q_mean = torch.mean(torch.cat(x.rollout_qs), dim=-1, keepdim=True)\n",
    "                q_max = torch.max(torch.cat(x.rollout_qs), dim=-1, keepdim=True)[0]\n",
    "            else:\n",
    "                q_mean = torch.tensor([0.], dtype=torch.float32)    \n",
    "                q_max = torch.tensor([0.], dtype=torch.float32)    \n",
    "            child_rollout_qs_mean.append(q_mean)\n",
    "            child_rollout_qs_max.append(q_max)\n",
    "        self.child_rollout_qs_mean = torch.concat(child_rollout_qs_mean)\n",
    "        self.child_rollout_qs_max = torch.concat(child_rollout_qs_max)\n",
    "        self.child_rollout_ns = torch.concat([x.rollout_n for x in self.children]) / self.rec_t       \n",
    "            \n",
    "        ret_list = [\"action\", \"r\", \"v\", \"child_logits\", \"child_rollout_qs_mean\",\n",
    "                    \"child_rollout_qs_max\", \"child_rollout_ns\"]\n",
    "        self.ret_dict = {x: getattr(self, x) for x in ret_list}\n",
    "        out = torch.concat(list(self.ret_dict.values()))        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae5f1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_parser():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n",
    "\n",
    "    parser.add_argument(\"--env\", type=str, default=\"Sokoban-v0\",\n",
    "                        help=\"Gym environment.\")\n",
    "    parser.add_argument(\"--env_disable_noop\", action=\"store_true\",\n",
    "                        help=\"Disable noop in environment or not. (sokoban only)\")\n",
    "\n",
    "    parser.add_argument(\"--xpid\", default=None,\n",
    "                        help=\"Experiment id (default: None).\")\n",
    "\n",
    "    parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n",
    "                        help=\"Disable saving checkpoint.\")\n",
    "    parser.add_argument(\"--load_checkpoint\", default=\"\",\n",
    "                        help=\"Load checkpoint directory.\")    \n",
    "    parser.add_argument(\"--savedir\", default=\"~/RS/thinker/logs/torchbeast\",\n",
    "                        help=\"Root dir where experiment data will be saved.\")\n",
    "\n",
    "    # Training settings.        \n",
    "    parser.add_argument(\"--num_actors\", default=48, type=int, metavar=\"N\",\n",
    "                        help=\"Number of actors (default: 48).\")\n",
    "    parser.add_argument(\"--total_steps\", default=500000000, type=int, metavar=\"T\",\n",
    "                        help=\"Total environment steps to train for.\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, metavar=\"B\",\n",
    "                        help=\"Learner batch size.\")\n",
    "    parser.add_argument(\"--unroll_length\", default=100, type=int, metavar=\"T\",\n",
    "                        help=\"The unroll length (time dimension).\")\n",
    "    parser.add_argument(\"--num_buffers\", default=None, type=int,\n",
    "                        metavar=\"N\", help=\"Number of shared-memory buffers.\")\n",
    "    parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=1, type=int,\n",
    "                        metavar=\"N\", help=\"Number learner threads.\")\n",
    "    parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n",
    "                        help=\"Disable CUDA.\")\n",
    "\n",
    "    # Architecture settings\n",
    "    parser.add_argument(\"--tran_dim\", default=64, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer hidden dim.\")\n",
    "    parser.add_argument(\"--tran_mem_n\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer memory.\")\n",
    "    parser.add_argument(\"--tran_layer_n\", default=3, type=int, metavar=\"N\",\n",
    "                        help=\"Number of transformer layer.\")\n",
    "    parser.add_argument(\"--tran_t\", default=1, type=int, metavar=\"T\",\n",
    "                        help=\"Number of recurrent step for transformer.\")\n",
    "    parser.add_argument(\"--tran_ff_n\", default=256, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer ff .\")\n",
    "    parser.add_argument(\"--tran_skip\", action=\"store_true\",\n",
    "                        help=\"Whether to enable skip conn.\")\n",
    "    parser.add_argument(\"--tran_norm_first\", action=\"store_true\",\n",
    "                        help=\"Whether to use norm first in transformer.\")\n",
    "    parser.add_argument(\"--tran_rpos\", action=\"store_true\",\n",
    "                        help=\"Whether to use relative position in transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm\", action=\"store_true\",\n",
    "                        help=\"Whether to use LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm_no_attn\", action=\"store_true\",\n",
    "                        help=\"Whether to disable attention in LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_erasep\", action=\"store_true\",\n",
    "                        help=\"Whether to erase past memories if not planning.\")\n",
    "\n",
    "    parser.add_argument(\"--rec_t\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Number of planning steps.\")\n",
    "    parser.add_argument(\"--stat_type\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"Staistic type (0: raw; 1: root node stat; 2. root & current node stat).\")    \n",
    "    parser.add_argument(\"--no_mem\", action=\"store_true\",\n",
    "                        help=\"Whether to erase all memories after each real action.\")   \n",
    "    \n",
    "    parser.add_argument(\"--model_type_nn\", default=0,\n",
    "                        type=float, help=\"Model type.\")     \n",
    "    parser.add_argument(\"--perfect_model\", action=\"store_true\",\n",
    "                        help=\"Whether to use perfect model.\")       \n",
    "    \n",
    "    # Loss settings.\n",
    "    parser.add_argument(\"--entropy_cost\", default=0.0001,\n",
    "                        type=float, help=\"Entropy cost/multiplier.\")\n",
    "    parser.add_argument(\"--im_entropy_cost\", default=0.0001,\n",
    "                        type=float, help=\"Imagainary Entropy cost/multiplier.\")   \n",
    "    parser.add_argument(\"--reset_entropy_cost\", default=0.0001,\n",
    "                        type=float, help=\"Reset Entropy cost/multiplier.\")       \n",
    "    parser.add_argument(\"--baseline_cost\", default=0.5,\n",
    "                        type=float, help=\"Baseline cost/multiplier.\")\n",
    "    parser.add_argument(\"--reg_cost\", default=0.1,\n",
    "                        type=float, help=\"Reg cost/multiplier.\")\n",
    "    parser.add_argument(\"--im_cost\", default=1,\n",
    "                        type=float, help=\"Imaginary reward cost/multiplier.\")    \n",
    "    parser.add_argument(\"--discounting\", default=0.99,\n",
    "                        type=float, help=\"Discounting factor.\")\n",
    "    parser.add_argument(\"--lamb\", default=1.,\n",
    "                        type=float, help=\"Lambda when computing trace.\")\n",
    "    parser.add_argument(\"--reward_clipping\", default=10, type=int, \n",
    "                        metavar=\"N\", help=\"Reward clipping.\")\n",
    "    parser.add_argument(\"--trun_bs\", action=\"store_true\",\n",
    "                        help=\"Whether to add baseline as reward when truncated.\")\n",
    "    parser.add_argument(\"--reward_type\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"Reward type\")    \n",
    "\n",
    "    # Optimizer settings.\n",
    "    parser.add_argument(\"--learning_rate\", default=0.00005,\n",
    "                        type=float, metavar=\"LR\", help=\"Learning rate.\")\n",
    "    parser.add_argument(\"--disable_adam\", action=\"store_true\",\n",
    "                        help=\"Use Aadm optimizer or not.\")\n",
    "    parser.add_argument(\"--alpha\", default=0.99, type=float,\n",
    "                        help=\"RMSProp smoothing constant.\")\n",
    "    parser.add_argument(\"--momentum\", default=0, type=float,\n",
    "                        help=\"RMSProp momentum.\")\n",
    "    parser.add_argument(\"--epsilon\", default=0.01, type=float,\n",
    "                        help=\"RMSProp epsilon.\")\n",
    "    parser.add_argument(\"--grad_norm_clipping\", default=0.0, type=float,\n",
    "                        help=\"Global gradient norm clip.\")\n",
    "    # yapf: enable\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.load_checkpoint = \"\"\n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.num_actors = 1\n",
    "flags.batch_size = 32\n",
    "flags.unroll_length = 200\n",
    "flags.learning_rate = 0.0001\n",
    "flags.grad_norm_clipping = 60\n",
    "\n",
    "flags.entropy_cost = 0.0001\n",
    "flags.im_entropy_cost = 0.0001\n",
    "flags.reset_entropy_cost = 0.0001\n",
    "flags.reg_cost = 0.01\n",
    "flags.im_cost = 1\n",
    "flags.discounting = 0.97\n",
    "flags.lamb = 1.\n",
    "\n",
    "flags.trun_bs = False\n",
    "flags.total_steps = 500000000\n",
    "flags.disable_adam = False\n",
    "\n",
    "flags.tran_t = 1\n",
    "flags.tran_mem_n = 5\n",
    "flags.tran_layer_n = 3\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = False\n",
    "flags.tran_norm_first = False\n",
    "flags.tran_ff_n = 256\n",
    "flags.tran_skip = False\n",
    "flags.tran_erasep = False\n",
    "flags.tran_dim = 64\n",
    "flags.tran_rpos = True\n",
    "\n",
    "flags.rec_t = 5\n",
    "flags.no_mem = True\n",
    "\n",
    "flags.model_type_nn = 0\n",
    "flags.perfect_model = True\n",
    "flags.stat_type = 2\n",
    "flags.reward_type = 1\n",
    "\n",
    "flags.savedir = \"~/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d04a968",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating log directory: /home/sc/tmp/torchbeast-20221113-223518\n",
      "Creating log directory: /home/sc/tmp/torchbeast-20221113-223518\n",
      "Symlinked log directory: /home/sc/tmp/latest\n",
      "Symlinked log directory: /home/sc/tmp/latest\n",
      "Saving arguments to /home/sc/tmp/torchbeast-20221113-223518/meta.json\n",
      "Saving arguments to /home/sc/tmp/torchbeast-20221113-223518/meta.json\n",
      "Saving messages to /home/sc/tmp/torchbeast-20221113-223518/out.log\n",
      "Saving messages to /home/sc/tmp/torchbeast-20221113-223518/out.log\n",
      "Saving logs data to /home/sc/tmp/torchbeast-20221113-223518/logs.csv\n",
      "Saving logs data to /home/sc/tmp/torchbeast-20221113-223518/logs.csv\n",
      "Saving logs' fields to /home/sc/tmp/torchbeast-20221113-223518/fields.csv\n",
      "Saving logs' fields to /home/sc/tmp/torchbeast-20221113-223518/fields.csv\n",
      "Using CUDA.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  396838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor 0 started.\n",
      "# Step\tmean_episode_return\tepisode_returns\ttotal_loss\tpg_loss\tbaseline_loss\tentropy_loss\tmax_rollout_depth\tim_pg_loss\tim_baseline_loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  396838\n",
      "Using Adam...\n",
      "All parameters: \n",
      "conv1.weight 2048\n",
      "conv1.bias 32\n",
      "conv2.weight 2048\n",
      "conv2.bias 64\n",
      "core.layers.0.pos_w 320\n",
      "core.layers.0.pos_b 40\n",
      "core.layers.0.conv.weight 81920\n",
      "core.layers.0.conv.bias 320\n",
      "core.layers.0.proj.weight 36864\n",
      "core.layers.0.proj.bias 192\n",
      "core.layers.0.out.weight 4096\n",
      "core.layers.0.out.bias 64\n",
      "core.layers.0.norm.weight 64\n",
      "core.layers.0.norm.bias 64\n",
      "core.layers.1.pos_w 320\n",
      "core.layers.1.pos_b 40\n",
      "core.layers.1.conv.weight 81920\n",
      "core.layers.1.conv.bias 320\n",
      "core.layers.1.proj.weight 36864\n",
      "core.layers.1.proj.bias 192\n",
      "core.layers.1.out.weight 4096\n",
      "core.layers.1.out.bias 64\n",
      "core.layers.1.norm.weight 64\n",
      "core.layers.1.norm.bias 64\n",
      "core.layers.2.pos_w 320\n",
      "core.layers.2.pos_b 40\n",
      "core.layers.2.conv.weight 81920\n",
      "core.layers.2.conv.bias 320\n",
      "core.layers.2.proj.weight 36864\n",
      "core.layers.2.proj.bias 192\n",
      "core.layers.2.out.weight 4096\n",
      "core.layers.2.out.bias 64\n",
      "core.layers.2.norm.weight 64\n",
      "core.layers.2.norm.bias 64\n",
      "core.proj_list.0.weight 128\n",
      "core.proj_list.0.bias 64\n",
      "core.proj_list.1.weight 128\n",
      "core.proj_list.1.bias 64\n",
      "core.proj_list.2.weight 128\n",
      "core.proj_list.2.bias 64\n",
      "fc.weight 16384\n",
      "fc.bias 256\n",
      "im_policy.weight 1280\n",
      "im_policy.bias 5\n",
      "policy.weight 1280\n",
      "policy.bias 5\n",
      "baseline.weight 512\n",
      "baseline.bias 2\n",
      "reset.weight 512\n",
      "reset.bias 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Updated log fields: ['_tick', '_time', 'step', 'mean_episode_return', 'episode_returns', 'total_loss', 'pg_loss', 'baseline_loss', 'entropy_loss', 'max_rollout_depth', 'im_pg_loss', 'im_baseline_loss', 'rmean_im_episode_return', 'rmean_episode_return', 'episode']\n",
      "Updated log fields: ['_tick', '_time', 'step', 'mean_episode_return', 'episode_returns', 'total_loss', 'pg_loss', 'baseline_loss', 'entropy_loss', 'max_rollout_depth', 'im_pg_loss', 'im_baseline_loss', 'rmean_im_episode_return', 'rmean_episode_return', 'episode']\n",
      "Steps 6400 @ 1279.2 SPS. Eps 10. L400 Return -0.967999 (0.078909). Loss -720.06 max_rollout_depth 2.39 pg_loss -417.26 baseline_loss 21.43 im_pg_loss -337.74 im_baseline_loss 16.01 entropy_loss -2.50 reg_loss 0.00 total_norm 456.66\n",
      "Steps 6400 @ 0.0 SPS. Eps 10. L400 Return -0.967999 (0.078909). Loss -720.06 max_rollout_depth 2.39 pg_loss -417.26 baseline_loss 21.43 im_pg_loss -337.74 im_baseline_loss 16.01 entropy_loss -2.50 reg_loss 0.00 total_norm 456.66\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_145130/3678687939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_145130/3678687939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;31m# Try joining actors then quit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if flags.reward_type == 0:\n",
    "    flags.num_rewards = num_rewards = 1\n",
    "else:\n",
    "    flags.num_rewards = num_rewards = 2\n",
    "flags.im_discounting = flags.discounting ** (1/flags.rec_t)    \n",
    "    \n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "\n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags))\n",
    "obs_shape = env.gym_env.observation_space.shape\n",
    "\n",
    "mp.set_sharing_strategy('file_system')\n",
    "\n",
    "if flags.load_checkpoint:\n",
    "    flags.savedir = os.path.split(os.path.split(flags.load_checkpoint)[0])[0]\n",
    "    flags.xpid = os.path.split(os.path.split(flags.load_checkpoint)[0])[-1]    \n",
    "else:\n",
    "    if flags.xpid is None:\n",
    "        flags.xpid = \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "plogger = file_writer.FileWriter(\n",
    "    xpid=flags.xpid, xp_args=flags.__dict__, rootdir=flags.savedir\n",
    ")\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available():\n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser(\"%s/%s/%s\" % (flags.savedir, flags.xpid, \"model.tar\"))\n",
    ")\n",
    "\n",
    "if flags.num_buffers is None:  # Set sensible default for num_buffers.\n",
    "    flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n",
    "if flags.num_actors >= flags.num_buffers:\n",
    "    raise ValueError(\"num_buffers should be larger than num_actors\")\n",
    "if flags.num_buffers < flags.batch_size:\n",
    "    raise ValueError(\"num_buffers should be larger than batch_size\")\n",
    "\n",
    "T = flags.unroll_length\n",
    "B = flags.batch_size\n",
    "\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags)\n",
    "buffers = create_buffers(flags, obs_shape, num_actions, num_rewards)\n",
    "\n",
    "if flags.load_checkpoint:\n",
    "    train_checkpoint = torch.load(flags.load_checkpoint)\n",
    "    actor_net.load_state_dict(train_checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "actor_net.share_memory()\n",
    "\n",
    "# Add initial RNN state.\n",
    "initial_agent_state_buffers = []\n",
    "for _ in range(flags.num_buffers):\n",
    "    state = actor_net.initial_state(batch_size=1)\n",
    "    for t in state:\n",
    "        t.share_memory_()\n",
    "    initial_agent_state_buffers.append(state)\n",
    "\n",
    "actor_processes = []\n",
    "ctx = mp.get_context()\n",
    "free_queue = ctx.SimpleQueue()\n",
    "full_queue = ctx.SimpleQueue()\n",
    "\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=act, args=(flags, i, free_queue, full_queue,\n",
    "            actor_net, model, buffers, initial_agent_state_buffers,),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)\n",
    "\n",
    "learner_net = Actor_net(obs_shape, num_actions, flags)\n",
    "if flags.load_checkpoint:\n",
    "    learner_net.load_state_dict(train_checkpoint[\"model_state_dict\"])\n",
    "learner_net = learner_net.to(device=flags.device)  \n",
    "\n",
    "if not flags.disable_adam:\n",
    "    print(\"Using Adam...\")        \n",
    "    optimizer = torch.optim.Adam(learner_net.parameters(),lr=flags.learning_rate)\n",
    "else:\n",
    "    print(\"Using RMS Prop...\")\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        learner_net.actor.parameters(),\n",
    "        lr=flags.learning_rate,\n",
    "        momentum=flags.momentum,\n",
    "        eps=flags.epsilon,\n",
    "        alpha=flags.alpha,)\n",
    "    \n",
    "if flags.load_checkpoint:\n",
    "    optimizer.load_state_dict(train_checkpoint[\"optimizer_state_dict\"])    \n",
    "    \n",
    "print(\"All parameters: \")\n",
    "for k, v in learner_net.named_parameters(): print(k, v.numel())    \n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "if flags.load_checkpoint:\n",
    "    scheduler.load_state_dict(train_checkpoint[\"scheduler_state_dict\"])\n",
    "    \n",
    "logger = logging.getLogger(\"logfile\")\n",
    "stat_keys = [\"mean_episode_return\", \"episode_returns\", \"total_loss\",\n",
    "    \"pg_loss\", \"baseline_loss\", \"entropy_loss\", \"max_rollout_depth\"]\n",
    "if flags.reward_type == 1:\n",
    "    stat_keys.extend([\"im_pg_loss\", \"im_baseline_loss\"])\n",
    "\n",
    "logger.info(\"# Step\\t%s\", \"\\t\".join(stat_keys))\n",
    "\n",
    "step, stats, last_returns, last_im_returns, tot_eps = 0, {}, deque(maxlen=400), deque(maxlen=40000), 0\n",
    "if flags.load_checkpoint:\n",
    "    step = train_checkpoint[\"scheduler_state_dict\"][\"_step_count\"] * T * B\n",
    "    \n",
    "def batch_and_learn(i, lock=threading.Lock()):\n",
    "    \"\"\"Thread target for the learning process.\"\"\"\n",
    "    #nonlocal step, stats, last_returns, tot_eps\n",
    "    global step, stats, last_returns, last_im_returns, tot_eps\n",
    "    timings = prof.Timings()\n",
    "    while step < flags.total_steps:\n",
    "        timings.reset()\n",
    "        batch, agent_state = get_batch(flags, free_queue, full_queue, buffers,\n",
    "            initial_agent_state_buffers, timings,)\n",
    "        stats = learn(flags, actor_net, learner_net, batch, agent_state, optimizer, \n",
    "            scheduler)\n",
    "        last_returns.extend(stats[\"episode_returns\"])\n",
    "        if \"im_episode_returns\" in stats:\n",
    "            last_im_returns.extend(stats[\"im_episode_returns\"])\n",
    "        tot_eps = tot_eps + len(stats[\"episode_returns\"])\n",
    "        timings.time(\"learn\")\n",
    "        with lock:\n",
    "            to_log = dict(step=step)\n",
    "            to_log.update({k: stats[k] for k in stat_keys})            \n",
    "            to_log.update({\"rmean_im_episode_return\": np.average(last_im_returns) if len(last_im_returns) > 0 else 0.,\n",
    "                           \"rmean_episode_return\": np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "                           \"episode\": tot_eps})\n",
    "            plogger.log(to_log)\n",
    "            step += T * B\n",
    "\n",
    "    if i == 0:\n",
    "        logging.info(\"Batch and learn: %s\", timings.summary())\n",
    "\n",
    "for m in range(flags.num_buffers):\n",
    "    free_queue.put(m)\n",
    "\n",
    "threads = []\n",
    "for i in range(flags.num_learner_threads):\n",
    "    thread = threading.Thread(\n",
    "        target=batch_and_learn, name=\"batch-and-learn-%d\" % i, args=(i,)\n",
    "    )\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "def checkpoint():\n",
    "    if flags.disable_checkpoint:\n",
    "        return\n",
    "    logging.info(\"Saving checkpoint to %s\", checkpointpath)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": actor_net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"flags\": vars(flags),\n",
    "        },\n",
    "        checkpointpath,\n",
    "    )\n",
    "\n",
    "timer = timeit.default_timer\n",
    "try:\n",
    "    last_checkpoint_time = timer()\n",
    "    while step < flags.total_steps:\n",
    "        start_step = step\n",
    "        start_time = timer()\n",
    "        time.sleep(5)\n",
    "\n",
    "        if timer() - last_checkpoint_time > 10 * 60:  # Save every 10 min.\n",
    "            checkpoint()\n",
    "            last_checkpoint_time = timer()\n",
    "\n",
    "        sps = (step - start_step) / (timer() - start_time)\n",
    "        if stats.get(\"episode_returns\", None):\n",
    "            mean_return = (\n",
    "                \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n",
    "            )\n",
    "        else:\n",
    "            mean_return = \"\"\n",
    "        total_loss = stats.get(\"total_loss\", float(\"inf\"))\n",
    "\n",
    "        print_str =  \"Steps %i @ %.1f SPS. Eps %i. L400 Return %f (%f). Loss %.2f\" % (step, sps, tot_eps, \n",
    "            np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "            np.average(last_im_returns) if len(last_im_returns) > 0 else 0.,\n",
    "            total_loss)\n",
    "\n",
    "        for s in [\"max_rollout_depth\", \"pg_loss\", \"baseline_loss\", \"im_pg_loss\", \n",
    "                  \"im_baseline_loss\", \"entropy_loss\", \"reg_loss\", \"total_norm\"]:\n",
    "            if s in stats: print_str += \" %s %.2f\" % (s, stats[s])\n",
    "\n",
    "        logging.info(print_str)\n",
    "except KeyboardInterrupt:\n",
    "    for thread in threads:\n",
    "        thread.join()        \n",
    "    # Try joining actors then quit.\n",
    "else:\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    logging.info(\"Learning finished after %d steps.\", step)\n",
    "finally:\n",
    "    for _ in range(flags.num_actors):\n",
    "        free_queue.put(None)\n",
    "    for actor in actor_processes:\n",
    "        actor.join(timeout=1)\n",
    "\n",
    "checkpoint()\n",
    "plogger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a726743",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = flags.total_steps + 1\n",
    "for thread in threads:\n",
    "     thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(flags.num_actors):\n",
    "    free_queue.put(None)\n",
    "for actor in actor_processes:\n",
    "    actor.join(timeout=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b71ad",
   "metadata": {},
   "source": [
    "<font size=\"5\">Agent Debug and Visualize</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8908d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  398078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "\n",
    "def plot_obs(x, ax=None, title=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    ax.imshow(torch.swapaxes(torch.swapaxes(x[0].cpu(),0,2),0,1), interpolation='nearest', aspect=\"auto\")\n",
    "    if title is not None: ax.set_title(title)\n",
    "    \n",
    "def plot_qn_sa(q_s_a, n_s_a, max_q_s_a=None, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    xs = np.arange(len(q_s_a))\n",
    "\n",
    "    ax.bar(xs - 0.3, q_s_a.cpu(), color = 'g', width = 0.3, label=\"q_s_a\")    \n",
    "    ax_n = ax.twinx()\n",
    "    if max_q_s_a is not None:\n",
    "        ax.bar(xs, max_q_s_a.cpu(), color = 'r', width = 0.3, label=\"max_q_s_a\")        \n",
    "    ax_n.bar(xs + (0.3 if max_q_s_a is not None else 0.), \n",
    "             n_s_a.cpu(), bottom=0, color = 'b', width = 0.3, label=\"n_s_a\")\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(q_s_a))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))    \n",
    "    ax.legend(loc=\"upper left\")   \n",
    "    ax_n.legend(loc=\"upper right\") \n",
    "    ax.set_title(\"q_s_a and n_s_a\")\n",
    "    \n",
    "def plot_policies(model_logit, actor_logit, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    model_prob = torch.softmax(model_logit, dim=-1).detach().cpu().numpy()\n",
    "    prob = torch.softmax(actor_logit, dim=-1).detach().cpu().numpy()\n",
    "    ax.set_title(\"Real policy prob\")\n",
    "    xs = np.arange(len(model_prob))\n",
    "    ax.bar(xs - 0.1, model_prob, color = 'g', width = 0.1, label=\"model policy prob\")\n",
    "    ax.bar(xs, prob, color = 'r', width = 0.1, label=\"agent policy prob\")\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(model_prob))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))\n",
    "    ax.set_ylim(0, 1)        \n",
    "    ax.legend()       \n",
    "        \n",
    "def plot_im_policies(im_policy_logits, reset_policy_logits, im_action, reset_action, \n",
    "                     one_hot=True, reset_ind=0, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "        \n",
    "    rec_t, num_actions = im_policy_logits.shape\n",
    "    num_actions += 1\n",
    "    rec_t -= 1\n",
    "        \n",
    "    im_prob = torch.softmax(im_policy_logits, dim=-1).detach().cpu().numpy()\n",
    "    reset_prob = torch.softmax(reset_policy_logits, dim=-1)[:,[reset_ind]].detach().cpu().numpy()\n",
    "    im_reset_prob = np.concatenate([im_prob, reset_prob], axis=-1)\n",
    "    \n",
    "    if not one_hot: im_action = F.one_hot(im_action, num_actions - 1)\n",
    "    im_action = im_action.detach().cpu().numpy()\n",
    "    reset_action = reset_action.unsqueeze(-1).detach().cpu().numpy()    \n",
    "    im_reset_action = np.concatenate([im_action, reset_action], axis=-1)\n",
    "    \n",
    "    im_reset_prob = im_reset_prob[:-1]\n",
    "    im_reset_action = im_reset_action[:-1]\n",
    "    \n",
    "    xs = np.arange(rec_t)\n",
    "    labels = ['noop', 'up', 'down', 'left', 'right', 'reset']\n",
    "    for i in range(num_actions):        \n",
    "        c = ax.bar(xs + 0.8 * (i / num_actions), im_reset_prob[:,i], width = 0.8 / (num_actions), label=labels[i])  \n",
    "        color = c.patches[0].get_facecolor()\n",
    "        color = color[:3] + (color[3] * 0.5,)\n",
    "        ax.bar(xs + 0.8 * (i / num_actions), im_reset_action[:,i], width = 0.8 / (num_actions), color=color)\n",
    "        \n",
    "    \n",
    "    #xs = np.arange(num_actions)\n",
    "    #for i in range(rec_t):\n",
    "    #    ax.bar(xs + 0.8 * (i / rec_t), im_reset_action[i], width = 0.8 / (rec_t), color=\"#cccccc\")\n",
    "    #    ax.bar(xs + 0.8 * (i / rec_t), im_reset_prob[i], width = 0.8 / (rec_t))        \n",
    "    #ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(num_actions)))\n",
    "    #ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right', 'reset'))    \n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)   \n",
    "    ax.set_title(\"Imagainary policy prob\")\n",
    "    plt.show()         \n",
    "    \n",
    "bsz = 1\n",
    "eps_n = 500\n",
    "flags.rec_t = 10\n",
    "flags.tran_mem_n = 10\n",
    "flags.no_mem = True\n",
    "flags.stat_type = 2\n",
    "flags.reward_type = 1\n",
    "flags.discounting = 0.97\n",
    "flags.tran_dim = 64\n",
    "\n",
    "if flags.reward_type == 0:\n",
    "    flags.num_rewards = num_rewards = 1\n",
    "else:\n",
    "    flags.num_rewards = num_rewards = 2\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])   \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags))\n",
    "obs_shape = env.gym_env.observation_space.shape\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available():\n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags).to(flags.device)\n",
    "#checkpoint = torch.load(\"/home/sc/RS/thinker/logs/planner_logs/alstm_3_1_rec_n_5_rei_clip_realnomem/model.tar\")\n",
    "#checkpoint = torch.load(\"/home/sc/RS/thinker/logs/planner_logs/alstm_3_1_rec_n_10_rei_aug_clip_realnomem/model.tar\")\n",
    "#checkpoint = torch.load(\"/home/sc/RS/thinker/logs/planner_logs/alstm_3_1_rec_n_10_rei_aug2_clip_realnomem_large/model.tar\")\n",
    "#checkpoint = torch.load(\"/home/sc/RS/thinker/logs/planner_logs/past/rei_rec_n_10_tem_4_base_1/model.tar\")\n",
    "checkpoint = torch.load(\"/home/sc/RS/thinker/logs/planner_logs/alstm_3_1_rec_n_10_rei_aug2_clip_mem0_imcost_0.5/model.tar\")\n",
    "\n",
    "\n",
    "actor_net.load_state_dict(checkpoint[\"model_state_dict\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb82bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "debug = True\n",
    "eps_n = 10\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "model.train(False)\n",
    "env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True), model=model, flags=flags)\n",
    "env = Environment(env)\n",
    "\n",
    "core_state = actor_net.initial_state(bsz)\n",
    "core_state = tuple(v.to(flags.device) for v in core_state)\n",
    "returns = []\n",
    "obs = env.initial()\n",
    "\n",
    "t = 0\n",
    "im_list = [\"im_policy_logits\", \"reset_policy_logits\", \"im_action\", \"reset_action\"]\n",
    "im_dict = {k: [] for k in im_list}\n",
    "while(True):\n",
    "    if len(returns) > (0 if debug else eps_n): break    \n",
    "    with torch.no_grad():\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}                             \n",
    "        actor_out, core_state = actor_net(obs, core_state, debug=False)\n",
    "        action = torch.cat([actor_out['action'][0].unsqueeze(-1), \n",
    "                            actor_out['im_action'][0].unsqueeze(-1), \n",
    "                            actor_out['reset_action'][0].unsqueeze(-1)], dim=-1)\n",
    "        for k in im_list: im_dict[k].append(actor_out[k][0,0].unsqueeze(0))        \n",
    "        if debug and obs[\"cur_t\"][0,0] == (flags.rec_t - 1): \n",
    "            for k in im_list: im_dict[k] = torch.concat(im_dict[k], dim=0)\n",
    "            fig, axs = plt.subplots(1, 4, figsize=(24,6))  \n",
    "            ret_dict = env.gym_env.ret_dict\n",
    "            title = \"step: %d; values: %.4f\" % (t, ret_dict[\"v0\"][0].cpu())\n",
    "            if flags.reward_type == 1: title += \" im_return: %.4f\" % cur_returns[..., 1]\n",
    "            plot_obs(env.gym_env.x/255, axs[0], title=title)          \n",
    "            max_q_s_a = ret_dict[\"max_q_s_a\"][0] if \"max_q_s_a\" in ret_dict else None\n",
    "            plot_qn_sa(ret_dict[\"q_s_a\"][0], ret_dict[\"n_s_a\"][0], max_q_s_a, ax=axs[2]) \n",
    "            plot_policies(ret_dict[\"logit0\"][0], actor_out[\"policy_logits\"][0,0], axs[3])    \n",
    "            plot_im_policies(**im_dict, one_hot=False, reset_ind=1, ax=axs[1])    \n",
    "            plt.show()\n",
    "            im_dict = {k: [] for k in im_list}\n",
    "        obs = env.step(action.unsqueeze(0))        \n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "    t += 1\n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "        np.average(returns), np.std(returns) / np.sqrt(len(returns))))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = True\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "\n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "                model=model, rec_t=flags.rec_t, flags=flags))\n",
    "obs = env.initial()\n",
    "\n",
    "obs_shape, num_actions = env.gym_env.observation_space.shape, env.gym_env.action_space.n   \n",
    "num_actions = env.gym_env.action_space.n\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags)\n",
    "core_state = actor_net.initial_state(1)\n",
    "\n",
    "for t in range(10):\n",
    "  actor_out, core_state = actor_net(obs, core_state=core_state)\n",
    "  action = torch.cat([actor_out['action'], actor_out['im_action'], actor_out['reset_action']], dim=-1)\n",
    "  obs = env.step(action.unsqueeze(0))\n",
    "  print(t, obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f6370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_actions = 4    \n",
    "root = Node(parent=None, action=2, logit=torch.tensor([0.3]), num_actions=num_actions, discounting=0.97)\n",
    "root.expand(r=torch.tensor([3.]), v=torch.tensor([2.]), logits=torch.tensor([0.3,0.4,0.5,0.6]), encoded=None)\n",
    "root.visit()\n",
    "cur_node = root\n",
    "print(1, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[3]\n",
    "cur_node.expand(r=torch.tensor([1.]), v=torch.tensor([4.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "\n",
    "print(2, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[2]\n",
    "cur_node.expand(r=torch.tensor([100.]), v=torch.tensor([0.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "print(3, root.rollout_qs, root.stat())\n",
    "\n",
    "root.visit()\n",
    "cur_node = root\n",
    "print(4, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[3]\n",
    "#cur_node.expand(r=torch.tensor([1.]), v=torch.tensor([3.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "\n",
    "print(5, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[1]\n",
    "cur_node.expand(r=torch.tensor([-10.]), v=torch.tensor([0.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "print(6, root.rollout_qs, root.stat())\n",
    "\n",
    "flags.rec_t = 20\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n     \n",
    "    \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags))\n",
    "_ = env.initial()\n",
    "\n",
    "act_seqs = [[2, 1, 1], [2, 2, 1], [2, 3, 1], [2, 4, 1], [2, 1, 0], [2, 2, 1]]\n",
    "#act_seqs = [[2, 4, 0], [2, 1, 1], [2, 4, 0], [2, 2, 1], [2, 2, 1]]\n",
    "\n",
    "for a in act_seqs:\n",
    "    obs = env.step(torch.tensor(a).unsqueeze(0).unsqueeze(0))\n",
    "    x = obs['frame'][0, 0, :, 0, 0]\n",
    "    r = obs['reward'][0, 0, :]\n",
    "    print(\"================================================\")\n",
    "    print(\"a\", a[1:])\n",
    "    print(\"x\", x)\n",
    "    print(\"r\", r)\n",
    "    print(\"rollout_qs\", torch.concat(env.gym_env.root_node.rollout_qs) / flags.discounting)\n",
    "    print(\"root stat\", env.gym_env.root_node.ret_dict)\n",
    "    print(\"last stat\", env.gym_env.last_node.ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c57f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class ModelWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, model, flags):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.env = env\n",
    "        self.model = model                \n",
    "        self.rec_t = flags.rec_t        \n",
    "        self.discounting = flags.discounting\n",
    "        self.stat_type = flags.stat_type    \n",
    "        self.reward_type = flags.reward_type    \n",
    "        self.no_mem = flags.no_mem\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        # 0 for the most basic; \n",
    "        # 1 for augmented input in root stat; \n",
    "        # 2 for tree stat\n",
    "        if self.stat_type == 0:\n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 5 + num_actions * 4 + self.rec_t\n",
    "        elif self.stat_type == 1:            \n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 7 + num_actions * 7 + self.rec_t\n",
    "        elif self.stat_type == 1.5:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 6 + num_actions * 10 + self.rec_t             \n",
    "        elif self.stat_type == 2:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 9 + num_actions * 10 + self.rec_t         \n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "          low=-np.inf, high=np.inf, shape=(obs_n, 1, 1), dtype=float)\n",
    "        self.model.train(False)        \n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        x = self.env.reset()\n",
    "        self.cur_t = 0        \n",
    "        out = self.use_model_raw(x, 0., 0, self.cur_t, 1.)\n",
    "        out_ = self.use_model_tree(x, 0., 0, self.cur_t, 1.)\n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q\n",
    "        self.max_rollout_depth = 0.\n",
    "        return out, out_\n",
    "    \n",
    "    def step(self, action):  \n",
    "        re_action, im_action, reset = action\n",
    "        info = {}\n",
    "        info[\"max_rollout_depth\"] = self.max_rollout_depth\n",
    "        if self.cur_t < self.rec_t - 1:\n",
    "          self.cur_t += 1\n",
    "          out = self.use_model_raw(None, None, im_action, self.cur_t, reset)      \n",
    "          out_ = self.use_model_tree(None, None, im_action, self.cur_t, reset)      \n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([0.])\n",
    "          else:\n",
    "            r = np.array([0., (self.root_max_q - self.last_root_max_q).item()], dtype=np.float32)\n",
    "          done = False\n",
    "          info['cur_t'] = self.cur_t   \n",
    "        else:\n",
    "          self.cur_t = 0\n",
    "          x, r, done, info_ = self.env.step(re_action)                    \n",
    "          out = self.use_model_raw(x, r, re_action, self.cur_t, 1.) \n",
    "          out_ = self.use_model_tree(x, r, re_action, self.cur_t, 1.) \n",
    "          info.update(info_)\n",
    "          info['cur_t'] = self.cur_t\n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([r])\n",
    "          else:\n",
    "            r = np.array([r, 0.], dtype=np.float32)   \n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q        \n",
    "        return out, out_, r, done, info        \n",
    "        \n",
    "    def use_model_raw(self, x, r, a, cur_t, reset):\n",
    "        # input: \n",
    "        # r: reward - [,]; x: frame - [C, H, W]; a: action - [,]\n",
    "        # cur_t: int; reset at cur_t == 0  \n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:                \n",
    "                self.rollout_depth_ = 0.\n",
    "                if self.no_mem:\n",
    "                    self.re_action = F.one_hot(torch.zeros(1, dtype=torch.long), self.num_actions)   \n",
    "                else:\n",
    "                    self.re_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)                   \n",
    "                \n",
    "                x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = x\n",
    "                _, vs, logits, encodeds = self.model(x, self.re_action.unsqueeze(0), one_hot=True)                \n",
    "                self.encoded = encodeds[-1]    \n",
    "                self.encoded_reset = self.encoded.clone()\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    self.re_reward = torch.tensor([[0.]], dtype=torch.float32)                \n",
    "                else:\n",
    "                    self.re_reward = torch.tensor([[r]], dtype=torch.float32)                \n",
    "                    \n",
    "                self.v0 = vs[-1].unsqueeze(-1).clone()\n",
    "                self.logit0 = logits[-1].clone()\n",
    "                \n",
    "                self.im_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.im_reset = torch.tensor([[1.]], dtype=torch.float32)\n",
    "                self.im_reward = torch.zeros(1, 1, dtype=torch.float32)                                \n",
    "                self.v = vs[-1].unsqueeze(-1)\n",
    "                self.logit = logits[-1]\n",
    "                self.rollout_first_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                self.rollout_return = torch.zeros(1, 1, dtype=torch.float32)                \n",
    "                self.q_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.n_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)                \n",
    "            else:\n",
    "                self.rollout_depth_ += 1                \n",
    "                \n",
    "                self.im_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)   \n",
    "                rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "                   self.im_action.unsqueeze(0), one_hot=True)\n",
    "                self.encoded = encodeds[-1]        \n",
    "                \n",
    "                self.im_reward = rs[-1].unsqueeze(-1)\n",
    "                self.v = vs[-1].unsqueeze(-1)    \n",
    "                self.logit = logits[-1]     \n",
    "                \n",
    "                if self.im_reset: \n",
    "                    # last action's reset is true; re-initialize everything                    \n",
    "                    self.rollout_first_action = self.im_action.clone()\n",
    "                    self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                    self.rollout_depth_ = 1                      \n",
    "                    \n",
    "                self.rollout_return_wo_v += (self.discounting ** (self.rollout_depth_-1)) * self.im_reward\n",
    "                self.rollout_return = self.rollout_return_wo_v + (\n",
    "                    self.discounting ** (self.rollout_depth_)) * self.v   \n",
    "                    \n",
    "                self.im_reset = torch.tensor([[reset]], dtype=torch.float32)\n",
    "                if self.im_reset:                    \n",
    "                    rollout_first_action_label = torch.argmax(self.rollout_first_action, dim=1)                    \n",
    "                    q = self.q_s_a[:, rollout_first_action_label]\n",
    "                    n = self.n_s_a[:, rollout_first_action_label]                    \n",
    "                    ret = self.rollout_return[:, 0]\n",
    "                    self.n_s_a[:, rollout_first_action_label] += 1                    \n",
    "                    self.q_s_a[:, rollout_first_action_label] = (n * q) / (n + 1) + ret / (n + 1)\n",
    "                    \n",
    "        time = F.one_hot(torch.tensor([cur_t]).long(), self.rec_t)\n",
    "        depc = torch.tensor([[self.discounting ** (self.rollout_depth_-1)]])\n",
    "        ret_dict = {\"re_action\": self.re_action,\n",
    "                    \"re_reward\": self.re_reward,\n",
    "                    \"v0\": self.v0,\n",
    "                    \"logit0\": self.logit0,\n",
    "                    \"im_action\": self.im_action,\n",
    "                    \"im_reset\": self.im_reset,\n",
    "                    \"im_reward\": self.im_reward,\n",
    "                    \"v\": self.v,\n",
    "                    \"logit\": self.logit,\n",
    "                    \"rollout_first_action\": self.rollout_first_action,\n",
    "                    \"rollout_return\": self.rollout_return,\n",
    "                    \"n_s_a\": self.n_s_a / self.rec_t,\n",
    "                    \"q_s_a\": self.q_s_a,\n",
    "                    \"time\": time,\n",
    "                    \"depc\": depc}        \n",
    "        self.ret_dict = ret_dict\n",
    "        if self.stat_type == 1:\n",
    "            out = torch.concat(list(ret_dict.values()), dim=-1)   \n",
    "        else:\n",
    "            core_inputs = [\"re_action\", \"re_reward\", \"v0\", \"logit0\", \"im_action\",\n",
    "                           \"im_reset\", \"im_reward\", \"v\", \"logit\", \"time\"]\n",
    "            out = torch.concat([ret_dict[v] for v in core_inputs], dim=-1)   \n",
    "        self.encoded = reset * self.encoded_reset + (1 - reset) * self.encoded\n",
    "        return out[0]      \n",
    "    \n",
    "    def use_model_tree(self, x, r, a, cur_t, reset):\n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:\n",
    "                self.rollout_depth = 0.\n",
    "                self.max_rollout_depth = 0.\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    re_action = 0\n",
    "                    re_reward = torch.tensor([0.], dtype=torch.float32)                \n",
    "                else:\n",
    "                    re_action = a                \n",
    "                    re_reward = torch.tensor([r], dtype=torch.float32)                \n",
    "                \n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = x_tensor\n",
    "                a_tensor = F.one_hot(torch.tensor(re_action, dtype=torch.long).unsqueeze(0), self.num_actions)\n",
    "                \n",
    "                _, vs, logits, encodeds = self.model(x_tensor, a_tensor.unsqueeze(0), one_hot=True)\n",
    "                \n",
    "                self.root_node = Node(parent=None, action=re_action, logit=None, \n",
    "                                      num_actions=self.num_actions,\n",
    "                                      discounting=self.discounting,\n",
    "                                      rec_t=self.rec_t)\n",
    "                self.root_node.expand(r=re_reward, v=vs[-1, 0].unsqueeze(-1), logits=logits[-1, 0],\n",
    "                                      encoded=encodeds[-1])\n",
    "                self.root_node.visit()\n",
    "                self.cur_node = self.root_node\n",
    "                self.rollout_first_action_ = torch.zeros(self.num_actions)\n",
    "            else:\n",
    "                if self.rollout_depth == 0:\n",
    "                    self.rollout_first_action_ = F.one_hot(torch.tensor(a, dtype=torch.long), self.num_actions)\n",
    "                self.rollout_depth += 1    \n",
    "                self.max_rollout_depth = max(self.max_rollout_depth, self.rollout_depth)\n",
    "                next_node = self.cur_node.children[a]\n",
    "                if not next_node.expanded():\n",
    "                    a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions) \n",
    "                    rs, vs, logits, encodeds = self.model.forward_encoded(self.cur_node.encoded, \n",
    "                        a_tensor.unsqueeze(0), one_hot=True)\n",
    "                    next_node.expand(r=rs[-1, 0].unsqueeze(-1), v=vs[-1, 0].unsqueeze(-1), \n",
    "                                     logits=logits[-1, 0], encoded=encodeds[-1])\n",
    "                next_node.visit()\n",
    "                self.cur_node = next_node\n",
    "            \n",
    "            root_node_stat = self.root_node.stat()\n",
    "            cur_node_stat = self.cur_node.stat()                        \n",
    "            reset = torch.tensor([reset], dtype=torch.float32)\n",
    "            time = F.one_hot(torch.tensor(cur_t).long(), self.rec_t)\n",
    "            depc = torch.tensor([self.discounting ** (self.rollout_depth-1)])\n",
    "            \n",
    "            root_trail_r = self.root_node.trail_r / self.discounting\n",
    "            root_rollout_q = self.root_node.rollout_q / self.discounting\n",
    "            root_max_q = torch.max(torch.concat(self.root_node.rollout_qs)).unsqueeze(-1) / self.discounting\n",
    "            \n",
    "            ret_list = [root_node_stat, cur_node_stat, reset, time, depc,]\n",
    "            if self.stat_type >= 2: ret_list.extend([root_trail_r, root_rollout_q, root_max_q])            \n",
    "                \n",
    "            if self.stat_type == 1:\n",
    "                ret_list = [self.root_node.ret_dict[\"action\"],\n",
    "                            self.root_node.ret_dict[\"r\"],\n",
    "                            self.root_node.ret_dict[\"v\"],\n",
    "                            self.root_node.ret_dict[\"child_logits\"],\n",
    "                            self.cur_node.ret_dict[\"action\"],\n",
    "                            reset,\n",
    "                            self.cur_node.ret_dict[\"r\"],\n",
    "                            self.cur_node.ret_dict[\"v\"],\n",
    "                            self.cur_node.ret_dict[\"child_logits\"],\n",
    "                            self.rollout_first_action_,\n",
    "                            self.root_node.rollout_q / self.discounting,\n",
    "                            self.root_node.ret_dict[\"child_rollout_ns\"],\n",
    "                            self.root_node.ret_dict[\"child_rollout_qs_mean\"],\n",
    "                            time,\n",
    "                            depc                            \n",
    "                            ]                 \n",
    "                \n",
    "                \n",
    "            out = torch.concat(ret_list, dim=-1)            \n",
    "            self.last_node = self.cur_node     \n",
    "            \n",
    "            self.root_max_q = root_max_q\n",
    "            self.ret_dict = {\"v0\": self.root_node.ret_dict[\"v\"].unsqueeze(0),\n",
    "                             \"q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_mean\"].unsqueeze(0),\n",
    "                             \"max_q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_max\"].unsqueeze(0),\n",
    "                             \"n_s_a\": self.root_node.ret_dict[\"child_rollout_ns\"].unsqueeze(0),\n",
    "                             \"logit0\": self.root_node.ret_dict[\"child_logits\"].unsqueeze(0),}\n",
    "            \n",
    "            if reset:\n",
    "                self.rollout_depth = 0\n",
    "                self.cur_node = self.root_node\n",
    "                self.cur_node.visit()\n",
    "                \n",
    "            return out\n",
    "        \n",
    "flags.stat_type = 1\n",
    "flags.rec_t = 10\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n         \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags)\n",
    "\n",
    "_ = env.reset()\n",
    "\n",
    "def debug_out(out, num_actions, rec_t):\n",
    "    struct = [[\"a0\", num_actions], \n",
    "        [\"r0\", 1], \n",
    "        [\"v0\", 1], \n",
    "        [\"pi0\", num_actions],\n",
    "        [\"a\", num_actions],\n",
    "        [\"reset\", 1],\n",
    "        [\"r\", 1],\n",
    "        [\"v\", 1],\n",
    "        [\"pi\",num_actions],\n",
    "        [\"first_a\", num_actions],\n",
    "        [\"rollout_v\", 1],\n",
    "        [\"ns\", num_actions],\n",
    "        [\"qs\", num_actions],\n",
    "        [\"t\", rec_t],\n",
    "        [\"depc\", 1]]\n",
    "    m = 0\n",
    "    for key, n in struct:\n",
    "        print(key, out[m:m+n])\n",
    "        m = m + n\n",
    "\n",
    "out, out_, r, done, info = env.step([1, 1, 0])\n",
    "out, out_, r, done, info = env.step([1, 2, 0])\n",
    "out, out_, r, done, info = env.step([1, 3, 1])\n",
    "out, out_, r, done, info = env.step([1, 1, 0])\n",
    "out, out_, r, done, info = env.step([1, 2, 1])\n",
    "out, out_, r, done, info = env.step([1, 1, 1])\n",
    "\n",
    "print(\"error:\", torch.sum((out - out_)**2))\n",
    "print(\"old\")\n",
    "debug_out(out, 5, 10)\n",
    "print(\"new\")\n",
    "debug_out(out_, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3228abc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flags.perfect_model = True\n",
    "flags.stat_type = 2\n",
    "flags.reward_type = 1\n",
    "flags.rec_t = 5\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n         \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags)\n",
    "env = Environment(env)\n",
    "obs = env.initial()\n",
    "plot_obs(env.gym_env.x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d794525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5374])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.gym_env.cur_node.v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba725051",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========0========\n",
      "eps_step tensor([[26]], dtype=torch.int32)\n",
      "eps_return tensor([[[-0.0500,  0.0000]]])\n",
      "reward tensor([[[0., 0.]]])\n",
      "cur_t tensor([[1]])\n",
      "=========1========\n",
      "eps_step tensor([[27]], dtype=torch.int32)\n",
      "eps_return tensor([[[-0.0500,  0.0635]]])\n",
      "reward tensor([[[0.0000, 0.0635]]])\n",
      "cur_t tensor([[2]])\n",
      "=========2========\n",
      "eps_step tensor([[28]], dtype=torch.int32)\n",
      "eps_return tensor([[[-0.0500,  0.0635]]])\n",
      "reward tensor([[[0., 0.]]])\n",
      "cur_t tensor([[3]])\n",
      "=========3========\n",
      "eps_step tensor([[29]], dtype=torch.int32)\n",
      "eps_return tensor([[[-0.0500,  0.0635]]])\n",
      "reward tensor([[[0., 0.]]])\n",
      "cur_t tensor([[4]])\n",
      "=========4========\n",
      "eps_step tensor([[30]], dtype=torch.int32)\n",
      "eps_return tensor([[[0.9400, 0.0635]]])\n",
      "reward tensor([[[0.9900, 0.0000]]])\n",
      "cur_t tensor([[0]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGgCAYAAAAKKQXsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAApq0lEQVR4nO3df3DV1Z3/8ddV4JJgclsV7s0dAqY2tChCEWwkuiYdTRyWOmXouLWxLtapCwVbsmwHTfh+x9sOvaHMNIP7daUFHYi62ewfKsuuVRJaDTjUlUVRGpxIh6zGH9eoxSRCSATO9w+Xu1yTzzWf5Obk3s99PmbujDmfz/28z7mfNHn1cPI5PmOMEQAAgCUXjHcHAABAdiF8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKvGLHw89NBDKioq0uTJk7VgwQLt27dvrEoBAIAMMmEsLvqv//qvqq6u1kMPPaTrrrtOv/3tb7V48WIdOXJEM2bMSPres2fP6t1331VeXp58Pt9YdA8AAKSYMUa9vb0Kh8O64ILkcxu+sdhYrqSkRFdffbW2bNkSb5s9e7aWLl2qurq6pO99++23VVhYmOouAQAACzo7OzV9+vSk56R85mNgYEAHDx7Ufffdl9BeWVmp/fv3Dzq/v79f/f398a/PZaGHr5Fyh9u7q250Pnb498O8yAhrpOr61Bj99W3UyKTPySs1uN/ZVYP7nbE1Tp6WfnRAysvL+8LLpjx8fPjhhzpz5oyCwWBCezAYVCwWG3R+XV2dfv7znw9qz53gInxMSnJiqkboVCOVnyA1Rnd9GzUy6XPySg3ud3bV4H5nfI3hLJkYswWnny9ujBmyQzU1Neru7o6/Ojs7x6pLAAAgDaR85uPSSy/VhRdeOGiWo6ura9BsiCT5/X75/f5UdwMAAKSplM98TJo0SQsWLFBLS0tCe0tLi0pLS1NdDgAAZJgx+VPbtWvX6o477tDChQu1aNEibd26VW+99ZZWrlw5FuUAAEAGGZPw8b3vfU8fffSRfvGLX+i9997TnDlz9Lvf/U4zZ84ci3IAACCDjMlzPkajp6dHgUBAjQ9ElZszeby7AwAAhuFk3ylVralVd3e38vPzk57L3i4AAMAqwgcAALCK8AEAAKwifAAAAKvG5K9dUqKpdnDv5t089Lmv7k5dXWqM7vo2amTS5+SVGtzv7KrB/c6uGqm636eHfyozHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACr2NUWAACMGrvaAgCAtEX4AAAAVhE+AACAVYQPAABgVWbtauuEHRizqwb3O7tqcL+zqwb3O3NrsKstAABIV4QPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFal70PGPGDpPpdv2JfCB8OkqMb1kyYN3T5xovObXn7B4cAUV7XdX1964dOh++vEcRxjPYYkNZzG8MLAgPsaafg9lXbXp0Z61RjB9Xf+1Rj0A2OKmQ8AAGAV4QMAAFhF+AAAAFYRPgAAgFUsOEVSTgsyN544Ybknw3PfFHcLQtNxHE5jGNGCUwBIQ8x8AAAAqwgfAADAKtfhY+/evbrlllsUDofl8/m0c+fOhOPGGEUiEYXDYeXk5Ki8vFxtbW2p6i8AAMhwrtd8nDhxQvPmzdMPf/hDffe73x10fNOmTaqvr9eOHTs0a9YsbdiwQRUVFWpvb1deXt7wC90WlXImu+3eYHNvHv01Rnr9fWvHtnaGud6h3fmRYenHaQxSZo0D8JQ76lNznbH+feH1Gn2npD/WDuvtrsPH4sWLtXjx4iGPGWO0efNmrV+/XsuWLZMkNTQ0KBgMqrGxUStWrHBbDgAAeExK13x0dHQoFoupsrIy3ub3+1VWVqb9+/cP+Z7+/n719PQkvAAAgHelNHzEYjFJUjAYTGgPBoPxY59XV1enQCAQfxUWFqaySwAAIM2MyV+7+Hy+hK+NMYPazqmpqVF3d3f81dnZORZdAgAAaSKlDxkLhUKSPpsBKSgoiLd3dXUNmg05x+/3y+/3Dz7QVDu4d/McFtG8msJdFm3U8DinRZlPX3rpkO37enuHbP92f3+KeuSe2zFI6TkOICs85rC43yu/M8a6htP13dY4PfxTUzrzUVRUpFAopJaWlnjbwMCAWltbVVpamspSAAAgQ7me+fjkk0/05z//Of51R0eHDh06pIsvvlgzZsxQdXW1otGoiouLVVxcrGg0qtzcXFVVVaW04wAAIDO5Dh//9V//pW9961vxr9eu/Wy6a/ny5dqxY4fWrVunvr4+rVq1SsePH1dJSYmam5vdPeMDAAB4luvwUV5eLmOM43Gfz6dIJKJIJDKafiGDOT1sy2lNxMY0XBPhdgxSeo4DANIRe7sAAACrCB8AAMAqwgcAALCK8AEAAKxK6UPGUsrNrrbpuksgu9om8MLDtrwwBsBz3O5qm66/MzK9hotdbZn5AAAAVhE+AACAVYQPAABgFeEDAABYlb4LTofa1dZJqnbkG0kNdrsFgPHltKutEy/sRJuONcZrV1sAAIAvQvgAAABWET4AAIBV6bvmA2nhhU8/HbL9vilTLPdkeJz66yQdx+F2DACQaZj5AAAAVhE+AACAVYQPAABgFeEDAABYxYJTJLXthwNDH5jk0C5Jc8emL3GvOx9afmro9tkPDd3+wkCScaSZv4x3B1LkYof2qXfnDdl+4mXne3TyoLtdhnMX+Idsn3L1pCHbP9jW6+r6GB9L97l8wz4LD4d0WWPnX41RP9IUMx8AAMAqwgcAALCK8AEAAKwifAAAAKtYcIrkJju0h5K8x2lFYXCUfTnn/STHLkxRjQzzfp3DgVkO7am6F5L0vEOJ/+PuMk6LO2e/FHZ3oRF4/ZvvjnkNAP+LmQ8AAGAV4QMAAFhF+AAAAFal75qP26JSjtOCAxfm3jz6a4z0+vvWjm1tG+Y4tE9N8p7nHNq/5dB+qUO704ODnPokSR8kOeZlsx3ax/peSMnvhwszfzN08Q8tPOjLqfabKz8c89qAJOmO+tRda6x/7znV6Dsl/bF2WG9n5gMAAFhF+AAAAFYRPgAAgFWEDwAAYFX6Ljhtqh3cu3kOi2heTeEOhTZqeIHTwkRJusGh/fcurzVvBLWzdcGp02ficC8mPebuOgOLR1DbgdPizlyHnWWd2m1w6qvEYlSk2GNJ/kBhrH8vOV3fbY3Twz+VmQ8AAGAV4QMAAFjlKnzU1dXpmmuuUV5enqZNm6alS5eqvb094RxjjCKRiMLhsHJyclReXq62traUdhoAAGQuV2s+WltbtXr1al1zzTU6ffq01q9fr8rKSh05ckRTpkyRJG3atEn19fXasWOHZs2apQ0bNqiiokLt7e3Ky8sbk0EgzaRy0zKMThrei/Fcw+FWJvUVyCSuwsezzz6b8PX27ds1bdo0HTx4UDfccIOMMdq8ebPWr1+vZcuWSZIaGhoUDAbV2NioFStWpK7nAAAgI41qzUd3d7ck6eKLP9tDvaOjQ7FYTJWVlfFz/H6/ysrKtH///iGv0d/fr56enoQXAADwrhGHD2OM1q5dq+uvv15z5ny2uUMsFpMkBYOJc73BYDB+7PPq6uoUCATir8LCwpF2CQAAZIARh4977rlHr732mv7lX/5l0DGfz5fwtTFmUNs5NTU16u7ujr86OztH2iUAAJABRvSQsZ/85CfatWuX9u7dq+nTp8fbQ6GQpM9mQAoKCuLtXV1dg2ZDzvH7/fL7/YMPuNnVdrx28PsiXtjV1uk5Su8nec8bDu03uqztdkfWbOZ0PxzuxcAdLq/vdC8k7geQCiPZ1TbdfveN1a62xhjdc889evLJJ/WHP/xBRUVFCceLiooUCoXU0tISbxsYGFBra6tKS0vdlAIAAB7lauZj9erVamxs1L/9278pLy8vvo4jEAgoJydHPp9P1dXVikajKi4uVnFxsaLRqHJzc1VVVTUmAwAAAJnFVfjYsmWLJKm8vDyhffv27brzzjslSevWrVNfX59WrVql48ePq6SkRM3NzTzjAwAASHIZPowxX3iOz+dTJBJRJBIZaZ8AAICHZdautk5StSPfSGp4fbfbof9CWno3yXucFpa63P3UcXfcZIsfs9XzDu1jfS8k1/fj9W8O/c0z9e6hZ0dzFwyxIP1/nDzY76q207WcrvPBtl5X1wdGLNmutk7Sbad3drUFAADpivABAACsInwAAACr0nfNB9LDKYf2ZM9/+8Bleyo59dfjgv/P4YBTexpyWl8xdQTvceJ0LdZ2AHYx8wEAAKwifAAAAKsIHwAAwCrCBwAAsIoFp2No51+5fEO6PTBGUvuREdQ4M8bjSPZQuSND1/DCvbh4X+pKZ5JULgZN5bW88D21NEu/pzD+mPkAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWOUzxpjx7sT5enp6FAgE1PhAVLk5ybZOBbLL0r9bO95dwHl2bq0f7y6MGt9T6cML308n+06pak2turu7lZ+fn/RcZj4AAIBVhA8AAGAV4QMAAFhF+AAAAFal7662TbWDe5eGu0JmbY1kO8uOdY1M+pxs1YB9jzks1uR7CiPh9P0kZc7P89PDP5WZDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBV7GoLZAh2IE0vXtiFlO+p9OGF7yd2tQUAAGmL8AEAAKwifAAAAKtchY8tW7Zo7ty5ys/PV35+vhYtWqRnnnkmftwYo0gkonA4rJycHJWXl6utrS3lnQYAAJnL1a6206dP18aNG/XVr35VktTQ0KDvfOc7euWVV3TllVdq06ZNqq+v144dOzRr1ixt2LBBFRUVam9vV15enrueDbWrrRN2WM2uGtxvpINku5AOhe8pJOP2+0lKv++psdrV9pZbbtFf//Vfa9asWZo1a5Z++ctf6qKLLtKLL74oY4w2b96s9evXa9myZZozZ44aGhp08uRJNTY2uikDAAA8bMRrPs6cOaOmpiadOHFCixYtUkdHh2KxmCorK+Pn+P1+lZWVaf/+/Y7X6e/vV09PT8ILAAB4l+vwcfjwYV100UXy+/1auXKlnnrqKV1xxRWKxWKSpGAwmHB+MBiMHxtKXV2dAoFA/FVYWOi2SwAAIIO4Dh9f+9rXdOjQIb344ov68Y9/rOXLl+vIkSPx4z6fL+F8Y8ygtvPV1NSou7s7/urs7HTbJQAAkEFcLTiVpEmTJsUXnC5cuFAHDhzQAw88oHvvvVeSFIvFVFBQED+/q6tr0GzI+fx+v/x+v9tuAACADDXq53wYY9Tf36+ioiKFQiG1tLTEjw0MDKi1tVWlpaWjLQMAADzC1cxHbW2tFi9erMLCQvX29qqpqUnPP/+8nn32Wfl8PlVXVysajaq4uFjFxcWKRqPKzc1VVVXVWPUfAABkGFfh4/3339cdd9yh9957T4FAQHPnztWzzz6riooKSdK6devU19enVatW6fjx4yopKVFzc7P7Z3wAAADPchU+HnnkkaTHfT6fIpGIIpHIaPoEpNzSfS7fsM/CQ5ls1EhDr69wODDJoX3uWPXkPK87tJ9yfsvsh1zW4HsKiGNvFwAAYBXhAwAAWEX4AAAAVhE+AACAVa4fMgYAozLZoT3k0H5xkms5P7/Qnfcd2i9M0fUBJGDmAwAAWEX4AAAAVhE+AACAVT5jjBnvTpyvp6dHgUBAjQ9ElZvj9I/DgDtL/27teHcB/+P1bQ4Hpjq0P5fkYt9yaL/Uod3pYXOzHdo/cC49+27nY4BbO7fWj3cXRu1k3ylVralVd3e38vPzk57LzAcAALCK8AEAAKwifAAAAKsIHwAAwKr0fchYU+3g3s27eehzX03hTo7UGN31bdRI5eeE9OG0SPSGJO/5vctrzXN5fpIFp0BKPZZkUXym/Dw/PfxTmfkAAABWET4AAIBVhA8AAGBV+q75AAApdZvHAUgbzHwAAACrCB8AAMAqwgcAALCK8AEAAKxK3wWnt0Wl4e5qOzfJA1JShRrpU2Mk19/HrrZp40OH9vcd2t9Icq0bXdZ22iHXaXdcwJY7RrCrbbr9PO87Jf2xdlinMvMBAACsInwAAACrCB8AAMAqwgcAALAqfRecDrWrrROv7LBKjdFdP5U1MHZiDu3vOrQnW1TqtButE6cdcp0WogK2JNvV1km6/TxnV1sAAJCuCB8AAMAqwgcAALAqfdd8APCmUw7tTs8U/CDJtZIdSwWnvgIYFWY+AACAVYQPAABgFeEDAABYNarwUVdXJ5/Pp+rq6nibMUaRSEThcFg5OTkqLy9XW1vbaPsJAAA8YsTh48CBA9q6davmzp2b0L5p0ybV19frwQcf1IEDBxQKhVRRUaHe3t5RdxaAh511eNlw4QheAEZsROHjk08+0e23365t27bpy1/+crzdGKPNmzdr/fr1WrZsmebMmaOGhgadPHlSjY2NKes0AADIXCMKH6tXr9aSJUt00003JbR3dHQoFoupsrIy3ub3+1VWVqb9+/cPea3+/n719PQkvAAAgHe5fs5HU1OTXn75ZR04cGDQsVjss00bgsFgQnswGNSbb7455PXq6ur085//3G03AABAhnI189HZ2ak1a9bo8ccf1+TJTk8Eknw+X8LXxphBbefU1NSou7s7/urs7HTTJQAAkGFczXwcPHhQXV1dWrBgQbztzJkz2rt3rx588EG1t7dL+mwGpKCgIH5OV1fXoNmQc/x+v/x+/0j6DiATOf3/lpBD+8VJrjX0jxX33ndoZ2EpMCZczXzceOONOnz4sA4dOhR/LVy4ULfffrsOHTqkr3zlKwqFQmppaYm/Z2BgQK2trSotLU155wEAQOZxNfORl5enOXPmJLRNmTJFl1xySby9urpa0WhUxcXFKi4uVjQaVW5urqqqqlLXawAAkLFSvrHcunXr1NfXp1WrVun48eMqKSlRc3Oz8vLyUl0KAABkIJ8xxox3J87X09OjQCCgxgeiys1xXtQKuLH079aOdxfwP17f5nBgqkP7c0ku9i2H9ksd2vc5tM92aE+ya+7su52PAW7t3Fo/3l0YtZN9p1S1plbd3d3Kz89Pei57uwAAAKsIHwAAwCrCBwAAsIrwAQAArEr5X7ukTFPt4N7Nu3noc1/dnbq61Bjd9W3USOXnhPThtEj0hiTv+b3La81zeX6SBadASj2WZFF8pvw8Pz38U5n5AAAAVhE+AACAVYQPAABgVfqu+QAAKXWbxwFIG8x8AAAAqwgfAADAKsIHAACwivABAACsSt8Fp7dFpeHuajs3yQNSUoUa6VNjJNffx662aeNDh/b3HdrfSHKtG13Wdtoh12l3XMCWO0awq226/TzvOyX9sXZYpzLzAQAArCJ8AAAAqwgfAADAKsIHAACwKn0XnA61q60Tr+ywSo3RXT+VNTB2Yg7t7zq0J1tU6rQbrROnHXKdFqICtiTb1dZJuv08Z1dbAACQrggfAADAKsIHAACwKn3XfADwplMO7U7PFPwgybWSHUsFp74CGBVmPgAAgFWEDwAAYBXhAwAAWEX4AAAAVrHgFFnh+kmThm6fONFyT77YC59+OnT7wIDlnoyN2b8d7x4AGG/MfAAAAKsIHwAAwCrCBwAAsIrwAQAArGLBKbKC08LSjSdOWO7JF7tvypQh272y4BQAmPkAAABWET4AAIBVrsJHJBKRz+dLeIVCofhxY4wikYjC4bBycnJUXl6utra2lHcaAABkLtdrPq688krt2bMn/vWFF14Y/+9Nmzapvr5eO3bs0KxZs7RhwwZVVFSovb1deXl57grdFpVynLa5dGHuzaO/xnhenxqpqfHy/03J5a9PcuyFlFQAkJXuqE/dtcbrZ23fKemPtcN6u+t/dpkwYYJCoVD8NXXqVEmfzXps3rxZ69ev17JlyzRnzhw1NDTo5MmTamxsdFsGAAB4lOvwcfToUYXDYRUVFem2227TsWPHJEkdHR2KxWKqrKyMn+v3+1VWVqb9+/c7Xq+/v189PT0JLwAA4F2uwkdJSYkeffRR7d69W9u2bVMsFlNpaak++ugjxWIxSVIwGEx4TzAYjB8bSl1dnQKBQPxVWFg4gmEAAIBM4Sp8LF68WN/97nd11VVX6aabbtLTTz8tSWpoaIif4/P5Et5jjBnUdr6amhp1d3fHX52dnW66BAAAMsyoHjI2ZcoUXXXVVTp69KiWLl0qSYrFYiooKIif09XVNWg25Hx+v19+v3/wgabawb2b57CI5tXdLnueBDVGd30bNUZ0/aEf3OXEaWHp05de6viefb29Q7Z/u7/fVW0AWeixtc7HMuXn+enhnzqq53z09/fr9ddfV0FBgYqKihQKhdTS0hI/PjAwoNbWVpWWlo6mDAAA8BBXMx8/+9nPdMstt2jGjBnq6urShg0b1NPTo+XLl8vn86m6ulrRaFTFxcUqLi5WNBpVbm6uqqqqxqr/AAAgw7gKH2+//ba+//3v68MPP9TUqVN17bXX6sUXX9TMmTMlSevWrVNfX59WrVql48ePq6SkRM3Nze6f8QEAADzLVfhoampKetzn8ykSiSgSiYymT8C4c3pgmNO6DknayNoOABgW9nYBAABWET4AAIBVhA8AAGAV4QMAAFg1qoeMjSk3u9p6fUdWaoz++ina1ZYHhgEYEyPZ1Tbdfp6P5a62AAAAo0H4AAAAVhE+AACAVYQPAABgVfouOB1qV1snGbfDKjXG5PpJa7jb1RYArEq2q62TdPt5bmtXWwAAALcIHwAAwCrCBwAAsCp913wAKfTCp58O2X7flPRbC+LUVwDwCmY+AACAVYQPAABgFeEDAABYRfgAAABWseAUWeFnJQMORxzax/HhPdc61PhZCmt4/6FyKaqRSZ+TV2pwv7MCMx8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAq3zGGDPenThfT0+PAoGAGh+IKjdn8nh3BwAADMPJvlOqWlOr7u5u5efnJz2XmQ8AAGAV4QMAAFhF+AAAAFYRPgAAgFXpu6ttU+3g3nllJ0Iv1GDnyeyqwf3Orhrc7+yqkar7fXr4pzLzAQAArCJ8AAAAq1yHj3feeUc/+MEPdMkllyg3N1ff+MY3dPDgwfhxY4wikYjC4bBycnJUXl6utra2lHYaAABkLlfh4/jx47ruuus0ceJEPfPMMzpy5Ih+/etf60tf+lL8nE2bNqm+vl4PPvigDhw4oFAopIqKCvX29qa67wAAIAO5WnD6q1/9SoWFhdq+fXu87bLLLov/tzFGmzdv1vr167Vs2TJJUkNDg4LBoBobG7VixYrU9BoAAGQsVzMfu3bt0sKFC3Xrrbdq2rRpmj9/vrZt2xY/3tHRoVgspsrKynib3+9XWVmZ9u/fP+Q1+/v71dPTk/ACAADe5Sp8HDt2TFu2bFFxcbF2796tlStX6qc//akeffRRSVIsFpMkBYPBhPcFg8H4sc+rq6tTIBCIvwoLC0cyDgAAkCFchY+zZ8/q6quvVjQa1fz587VixQrdfffd2rJlS8J5Pp8v4WtjzKC2c2pqatTd3R1/dXZ2uhwCAADIJK52tZ05c6YqKir08MMPx9u2bNmiDRs26J133tGxY8d0+eWX6+WXX9b8+fPj53znO9/Rl770JTU0NHxhDXa1BQAg84zZrrbXXXed2tvbE9reeOMNzZw5U5JUVFSkUCiklpaW+PGBgQG1traqtLTUTSkAAOBRrv7a5e///u9VWlqqaDSqv/mbv9FLL72krVu3auvWrZI+++eW6upqRaNRFRcXq7i4WNFoVLm5uaqqqhqTAQAAgMziKnxcc801euqpp1RTU6Nf/OIXKioq0ubNm3X77bfHz1m3bp36+vq0atUqHT9+XCUlJWpublZeXl7KOw8AADKPqzUfNrDmAwCAzONmzUdm7WrrhB0Ys6sG9zu7anC/s6sG9ztza7CrLQAASFeEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVT5jjBnvTpyvp6dHgUBAjQ9ElZszeby7AwAAhuFk3ylVralVd3e38vPzk57LzAcAALCK8AEAAKwifAAAAKsIHwAAwKoJ490BR021g3s37+ahz311d+rqUmN017dRI5M+J6/U4H5nVw3ud3bVSNX9Pj38U5n5AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFjFrrYAAGDU2NUWAACkLcIHAACwylX4uOyyy+Tz+Qa9Vq9eLUkyxigSiSgcDisnJ0fl5eVqa2sbk44DAIDM5Cp8HDhwQO+991781dLSIkm69dZbJUmbNm1SfX29HnzwQR04cEChUEgVFRXq7e1Nfc8BAEBGGtWC0+rqav3Hf/yHjh49KkkKh8Oqrq7WvffeK0nq7+9XMBjUr371K61YsWJY14wvOF0k5Q53z112YMyuGtzv7KrB/c6uGtzvjK1x8rRU9UeN7YLTgYEBPf7447rrrrvk8/nU0dGhWCymysrK+Dl+v19lZWXav3+/43X6+/vV09OT8AIAAN414vCxc+dOffzxx7rzzjslSbFYTJIUDAYTzgsGg/FjQ6mrq1MgEIi/CgsLR9olAACQAUYcPh555BEtXrxY4XA4od3n8yV8bYwZ1Ha+mpoadXd3x1+dnZ0j7RIAAMgAw11VkeDNN9/Unj179OSTT8bbQqGQpM9mQAoKCuLtXV1dg2ZDzuf3++X3+0fSDQAAkIFGNPOxfft2TZs2TUuWLIm3FRUVKRQKxf8CRvpsXUhra6tKS0tH31MAAOAJrmc+zp49q+3bt2v58uWaMOF/3+7z+VRdXa1oNKri4mIVFxcrGo0qNzdXVVVVKe00AADIXK7Dx549e/TWW2/prrvuGnRs3bp16uvr06pVq3T8+HGVlJSoublZeXl5KeksAADIfK7DR2VlpZweDeLz+RSJRBSJREbbLwAA4FHs7QIAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACs8hmnx5WOk56eHgUCATU+EFVuzuTx7g4AABiGk32nVLWmVt3d3crPz096LjMfAADAKsIHAACwivABAACsInwAAACrJox3Bxw11Q7u3bybhz731d2pq0uN0V3fRo1M+py8UoP7nV01uN/ZVSNV9/v08E9l5gMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFbvaAgCAUWNXWwAAkLYIHwAAwCrCBwAAsIrwAQAArMqsXW2dsANjdtXgfmdXDe53dtXgfmduDXa1BQAA6YrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACr0u45H+f2uTvp4u+FNZDkZDfXGUmNVF2fGqO/vo0amfQ5eaUG9zu7anC/M7bGud/bw9mvNu12tX377bdVWFg43t0AAAAj0NnZqenTpyc9J+3Cx9mzZ/Xuu+8qLy9PPp9PPT09KiwsVGdn5xdu0esljJtxZwPGzbizQbaM2xij3t5ehcNhXXBB8lUdaffPLhdccMGQiSk/P9/TN80J484ujDu7MO7skg3jDgQCwzqPBacAAMAqwgcAALAq7cOH3+/X/fffL7/fP95dsYpxM+5swLgZdzbI1nEnk3YLTgEAgLel/cwHAADwFsIHAACwivABAACsInwAAACrCB8AAMCqtA4fDz30kIqKijR58mQtWLBA+/btG+8updTevXt1yy23KBwOy+fzaefOnQnHjTGKRCIKh8PKyclReXm52traxqezKVRXV6drrrlGeXl5mjZtmpYuXar29vaEc7w49i1btmju3LnxpxwuWrRIzzzzTPy4F8f8eXV1dfL5fKquro63eXXckUhEPp8v4RUKheLHvTpuSXrnnXf0gx/8QJdccolyc3P1jW98QwcPHowf9+LYL7vsskH32+fzafXq1ZK8OeZRMWmqqanJTJw40Wzbts0cOXLErFmzxkyZMsW8+eab4921lPnd735n1q9fb5544gkjyTz11FMJxzdu3Gjy8vLME088YQ4fPmy+973vmYKCAtPT0zM+HU6Rm2++2Wzfvt386U9/MocOHTJLliwxM2bMMJ988kn8HC+OfdeuXebpp5827e3tpr293dTW1pqJEyeaP/3pT8YYb475fC+99JK57LLLzNy5c82aNWvi7V4d9/3332+uvPJK895778VfXV1d8eNeHfdf/vIXM3PmTHPnnXea//zP/zQdHR1mz5495s9//nP8HC+OvaurK+Fet7S0GEnmueeeM8Z4c8yjkbbh45vf/KZZuXJlQtvXv/51c999941Tj8bW58PH2bNnTSgUMhs3boy3nTp1ygQCAfOb3/xmHHo4drq6uowk09raaozJrrF/+ctfNg8//LDnx9zb22uKi4tNS0uLKSsri4cPL4/7/vvvN/PmzRvymJfHfe+995rrr7/e8biXx36+NWvWmMsvv9ycPXs2a8bsRlr+s8vAwIAOHjyoysrKhPbKykrt379/nHplV0dHh2KxWMJn4Pf7VVZW5rnPoLu7W5J08cUXS8qOsZ85c0ZNTU06ceKEFi1a5Pkxr169WkuWLNFNN92U0O71cR89elThcFhFRUW67bbbdOzYMUneHveuXbu0cOFC3XrrrZo2bZrmz5+vbdu2xY97eeznDAwM6PHHH9ddd90ln8+XFWN2Ky3Dx4cffqgzZ84oGAwmtAeDQcVisXHqlV3nxun1z8AYo7Vr1+r666/XnDlzJHl77IcPH9ZFF10kv9+vlStX6qmnntIVV1zh6TE3NTXp5ZdfVl1d3aBjXh53SUmJHn30Ue3evVvbtm1TLBZTaWmpPvroI0+P+9ixY9qyZYuKi4u1e/durVy5Uj/96U/16KOPSvL2PT9n586d+vjjj3XnnXdKyo4xuzVhvDuQjM/nS/jaGDOozeu8/hncc889eu211/TCCy8MOubFsX/ta1/ToUOH9PHHH+uJJ57Q8uXL1draGj/utTF3dnZqzZo1am5u1uTJkx3P89q4JWnx4sXx/77qqqu0aNEiXX755WpoaNC1114ryZvjPnv2rBYuXKhoNCpJmj9/vtra2rRlyxb97d/+bfw8L479nEceeUSLFy9WOBxOaPfymN1Ky5mPSy+9VBdeeOGgRNjV1TUoOXrVuVXxXv4MfvKTn2jXrl167rnnNH369Hi7l8c+adIkffWrX9XChQtVV1enefPm6YEHHvDsmA8ePKiuri4tWLBAEyZM0IQJE9Ta2qp//Md/1IQJE+Jj89q4hzJlyhRdddVVOnr0qGfvtyQVFBToiiuuSGibPXu23nrrLUne/t+3JL355pvas2ePfvSjH8XbvD7mkUjL8DFp0iQtWLBALS0tCe0tLS0qLS0dp17ZVVRUpFAolPAZDAwMqLW1NeM/A2OM7rnnHj355JP6wx/+oKKiooTjXh775xlj1N/f79kx33jjjTp8+LAOHToUfy1cuFC33367Dh06pK985SueHPdQ+vv79frrr6ugoMCz91uSrrvuukF/Ov/GG29o5syZkrz/v+/t27dr2rRpWrJkSbzN62MekXFa6PqFzv2p7SOPPGKOHDliqqurzZQpU8x///d/j3fXUqa3t9e88sor5pVXXjGSTH19vXnllVfif068ceNGEwgEzJNPPmkOHz5svv/973viT7N+/OMfm0AgYJ5//vmEP007efJk/Bwvjr2mpsbs3bvXdHR0mNdee83U1taaCy64wDQ3NxtjvDnmoZz/1y7GeHfc//AP/2Cef/55c+zYMfPiiy+ab3/72yYvLy/+M8yr437ppZfMhAkTzC9/+Utz9OhR88///M8mNzfXPP744/FzvDr2M2fOmBkzZph777130DGvjnmk0jZ8GGPMP/3TP5mZM2eaSZMmmauvvjr+p5he8dxzzxlJg17Lly83xnz2J2n333+/CYVCxu/3mxtuuMEcPnx4fDudAkONWZLZvn17/Bwvjv2uu+6Kfz9PnTrV3HjjjfHgYYw3xzyUz4cPr4773HMcJk6caMLhsFm2bJlpa2uLH/fquI0x5t///d/NnDlzjN/vN1//+tfN1q1bE457dey7d+82kkx7e/ugY14d80j5jDFmXKZcAABAVkrLNR8AAMC7CB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACw6v8DizbhX+TUKq8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a_seqs = ([2,2,0],[4,3,0],[4,3,0],[4,3,0],[3,3,0])\n",
    "\n",
    "for n, a in enumerate(a_seqs):\n",
    "    print(\"=========%d========\"%n)\n",
    "    a_tensor = torch.tensor([[a]]).long()\n",
    "    obs = env.step(a_tensor)\n",
    "    print(\"eps_step\", obs['episode_step'])\n",
    "    print(\"eps_return\", obs['episode_return'])\n",
    "    print(\"reward\", obs['reward'])\n",
    "    print(\"cur_t\", obs['cur_t'])\n",
    "    plot_obs(env.gym_env.x_/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712bd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
