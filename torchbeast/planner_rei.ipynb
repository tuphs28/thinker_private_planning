{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c5b1741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core import file_writer\n",
    "from torchbeast.core import prof\n",
    "from torchbeast.core import vtrace\n",
    "from torchbeast.atari_wrappers import *\n",
    "from torchbeast.transformer_rnn import *\n",
    "from torchbeast.train import *\n",
    "from torchbeast.model import Model\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41d1c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to original core funct\n",
    "\n",
    "def create_buffers(flags, obs_shape, num_actions, num_rewards) -> Buffers:\n",
    "    T = flags.unroll_length\n",
    "    specs = dict(\n",
    "        frame=dict(size=(T + 1, *obs_shape), dtype=torch.float32),\n",
    "        reward=dict(size=(T + 1, num_rewards), dtype=torch.float32),\n",
    "        done=dict(size=(T + 1,), dtype=torch.bool),\n",
    "        truncated_done=dict(size=(T + 1,), dtype=torch.bool),\n",
    "        episode_return=dict(size=(T + 1, num_rewards), dtype=torch.float32),\n",
    "        episode_step=dict(size=(T + 1,), dtype=torch.int32),\n",
    "        policy_logits=dict(size=(T + 1, num_actions), dtype=torch.float32),\n",
    "        im_policy_logits=dict(size=(T + 1, num_actions), dtype=torch.float32),\n",
    "        reset_policy_logits=dict(size=(T + 1, 2), dtype=torch.float32),\n",
    "        baseline=dict(size=(T + 1, num_rewards), dtype=torch.float32),\n",
    "        last_action=dict(size=(T + 1, 3), dtype=torch.int64),\n",
    "        action=dict(size=(T + 1,), dtype=torch.int64),\n",
    "        im_action=dict(size=(T + 1,), dtype=torch.int64),\n",
    "        reset_action=dict(size=(T + 1,), dtype=torch.int64),        \n",
    "        reg_loss=dict(size=(T + 1,), dtype=torch.float32),  \n",
    "        cur_t=dict(size=(T + 1,), dtype=torch.int64),             \n",
    "        max_rollout_depth=dict(size=(T + 1,), dtype=torch.float32),  \n",
    "    )\n",
    "    buffers: Buffers = {key: [] for key in specs}\n",
    "    for _ in range(flags.num_buffers):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "    return buffers  \n",
    "\n",
    "def act(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "    full_queue: mp.SimpleQueue,\n",
    "    actor_net: torch.nn.Module,\n",
    "    model: torch.nn.Module,\n",
    "    buffers: Buffers,\n",
    "    initial_agent_state_buffers,\n",
    "):\n",
    "    try:\n",
    "        logging.info(\"Actor %i started.\", actor_index)\n",
    "        timings = prof.Timings()  # Keep track of how fast things are.\n",
    "\n",
    "        gym_env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "                               model=model, flags=flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()\n",
    "        agent_state = actor_net.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = actor_net(env_output, agent_state)\n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break\n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:           \n",
    "                if key in buffers: buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                if key in buffers: buffers[key][index][0, ...] = agent_output[key]                    \n",
    "            for i, tensor in enumerate(agent_state):\n",
    "                initial_agent_state_buffers[index][i][...] = tensor\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.unroll_length):\n",
    "                timings.reset()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = actor_net(env_output, agent_state)                    \n",
    "\n",
    "                timings.time(\"actor_net\")\n",
    "                \n",
    "                action = torch.cat([agent_output['action'], agent_output['im_action'], agent_output['reset_action']], dim=-1)\n",
    "                env_output = env.step(action.unsqueeze(0))\n",
    "\n",
    "                if flags.trun_bs:\n",
    "                    if env_output['truncated_done']: \n",
    "                        env_output['reward'] = env_output['reward'] + flags.im_discounting * agent_output['baseline']\n",
    "\n",
    "                timings.time(\"step\")\n",
    "\n",
    "                for key in env_output:\n",
    "                    if key in buffers:\n",
    "                        #print(key, env_output[key].shape, env_output[key])\n",
    "                        buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    if key in buffers:\n",
    "                        buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "\n",
    "                timings.time(\"write\")\n",
    "            full_queue.put(index)\n",
    "\n",
    "        if actor_index == 0:\n",
    "            logging.info(\"Actor %i: %s\", actor_index, timings.summary())\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        print()\n",
    "        raise e\n",
    "\n",
    "def compute_policy_gradient_loss(logits_ls, actions_ls, advantages_ls, masks_ls):\n",
    "    loss = 0.\n",
    "    for logits, actions, advantages, masks in zip(logits_ls, actions_ls, advantages_ls, masks_ls):\n",
    "        cross_entropy = F.nll_loss(\n",
    "            F.log_softmax(torch.flatten(logits, 0, 1), dim=-1),\n",
    "            target=torch.flatten(actions, 0, 1),\n",
    "            reduction=\"none\",)\n",
    "        cross_entropy = cross_entropy.view_as(advantages)\n",
    "        adv_cross_entropy = cross_entropy * advantages.detach()\n",
    "        loss = loss + torch.sum(adv_cross_entropy * (1-masks))\n",
    "    return loss  \n",
    "\n",
    "def compute_entropy_loss(logits_ls, masks_ls, c_ls):\n",
    "    \"\"\"Return the entropy loss, i.e., the negative entropy of the policy.\"\"\"\n",
    "    loss = 0.\n",
    "    for logits, masks, c in zip(logits_ls, masks_ls, c_ls):\n",
    "        policy = F.softmax(logits, dim=-1)\n",
    "        log_policy = F.log_softmax(logits, dim=-1)\n",
    "        ent = torch.sum(policy * log_policy, dim=-1) #* (1-masks)\n",
    "        loss = loss + torch.sum(ent) * c \n",
    "    return loss\n",
    "\n",
    "def action_log_probs(policy_logits, actions):\n",
    "    return -F.nll_loss(\n",
    "        F.log_softmax(torch.flatten(policy_logits, 0, -2), dim=-1),\n",
    "        torch.flatten(actions),\n",
    "        reduction=\"none\",\n",
    "    ).view_as(actions) \n",
    "  \n",
    "def from_logits(\n",
    "    behavior_logits_ls, target_logits_ls, actions_ls, masks_ls,\n",
    "    discounts, rewards, values, bootstrap_value, clip_rho_threshold=1.0,\n",
    "    clip_pg_rho_threshold=1.0, lamb=1.0,):\n",
    "    \"\"\"V-trace for softmax policies.\"\"\"\n",
    "    \n",
    "    log_rhos = 0.\n",
    "    \n",
    "    for behavior_logits, target_logits, actions, masks in zip(behavior_logits_ls, \n",
    "             target_logits_ls, actions_ls, masks_ls):\n",
    "        behavior_log_probs = action_log_probs(behavior_logits, actions)        \n",
    "        target_log_probs = action_log_probs(target_logits, actions)\n",
    "        log_rho = target_log_probs - behavior_log_probs\n",
    "        log_rhos = log_rhos + log_rho * (1-masks)\n",
    "    \n",
    "    vtrace_returns = vtrace.from_importance_weights(\n",
    "        log_rhos=log_rhos,\n",
    "        discounts=discounts,\n",
    "        rewards=rewards,\n",
    "        values=values,\n",
    "        bootstrap_value=bootstrap_value,\n",
    "        clip_rho_threshold=clip_rho_threshold,\n",
    "        clip_pg_rho_threshold=clip_pg_rho_threshold,\n",
    "        lamb=lamb\n",
    "    )\n",
    "    return vtrace.VTraceFromLogitsReturns(\n",
    "        log_rhos=log_rhos,\n",
    "        behavior_action_log_probs=None,\n",
    "        target_action_log_probs=None,\n",
    "        **vtrace_returns._asdict(),\n",
    "    )  \n",
    "  \n",
    "def learn(\n",
    "    flags,\n",
    "    actor_model,\n",
    "    model,\n",
    "    batch,\n",
    "    initial_agent_state,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    lock=threading.Lock(),  # noqa: B008\n",
    "):\n",
    "    \"\"\"Performs a learning (optimization) step.\"\"\"\n",
    "    with lock:        \n",
    "        learner_outputs, unused_state = model(batch, initial_agent_state)\n",
    "        #learner_outputs[\"im_policy_logits\"].register_hook(lambda grad: grad / (flags.rec_t - 1))\n",
    "        #learner_outputs[\"reset_policy_logits\"].register_hook(lambda grad: grad / (flags.rec_t - 1))\n",
    "        #learner_outputs[\"baseline\"].register_hook(lambda grad: grad / (flags.rec_t - 1))\n",
    "        \n",
    "        # Take final value function slice for bootstrapping.\n",
    "        bootstrap_value = learner_outputs[\"baseline\"][-1]        \n",
    "\n",
    "        # Move from obs[t] -> action[t] to action[t] -> obs[t].\n",
    "        batch = {key: tensor[1:] for key, tensor in batch.items()}\n",
    "        learner_outputs = {key: tensor[:-1] for key, tensor in learner_outputs.items()}\n",
    "\n",
    "        rewards = batch[\"reward\"]\n",
    "        if flags.reward_clipping > 0:\n",
    "            clipped_rewards = torch.clamp(rewards, -flags.reward_clipping, flags.reward_clipping)\n",
    "        else:\n",
    "            clipped_rewards = rewards\n",
    "        \n",
    "        # compute advantage w.r.t real rewards\n",
    "        \n",
    "        discounts = (~batch[\"done\"]).float() * flags.im_discounting        \n",
    "        behavior_logits_ls = [batch[\"policy_logits\"], batch[\"im_policy_logits\"], batch[\"reset_policy_logits\"]]\n",
    "        target_logits_ls = [learner_outputs[\"policy_logits\"], learner_outputs[\"im_policy_logits\"], learner_outputs[\"reset_policy_logits\"]]\n",
    "        actions_ls = [batch[\"action\"], batch[\"im_action\"], batch[\"reset_action\"]]        \n",
    "        im_mask = (batch[\"cur_t\"] == 0).float()\n",
    "        real_mask = 1 - im_mask\n",
    "        masks_ls = [real_mask, im_mask, im_mask]        \n",
    "\n",
    "        vtrace_returns = from_logits(\n",
    "            behavior_logits_ls, target_logits_ls, actions_ls, masks_ls,\n",
    "            discounts=discounts,\n",
    "            rewards=clipped_rewards[:, :, 0],\n",
    "            values=learner_outputs[\"baseline\"][:, :, 0],\n",
    "            bootstrap_value=bootstrap_value[:, 0],\n",
    "            lamb=flags.lamb\n",
    "        )\n",
    "        \n",
    "        advantages_ls = [vtrace_returns.pg_advantages, vtrace_returns.pg_advantages, vtrace_returns.pg_advantages]\n",
    "        pg_loss = compute_policy_gradient_loss(target_logits_ls, actions_ls, advantages_ls, masks_ls)         \n",
    "        baseline_loss = flags.baseline_cost * compute_baseline_loss(\n",
    "            vtrace_returns.vs - learner_outputs[\"baseline\"][:, :, 0])        \n",
    "       \n",
    "        # compute advantage w.r.t imagainary rewards\n",
    "        \n",
    "        cs_ls = [flags.entropy_cost, flags.im_entropy_cost, flags.reset_entropy_cost]\n",
    "        entropy_loss = compute_entropy_loss(target_logits_ls, masks_ls, cs_ls)        \n",
    "\n",
    "        if flags.reward_type == 1:\n",
    "            if flags.reward_carry:                \n",
    "                discounts = (~batch[\"done\"]).float() * flags.im_discounting \n",
    "            else:\n",
    "                discounts = (~(batch[\"cur_t\"] == 0)).float() * flags.im_discounting        \n",
    "            behavior_logits_ls = [batch[\"im_policy_logits\"], batch[\"reset_policy_logits\"]]\n",
    "            target_logits_ls = [learner_outputs[\"im_policy_logits\"], learner_outputs[\"reset_policy_logits\"]]\n",
    "            actions_ls = [batch[\"im_action\"], batch[\"reset_action\"]] \n",
    "            im_mask = (batch[\"cur_t\"] == 0).float()\n",
    "            masks_ls = [im_mask, im_mask]  \n",
    "            \n",
    "            vtrace_returns = from_logits(\n",
    "                behavior_logits_ls, target_logits_ls, actions_ls, masks_ls,\n",
    "                discounts=discounts,\n",
    "                rewards=clipped_rewards[:, :, 1],\n",
    "                values=learner_outputs[\"baseline\"][:, :, 1],\n",
    "                bootstrap_value=bootstrap_value[:, 1],\n",
    "                lamb=flags.lamb\n",
    "            )\n",
    "            advantages_ls = [vtrace_returns.pg_advantages, vtrace_returns.pg_advantages]\n",
    "            im_pg_loss = compute_policy_gradient_loss(target_logits_ls, actions_ls, advantages_ls, masks_ls)   \n",
    "            im_baseline_loss = flags.baseline_cost * compute_baseline_loss(\n",
    "                vtrace_returns.vs - learner_outputs[\"baseline\"][:, :, 1])     \n",
    "\n",
    "        reg_loss = flags.reg_cost * torch.sum(learner_outputs[\"reg_loss\"])\n",
    "        total_loss = pg_loss + baseline_loss + entropy_loss + reg_loss\n",
    "        \n",
    "        if flags.reward_type == 1:\n",
    "            total_loss = total_loss + flags.im_cost * (im_pg_loss + im_baseline_loss)\n",
    "        \n",
    "        episode_returns = batch[\"episode_return\"][batch[\"done\"]][:, 0]  \n",
    "        max_rollout_depth = (batch[\"max_rollout_depth\"][batch[\"cur_t\"] == 0]).detach().cpu().numpy()\n",
    "        max_rollout_depth = np.average(max_rollout_depth) if len (max_rollout_depth) > 0 else 0.        \n",
    "        stats = {\n",
    "            \"episode_returns\": tuple(episode_returns.detach().cpu().numpy()),\n",
    "            \"mean_episode_return\": torch.mean(episode_returns).item(),\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"pg_loss\": pg_loss.item(),\n",
    "            \"baseline_loss\": baseline_loss.item(),\n",
    "            \"entropy_loss\": entropy_loss.item(),\n",
    "            \"reg_loss\": reg_loss.item(),\n",
    "            \"max_rollout_depth\": max_rollout_depth\n",
    "        }\n",
    "        \n",
    "        if flags.reward_type == 1:            \n",
    "            im_episode_returns = batch[\"episode_return\"][batch[\"cur_t\"] == 0][:, 1]\n",
    "            stats[\"im_episode_returns\"] = tuple(im_episode_returns.detach().cpu().numpy())\n",
    "            stats[\"im_pg_loss\"] = im_pg_loss.item()\n",
    "            stats[\"im_baseline_loss\"] = im_baseline_loss.item()   \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimize_params = optimizer.param_groups[0]['params']\n",
    "        if flags.grad_norm_clipping > 0:\n",
    "            total_norm = nn.utils.clip_grad_norm_(optimize_params, flags.grad_norm_clipping)\n",
    "        else:\n",
    "            total_norm = 0.\n",
    "            parameters = [p for p in optimize_params if p.grad is not None and p.requires_grad]\n",
    "            for p in parameters:\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "        stats[\"total_norm\"] = total_norm\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        actor_model.load_state_dict(model.state_dict())\n",
    "        return stats  \n",
    "\n",
    "# Wrap the environment with a model\n",
    "\n",
    "def _format_frame(frame, bsz=None):\n",
    "    if type(frame) == np.ndarray:\n",
    "        frame = torch.from_numpy(frame).float()\n",
    "    if bsz is not None:\n",
    "        return frame.view((1,) + frame.shape)\n",
    "    else:\n",
    "        return frame.view((1, 1) + frame.shape)\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, gym_env):\n",
    "        self.gym_env = gym_env\n",
    "        self.episode_return = None\n",
    "        self.episode_step = None\n",
    "\n",
    "    def initial(self):\n",
    "        initial_reward = torch.zeros(1, 1)\n",
    "        # This supports only single-tensor actions ATM.\n",
    "        initial_last_action = torch.zeros(1, 1, dtype=torch.int64)\n",
    "        self.episode_return = torch.zeros(1, 1, 1)\n",
    "        self.episode_step = torch.zeros(1, 1, dtype=torch.int32)\n",
    "        initial_done = torch.ones(1, 1, dtype=torch.bool)\n",
    "        initial_frame = _format_frame(self.gym_env.reset())\n",
    "        return dict(\n",
    "            frame=initial_frame,\n",
    "            reward=initial_reward,\n",
    "            done=initial_done,\n",
    "            truncated_done=torch.tensor(0).view(1, 1).bool(),\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=torch.tensor(0).view(1, 1),\n",
    "            last_action=initial_last_action,\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        frame, reward, done, unused_info = self.gym_env.step(action[0,0].cpu().detach().numpy())     \n",
    "        self.episode_step += 1\n",
    "        self.episode_return = self.episode_return + torch.tensor(reward).unsqueeze(0).unsqueeze(0)\n",
    "        episode_step = self.episode_step\n",
    "        episode_return = self.episode_return.clone()\n",
    "        if done:\n",
    "            frame = self.gym_env.reset()\n",
    "            self.episode_return = torch.zeros(1, 1, 1)\n",
    "            self.episode_step = torch.zeros(1, 1, dtype=torch.int32)        \n",
    "        frame = _format_frame(frame)\n",
    "        reward = torch.tensor(reward).view(1, 1, -1)\n",
    "        done = torch.tensor(done).view(1, 1)\n",
    "        truncated_done = 'TimeLimit.truncated' in unused_info and unused_info['TimeLimit.truncated']\n",
    "        truncated_done = torch.tensor(truncated_done).view(1, 1)\n",
    "        cur_t = torch.tensor(unused_info[\"cur_t\"]).view(1, 1)\n",
    "        if cur_t == 0 and self.episode_return.shape[2] > 1:\n",
    "            self.episode_return[:, :, 1] = 0.\n",
    "        if 'max_rollout_depth' in unused_info:\n",
    "            max_rollout_depth = torch.tensor(unused_info[\"max_rollout_depth\"]).view(1, 1)\n",
    "        else:\n",
    "            max_rollout_depth = torch.tensor(0.).view(1, 1)\n",
    "        \n",
    "        return dict(\n",
    "            frame=frame,\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            truncated_done=truncated_done,          \n",
    "            episode_return=episode_return,\n",
    "            episode_step=episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=action,\n",
    "            max_rollout_depth=max_rollout_depth\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        self.gym_env.close()\n",
    "\n",
    "    def clone_state(self):\n",
    "        state = self.gym_env.clone_state()\n",
    "        state[\"env_episode_return\"] = self.episode_return.clone()\n",
    "        state[\"env_episode_step\"] = self.episode_step.clone()\n",
    "        return state\n",
    "        \n",
    "    def restore_state(self, state):\n",
    "        self.episode_return = state[\"env_episode_return\"].clone()\n",
    "        self.episode_step = state[\"env_episode_step\"].clone()\n",
    "        self.gym_env.restore_state(state)\n",
    "        \n",
    "class Vec_Environment:\n",
    "    # deprecated\n",
    "    def __init__(self, gym_env, bsz):\n",
    "        self.gym_env = gym_env\n",
    "        self.bsz = bsz\n",
    "        self.episode_return = torch.zeros(1, self.bsz)\n",
    "        self.episode_step = torch.zeros(1, self.bsz)        \n",
    "\n",
    "    def initial(self):\n",
    "        initial_reward = torch.zeros(1, self.bsz, 1)\n",
    "        # This supports only single-tensor actions ATM.\n",
    "        initial_last_action = torch.zeros(1, self.bsz, dtype=torch.int64)\n",
    "        self.episode_return = torch.zeros(1, self.bsz)\n",
    "        self.episode_step = torch.zeros(1, self.bsz, dtype=torch.int32)\n",
    "        initial_done = torch.ones(1, self.bsz, dtype=torch.uint8)\n",
    "        initial_frame = _format_frame(self.gym_env.reset(), self.bsz)\n",
    "        cur_t = torch.zeros(1, self.bsz)\n",
    "        \n",
    "        return dict(\n",
    "            frame=initial_frame,\n",
    "            reward=initial_reward,\n",
    "            done=initial_done,\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=initial_last_action,\n",
    "        )\n",
    "\n",
    "    def step(self, action):\n",
    "        frame, reward, done, unused_info = self.gym_env.step(action.detach().cpu().numpy())   \n",
    "        \n",
    "        self.episode_step += 1\n",
    "        self.episode_return += torch.Tensor(reward).unsqueeze(0)\n",
    "        \n",
    "        done = torch.tensor(done).view(1, self.bsz)\n",
    "        truncated_done = ['TimeLimit.truncated' in x and x['TimeLimit.truncated'] for x in unused_info]\n",
    "        truncated_done = torch.tensor(truncated_done).view(1, self.bsz)\n",
    "        \n",
    "        self.episode_return = (~done).float().unsqueeze(-1) * self.episode_return\n",
    "        self.episode_step = (~done).float() * self.episode_step\n",
    "        \n",
    "        frame = _format_frame(frame, self.bsz)\n",
    "        reward = torch.tensor(reward).view(1, self.bsz).float()\n",
    "        \n",
    "        cur_t = [x[\"cur_t\"] for x in unused_info]  \n",
    "        cur_t = torch.tensor(cur_t).view(1, self.bsz)\n",
    "        \n",
    "        return dict(\n",
    "            frame=frame,\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "            truncated_done=truncated_done,\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=cur_t,\n",
    "            last_action=action.unsqueeze(0),\n",
    "        )\n",
    "    \n",
    "    def clone_state(self):        \n",
    "        state = {}\n",
    "        state[\"env_episode_return\"] = self.episode_return.clone()\n",
    "        state[\"env_episode_step\"] = self.episode_step.clone()\n",
    "        for n, k in enumerate(self.gym_env.envs): \n",
    "            state[\"env_%d\"%n] = k.clone_state()\n",
    "        return state\n",
    "        \n",
    "    def restore_state(self, state):\n",
    "        self.episode_return = state[\"env_episode_return\"].clone()\n",
    "        self.episode_step = state[\"env_episode_step\"].clone()\n",
    "        for n, k in enumerate(self.gym_env.envs): \n",
    "            k.restore_state(state[\"env_%d\"%n])\n",
    "\n",
    "    def close(self):\n",
    "        self.gym_env.close()  \n",
    "\n",
    "class Actor_net(nn.Module):    \n",
    "    def __init__(self, obs_shape, num_actions, flags):\n",
    "\n",
    "        super(Actor_net, self).__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions  \n",
    "        \n",
    "        self.tran_t = flags.tran_t                   # number of recurrence of RNN\n",
    "        self.tran_mem_n = flags.tran_mem_n           # size of memory for the attn modules\n",
    "        self.tran_layer_n = flags.tran_layer_n       # number of layers\n",
    "        self.tran_lstm = flags.tran_lstm             # to use lstm or not\n",
    "        self.tran_lstm_no_attn = flags.tran_lstm_no_attn  # to use attention in lstm or not\n",
    "        self.tran_norm_first = flags.tran_norm_first # to use norm first in transformer (not on LSTM)\n",
    "        self.tran_ff_n = flags.tran_ff_n             # number of dim of ff in transformer (not on LSTM)        \n",
    "        self.tran_skip = flags.tran_skip             # whether to add skip connection\n",
    "        self.conv_out = flags.tran_dim               # size of transformer / LSTM embedding dim\n",
    "        self.no_mem = flags.no_mem\n",
    "        self.num_rewards = flags.num_rewards\n",
    "        \n",
    "        self.conv_out_hw = 1   \n",
    "        self.d_model = self.conv_out\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=self.obs_shape[0], out_channels=self.conv_out//2, kernel_size=1, stride=1)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv_out//2, out_channels=self.conv_out, kernel_size=1, stride=1)        \n",
    "        self.frame_conv = torch.nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU())\n",
    "        self.env_input_size = self.conv_out\n",
    "        d_in = self.env_input_size + self.d_model \n",
    "        \n",
    "        if self.tran_lstm:\n",
    "            self.core = ConvAttnLSTM(h=self.conv_out_hw, w=self.conv_out_hw,\n",
    "                                 input_dim=d_in-self.d_model, hidden_dim=self.d_model,\n",
    "                                 kernel_size=1, num_layers=self.tran_layer_n,\n",
    "                                 num_heads=8, mem_n=self.tran_mem_n, attn=not self.tran_lstm_no_attn)\n",
    "        else:            \n",
    "            self.core = ConvTransformerRNN(d_in=d_in,\n",
    "                                       h=self.conv_out_hw, w=self.conv_out_hw, d_model=self.d_model, \n",
    "                                       num_heads=8, dim_feedforward=self.tran_ff_n, \n",
    "                                       mem_n=self.tran_mem_n, norm_first=self.tran_norm_first,\n",
    "                                       num_layers=self.tran_layer_n, rpos=True, conv=False)   \n",
    "                         \n",
    "        \n",
    "        if self.tran_skip:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * (self.d_model + self.env_input_size)\n",
    "        else:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * self.d_model\n",
    "                \n",
    "        self.fc = nn.Linear(rnn_out_size, 256)        \n",
    "        \n",
    "        self.im_policy = nn.Linear(256, self.num_actions)        \n",
    "        self.policy = nn.Linear(256, self.num_actions)        \n",
    "        self.baseline = nn.Linear(256, self.num_rewards)        \n",
    "        self.reset = nn.Linear(256, 2)        \n",
    "        \n",
    "        print(\"actor size: \", sum(p.numel() for p in self.parameters()))\n",
    "        #for k, v in self.named_parameters(): print(k, v.numel())   \n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        state = self.core.init_state(batch_size) + (torch.zeros(1, batch_size, \n",
    "               self.env_input_size, self.conv_out_hw, self.conv_out_hw),)\n",
    "        return state\n",
    "\n",
    "    def forward(self, obs, core_state=(), debug=False):\n",
    "        # one-step forward for the actor\n",
    "        # input / done shape x: T x B x C x 1 x 1 / B x C x 1 x 1\n",
    "        # only supports T = 1 at the moment; all output does not have T dim.        \n",
    "        \n",
    "        x = obs[\"frame\"]\n",
    "        done = obs[\"done\"]\n",
    "        \n",
    "        if len(x.shape) == 4: x = x.unsqueeze(0)\n",
    "        if len(done.shape) == 1: done = done.unsqueeze(0)  \n",
    "            \n",
    "        T, B, *_ = x.shape\n",
    "        x = torch.flatten(x, 0, 1)  # Merge time and batch.  \n",
    "        env_input = self.frame_conv(x)                \n",
    "        core_input = env_input.view(T, B, -1, self.conv_out_hw, self.conv_out_hw)\n",
    "        core_output_list = []\n",
    "        notdone = ~(done.bool())\n",
    "        \n",
    "        for n, (input, nd) in enumerate(zip(core_input.unbind(), notdone.unbind())):       \n",
    "            if self.no_mem and obs[\"cur_t\"][n, 0] == 0:\n",
    "                core_state = self.initial_state(B)\n",
    "                core_state = tuple(v.to(x.device) for v in core_state)\n",
    "                \n",
    "            # Input shape: B, self.conv_out + self.num_actions + 1, H, W\n",
    "            for t in range(self.tran_t):                \n",
    "                if t > 0: nd = torch.ones(B).to(x.device).bool()                    \n",
    "                nd = nd.view(-1)      \n",
    "                output, core_state = self.core(input, core_state, nd, nd) # output shape: 1, B, core_output_size \n",
    "                \n",
    "            last_input = input   \n",
    "            core_output_list.append(output)\n",
    "                                   \n",
    "        core_output = torch.cat(core_output_list)  \n",
    "        if self.tran_skip: core_output = torch.concat([core_output, core_input], dim=-3)\n",
    "        core_output = torch.flatten(core_output, 0, 1)        \n",
    "        core_output = F.relu(self.fc(torch.flatten(core_output, start_dim=1)))   \n",
    "        \n",
    "        policy_logits = self.policy(core_output)\n",
    "        im_policy_logits = self.im_policy(core_output)\n",
    "        reset_policy_logits = self.reset(core_output)\n",
    "        \n",
    "        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        im_action = torch.multinomial(F.softmax(im_policy_logits, dim=1), num_samples=1)\n",
    "        reset_action = torch.multinomial(F.softmax(reset_policy_logits, dim=1), num_samples=1)\n",
    "                \n",
    "        baseline = self.baseline(core_output)\n",
    "                   \n",
    "        reg_loss = (1e-3 * torch.sum(policy_logits**2, dim=-1) / 2 + \n",
    "                    1e-5 * torch.sum(core_output**2, dim=-1) / 2)\n",
    "        reg_loss = reg_loss.view(T, B)\n",
    "        \n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        im_policy_logits = im_policy_logits.view(T, B, self.num_actions)\n",
    "        reset_policy_logits = reset_policy_logits.view(T, B, 2)\n",
    "        \n",
    "        action = action.view(T, B)      \n",
    "        im_action = im_action.view(T, B)      \n",
    "        reset_action = reset_action.view(T, B)             \n",
    "        baseline = baseline.view(T, B, self.num_rewards)\n",
    "        \n",
    "        ret_dict = dict(policy_logits=policy_logits,                         \n",
    "                        im_policy_logits=im_policy_logits,                         \n",
    "                        reset_policy_logits=reset_policy_logits,     \n",
    "                        action=action,     \n",
    "                        im_action=im_action,\n",
    "                        reset_action=reset_action,\n",
    "                        baseline=baseline, \n",
    "                        reg_loss=reg_loss, )\n",
    "        \n",
    "        return (ret_dict, core_state) \n",
    "    \n",
    "        \n",
    "class ModelWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, model, flags):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.env = env\n",
    "        self.model = model                \n",
    "        self.rec_t = flags.rec_t        \n",
    "        self.discounting = flags.discounting\n",
    "        self.stat_type = flags.stat_type    \n",
    "        self.reward_type = flags.reward_type    \n",
    "        self.no_mem = flags.no_mem\n",
    "        self.perfect_model = flags.perfect_model\n",
    "        self.reset_m = flags.reset_m\n",
    "        self.tree_carry = flags.tree_carry\n",
    "        self.thres_carry = flags.thres_carry\n",
    "        self.thres_discounting = flags.thres_discounting\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.root_node = None\n",
    "        \n",
    "        # 0 for the most basic; \n",
    "        # 1 for augmented input in root stat; \n",
    "        # 2 for tree stat\n",
    "        if self.stat_type == 0:\n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 5 + num_actions * 4 + self.rec_t\n",
    "        elif self.stat_type == 1:            \n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 7 + num_actions * 7 + self.rec_t\n",
    "        elif self.stat_type == 1.5:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 6 + num_actions * 10 + self.rec_t             \n",
    "        elif self.stat_type == 2:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 9 + num_actions * 10 + self.rec_t   \n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "          low=-np.inf, high=np.inf, shape=(obs_n, 1, 1), dtype=float)\n",
    "        self.model.train(False)        \n",
    "        \n",
    "        self.max_rollout_depth = 0.\n",
    "        self.thres = None\n",
    "        self.root_max_q = None\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        x = self.env.reset()\n",
    "        self.cur_t = 0        \n",
    "        out = self.use_model(x, 0., 0, self.cur_t, 1.)\n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q\n",
    "        return out.unsqueeze(-1).unsqueeze(-1)\n",
    "    \n",
    "    def step(self, action):  \n",
    "        re_action, im_action, reset = action\n",
    "        info = {}\n",
    "        info[\"max_rollout_depth\"] = self.max_rollout_depth\n",
    "        if self.cur_t < self.rec_t - 1:\n",
    "          self.cur_t += 1\n",
    "          out = self.use_model(None, None, im_action, self.cur_t, reset)          \n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([0.])\n",
    "          else:\n",
    "            r = np.array([0., (self.root_max_q - self.last_root_max_q).item()], dtype=np.float32)\n",
    "          done = False\n",
    "          info['cur_t'] = self.cur_t   \n",
    "        else:\n",
    "          self.cur_t = 0\n",
    "          if self.perfect_model:\n",
    "                self.env.restore_state(self.root_node.encoded)\n",
    "          x, r, done, info_ = self.env.step(re_action)                    \n",
    "          out = self.use_model(x, r, re_action, self.cur_t, 1., done) \n",
    "          info.update(info_)\n",
    "          info['cur_t'] = self.cur_t\n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([r])\n",
    "          else:\n",
    "            r = np.array([r, 0.], dtype=np.float32)   \n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q   \n",
    "        \n",
    "        return out.unsqueeze(-1).unsqueeze(-1), r, done, info        \n",
    "        \n",
    "    def use_model_raw(self, x, r, a, cur_t, reset, done):\n",
    "        # mostly deprecated\n",
    "        # input: \n",
    "        # r: reward - [,]; x: frame - [C, H, W]; a: action - [,]\n",
    "        # cur_t: int; reset at cur_t == 0  \n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:\n",
    "                self.rollout_depth = 0.\n",
    "                if self.no_mem:\n",
    "                    self.re_action = F.one_hot(torch.zeros(1, dtype=torch.long), self.num_actions)   \n",
    "                else:\n",
    "                    self.re_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)                   \n",
    "                \n",
    "                x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = x\n",
    "                _, vs, logits, encodeds = self.model(x, self.re_action.unsqueeze(0), one_hot=True)                \n",
    "                self.encoded = encodeds[-1]    \n",
    "                self.encoded_reset = self.encoded.clone()\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    self.re_reward = torch.tensor([[0.]], dtype=torch.float32)                \n",
    "                else:\n",
    "                    self.re_reward = torch.tensor([[r]], dtype=torch.float32)                \n",
    "                    \n",
    "                self.v0 = vs[-1].unsqueeze(-1).clone()\n",
    "                self.logit0 = logits[-1].clone()\n",
    "                \n",
    "                self.im_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.im_reset = torch.tensor([[1.]], dtype=torch.float32)\n",
    "                self.im_reward = torch.zeros(1, 1, dtype=torch.float32)                                \n",
    "                self.v = vs[-1].unsqueeze(-1)\n",
    "                self.logit = logits[-1]\n",
    "                self.rollout_first_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                self.rollout_return = torch.zeros(1, 1, dtype=torch.float32)                \n",
    "                self.q_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.n_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)                \n",
    "            else:\n",
    "                self.rollout_depth += 1                \n",
    "                \n",
    "                self.im_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)   \n",
    "                rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "                   self.im_action.unsqueeze(0), one_hot=True)\n",
    "                self.encoded = encodeds[-1]        \n",
    "                \n",
    "                self.im_reward = rs[-1].unsqueeze(-1)\n",
    "                self.v = vs[-1].unsqueeze(-1)    \n",
    "                self.logit = logits[-1]     \n",
    "                \n",
    "                if self.im_reset: \n",
    "                    # last action's reset is true; re-initialize everything                    \n",
    "                    self.rollout_first_action = self.im_action.clone()\n",
    "                    self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                    self.rollout_depth = 1                      \n",
    "                    \n",
    "                self.rollout_return_wo_v += (self.discounting ** (self.rollout_depth-1)) * self.im_reward\n",
    "                self.rollout_return = self.rollout_return_wo_v + (\n",
    "                    self.discounting ** (self.rollout_depth)) * self.v                    \n",
    "                    \n",
    "                self.im_reset = torch.tensor([[reset]], dtype=torch.float32)\n",
    "                if self.im_reset:                    \n",
    "                    rollout_first_action_label = torch.argmax(self.rollout_first_action, dim=1)                    \n",
    "                    q = self.q_s_a[:, rollout_first_action_label]\n",
    "                    n = self.n_s_a[:, rollout_first_action_label]                    \n",
    "                    ret = self.rollout_return[:, 0]\n",
    "                    self.n_s_a[:, rollout_first_action_label] += 1                    \n",
    "                    self.q_s_a[:, rollout_first_action_label] = (n * q) / (n + 1) + ret / (n + 1)\n",
    "                    \n",
    "        time = F.one_hot(torch.tensor([cur_t]).long(), self.rec_t)\n",
    "        depc = torch.tensor([[self.discounting ** (self.rollout_depth-1)]])\n",
    "        ret_dict = {\"re_action\": self.re_action,\n",
    "                    \"re_reward\": self.re_reward,\n",
    "                    \"v0\": self.v0,\n",
    "                    \"logit0\": self.logit0,\n",
    "                    \"im_action\": self.im_action,\n",
    "                    \"im_reset\": self.im_reset,\n",
    "                    \"im_reward\": self.im_reward,\n",
    "                    \"v\": self.v,\n",
    "                    \"logit\": self.logit,\n",
    "                    \"rollout_first_action\": self.rollout_first_action,\n",
    "                    \"rollout_return\": self.rollout_return,\n",
    "                    \"n_s_a\": self.n_s_a,\n",
    "                    \"q_s_a\": self.q_s_a,\n",
    "                    \"time\": time,\n",
    "                    \"depc\": depc}        \n",
    "        self.ret_dict = ret_dict\n",
    "        if self.stat_type == 1:\n",
    "            out = torch.concat(list(ret_dict.values()), dim=-1)   \n",
    "        else:\n",
    "            core_inputs = [\"re_action\", \"re_reward\", \"v0\", \"logit0\", \"im_action\",\n",
    "                           \"im_reset\", \"im_reward\", \"v\", \"logit\", \"time\"]\n",
    "            out = torch.concat([ret_dict[v] for v in core_inputs], dim=-1)   \n",
    "        self.encoded = reset * self.encoded_reset + (1 - reset) * self.encoded\n",
    "        return out[0]      \n",
    "    \n",
    "    def use_model_tree(self, x, r, a, cur_t, reset, done=False):\n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:\n",
    "                self.rollout_depth = 0.\n",
    "                self.unexpand_rollout_depth = 0.\n",
    "                self.pass_unexpand = False\n",
    "                self.max_rollout_depth = 0.\n",
    "                \n",
    "                if self.root_max_q is not None:\n",
    "                    self.thres = (self.root_max_q - r) / self.discounting\n",
    "                    self.thres *= self.thres_discounting\n",
    "                if done:\n",
    "                    self.thres = None\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    re_action = 0\n",
    "                    re_reward = torch.tensor([0.], dtype=torch.float32)                \n",
    "                else:\n",
    "                    re_action = a                \n",
    "                    re_reward = torch.tensor([r], dtype=torch.float32)                \n",
    "                \n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = self.x_ = x_tensor\n",
    "                a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)                \n",
    "                _, vs, logits, encodeds = self.model(x_tensor, a_tensor.unsqueeze(0), one_hot=True) \n",
    "                \n",
    "                if self.perfect_model: \n",
    "                    encoded = self.clone_state()\n",
    "                else:\n",
    "                    encoded=encodeds[-1]\n",
    "                \n",
    "                if (not self.tree_carry or self.root_node is None or \n",
    "                    not self.root_node.children[a].expanded() or done):\n",
    "                \n",
    "                    self.root_node = Node(parent=None, action=re_action, logit=None, \n",
    "                                          num_actions=self.num_actions,\n",
    "                                          discounting=self.discounting,\n",
    "                                          rec_t=self.rec_t)\n",
    "                    self.root_node.expand(r=torch.tensor([0.], dtype=torch.float32), \n",
    "                                          v=vs[-1, 0].unsqueeze(-1), logits=logits[-1, 0],\n",
    "                                          encoded=encoded)\n",
    "                else:\n",
    "                    self.root_node = self.root_node.children[a]\n",
    "                    self.root_node.expand(r=torch.tensor([0.], dtype=torch.float32), \n",
    "                                          v=vs[-1, 0].unsqueeze(-1), logits=logits[-1, 0],\n",
    "                                          encoded=encoded, override=True)\n",
    "                    self.parent = None\n",
    "                \n",
    "                self.root_node.visit()\n",
    "                self.cur_node = self.root_node\n",
    "                \n",
    "            else:\n",
    "                self.rollout_depth += 1                    \n",
    "                self.max_rollout_depth = max(self.max_rollout_depth, self.rollout_depth)\n",
    "                next_node = self.cur_node.children[a]\n",
    "                \n",
    "                if not next_node.expanded():\n",
    "                    self.pass_unexpand = True\n",
    "                    a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions) \n",
    "                    if not self.perfect_model:\n",
    "                        rs, vs, logits, encodeds = self.model.forward_encoded(self.cur_node.encoded, \n",
    "                            a_tensor.unsqueeze(0), one_hot=True)\n",
    "                        next_node.expand(r=rs[-1, 0].unsqueeze(-1), v=vs[-1, 0].unsqueeze(-1), \n",
    "                                     logits=logits[-1, 0], encoded=encodeds[-1])\n",
    "                    else:                        \n",
    "                        if \"done\" not in self.cur_node.encoded:                            \n",
    "                            self.env.restore_state(self.cur_node.encoded)                        \n",
    "                            x, r, done, info = self.env.step(a)                        \n",
    "                            encoded = self.env.clone_state()\n",
    "                            if done: encoded[\"done\"] = True                        \n",
    "                            x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                            self.x_ = x_tensor\n",
    "                            a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions) \n",
    "                            _, vs, logits, _ = self.model(x_tensor, a_tensor.unsqueeze(0), one_hot=True)                        \n",
    "\n",
    "                            if done:\n",
    "                                v = torch.tensor([0.], dtype=torch.float32)\n",
    "                            else:\n",
    "                                v = vs[-1, 0].unsqueeze(-1)\n",
    "\n",
    "                            next_node.expand(r=torch.tensor([r], dtype=torch.float32), \n",
    "                                             v=v, \n",
    "                                             logits=logits[-1, 0], \n",
    "                                             encoded=encoded)\n",
    "                        else:\n",
    "                            logits = torch.concat([x.logit for x in self.cur_node.children])  \n",
    "                            next_node.expand(r=torch.tensor([0.], dtype=torch.float32), \n",
    "                                             v=torch.tensor([0.], dtype=torch.float32),\n",
    "                                             logits=logits, \n",
    "                                             encoded=self.cur_node.encoded)                            \n",
    "                            \n",
    "                next_node.visit()\n",
    "                self.cur_node = next_node\n",
    "            \n",
    "            if self.pass_unexpand:                 \n",
    "                self.unexpand_rollout_depth += 1    \n",
    "                if self.reset_m >= 0 and self.unexpand_rollout_depth > self.reset_m:\n",
    "                    reset = True\n",
    "            \n",
    "            root_node_stat = self.root_node.stat()\n",
    "            cur_node_stat = self.cur_node.stat()                        \n",
    "            reset = torch.tensor([reset], dtype=torch.float32)\n",
    "            time = F.one_hot(torch.tensor(cur_t).long(), self.rec_t)\n",
    "            depc = torch.tensor([self.discounting ** (self.rollout_depth-1)])\n",
    "            \n",
    "            root_trail_r = self.root_node.trail_r / self.discounting\n",
    "            root_rollout_q = self.root_node.rollout_q / self.discounting\n",
    "            root_max_q = torch.max(torch.concat(self.root_node.rollout_qs)).unsqueeze(-1) / self.discounting\n",
    "            if self.thres_carry and self.thres is not None:\n",
    "                root_max_q = torch.max(root_max_q, self.thres)                \n",
    "            \n",
    "            ret_list = [root_node_stat, cur_node_stat, reset, time, depc]\n",
    "            if self.stat_type >= 2: ret_list.extend([root_trail_r, root_rollout_q, root_max_q])            \n",
    "            out = torch.concat(ret_list, dim=-1)            \n",
    "            self.last_node = self.cur_node     \n",
    "            \n",
    "            self.root_max_q = root_max_q\n",
    "            self.ret_dict = {\"v0\": self.root_node.ret_dict[\"v\"].unsqueeze(0),\n",
    "                             \"q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_mean\"].unsqueeze(0),\n",
    "                             \"max_q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_max\"].unsqueeze(0),\n",
    "                             \"n_s_a\": self.root_node.ret_dict[\"child_rollout_ns\"].unsqueeze(0),\n",
    "                             \"logit0\": self.root_node.ret_dict[\"child_logits\"].unsqueeze(0),\n",
    "                             \"reset\": reset}\n",
    "            \n",
    "            if reset:\n",
    "                self.rollout_depth = 0\n",
    "                self.unexpand_rollout_depth = 0.\n",
    "                self.cur_node = self.root_node\n",
    "                self.cur_node.visit()\n",
    "                self.pass_unexpand = False\n",
    "                \n",
    "            return out\n",
    "                \n",
    "class Node:\n",
    "    def __init__(self, parent, action, logit, num_actions, discounting, rec_t):        \n",
    "        \n",
    "        self.action = F.one_hot(torch.tensor(action).long(), num_actions) # shape (1, num_actions)        \n",
    "        self.r = torch.tensor([0.], dtype=torch.float32)    \n",
    "        self.v = torch.tensor([0.], dtype=torch.float32)            \n",
    "        self.logit = logit # shape (1,)        \n",
    "        \n",
    "        self.rollout_qs = []  # list of tensors of shape (1,)\n",
    "        self.rollout_n = torch.tensor([0.], dtype=torch.float32)    \n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.encoded = None \n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.discounting = discounting\n",
    "        self.rec_t = rec_t        \n",
    "        \n",
    "        self.visited = False\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def expand(self, r, v, logits, encoded, override=False):\n",
    "        \"\"\"\n",
    "        First time arriving a node and so we expand it\n",
    "        r, v: tensor of shape (1,)\n",
    "        logits: tensor of shape (num_actions,)\n",
    "        \"\"\"\n",
    "        if not override: assert not self.expanded()\n",
    "        if override:\n",
    "            self.rollout_qs = [x - self.r + r for x in self.rollout_qs]\n",
    "            self.rollout_qs[0] = v * self.discounting\n",
    "        self.r = r\n",
    "        self.v = v\n",
    "        self.encoded = encoded\n",
    "        for a in range(self.num_actions):\n",
    "            if not override:\n",
    "                child = self.children.append(Node(self, a, logits[[a]], \n",
    "                   self.num_actions, self.discounting, self.rec_t))\n",
    "            else:\n",
    "                self.children[a].logit = logits[[a]]        \n",
    "            \n",
    "    def visit(self):\n",
    "        self.trail_r = torch.tensor([0.], dtype=torch.float32)    \n",
    "        self.trail_discount = 1.\n",
    "        self.propagate(self.r, self.v, not self.visited)        \n",
    "        self.visited = True\n",
    "        \n",
    "    def propagate(self, r, v, new_rollout):\n",
    "        self.trail_r = self.trail_r + self.trail_discount * r\n",
    "        self.trail_discount = self.trail_discount * self.discounting\n",
    "        self.rollout_q = self.trail_r + self.trail_discount * v\n",
    "        if new_rollout:\n",
    "            self.rollout_qs.append(self.rollout_q)\n",
    "            self.rollout_n = self.rollout_n + 1\n",
    "        if self.parent is not None: self.parent.propagate(r, v, new_rollout)\n",
    "            \n",
    "    def stat(self):\n",
    "        assert self.expanded()\n",
    "        self.child_logits = torch.concat([x.logit for x in self.children])        \n",
    "        child_rollout_qs_mean = []\n",
    "        child_rollout_qs_max = []\n",
    "        for x in self.children:\n",
    "            if len(x.rollout_qs) > 0:                \n",
    "                q_mean = torch.mean(torch.cat(x.rollout_qs), dim=-1, keepdim=True)\n",
    "                q_max = torch.max(torch.cat(x.rollout_qs), dim=-1, keepdim=True)[0]\n",
    "            else:\n",
    "                q_mean = torch.tensor([0.], dtype=torch.float32)    \n",
    "                q_max = torch.tensor([0.], dtype=torch.float32)    \n",
    "            child_rollout_qs_mean.append(q_mean)\n",
    "            child_rollout_qs_max.append(q_max)\n",
    "        self.child_rollout_qs_mean = torch.concat(child_rollout_qs_mean)\n",
    "        self.child_rollout_qs_max = torch.concat(child_rollout_qs_max)\n",
    "        self.child_rollout_ns = torch.concat([x.rollout_n for x in self.children]) / self.rec_t       \n",
    "            \n",
    "        ret_list = [\"action\", \"r\", \"v\", \"child_logits\", \"child_rollout_qs_mean\",\n",
    "                    \"child_rollout_qs_max\", \"child_rollout_ns\"]\n",
    "        self.ret_dict = {x: getattr(self, x) for x in ret_list}\n",
    "        out = torch.concat(list(self.ret_dict.values()))        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae5f1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_parser():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n",
    "\n",
    "    parser.add_argument(\"--env\", type=str, default=\"Sokoban-v0\",\n",
    "                        help=\"Gym environment.\")\n",
    "    parser.add_argument(\"--env_disable_noop\", action=\"store_true\",\n",
    "                        help=\"Disable noop in environment or not. (sokoban only)\")\n",
    "\n",
    "    parser.add_argument(\"--xpid\", default=None,\n",
    "                        help=\"Experiment id (default: None).\")\n",
    "\n",
    "    parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n",
    "                        help=\"Disable saving checkpoint.\")\n",
    "    parser.add_argument(\"--load_checkpoint\", default=\"\",\n",
    "                        help=\"Load checkpoint directory.\")    \n",
    "    parser.add_argument(\"--savedir\", default=\"~/RS/thinker/logs/torchbeast\",\n",
    "                        help=\"Root dir where experiment data will be saved.\")\n",
    "\n",
    "    # Training settings.        \n",
    "    parser.add_argument(\"--num_actors\", default=48, type=int, metavar=\"N\",\n",
    "                        help=\"Number of actors (default: 48).\")\n",
    "    parser.add_argument(\"--total_steps\", default=500000000, type=int, metavar=\"T\",\n",
    "                        help=\"Total environment steps to train for.\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, metavar=\"B\",\n",
    "                        help=\"Learner batch size.\")\n",
    "    parser.add_argument(\"--unroll_length\", default=100, type=int, metavar=\"T\",\n",
    "                        help=\"The unroll length (time dimension).\")\n",
    "    parser.add_argument(\"--num_buffers\", default=None, type=int,\n",
    "                        metavar=\"N\", help=\"Number of shared-memory buffers.\")\n",
    "    parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=1, type=int,\n",
    "                        metavar=\"N\", help=\"Number learner threads.\")\n",
    "    parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n",
    "                        help=\"Disable CUDA.\")\n",
    "\n",
    "    # Architecture settings\n",
    "    parser.add_argument(\"--tran_dim\", default=64, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer hidden dim.\")\n",
    "    parser.add_argument(\"--tran_mem_n\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer memory.\")\n",
    "    parser.add_argument(\"--tran_layer_n\", default=3, type=int, metavar=\"N\",\n",
    "                        help=\"Number of transformer layer.\")\n",
    "    parser.add_argument(\"--tran_t\", default=1, type=int, metavar=\"T\",\n",
    "                        help=\"Number of recurrent step for transformer.\")\n",
    "    parser.add_argument(\"--tran_ff_n\", default=256, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer ff .\")\n",
    "    parser.add_argument(\"--tran_skip\", action=\"store_true\",\n",
    "                        help=\"Whether to enable skip conn.\")\n",
    "    parser.add_argument(\"--tran_norm_first\", action=\"store_true\",\n",
    "                        help=\"Whether to use norm first in transformer.\")\n",
    "    parser.add_argument(\"--tran_rpos\", action=\"store_true\",\n",
    "                        help=\"Whether to use relative position in transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm\", action=\"store_true\",\n",
    "                        help=\"Whether to use LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm_no_attn\", action=\"store_true\",\n",
    "                        help=\"Whether to disable attention in LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_erasep\", action=\"store_true\",\n",
    "                        help=\"Whether to erase past memories if not planning.\")\n",
    "    \n",
    "    # Loss settings.\n",
    "    parser.add_argument(\"--entropy_cost\", default=0.0001,\n",
    "                        type=float, help=\"Entropy cost/multiplier.\")\n",
    "    parser.add_argument(\"--im_entropy_cost\", default=0.0001,\n",
    "                        type=float, help=\"Imagainary Entropy cost/multiplier.\")   \n",
    "    parser.add_argument(\"--reset_entropy_cost\", default=0.0001,\n",
    "                        type=float, help=\"Reset Entropy cost/multiplier.\")       \n",
    "    parser.add_argument(\"--baseline_cost\", default=0.5,\n",
    "                        type=float, help=\"Baseline cost/multiplier.\")\n",
    "    parser.add_argument(\"--reg_cost\", default=0.1,\n",
    "                        type=float, help=\"Reg cost/multiplier.\")\n",
    "    parser.add_argument(\"--im_cost\", default=1,\n",
    "                        type=float, help=\"Imaginary reward cost/multiplier.\")    \n",
    "    parser.add_argument(\"--discounting\", default=0.99,\n",
    "                        type=float, help=\"Discounting factor.\")\n",
    "    parser.add_argument(\"--lamb\", default=1.,\n",
    "                        type=float, help=\"Lambda when computing trace.\")\n",
    "    parser.add_argument(\"--reward_clipping\", default=10, type=int, \n",
    "                        metavar=\"N\", help=\"Reward clipping.\")\n",
    "    parser.add_argument(\"--trun_bs\", action=\"store_true\",\n",
    "                        help=\"Whether to add baseline as reward when truncated.\")\n",
    "    \n",
    "    # Model settings\n",
    "    parser.add_argument(\"--reward_type\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"Reward type\")   \n",
    "    parser.add_argument(\"--reset_m\", default=-1, type=int, metavar=\"N\",\n",
    "                        help=\"Auto reset after passing m node since an unexpanded noded\")    \n",
    "    parser.add_argument(\"--model_type_nn\", default=0,\n",
    "                        type=float, help=\"Model type.\")     \n",
    "    parser.add_argument(\"--perfect_model\", action=\"store_true\",\n",
    "                        help=\"Whether to use perfect model.\")    \n",
    "    parser.add_argument(\"--rec_t\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Number of planning steps.\")\n",
    "    parser.add_argument(\"--stat_type\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"Staistic type (0: raw; 1: root node stat; 2. root & current node stat).\")    \n",
    "    parser.add_argument(\"--no_mem\", action=\"store_true\",\n",
    "                        help=\"Whether to erase all memories after each real action.\")   \n",
    "    parser.add_argument(\"--tree_carry\", action=\"store_true\",\n",
    "                        help=\"Whether to carry over the tree.\")   \n",
    "    parser.add_argument(\"--thres_carry\", action=\"store_true\",\n",
    "                        help=\"Whether to carry threshold over.\")   \n",
    "    parser.add_argument(\"--reward_carry\", action=\"store_true\",\n",
    "                        help=\"Whether to carry planning reward over.\")      \n",
    "    parser.add_argument(\"--thres_discounting\", default=0.99,\n",
    "                        type=float, help=\"Threshold discounting factor.\")    \n",
    "    \n",
    "\n",
    "    # Optimizer settings.\n",
    "    parser.add_argument(\"--learning_rate\", default=0.00005,\n",
    "                        type=float, metavar=\"LR\", help=\"Learning rate.\")\n",
    "    parser.add_argument(\"--disable_adam\", action=\"store_true\",\n",
    "                        help=\"Use Aadm optimizer or not.\")\n",
    "    parser.add_argument(\"--alpha\", default=0.99, type=float,\n",
    "                        help=\"RMSProp smoothing constant.\")\n",
    "    parser.add_argument(\"--momentum\", default=0, type=float,\n",
    "                        help=\"RMSProp momentum.\")\n",
    "    parser.add_argument(\"--epsilon\", default=0.01, type=float,\n",
    "                        help=\"RMSProp epsilon.\")\n",
    "    parser.add_argument(\"--grad_norm_clipping\", default=0.0, type=float,\n",
    "                        help=\"Global gradient norm clip.\")\n",
    "    # yapf: enable\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.load_checkpoint = \"\"\n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.num_actors = 1\n",
    "flags.batch_size = 32\n",
    "flags.unroll_length = 200\n",
    "flags.learning_rate = 0.0001\n",
    "flags.grad_norm_clipping = 60\n",
    "\n",
    "flags.entropy_cost = 0.0001\n",
    "flags.im_entropy_cost = 0.0001\n",
    "flags.reset_entropy_cost = 0.0001\n",
    "flags.reg_cost = 0.01\n",
    "flags.im_cost = 1\n",
    "flags.discounting = 0.97\n",
    "flags.lamb = 1.\n",
    "\n",
    "flags.trun_bs = False\n",
    "flags.total_steps = 500000000\n",
    "flags.disable_adam = False\n",
    "\n",
    "flags.tran_t = 1\n",
    "flags.tran_mem_n = 5\n",
    "flags.tran_layer_n = 3\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = False\n",
    "flags.tran_norm_first = False\n",
    "flags.tran_ff_n = 256\n",
    "flags.tran_skip = False\n",
    "flags.tran_erasep = False\n",
    "flags.tran_dim = 64\n",
    "flags.tran_rpos = True\n",
    "\n",
    "flags.no_mem = True\n",
    "flags.rec_t = 5\n",
    "flags.model_type_nn = 0\n",
    "flags.perfect_model = True\n",
    "flags.stat_type = 2\n",
    "flags.reward_type = 1\n",
    "\n",
    "flags.reset_m = -1\n",
    "flags.tree_carry = True\n",
    "flags.thres_carry = True\n",
    "flags.reward_carry = True\n",
    "flags.thres_discounting = 0.97\n",
    "\n",
    "flags.savedir = \"~/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d04a968",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating log directory: /home/sc/tmp/torchbeast-20221117-220137\n",
      "Creating log directory: /home/sc/tmp/torchbeast-20221117-220137\n",
      "Creating log directory: /home/sc/tmp/torchbeast-20221117-220137\n",
      "Symlinked log directory: /home/sc/tmp/latest\n",
      "Symlinked log directory: /home/sc/tmp/latest\n",
      "Symlinked log directory: /home/sc/tmp/latest\n",
      "Saving arguments to /home/sc/tmp/torchbeast-20221117-220137/meta.json\n",
      "Saving arguments to /home/sc/tmp/torchbeast-20221117-220137/meta.json\n",
      "Saving arguments to /home/sc/tmp/torchbeast-20221117-220137/meta.json\n",
      "Saving messages to /home/sc/tmp/torchbeast-20221117-220137/out.log\n",
      "Saving messages to /home/sc/tmp/torchbeast-20221117-220137/out.log\n",
      "Saving messages to /home/sc/tmp/torchbeast-20221117-220137/out.log\n",
      "Saving logs data to /home/sc/tmp/torchbeast-20221117-220137/logs.csv\n",
      "Saving logs data to /home/sc/tmp/torchbeast-20221117-220137/logs.csv\n",
      "Saving logs data to /home/sc/tmp/torchbeast-20221117-220137/logs.csv\n",
      "Saving logs' fields to /home/sc/tmp/torchbeast-20221117-220137/fields.csv\n",
      "Saving logs' fields to /home/sc/tmp/torchbeast-20221117-220137/fields.csv\n",
      "Saving logs' fields to /home/sc/tmp/torchbeast-20221117-220137/fields.csv\n",
      "Using CUDA.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  396838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Actor 0 started.\n",
      "# Step\tmean_episode_return\tepisode_returns\ttotal_loss\tpg_loss\tbaseline_loss\tentropy_loss\tmax_rollout_depth\tim_pg_loss\tim_baseline_loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  396838\n",
      "Using Adam...\n",
      "All parameters: \n",
      "conv1.weight 2048\n",
      "conv1.bias 32\n",
      "conv2.weight 2048\n",
      "conv2.bias 64\n",
      "core.layers.0.pos_w 320\n",
      "core.layers.0.pos_b 40\n",
      "core.layers.0.conv.weight 81920\n",
      "core.layers.0.conv.bias 320\n",
      "core.layers.0.proj.weight 36864\n",
      "core.layers.0.proj.bias 192\n",
      "core.layers.0.out.weight 4096\n",
      "core.layers.0.out.bias 64\n",
      "core.layers.0.norm.weight 64\n",
      "core.layers.0.norm.bias 64\n",
      "core.layers.1.pos_w 320\n",
      "core.layers.1.pos_b 40\n",
      "core.layers.1.conv.weight 81920\n",
      "core.layers.1.conv.bias 320\n",
      "core.layers.1.proj.weight 36864\n",
      "core.layers.1.proj.bias 192\n",
      "core.layers.1.out.weight 4096\n",
      "core.layers.1.out.bias 64\n",
      "core.layers.1.norm.weight 64\n",
      "core.layers.1.norm.bias 64\n",
      "core.layers.2.pos_w 320\n",
      "core.layers.2.pos_b 40\n",
      "core.layers.2.conv.weight 81920\n",
      "core.layers.2.conv.bias 320\n",
      "core.layers.2.proj.weight 36864\n",
      "core.layers.2.proj.bias 192\n",
      "core.layers.2.out.weight 4096\n",
      "core.layers.2.out.bias 64\n",
      "core.layers.2.norm.weight 64\n",
      "core.layers.2.norm.bias 64\n",
      "core.proj_list.0.weight 128\n",
      "core.proj_list.0.bias 64\n",
      "core.proj_list.1.weight 128\n",
      "core.proj_list.1.bias 64\n",
      "core.proj_list.2.weight 128\n",
      "core.proj_list.2.bias 64\n",
      "fc.weight 16384\n",
      "fc.bias 256\n",
      "im_policy.weight 1280\n",
      "im_policy.bias 5\n",
      "policy.weight 1280\n",
      "policy.bias 5\n",
      "baseline.weight 512\n",
      "baseline.bias 2\n",
      "reset.weight 512\n",
      "reset.bias 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n",
      "Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000 (0.000000). Loss inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434583/3678687939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_434583/3678687939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m     \u001b[0;31m# Try joining actors then quit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if flags.reward_type == 0:\n",
    "    flags.num_rewards = num_rewards = 1\n",
    "else:\n",
    "    flags.num_rewards = num_rewards = 2\n",
    "flags.im_discounting = flags.discounting ** (1/flags.rec_t)    \n",
    "    \n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "\n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags))\n",
    "obs_shape = env.gym_env.observation_space.shape\n",
    "\n",
    "mp.set_sharing_strategy('file_system')\n",
    "\n",
    "if flags.load_checkpoint:\n",
    "    flags.savedir = os.path.split(os.path.split(flags.load_checkpoint)[0])[0]\n",
    "    flags.xpid = os.path.split(os.path.split(flags.load_checkpoint)[0])[-1]    \n",
    "else:\n",
    "    if flags.xpid is None:\n",
    "        flags.xpid = \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "plogger = file_writer.FileWriter(\n",
    "    xpid=flags.xpid, xp_args=flags.__dict__, rootdir=flags.savedir\n",
    ")\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available():\n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser(\"%s/%s/%s\" % (flags.savedir, flags.xpid, \"model.tar\"))\n",
    ")\n",
    "\n",
    "if flags.num_buffers is None:  # Set sensible default for num_buffers.\n",
    "    flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n",
    "if flags.num_actors >= flags.num_buffers:\n",
    "    raise ValueError(\"num_buffers should be larger than num_actors\")\n",
    "if flags.num_buffers < flags.batch_size:\n",
    "    raise ValueError(\"num_buffers should be larger than batch_size\")\n",
    "\n",
    "T = flags.unroll_length\n",
    "B = flags.batch_size\n",
    "\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags)\n",
    "buffers = create_buffers(flags, obs_shape, num_actions, num_rewards)\n",
    "\n",
    "if flags.load_checkpoint:\n",
    "    train_checkpoint = torch.load(flags.load_checkpoint)\n",
    "    actor_net.load_state_dict(train_checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "actor_net.share_memory()\n",
    "\n",
    "# Add initial RNN state.\n",
    "initial_agent_state_buffers = []\n",
    "for _ in range(flags.num_buffers):\n",
    "    state = actor_net.initial_state(batch_size=1)\n",
    "    for t in state:\n",
    "        t.share_memory_()\n",
    "    initial_agent_state_buffers.append(state)\n",
    "\n",
    "actor_processes = []\n",
    "ctx = mp.get_context()\n",
    "free_queue = ctx.SimpleQueue()\n",
    "full_queue = ctx.SimpleQueue()\n",
    "\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=act, args=(flags, i, free_queue, full_queue,\n",
    "            actor_net, model, buffers, initial_agent_state_buffers,),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)\n",
    "\n",
    "learner_net = Actor_net(obs_shape, num_actions, flags)\n",
    "if flags.load_checkpoint:\n",
    "    learner_net.load_state_dict(train_checkpoint[\"model_state_dict\"])\n",
    "learner_net = learner_net.to(device=flags.device)  \n",
    "\n",
    "if not flags.disable_adam:\n",
    "    print(\"Using Adam...\")        \n",
    "    optimizer = torch.optim.Adam(learner_net.parameters(),lr=flags.learning_rate)\n",
    "else:\n",
    "    print(\"Using RMS Prop...\")\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        learner_net.actor.parameters(),\n",
    "        lr=flags.learning_rate,\n",
    "        momentum=flags.momentum,\n",
    "        eps=flags.epsilon,\n",
    "        alpha=flags.alpha,)\n",
    "    \n",
    "if flags.load_checkpoint:\n",
    "    optimizer.load_state_dict(train_checkpoint[\"optimizer_state_dict\"])    \n",
    "    \n",
    "print(\"All parameters: \")\n",
    "for k, v in learner_net.named_parameters(): print(k, v.numel())    \n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "if flags.load_checkpoint:\n",
    "    scheduler.load_state_dict(train_checkpoint[\"scheduler_state_dict\"])\n",
    "    \n",
    "logger = logging.getLogger(\"logfile\")\n",
    "stat_keys = [\"mean_episode_return\", \"episode_returns\", \"total_loss\",\n",
    "    \"pg_loss\", \"baseline_loss\", \"entropy_loss\", \"max_rollout_depth\"]\n",
    "if flags.reward_type == 1:\n",
    "    stat_keys.extend([\"im_pg_loss\", \"im_baseline_loss\"])\n",
    "\n",
    "logger.info(\"# Step\\t%s\", \"\\t\".join(stat_keys))\n",
    "\n",
    "step, stats, last_returns, last_im_returns, tot_eps = 0, {}, deque(maxlen=400), deque(maxlen=40000), 0\n",
    "if flags.load_checkpoint:\n",
    "    step = train_checkpoint[\"scheduler_state_dict\"][\"_step_count\"] * T * B\n",
    "    \n",
    "def batch_and_learn(i, lock=threading.Lock()):\n",
    "    \"\"\"Thread target for the learning process.\"\"\"\n",
    "    #nonlocal step, stats, last_returns, tot_eps\n",
    "    global step, stats, last_returns, last_im_returns, tot_eps\n",
    "    timings = prof.Timings()\n",
    "    while step < flags.total_steps:\n",
    "        timings.reset()\n",
    "        batch, agent_state = get_batch(flags, free_queue, full_queue, buffers,\n",
    "            initial_agent_state_buffers, timings,)\n",
    "        stats = learn(flags, actor_net, learner_net, batch, agent_state, optimizer, \n",
    "            scheduler)\n",
    "        last_returns.extend(stats[\"episode_returns\"])\n",
    "        if \"im_episode_returns\" in stats:\n",
    "            last_im_returns.extend(stats[\"im_episode_returns\"])\n",
    "        tot_eps = tot_eps + len(stats[\"episode_returns\"])\n",
    "        timings.time(\"learn\")\n",
    "        with lock:\n",
    "            to_log = dict(step=step)\n",
    "            to_log.update({k: stats[k] for k in stat_keys})            \n",
    "            to_log.update({\"rmean_im_episode_return\": np.average(last_im_returns) if len(last_im_returns) > 0 else 0.,\n",
    "                           \"rmean_episode_return\": np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "                           \"episode\": tot_eps})\n",
    "            plogger.log(to_log)\n",
    "            step += T * B\n",
    "\n",
    "    if i == 0:\n",
    "        logging.info(\"Batch and learn: %s\", timings.summary())\n",
    "\n",
    "for m in range(flags.num_buffers):\n",
    "    free_queue.put(m)\n",
    "\n",
    "threads = []\n",
    "for i in range(flags.num_learner_threads):\n",
    "    thread = threading.Thread(\n",
    "        target=batch_and_learn, name=\"batch-and-learn-%d\" % i, args=(i,)\n",
    "    )\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "def checkpoint():\n",
    "    if flags.disable_checkpoint:\n",
    "        return\n",
    "    logging.info(\"Saving checkpoint to %s\", checkpointpath)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": actor_net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"flags\": vars(flags),\n",
    "        },\n",
    "        checkpointpath,\n",
    "    )\n",
    "\n",
    "timer = timeit.default_timer\n",
    "try:\n",
    "    last_checkpoint_time = timer()\n",
    "    while step < flags.total_steps:\n",
    "        start_step = step\n",
    "        start_time = timer()\n",
    "        time.sleep(5)\n",
    "\n",
    "        if timer() - last_checkpoint_time > 10 * 60:  # Save every 10 min.\n",
    "            checkpoint()\n",
    "            last_checkpoint_time = timer()\n",
    "\n",
    "        sps = (step - start_step) / (timer() - start_time)\n",
    "        if stats.get(\"episode_returns\", None):\n",
    "            mean_return = (\n",
    "                \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n",
    "            )\n",
    "        else:\n",
    "            mean_return = \"\"\n",
    "        total_loss = stats.get(\"total_loss\", float(\"inf\"))\n",
    "\n",
    "        print_str =  \"Steps %i @ %.1f SPS. Eps %i. L400 Return %f (%f). Loss %.2f\" % (step, sps, tot_eps, \n",
    "            np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "            np.average(last_im_returns) if len(last_im_returns) > 0 else 0.,\n",
    "            total_loss)\n",
    "\n",
    "        for s in [\"max_rollout_depth\", \"pg_loss\", \"baseline_loss\", \"im_pg_loss\", \n",
    "                  \"im_baseline_loss\", \"entropy_loss\", \"reg_loss\", \"total_norm\"]:\n",
    "            if s in stats: print_str += \" %s %.2f\" % (s, stats[s])\n",
    "\n",
    "        logging.info(print_str)\n",
    "except KeyboardInterrupt:\n",
    "    for thread in threads:\n",
    "        thread.join()        \n",
    "    # Try joining actors then quit.\n",
    "else:\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    logging.info(\"Learning finished after %d steps.\", step)\n",
    "finally:\n",
    "    for _ in range(flags.num_actors):\n",
    "        free_queue.put(None)\n",
    "    for actor in actor_processes:\n",
    "        actor.join(timeout=1)\n",
    "\n",
    "checkpoint()\n",
    "plogger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a726743",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = flags.total_steps + 1\n",
    "for thread in threads:\n",
    "     thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(flags.num_actors):\n",
    "    free_queue.put(None)\n",
    "for actor in actor_processes:\n",
    "    actor.join(timeout=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b71ad",
   "metadata": {},
   "source": [
    "<font size=\"5\">Agent Debug and Visualize</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d487f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker\n",
    "\n",
    "def plot_obs(x, ax=None, title=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    ax.imshow(torch.swapaxes(torch.swapaxes(x[0].cpu(),0,2),0,1), interpolation='nearest', aspect=\"auto\")\n",
    "    if title is not None: ax.set_title(title)\n",
    "    \n",
    "def plot_qn_sa(q_s_a, n_s_a, max_q_s_a=None, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    xs = np.arange(len(q_s_a))\n",
    "\n",
    "    ax.bar(xs - 0.3, q_s_a.cpu(), color = 'g', width = 0.3, label=\"q_s_a\")    \n",
    "    ax_n = ax.twinx()\n",
    "    if max_q_s_a is not None:\n",
    "        ax.bar(xs, max_q_s_a.cpu(), color = 'r', width = 0.3, label=\"max_q_s_a\")        \n",
    "    ax_n.bar(xs + (0.3 if max_q_s_a is not None else 0.), \n",
    "             n_s_a.cpu(), bottom=0, color = 'b', width = 0.3, label=\"n_s_a\")\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(q_s_a))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))    \n",
    "    ax.legend(loc=\"upper left\")   \n",
    "    ax_n.legend(loc=\"upper right\") \n",
    "    ax.set_title(\"q_s_a and n_s_a\")\n",
    "    \n",
    "def plot_policies(model_logit, actor_logit, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    model_prob = torch.softmax(model_logit, dim=-1).detach().cpu().numpy()\n",
    "    prob = torch.softmax(actor_logit, dim=-1).detach().cpu().numpy()\n",
    "    ax.set_title(\"Real policy prob\")\n",
    "    xs = np.arange(len(model_prob))\n",
    "    ax.bar(xs - 0.1, model_prob, color = 'g', width = 0.1, label=\"model policy prob\")\n",
    "    ax.bar(xs, prob, color = 'r', width = 0.1, label=\"agent policy prob\")\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(model_prob))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))\n",
    "    ax.set_ylim(0, 1)        \n",
    "    ax.legend()       \n",
    "        \n",
    "def plot_im_policies(im_policy_logits, reset_policy_logits, im_action, reset_action, \n",
    "                     one_hot=True, reset_ind=0, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "        \n",
    "    rec_t, num_actions = im_policy_logits.shape\n",
    "    num_actions += 1\n",
    "    rec_t -= 1\n",
    "        \n",
    "    im_prob = torch.softmax(im_policy_logits, dim=-1).detach().cpu().numpy()\n",
    "    reset_prob = torch.softmax(reset_policy_logits, dim=-1)[:,[reset_ind]].detach().cpu().numpy()\n",
    "    im_reset_prob = np.concatenate([im_prob, reset_prob], axis=-1)\n",
    "    \n",
    "    if not one_hot: im_action = F.one_hot(im_action, num_actions - 1)\n",
    "    im_action = im_action.detach().cpu().numpy()\n",
    "    reset_action = reset_action.unsqueeze(-1).detach().cpu().numpy()    \n",
    "    im_reset_action = np.concatenate([im_action, reset_action], axis=-1)\n",
    "    \n",
    "    im_reset_prob = im_reset_prob[:-1]\n",
    "    im_reset_action = im_reset_action[:-1]\n",
    "    \n",
    "    xs = np.arange(rec_t)\n",
    "    labels = ['noop', 'up', 'down', 'left', 'right', 'reset']\n",
    "    for i in range(num_actions):        \n",
    "        c = ax.bar(xs + 0.8 * (i / num_actions), im_reset_prob[:,i], width = 0.8 / (num_actions), label=labels[i])  \n",
    "        color = c.patches[0].get_facecolor()\n",
    "        color = color[:3] + (color[3] * 0.5,)\n",
    "        ax.bar(xs + 0.8 * (i / num_actions), im_reset_action[:,i], width = 0.8 / (num_actions), color=color)\n",
    "        \n",
    "    \n",
    "    #xs = np.arange(num_actions)\n",
    "    #for i in range(rec_t):\n",
    "    #    ax.bar(xs + 0.8 * (i / rec_t), im_reset_action[i], width = 0.8 / (rec_t), color=\"#cccccc\")\n",
    "    #    ax.bar(xs + 0.8 * (i / rec_t), im_reset_prob[i], width = 0.8 / (rec_t))        \n",
    "    #ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(num_actions)))\n",
    "    #ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right', 'reset'))    \n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)   \n",
    "    ax.set_title(\"Imagainary policy prob\")\n",
    "    plt.show()         \n",
    "    \n",
    "def print_im_actions(im_dict, print_stat=False):\n",
    "\n",
    "    lookup_dict = {0:\"Noop\",\n",
    "                   1:\"Up\",\n",
    "                   2:\"Down\",\n",
    "                   3:\"Left\",\n",
    "                   4:\"Right\"}\n",
    "\n",
    "    print_strs = []\n",
    "    n, s = 1, \"\"\n",
    "    reset = False\n",
    "    for im, reset in zip(im_dict[\"im_action\"][:-1], im_dict[\"reset_action\"][:-1]):\n",
    "        s += lookup_dict[im.item()] + \", \"\n",
    "        if reset:        \n",
    "            s += \"Reset\"\n",
    "            print_strs.append(\"%d: %s\" %(n, s))\n",
    "            s = \"\"\n",
    "            n += 1\n",
    "    if not reset: print_strs.append(\"%d: %s\" %(n, s[:-2]))\n",
    "    if print_stat: \n",
    "        for s in print_strs: print(s) \n",
    "    return print_strs\n",
    "\n",
    "def plot_im_actions(im_dict, ax=None):    \n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    ax.axis('off')\n",
    "    print_strs = print_im_actions(im_dict, print_stat=False)\n",
    "    for n, s in enumerate(print_strs):  \n",
    "        txt = ax.text(0, 0.8 - 0.1 * n, s, size='x-large')        \n",
    "        txt.set_clip_on(True) \n",
    "    ax.set_xlim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8908d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CUDA.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  398078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bsz = 1\n",
    "\n",
    "name = \"alstm_3_1_rec_n_10_rei_aug2_clip_mem1_imcost_0.5_perfect\"\n",
    "checkpoint = torch.load(\"/home/sc/RS/thinker/logs/planner_logs/%s/model.tar\" % name)\n",
    "\n",
    "flags_ = checkpoint[\"flags\"]\n",
    "\n",
    "flags.tran_dim = flags_[\"tran_dim\"]\n",
    "\n",
    "flags.rec_t = flags_[\"rec_t\"]\n",
    "flags.tran_mem_n = flags_[\"tran_mem_n\"]\n",
    "flags.no_mem = flags_[\"no_mem\"] \n",
    "\n",
    "flags.reward_type = flags_[\"reward_type\"] \n",
    "flags.stat_type = flags_[\"stat_type\"]\n",
    "flags.perfect_model = flags_[\"perfect_model\"]\n",
    "flags.reset_m = flags_[\"reset_m\"] if \"reset_m\" in flags_.keys() else -1\n",
    "\n",
    "flags.discounting = flags_[\"discounting\"]\n",
    "\n",
    "if flags.reward_type == 0:\n",
    "    flags.num_rewards = num_rewards = 1\n",
    "else:\n",
    "    flags.num_rewards = num_rewards = 2\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "model_checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(model_checkpoint[\"model_state_dict\"])   \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags))\n",
    "obs_shape = env.gym_env.observation_space.shape\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available():\n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags).to(flags.device)\n",
    "actor_net.load_state_dict(checkpoint[\"model_state_dict\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb82bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "debug = True\n",
    "eps_n = 1000\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "model.train(False)\n",
    "env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True), model=model, flags=flags)\n",
    "env = Environment(env)\n",
    "\n",
    "core_state = actor_net.initial_state(bsz)\n",
    "core_state = tuple(v.to(flags.device) for v in core_state)\n",
    "returns = []\n",
    "obs = env.initial()\n",
    "\n",
    "t = 0\n",
    "im_list = [\"im_policy_logits\", \"reset_policy_logits\", \"im_action\", \"reset_action\"]\n",
    "im_dict = {k: [] for k in im_list}\n",
    "while(True):\n",
    "    if len(returns) > (0 if debug else eps_n): break    \n",
    "    with torch.no_grad():\n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}                             \n",
    "        actor_out, core_state = actor_net(obs, core_state, debug=False)\n",
    "        action = torch.cat([actor_out['action'][0].unsqueeze(-1), \n",
    "                            actor_out['im_action'][0].unsqueeze(-1), \n",
    "                            actor_out['reset_action'][0].unsqueeze(-1)], dim=-1)\n",
    "        if len(im_dict['reset_action']) > 0:\n",
    "            im_dict['reset_action'][-1] = env.gym_env.ret_dict['reset'].to(flags.device)\n",
    "        for k in im_list: im_dict[k].append(actor_out[k][0,0].unsqueeze(0))        \n",
    "        if debug and obs[\"cur_t\"][0,0] == (flags.rec_t - 1): \n",
    "            for k in im_list: im_dict[k] = torch.concat(im_dict[k], dim=0)\n",
    "            \n",
    "            fig, axs = plt.subplots(1, 4, figsize=(24,6))  \n",
    "            ret_dict = env.gym_env.ret_dict\n",
    "            title = \"step: %d; values: %.4f\" % (t, ret_dict[\"v0\"][0].cpu())\n",
    "            if flags.reward_type == 1: title += \" im_return: %.4f\" % obs['episode_return'][..., 1]            \n",
    "            plot_obs(env.gym_env.x/255, axs[0], title=title)          \n",
    "            max_q_s_a = ret_dict[\"max_q_s_a\"][0] if \"max_q_s_a\" in ret_dict else None\n",
    "            plot_qn_sa(ret_dict[\"q_s_a\"][0], ret_dict[\"n_s_a\"][0], max_q_s_a, ax=axs[2]) \n",
    "            plot_policies(ret_dict[\"logit0\"][0], actor_out[\"policy_logits\"][0,0], ax=axs[3])    \n",
    "            plot_im_policies(**im_dict, one_hot=False, reset_ind=1, ax=axs[1])    \n",
    "            plt.show()            \n",
    "            print_im_actions(im_dict, print_stat=True)\n",
    "            im_dict = {k: [] for k in im_list}            \n",
    "        obs = env.step(action.unsqueeze(0))        \n",
    "        if torch.any(obs['done']):\n",
    "            new_rets = obs['episode_return'][obs['done']][:,0].numpy()\n",
    "            returns.extend(new_rets)\n",
    "            print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n",
    "    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd9ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6346fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = True\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n \n",
    "\n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "                model=model, rec_t=flags.rec_t, flags=flags))\n",
    "obs = env.initial()\n",
    "\n",
    "obs_shape, num_actions = env.gym_env.observation_space.shape, env.gym_env.action_space.n   \n",
    "num_actions = env.gym_env.action_space.n\n",
    "actor_net = Actor_net(obs_shape, num_actions, flags)\n",
    "core_state = actor_net.initial_state(1)\n",
    "\n",
    "for t in range(10):\n",
    "  actor_out, core_state = actor_net(obs, core_state=core_state)\n",
    "  action = torch.cat([actor_out['action'], actor_out['im_action'], actor_out['reset_action']], dim=-1)\n",
    "  obs = env.step(action.unsqueeze(0))\n",
    "  print(t, obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deef3add",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_actions = 4    \n",
    "root = Node(parent=None, action=2, logit=torch.tensor([0.3]), num_actions=num_actions, discounting=0.97)\n",
    "root.expand(r=torch.tensor([3.]), v=torch.tensor([2.]), logits=torch.tensor([0.3,0.4,0.5,0.6]), encoded=None)\n",
    "root.visit()\n",
    "cur_node = root\n",
    "print(1, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[3]\n",
    "cur_node.expand(r=torch.tensor([1.]), v=torch.tensor([4.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "\n",
    "print(2, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[2]\n",
    "cur_node.expand(r=torch.tensor([100.]), v=torch.tensor([0.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "print(3, root.rollout_qs, root.stat())\n",
    "\n",
    "root.visit()\n",
    "cur_node = root\n",
    "print(4, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[3]\n",
    "#cur_node.expand(r=torch.tensor([1.]), v=torch.tensor([3.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "\n",
    "print(5, root.rollout_qs, root.stat())\n",
    "cur_node = cur_node.children[1]\n",
    "cur_node.expand(r=torch.tensor([-10.]), v=torch.tensor([0.]), logits=torch.tensor([0.4,0.4,0.5,0.6]), encoded=None)\n",
    "cur_node.visit()\n",
    "print(6, root.rollout_qs, root.stat())\n",
    "\n",
    "flags.rec_t = 20\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n     \n",
    "    \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = Environment(ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags))\n",
    "_ = env.initial()\n",
    "\n",
    "act_seqs = [[2, 1, 1], [2, 2, 1], [2, 3, 1], [2, 4, 1], [2, 1, 0], [2, 2, 1]]\n",
    "#act_seqs = [[2, 4, 0], [2, 1, 1], [2, 4, 0], [2, 2, 1], [2, 2, 1]]\n",
    "\n",
    "for a in act_seqs:\n",
    "    obs = env.step(torch.tensor(a).unsqueeze(0).unsqueeze(0))\n",
    "    x = obs['frame'][0, 0, :, 0, 0]\n",
    "    r = obs['reward'][0, 0, :]\n",
    "    print(\"================================================\")\n",
    "    print(\"a\", a[1:])\n",
    "    print(\"x\", x)\n",
    "    print(\"r\", r)\n",
    "    print(\"rollout_qs\", torch.concat(env.gym_env.root_node.rollout_qs) / flags.discounting)\n",
    "    print(\"root stat\", env.gym_env.root_node.ret_dict)\n",
    "    print(\"last stat\", env.gym_env.last_node.ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e62d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class ModelWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, model, flags):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.env = env\n",
    "        self.model = model                \n",
    "        self.rec_t = flags.rec_t        \n",
    "        self.discounting = flags.discounting\n",
    "        self.stat_type = flags.stat_type    \n",
    "        self.reward_type = flags.reward_type    \n",
    "        self.no_mem = flags.no_mem\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        # 0 for the most basic; \n",
    "        # 1 for augmented input in root stat; \n",
    "        # 2 for tree stat\n",
    "        if self.stat_type == 0:\n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 5 + num_actions * 4 + self.rec_t\n",
    "        elif self.stat_type == 1:            \n",
    "            self.use_model = self.use_model_raw\n",
    "            obs_n = 7 + num_actions * 7 + self.rec_t\n",
    "        elif self.stat_type == 1.5:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 6 + num_actions * 10 + self.rec_t             \n",
    "        elif self.stat_type == 2:\n",
    "            self.use_model = self.use_model_tree    \n",
    "            obs_n = 9 + num_actions * 10 + self.rec_t         \n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "          low=-np.inf, high=np.inf, shape=(obs_n, 1, 1), dtype=float)\n",
    "        self.model.train(False)        \n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        x = self.env.reset()\n",
    "        self.cur_t = 0        \n",
    "        out = self.use_model_raw(x, 0., 0, self.cur_t, 1.)\n",
    "        out_ = self.use_model_tree(x, 0., 0, self.cur_t, 1.)\n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q\n",
    "        self.max_rollout_depth = 0.\n",
    "        return out, out_\n",
    "    \n",
    "    def step(self, action):  \n",
    "        re_action, im_action, reset = action\n",
    "        info = {}\n",
    "        info[\"max_rollout_depth\"] = self.max_rollout_depth\n",
    "        if self.cur_t < self.rec_t - 1:\n",
    "          self.cur_t += 1\n",
    "          out = self.use_model_raw(None, None, im_action, self.cur_t, reset)      \n",
    "          out_ = self.use_model_tree(None, None, im_action, self.cur_t, reset)      \n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([0.])\n",
    "          else:\n",
    "            r = np.array([0., (self.root_max_q - self.last_root_max_q).item()], dtype=np.float32)\n",
    "          done = False\n",
    "          info['cur_t'] = self.cur_t   \n",
    "        else:\n",
    "          self.cur_t = 0\n",
    "          x, r, done, info_ = self.env.step(re_action)                    \n",
    "          out = self.use_model_raw(x, r, re_action, self.cur_t, 1.) \n",
    "          out_ = self.use_model_tree(x, r, re_action, self.cur_t, 1.) \n",
    "          info.update(info_)\n",
    "          info['cur_t'] = self.cur_t\n",
    "          if self.reward_type == 0:\n",
    "            r = np.array([r])\n",
    "          else:\n",
    "            r = np.array([r, 0.], dtype=np.float32)   \n",
    "        if self.reward_type == 1:\n",
    "            self.last_root_max_q = self.root_max_q        \n",
    "        return out, out_, r, done, info        \n",
    "        \n",
    "    def use_model_raw(self, x, r, a, cur_t, reset):\n",
    "        # input: \n",
    "        # r: reward - [,]; x: frame - [C, H, W]; a: action - [,]\n",
    "        # cur_t: int; reset at cur_t == 0  \n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:                \n",
    "                self.rollout_depth_ = 0.\n",
    "                if self.no_mem:\n",
    "                    self.re_action = F.one_hot(torch.zeros(1, dtype=torch.long), self.num_actions)   \n",
    "                else:\n",
    "                    self.re_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)                   \n",
    "                \n",
    "                x = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = x\n",
    "                _, vs, logits, encodeds = self.model(x, self.re_action.unsqueeze(0), one_hot=True)                \n",
    "                self.encoded = encodeds[-1]    \n",
    "                self.encoded_reset = self.encoded.clone()\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    self.re_reward = torch.tensor([[0.]], dtype=torch.float32)                \n",
    "                else:\n",
    "                    self.re_reward = torch.tensor([[r]], dtype=torch.float32)                \n",
    "                    \n",
    "                self.v0 = vs[-1].unsqueeze(-1).clone()\n",
    "                self.logit0 = logits[-1].clone()\n",
    "                \n",
    "                self.im_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.im_reset = torch.tensor([[1.]], dtype=torch.float32)\n",
    "                self.im_reward = torch.zeros(1, 1, dtype=torch.float32)                                \n",
    "                self.v = vs[-1].unsqueeze(-1)\n",
    "                self.logit = logits[-1]\n",
    "                self.rollout_first_action = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                self.rollout_return = torch.zeros(1, 1, dtype=torch.float32)                \n",
    "                self.q_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)\n",
    "                self.n_s_a = torch.zeros(1, self.num_actions, dtype=torch.float32)                \n",
    "            else:\n",
    "                self.rollout_depth_ += 1                \n",
    "                \n",
    "                self.im_action = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions)   \n",
    "                rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "                   self.im_action.unsqueeze(0), one_hot=True)\n",
    "                self.encoded = encodeds[-1]        \n",
    "                \n",
    "                self.im_reward = rs[-1].unsqueeze(-1)\n",
    "                self.v = vs[-1].unsqueeze(-1)    \n",
    "                self.logit = logits[-1]     \n",
    "                \n",
    "                if self.im_reset: \n",
    "                    # last action's reset is true; re-initialize everything                    \n",
    "                    self.rollout_first_action = self.im_action.clone()\n",
    "                    self.rollout_return_wo_v = torch.zeros(1, 1, dtype=torch.float32)   \n",
    "                    self.rollout_depth_ = 1                      \n",
    "                    \n",
    "                self.rollout_return_wo_v += (self.discounting ** (self.rollout_depth_-1)) * self.im_reward\n",
    "                self.rollout_return = self.rollout_return_wo_v + (\n",
    "                    self.discounting ** (self.rollout_depth_)) * self.v   \n",
    "                    \n",
    "                self.im_reset = torch.tensor([[reset]], dtype=torch.float32)\n",
    "                if self.im_reset:                    \n",
    "                    rollout_first_action_label = torch.argmax(self.rollout_first_action, dim=1)                    \n",
    "                    q = self.q_s_a[:, rollout_first_action_label]\n",
    "                    n = self.n_s_a[:, rollout_first_action_label]                    \n",
    "                    ret = self.rollout_return[:, 0]\n",
    "                    self.n_s_a[:, rollout_first_action_label] += 1                    \n",
    "                    self.q_s_a[:, rollout_first_action_label] = (n * q) / (n + 1) + ret / (n + 1)\n",
    "                    \n",
    "        time = F.one_hot(torch.tensor([cur_t]).long(), self.rec_t)\n",
    "        depc = torch.tensor([[self.discounting ** (self.rollout_depth_-1)]])\n",
    "        ret_dict = {\"re_action\": self.re_action,\n",
    "                    \"re_reward\": self.re_reward,\n",
    "                    \"v0\": self.v0,\n",
    "                    \"logit0\": self.logit0,\n",
    "                    \"im_action\": self.im_action,\n",
    "                    \"im_reset\": self.im_reset,\n",
    "                    \"im_reward\": self.im_reward,\n",
    "                    \"v\": self.v,\n",
    "                    \"logit\": self.logit,\n",
    "                    \"rollout_first_action\": self.rollout_first_action,\n",
    "                    \"rollout_return\": self.rollout_return,\n",
    "                    \"n_s_a\": self.n_s_a / self.rec_t,\n",
    "                    \"q_s_a\": self.q_s_a,\n",
    "                    \"time\": time,\n",
    "                    \"depc\": depc}        \n",
    "        self.ret_dict = ret_dict\n",
    "        if self.stat_type == 1:\n",
    "            out = torch.concat(list(ret_dict.values()), dim=-1)   \n",
    "        else:\n",
    "            core_inputs = [\"re_action\", \"re_reward\", \"v0\", \"logit0\", \"im_action\",\n",
    "                           \"im_reset\", \"im_reward\", \"v\", \"logit\", \"time\"]\n",
    "            out = torch.concat([ret_dict[v] for v in core_inputs], dim=-1)   \n",
    "        self.encoded = reset * self.encoded_reset + (1 - reset) * self.encoded\n",
    "        return out[0]      \n",
    "    \n",
    "    def use_model_tree(self, x, r, a, cur_t, reset):\n",
    "        with torch.no_grad():\n",
    "            if cur_t == 0:\n",
    "                self.rollout_depth = 0.\n",
    "                self.max_rollout_depth = 0.\n",
    "                \n",
    "                if self.no_mem:\n",
    "                    re_action = 0\n",
    "                    re_reward = torch.tensor([0.], dtype=torch.float32)                \n",
    "                else:\n",
    "                    re_action = a                \n",
    "                    re_reward = torch.tensor([r], dtype=torch.float32)                \n",
    "                \n",
    "                x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(0)\n",
    "                self.x = x_tensor\n",
    "                a_tensor = F.one_hot(torch.tensor(re_action, dtype=torch.long).unsqueeze(0), self.num_actions)\n",
    "                \n",
    "                _, vs, logits, encodeds = self.model(x_tensor, a_tensor.unsqueeze(0), one_hot=True)\n",
    "                \n",
    "                self.root_node = Node(parent=None, action=re_action, logit=None, \n",
    "                                      num_actions=self.num_actions,\n",
    "                                      discounting=self.discounting,\n",
    "                                      rec_t=self.rec_t)\n",
    "                self.root_node.expand(r=re_reward, v=vs[-1, 0].unsqueeze(-1), logits=logits[-1, 0],\n",
    "                                      encoded=encodeds[-1])\n",
    "                self.root_node.visit()\n",
    "                self.cur_node = self.root_node\n",
    "                self.rollout_first_action_ = torch.zeros(self.num_actions)\n",
    "            else:\n",
    "                if self.rollout_depth == 0:\n",
    "                    self.rollout_first_action_ = F.one_hot(torch.tensor(a, dtype=torch.long), self.num_actions)\n",
    "                self.rollout_depth += 1    \n",
    "                self.max_rollout_depth = max(self.max_rollout_depth, self.rollout_depth)\n",
    "                next_node = self.cur_node.children[a]\n",
    "                if not next_node.expanded():\n",
    "                    a_tensor = F.one_hot(torch.tensor(a, dtype=torch.long).unsqueeze(0), self.num_actions) \n",
    "                    rs, vs, logits, encodeds = self.model.forward_encoded(self.cur_node.encoded, \n",
    "                        a_tensor.unsqueeze(0), one_hot=True)\n",
    "                    next_node.expand(r=rs[-1, 0].unsqueeze(-1), v=vs[-1, 0].unsqueeze(-1), \n",
    "                                     logits=logits[-1, 0], encoded=encodeds[-1])\n",
    "                next_node.visit()\n",
    "                self.cur_node = next_node\n",
    "            \n",
    "            root_node_stat = self.root_node.stat()\n",
    "            cur_node_stat = self.cur_node.stat()                        \n",
    "            reset = torch.tensor([reset], dtype=torch.float32)\n",
    "            time = F.one_hot(torch.tensor(cur_t).long(), self.rec_t)\n",
    "            depc = torch.tensor([self.discounting ** (self.rollout_depth-1)])\n",
    "            \n",
    "            root_trail_r = self.root_node.trail_r / self.discounting\n",
    "            root_rollout_q = self.root_node.rollout_q / self.discounting\n",
    "            root_max_q = torch.max(torch.concat(self.root_node.rollout_qs)).unsqueeze(-1) / self.discounting\n",
    "            \n",
    "            ret_list = [root_node_stat, cur_node_stat, reset, time, depc,]\n",
    "            if self.stat_type >= 2: ret_list.extend([root_trail_r, root_rollout_q, root_max_q])            \n",
    "                \n",
    "            if self.stat_type == 1:\n",
    "                ret_list = [self.root_node.ret_dict[\"action\"],\n",
    "                            self.root_node.ret_dict[\"r\"],\n",
    "                            self.root_node.ret_dict[\"v\"],\n",
    "                            self.root_node.ret_dict[\"child_logits\"],\n",
    "                            self.cur_node.ret_dict[\"action\"],\n",
    "                            reset,\n",
    "                            self.cur_node.ret_dict[\"r\"],\n",
    "                            self.cur_node.ret_dict[\"v\"],\n",
    "                            self.cur_node.ret_dict[\"child_logits\"],\n",
    "                            self.rollout_first_action_,\n",
    "                            self.root_node.rollout_q / self.discounting,\n",
    "                            self.root_node.ret_dict[\"child_rollout_ns\"],\n",
    "                            self.root_node.ret_dict[\"child_rollout_qs_mean\"],\n",
    "                            time,\n",
    "                            depc                            \n",
    "                            ]                 \n",
    "                \n",
    "                \n",
    "            out = torch.concat(ret_list, dim=-1)            \n",
    "            self.last_node = self.cur_node     \n",
    "            \n",
    "            self.root_max_q = root_max_q\n",
    "            self.ret_dict = {\"v0\": self.root_node.ret_dict[\"v\"].unsqueeze(0),\n",
    "                             \"q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_mean\"].unsqueeze(0),\n",
    "                             \"max_q_s_a\": self.root_node.ret_dict[\"child_rollout_qs_max\"].unsqueeze(0),\n",
    "                             \"n_s_a\": self.root_node.ret_dict[\"child_rollout_ns\"].unsqueeze(0),\n",
    "                             \"logit0\": self.root_node.ret_dict[\"child_logits\"].unsqueeze(0),}\n",
    "            \n",
    "            if reset:\n",
    "                self.rollout_depth = 0\n",
    "                self.cur_node = self.root_node\n",
    "                self.cur_node.visit()\n",
    "                \n",
    "            return out\n",
    "        \n",
    "flags.stat_type = 1\n",
    "flags.rec_t = 10\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n         \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags)\n",
    "\n",
    "_ = env.reset()\n",
    "\n",
    "def debug_out(out, num_actions, rec_t):\n",
    "    struct = [[\"a0\", num_actions], \n",
    "        [\"r0\", 1], \n",
    "        [\"v0\", 1], \n",
    "        [\"pi0\", num_actions],\n",
    "        [\"a\", num_actions],\n",
    "        [\"reset\", 1],\n",
    "        [\"r\", 1],\n",
    "        [\"v\", 1],\n",
    "        [\"pi\",num_actions],\n",
    "        [\"first_a\", num_actions],\n",
    "        [\"rollout_v\", 1],\n",
    "        [\"ns\", num_actions],\n",
    "        [\"qs\", num_actions],\n",
    "        [\"t\", rec_t],\n",
    "        [\"depc\", 1]]\n",
    "    m = 0\n",
    "    for key, n in struct:\n",
    "        print(key, out[m:m+n])\n",
    "        m = m + n\n",
    "\n",
    "out, out_, r, done, info = env.step([1, 1, 0])\n",
    "out, out_, r, done, info = env.step([1, 2, 0])\n",
    "out, out_, r, done, info = env.step([1, 3, 1])\n",
    "out, out_, r, done, info = env.step([1, 1, 0])\n",
    "out, out_, r, done, info = env.step([1, 2, 1])\n",
    "out, out_, r, done, info = env.step([1, 1, 1])\n",
    "\n",
    "print(\"error:\", torch.sum((out - out_)**2))\n",
    "print(\"old\")\n",
    "debug_out(out, 5, 10)\n",
    "print(\"new\")\n",
    "debug_out(out_, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "322b0170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "flags.perfect_model = True\n",
    "flags.stat_type = 2\n",
    "flags.reward_type = 1\n",
    "flags.rec_t = 5\n",
    "flags.reset_m = -1\n",
    "flags.tree_carry = True\n",
    "flags.thres_carry = True\n",
    "flags.thres_discounting = 0.9\n",
    "\n",
    "raw_env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop)\n",
    "raw_obs_shape, num_actions = raw_env.observation_space.shape, raw_env.action_space.n         \n",
    "model = Model(flags, raw_obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])    \n",
    "\n",
    "env = ModelWrapper(SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=not flags.env_disable_noop), \n",
    "     model=model, flags=flags)\n",
    "env = Environment(env)\n",
    "obs = env.initial()\n",
    "plot_obs(env.gym_env.x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7e543ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========0========\n",
      "a_tensor tensor([[[3, 3, 0]]])\n",
      "eps_step tensor([[51]], dtype=torch.int32)\n",
      "eps_return tensor([[[-0.1000,  0.0000]]])\n",
      "reward tensor([[[0., 0.]]])\n",
      "cur_t tensor([[1]])\n",
      "thres tensor([0.1688])\n",
      "root_node qs [tensor([-0.1012]), tensor([-0.1082]), tensor([-0.1150]), tensor([-0.1215])]\n",
      "child_node qs []\n",
      "=========1========\n",
      "a_tensor tensor([[[3, 3, 0]]])\n",
      "eps_step tensor([[52]], dtype=torch.int32)\n",
      "eps_return tensor([[[-0.1000,  0.0000]]])\n",
      "reward tensor([[[0., 0.]]])\n",
      "cur_t tensor([[2]])\n",
      "thres tensor([0.1688])\n",
      "root_node qs [tensor([-0.1012]), tensor([-0.1082]), tensor([-0.1150]), tensor([-0.1215])]\n",
      "child_node qs []\n",
      "=========2========\n",
      "a_tensor tensor([[[3, 3, 0]]])\n",
      "eps_step tensor([[53]], dtype=torch.int32)\n",
      "eps_return tensor([[[-0.1000,  0.0000]]])\n",
      "reward tensor([[[0., 0.]]])\n",
      "cur_t tensor([[3]])\n",
      "thres tensor([0.1688])\n",
      "root_node qs [tensor([-0.1012]), tensor([-0.1082]), tensor([-0.1150]), tensor([-0.1215])]\n",
      "child_node qs []\n",
      "=========3========\n",
      "a_tensor tensor([[[3, 3, 0]]])\n",
      "eps_step tensor([[54]], dtype=torch.int32)\n",
      "eps_return tensor([[[-0.1000,  0.0000]]])\n",
      "reward tensor([[[0., 0.]]])\n",
      "cur_t tensor([[4]])\n",
      "thres tensor([0.1688])\n",
      "root_node qs [tensor([-0.1012]), tensor([-0.1082]), tensor([-0.1150]), tensor([-0.1215]), tensor([-0.1279])]\n",
      "child_node qs []\n",
      "=========4========\n",
      "a_tensor tensor([[[3, 3, 0]]])\n",
      "eps_step tensor([[55]], dtype=torch.int32)\n",
      "eps_return tensor([[[-0.1100,  0.0000]]])\n",
      "reward tensor([[[-0.0100,  0.0000]]])\n",
      "cur_t tensor([[0]])\n",
      "thres tensor([0.1659])\n",
      "root_node qs [tensor([-0.1012]), tensor([-0.1082]), tensor([-0.1150]), tensor([-0.1215])]\n",
      "child_node qs []\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = 3\n",
    "a_seqs = ([a,a,0],[a,a,0],[a,a,0],[a,a,0],[a,a,0])\n",
    "\n",
    "#a_seqs = ([2,4,0],[2,2,0],[2,2,0],[2,2,0],[2,3,0])\n",
    "\n",
    "for n, a in enumerate(a_seqs):\n",
    "    print(\"=========%d========\"%n)\n",
    "    a_tensor = torch.tensor([[a]]).long()\n",
    "    print(\"a_tensor\", a_tensor)\n",
    "    obs = env.step(a_tensor)\n",
    "    print(\"eps_step\", obs['episode_step'])\n",
    "    print(\"eps_return\", obs['episode_return'])\n",
    "    print(\"reward\", obs['reward'])\n",
    "    print(\"cur_t\", obs['cur_t'])\n",
    "    print(\"thres\", env.gym_env.thres)\n",
    "    print(\"root_node qs\", [x / 0.97 for x in env.gym_env.root_node.rollout_qs])\n",
    "    print(\"child_node qs\", env.gym_env.root_node.children[1].rollout_qs)\n",
    "    plot_obs(env.gym_env.x_/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2df76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gym_env.root_node.expanded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_seqs = [[0,0,0]]\n",
    "\n",
    "for n, a in enumerate(a_seqs):\n",
    "    print(\"=========%d========\"%n)\n",
    "    a_tensor = torch.tensor([[a]]).long()\n",
    "    print(\"a_tensor\", a_tensor)\n",
    "    obs = env.step(a_tensor)\n",
    "    print(\"eps_step\", obs['episode_step'])\n",
    "    print(\"eps_return\", obs['episode_return'])\n",
    "    print(\"reward\", obs['reward'])\n",
    "    print(\"cur_t\", obs['cur_t'])\n",
    "    plot_obs(env.gym_env.x_/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343eadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.gym_env.root_node.encoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
