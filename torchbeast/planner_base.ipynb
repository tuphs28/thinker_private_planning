{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc8a370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG:25924 __init__:277 2022-10-31 09:09:39,743] matplotlib data path: /home/schk/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data\n",
      "[DEBUG:25924 __init__:277 2022-10-31 09:09:39,748] CONFIGDIR=/home/schk/.config/matplotlib\n",
      "[DEBUG:25924 __init__:1447 2022-10-31 09:09:39,750] interactive is False\n",
      "[DEBUG:25924 __init__:1448 2022-10-31 09:09:39,751] platform is linux\n",
      "[DEBUG:25924 __init__:1449 2022-10-31 09:09:39,751] loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', 'zipimport', '_frozen_importlib_external', '_io', 'marshal', 'posix', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_abc', '_bootlocale', '_locale', 'site', 'os', 'stat', '_stat', 'posixpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', '_distutils_hack', 'types', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'functools', '_functools', 'mpl_toolkits', 'google', 'runpy', 'pkgutil', 'weakref', '_weakrefset', 'ipykernel', 'ipykernel._version', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'typing', 'collections.abc', 'typing.io', 'typing.re', 'ipykernel.connect', 'json', 'json.decoder', 'json.scanner', '_json', 'json.encoder', 'subprocess', 'time', 'signal', 'errno', '_posixsubprocess', 'select', 'selectors', 'math', 'threading', 'traceback', 'linecache', 'tokenize', 'token', 'jupyter_client', 'jupyter_client._version', 'jupyter_client.asynchronous', 'jupyter_client.asynchronous.client', 'traitlets', 'traitlets.traitlets', 'ast', '_ast', 'inspect', 'dis', 'opcode', '_opcode', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets.utils.descriptions', 'traitlets.utils.decorators', 'copy', 'traitlets._version', 'jupyter_client.channels', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'logging', 'string', '_string', 'atexit', 'socket', '_socket', 'ssl', '_ssl', 'base64', 'struct', '_struct', 'binascii', 'asyncio.constants', 'asyncio.coroutines', 'asyncio.base_futures', 'asyncio.format_helpers', 'asyncio.log', 'asyncio.events', 'contextvars', '_contextvars', 'asyncio.base_tasks', '_asyncio', 'asyncio.futures', 'asyncio.protocols', 'asyncio.sslproto', 'asyncio.transports', 'asyncio.tasks', 'asyncio.locks', 'asyncio.runners', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.unix_events', 'asyncio.base_subprocess', 'asyncio.selector_events', 'queue', '_queue', 'zmq', 'platform', 'ctypes', '_ctypes', 'ctypes._endian', 'zmq.backend', 'zmq.backend.select', 'zmq.backend.cython', 'zmq.backend.cython._device', '_cython_0_29_30', 'cython_runtime', 'zmq.backend.cython.context', 'zmq.error', 'zmq.backend.cython.socket', 'zmq.backend.cython.message', 'random', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'pickle', '_compat_pickle', '_pickle', 'zmq.constants', 'zmq.backend.cython._poll', 'zmq.backend.cython._proxy_steerable', 'zmq.backend.cython._version', 'zmq.backend.cython.error', 'zmq.backend.cython.utils', 'zmq.sugar', 'zmq.sugar.context', 'zmq.sugar.attrsettr', 'zmq.sugar.socket', 'zmq._typing', 'typing_extensions', 'zmq.utils', 'zmq.utils.jsonapi', 'zmq.sugar.poll', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'zmq.asyncio', 'zmq._future', 'jupyter_client.channelsabc', 'jupyter_client.session', 'hmac', 'pprint', 'datetime', '_datetime', 'traitlets.config', 'traitlets.config.application', 'traitlets.config.configurable', 'traitlets.config.loader', 'argparse', 'gettext', 'locale', 'traitlets.utils.text', 'textwrap', 'traitlets.log', 'zmq.eventloop', 'zmq.eventloop.ioloop', 'tornado', 'tornado.ioloop', 'numbers', 'tornado.concurrent', 'tornado.log', 'logging.handlers', 'tornado.escape', 'html', 'html.entities', 'urllib', 'urllib.parse', 'tornado.util', 'array', 'zlib', 'tornado.speedups', 'curses', '_curses', 'tornado.platform', 'tornado.platform.asyncio', 'tornado.gen', 'zmq.eventloop.zmqstream', 'jupyter_client.adapter', 'jupyter_client.jsonutil', 'dateutil', 'dateutil._version', 'dateutil.parser', 'dateutil.parser._parser', '__future__', 'calendar', 'six', 'decimal', '_decimal', 'dateutil.relativedelta', 'dateutil._common', 'dateutil.tz', 'dateutil.tz.tz', 'six.moves', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.parser.isoparser', '_strptime', 'jupyter_client.client', 'getpass', 'termios', 'jupyter_client.clientabc', 'jupyter_client.connect', 'glob', 'fnmatch', 'tempfile', 'shutil', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'pwd', 'grp', 'jupyter_core', 'jupyter_core.version', 'jupyter_core.paths', 'pathlib', 'ntpath', 'jupyter_client.localinterfaces', 'jupyter_client.utils', 'jupyter_client.blocking', 'jupyter_client.blocking.client', 'jupyter_client.launcher', 'jupyter_client.manager', 'uuid', 'jupyter_client.managerabc', 'jupyter_client.provisioning', 'jupyter_client.provisioning.factory', 'entrypoints', 'zipfile', 'configparser', 'jupyter_client.provisioning.provisioner_base', 'jupyter_client.provisioning.local_provisioner', 'jupyter_client.kernelspec', 'jupyter_client.multikernelmanager', 'ipykernel.kernelapp', 'IPython', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'IPython.core.application', 'IPython.core.crashhandler', 'IPython.core.ultratb', 'pydoc', 'IPython.core.debugger', 'bdb', 'IPython.utils', 'IPython.utils.PyColorize', 'IPython.utils.coloransi', 'IPython.utils.ipstruct', 'IPython.utils.colorable', 'pygments', 'IPython.utils.py3compat', 'IPython.utils.encoding', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'pdb', 'cmd', 'code', 'codeop', 'IPython.core.display_trap', 'IPython.utils.path', 'IPython.utils.process', 'IPython.utils._process_posix', 'pexpect', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'pexpect.pty_spawn', 'pty', 'tty', 'ptyprocess', 'ptyprocess.ptyprocess', 'fcntl', 'resource', 'ptyprocess.util', 'pexpect.spawnbase', 'pexpect.run', 'IPython.utils._process_common', 'shlex', 'IPython.utils.decorators', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.utils.sysinfo', 'IPython.utils._sysinfo', 'IPython.core.profiledir', 'IPython.paths', 'IPython.utils.importstring', 'IPython.terminal', 'IPython.terminal.embed', 'IPython.core.compilerop', 'IPython.core.magic_arguments', 'IPython.core.error', 'IPython.utils.text', 'IPython.core.magic', 'getopt', 'IPython.core.oinspect', 'IPython.core.page', 'IPython.core.display', 'mimetypes', 'IPython.lib', 'IPython.lib.security', 'IPython.lib.pretty', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.util', 'pygments.lexers.python', 'pygments.lexer', 'pygments.filter', 'pygments.filters', 'pygments.token', 'pygments.regexopt', 'pygments.unistring', 'pygments.formatters', 'pygments.formatters._mapping', 'pygments.formatters.html', 'pygments.formatter', 'pygments.styles', 'IPython.core.inputtransformer2', 'decorator', 'IPython.core.interactiveshell', 'pickleshare', 'IPython.core.prefilter', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.alias', 'IPython.core.builtin_trap', 'IPython.core.events', 'backcall', 'backcall.backcall', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.core.formatters', 'IPython.utils.sentinel', 'IPython.core.history', 'sqlite3', 'sqlite3.dbapi2', '_sqlite3', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.display', 'IPython.lib.display', 'IPython.utils.io', 'IPython.utils.capture', 'IPython.utils.strdispatch', 'IPython.core.hooks', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.terminal.interactiveshell', 'prompt_toolkit', 'prompt_toolkit.application', 'prompt_toolkit.application.application', 'prompt_toolkit.buffer', 'prompt_toolkit.application.current', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.eventloop', 'prompt_toolkit.eventloop.async_generator', 'prompt_toolkit.eventloop.utils', 'prompt_toolkit.eventloop.inputhook', 'prompt_toolkit.utils', 'wcwidth', 'wcwidth.wcwidth', 'wcwidth.table_wide', 'wcwidth.table_zero', 'wcwidth.unicode_versions', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.document', 'prompt_toolkit.clipboard', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.filters', 'prompt_toolkit.filters.app', 'prompt_toolkit.cache', 'prompt_toolkit.enums', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.cli', 'prompt_toolkit.filters.utils', 'prompt_toolkit.completion', 'prompt_toolkit.completion.base', 'prompt_toolkit.formatted_text', 'prompt_toolkit.formatted_text.ansi', 'prompt_toolkit.output', 'prompt_toolkit.output.base', 'prompt_toolkit.data_structures', 'prompt_toolkit.styles', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.pygments', 'prompt_toolkit.styles.style_transformation', 'colorsys', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output.vt100', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.mouse_events', 'prompt_toolkit.formatted_text.html', 'xml', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.completion.deduplicate', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.completion.nested', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.validation', 'prompt_toolkit.input', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.layout', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.controls', 'prompt_toolkit.lexers', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.layout.screen', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.layout.scrollable_pane', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.renderer', 'prompt_toolkit.application.dummy', 'prompt_toolkit.shortcuts', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.patch_stdout', 'pygments.style', 'IPython.terminal.debugger', 'IPython.core.completer', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso.utils', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.grammar', 'parso._compatibility', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'parso.file_io', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.file_io', 'jedi.api.classes', 'jedi.inference', 'jedi.inference.imports', 'jedi.inference.sys_path', 'jedi.inference.cache', 'jedi.inference.base_value', 'jedi.inference.helpers', 'jedi.inference.utils', 'jedi.inference.compiled', 'jedi.inference.compiled.value', 'jedi.inference.filters', 'jedi.inference.flow_analysis', 'jedi.inference.recursion', 'jedi.inference.names', 'jedi.inference.docstrings', 'jedi.inference.lazy_value', 'jedi.common', 'jedi.plugins', 'jedi.inference.compiled.access', 'jedi.inference.compiled.getattr_static', 'jedi.inference.signature', 'jedi.inference.context', 'jedi.inference.analysis', 'jedi.inference.gradual', 'jedi.inference.gradual.typeshed', 'jedi.inference.gradual.stub_value', 'jedi.inference.value', 'jedi.inference.value.module', 'jedi.inference.value.klass', 'jedi.inference.arguments', 'jedi.inference.value.iterable', 'jedi.inference.value.dynamic_arrays', 'jedi.inference.value.function', 'jedi.inference.parser_cache', 'jedi.inference.gradual.generics', 'jedi.inference.value.instance', 'jedi.inference.gradual.typing', 'jedi.inference.gradual.base', 'jedi.inference.gradual.type_var', 'jedi.inference.compiled.subprocess', 'jedi._compatibility', 'jedi.inference.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.inference.syntax_tree', 'jedi.inference.gradual.annotation', 'jedi.inference.param', 'jedi.inference.value.decorator', 'jedi.inference.compiled.mixed', 'jedi.inference.gradual.conversion', 'jedi.api.keywords', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.completion_cache', 'jedi.api.helpers', 'jedi.api.interpreter', 'jedi.api.completion', 'jedi.api.strings', 'jedi.api.file_name', 'jedi.inference.docstring_utils', 'jedi.api.environment', 'filecmp', 'jedi.api.project', 'jedi.inference.references', 'jedi.api.errors', 'jedi.api.refactoring', 'jedi.api.refactoring.extract', 'jedi.inference.gradual.utils', 'jedi.plugins.registry', 'jedi.plugins.stdlib', 'jedi.plugins.flask', 'jedi.plugins.pytest', 'jedi.plugins.django', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'concurrent.futures.thread', 'IPython.terminal.magics', 'IPython.lib.clipboard', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.ipapp', 'IPython.core.magics', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'IPython.core.magics.code', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'urllib.error', 'urllib.response', 'IPython.core.magics.config', 'IPython.core.magics.display', 'IPython.core.magics.execution', 'timeit', 'cProfile', '_lsprof', 'profile', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.magics.pylab', 'IPython.core.pylabtools', 'IPython.core.magics.script', 'IPython.lib.backgroundjobs', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.utils.frame', 'ipykernel.control', 'ipykernel.heartbeat', 'ipykernel.iostream', 'ipykernel.ipkernel', 'IPython.utils.tokenutil', 'ipykernel.comm', 'ipykernel.comm.comm', 'ipykernel.jsonutil', 'ipykernel.kernelbase', 'psutil', 'psutil._common', 'psutil._compat', 'psutil._pslinux', 'psutil._psposix', 'psutil._psutil_linux', 'psutil._psutil_posix', 'tornado.queues', 'tornado.locks', 'ipykernel.comm.manager', 'ipykernel.compiler', 'ipykernel.debugger', 'debugpy', 'debugpy._version', 'debugpy.common', 'debugpy.common.compat', 'debugpy.common.fmt', 'debugpy.common.json', 'debugpy.server', 'debugpy._vendored', 'debugpy._vendored._util', 'debugpy._vendored.force_pydevd', '_pydevd_bundle', '_pydevd_bundle.pydevd_constants', 'encodings.ascii', 'encodings.idna', 'stringprep', '_pydevd_bundle.pydevd_vm_type', '_pydev_imps', '_pydev_imps._pydev_saved_modules', 'xmlrpc', 'xmlrpc.client', 'xml.parsers', 'xml.parsers.expat', 'pyexpat.errors', 'pyexpat.model', 'pyexpat', 'xml.parsers.expat.model', 'xml.parsers.expat.errors', 'gzip', 'xmlrpc.server', 'http.server', 'socketserver', '_pydev_bundle', '_pydev_runfiles', '_pydevd_frame_eval', 'pydev_ipython', 'pydevd_concurrency_analyser', 'pydevd_plugins', 'pkg_resources', 'plistlib', 'pkg_resources.extern', 'pkg_resources._vendor', 'pkg_resources._vendor.jaraco', 'pkg_resources.extern.jaraco', 'pkg_resources.extern.jaraco.text', 'importlib.resources', 'pkg_resources._vendor.importlib_resources', 'pkg_resources._vendor.importlib_resources._common', 'pkg_resources._vendor.importlib_resources.abc', 'pkg_resources._vendor.importlib_resources._compat', 'pkg_resources._vendor.zipp', 'pkg_resources._vendor.importlib_resources._legacy', 'pkg_resources.extern.importlib_resources', 'pkg_resources.extern.jaraco.functools', 'pkg_resources._vendor.more_itertools', 'pkg_resources._vendor.more_itertools.more', 'pkg_resources._vendor.more_itertools.recipes', 'pkg_resources.extern.more_itertools', 'pkg_resources.extern.jaraco.context', 'pkg_resources._vendor.appdirs', 'pkg_resources.extern.appdirs', 'pkg_resources._vendor.packaging', 'pkg_resources._vendor.packaging.__about__', 'pkg_resources.extern.packaging', 'pkg_resources.extern.packaging.version', 'pkg_resources.extern.packaging._structures', 'pkg_resources.extern.packaging.specifiers', 'pkg_resources.extern.packaging.utils', 'pkg_resources.extern.packaging.tags', 'sysconfig', 'pkg_resources._vendor.packaging._manylinux', 'pkg_resources._vendor.packaging._musllinux', 'pkg_resources.extern.packaging.requirements', 'pkg_resources._vendor.pyparsing', 'pkg_resources._vendor.pyparsing.util', 'pkg_resources._vendor.pyparsing.exceptions', 'pkg_resources._vendor.pyparsing.unicode', 'pkg_resources._vendor.pyparsing.actions', 'pkg_resources._vendor.pyparsing.core', 'pkg_resources._vendor.pyparsing.results', 'pkg_resources._vendor.pyparsing.helpers', 'pkg_resources._vendor.pyparsing.testing', 'pkg_resources._vendor.pyparsing.common', 'pkg_resources.extern.pyparsing', 'pkg_resources.extern.packaging.markers', 'pydevd', 'pydevd_file_utils', '_pydev_bundle.pydev_log', '_pydev_bundle._pydev_filesystem_encoding', '_pydevd_bundle.pydevd_comm_constants', '_sysconfigdata_m_linux_x86_64-linux-gnu', '_pydev_bundle.pydev_imports', '_pydev_imps._pydev_execfile', '_pydevd_bundle.pydevd_exec2', '_pydev_bundle.pydev_is_thread_alive', '_pydev_bundle.pydev_override', '_pydevd_bundle.pydevd_extension_utils', 'pydevd_plugins.extensions', '_pydevd_bundle.pydevd_frame_utils', '_pydevd_bundle.pydevd_filtering', '_pydevd_bundle.pydevd_io', '_pydevd_bundle.pydevd_utils', '_pydev_bundle.pydev_console_utils', '_pydev_bundle._pydev_calltip_util', '_pydev_bundle._pydev_imports_tipper', '_pydev_bundle._pydev_tipper_common', '_pydevd_bundle.pydevd_vars', '_pydevd_bundle.pydevd_xml', '_pydevd_bundle.pydevd_resolver', '_pydevd_bundle.pydevd_safe_repr', '_pydevd_bundle.pydevd_extension_api', '_pydevd_bundle.pydevd_thread_lifecycle', '_pydevd_bundle.pydevd_additional_thread_info', '_pydevd_bundle.pydevd_cython_wrapper', '_pydevd_bundle.pydevd_cython', '_pydevd_bundle.pydevd_dont_trace', '_pydevd_bundle.pydevd_bytecode_utils', '_pydevd_frame_eval.vendored', '_pydevd_frame_eval.vendored.bytecode', '_pydevd_frame_eval.vendored.bytecode.flags', '_pydevd_frame_eval.vendored.bytecode.instr', '_pydevd_frame_eval.vendored.bytecode.bytecode', '_pydevd_frame_eval.vendored.bytecode.concrete', '_pydevd_frame_eval.vendored.bytecode.cfg', '_pydevd_bundle.pydevd_save_locals', '_pydevd_bundle.pydevd_timeout', '_pydevd_bundle.pydevd_daemon_thread', 'pydevd_tracing', '_pydev_bundle.pydev_monkey', '_pydevd_bundle.pydevd_defaults', '_pydevd_bundle.pydevd_breakpoints', '_pydevd_bundle.pydevd_import_class', '_pydevd_bundle.pydevd_custom_frames', '_pydevd_bundle.pydevd_dont_trace_files', '_pydevd_bundle.pydevd_net_command_factory_xml', '_pydevd_bundle.pydevd_net_command', '_pydev_bundle._pydev_completer', 'pydevconsole', '_pydev_bundle.pydev_umd', '_pydevd_bundle.pydevd_trace_dispatch', '_pydevd_bundle.pydevd_additional_thread_info_regular', '_pydevd_bundle.pydevd_frame', '_pydevd_frame_eval.pydevd_frame_eval_main', '_pydevd_bundle.pydevd_source_mapping', 'pydevd_concurrency_analyser.pydevd_concurrency_logger', 'pydevd_concurrency_analyser.pydevd_thread_wrappers', '_pydevd_bundle.pydevd_comm', '_pydevd_bundle._debug_adapter', '_pydevd_bundle._debug_adapter.pydevd_schema', '_pydevd_bundle._debug_adapter.pydevd_base_schema', '_pydevd_bundle._debug_adapter.pydevd_schema_log', '_pydevd_bundle.pydevd_reload', '_pydev_bundle.fsnotify', '_pydevd_bundle.pydevd_console', '_pydevd_bundle.pydevd_process_net_command_json', '_pydevd_bundle.pydevd_api', '_pydevd_bundle.pydevd_net_command_factory_json', '_pydevd_bundle.pydevd_collect_bytecode_info', '_pydevd_bundle.pydevd_json_debug_options', '_pydevd_bundle.pydevd_process_net_command', '_pydevd_bundle.pydevd_traceproperty', '_pydevd_bundle.pydevd_suspended_frames', '_pydevd_bundle.pydevd_plugin_utils', '_pydevd_bundle.pydevd_trace_api', 'pydevd_plugins.django_debug', 'pydevd_plugins.jinja2_debug', 'pydevd_plugins.extensions.types', 'pydevd_plugins.extensions.types.pydevd_plugin_numpy_types', 'pydevd_plugins.extensions.types.pydevd_helpers', 'pydevd_plugins.extensions.types.pydevd_plugins_django_form_str', 'debugpy.server.api', 'debugpy.adapter', 'debugpy.common.log', 'debugpy.common.timestamp', 'debugpy.common.util', 'debugpy.common.sockets', 'ipykernel.eventloops', 'packaging', 'packaging.__about__', 'packaging.version', 'packaging._structures', 'ipykernel.zmqshell', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.parentpoller', 'faulthandler', 'IPython.core.completerlib', 'storemagic', 'torch', 'torch._utils', 'torch._utils_internal', 'torch.torch_version', 'torch.version', 'torch._six', 'torch._C._onnx', 'torch._C._jit', 'torch._C._jit_tree_views', 'torch._C._te', 'torch._C._nvfuser', 'torch._C._monitor', 'torch._C.cpp', 'torch._C.cpp.nn', 'torch._C._lazy', 'torch._C._lazy_ts_backend', 'torch._C._cudart', 'torch._C._nvtx', 'torch._C._cudnn', 'torch._C', 'torch._C._fft', 'torch._C._linalg', 'torch._C._nn', 'torch._C._return_types', 'torch._C._sparse', 'torch._C._special', 'torch._tensor', 'torch._namedtensor_internals', 'torch.overrides', 'torch.utils', 'torch.utils.throughput_benchmark', 'torch.utils._crash_handler', 'torch.utils._mode_utils', 'dataclasses', 'torch.utils.hooks', 'torch.storage', 'torch.types', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy._version', 'numpy._distributor_init', 'mkl', 'mkl._mklinit', 'mkl._py_mkl_service', 'numpy.core', 'numpy.version', 'numpy.core.multiarray', 'numpy.core.overrides', 'numpy.core._multiarray_umath', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.umath', 'numpy.core.numerictypes', 'numpy.core._string_helpers', 'numpy.core._type_aliases', 'numpy.core._dtype', 'numpy.core.numeric', 'numpy.core.shape_base', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core._exceptions', 'numpy.core._ufunc_config', 'numpy.core.arrayprint', 'numpy.core._asarray', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.einsumfunc', 'numpy.core._add_newdocs', 'numpy.core._multiarray_tests', 'numpy.core._add_newdocs_scalars', 'numpy.core._dtype_ctypes', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.lib', 'numpy.lib.mixins', 'numpy.lib.scimath', 'numpy.lib.type_check', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.linalg', 'numpy.linalg.linalg', 'numpy.lib.twodim_base', 'numpy.lib.stride_tricks', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.function_base', 'numpy.lib.histograms', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.fft', 'numpy.fft._pocketfft', 'numpy.fft._pocketfft_internal', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random._pickle', 'numpy.random.mtrand', 'numpy.random.bit_generator', 'numpy.random._common', 'secrets', 'numpy.random._bounded_integers', 'numpy.random._mt19937', 'numpy.random._philox', 'numpy.random._pcg64', 'numpy.random._sfc64', 'numpy.random._generator', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'torch.random', 'torch.serialization', 'tarfile', 'torch._sources', 'torch._tensor_str', 'torch.amp', 'torch.amp.autocast_mode', 'torch.cuda', 'torch.cuda._utils', 'torch.cuda.graphs', 'torch.cuda.streams', 'torch.cuda.memory', 'torch.cuda.random', 'torch.cuda.sparse', 'torch.cuda.profiler', 'torch.cuda.nvtx', 'torch.cuda.amp', 'torch.cuda.amp.autocast_mode', 'torch.cuda.amp.grad_scaler', 'torch.cuda.amp.common', 'torch.cuda.jiterator', 'torch.sparse', 'torch.functional', 'torch.nn', 'torch.nn.modules', 'torch.nn.modules.module', 'torch.nn.parameter', 'torch.nn.modules.linear', 'torch.nn.functional', 'torch._VF', 'torch._torch_docs', 'torch._jit_internal', 'torch.distributed', 'torch._C._distributed_c10d', 'torch.distributed.distributed_c10d', 'torch.distributed.constants', 'torch.distributed.rendezvous', 'torch.distributed.remote_device', 'torch.distributed.rpc', 'torch._C._distributed_rpc', 'torch.distributed.rpc.api', 'torch.futures', 'torch.distributed.rpc.internal', 'torch.distributed.rpc.constants', 'torch.distributed.rpc._utils', 'torch.distributed.rpc.backend_registry', 'torch.distributed.rpc.functions', 'torch.distributed.autograd', 'torch._C._distributed_autograd', 'torch.distributed.rpc.options', 'torch.distributed.rpc.server_process_global_profiler', 'torch.autograd', 'torch.autograd.variable', 'torch.autograd.function', 'torch.autograd.gradcheck', 'torch.testing', 'torch.testing._comparison', 'cmath', 'torch.testing._creation', 'torch.testing._deprecated', 'torch.testing._legacy', 'torch._vmap_internals', 'torch.utils._pytree', 'torch.autograd.grad_mode', 'torch.autograd.anomaly_mode', 'torch.autograd.functional', 'torch.autograd.forward_ad', 'torch.autograd.graph', 'torch._C._autograd', 'torch.autograd.profiler', 'torch.autograd.profiler_util', 'torch.autograd.profiler_legacy', 'torch.package', 'torch.package.analyze', 'torch.package.analyze.find_first_use_of_broken_modules', 'torch.package.package_exporter', 'pickletools', 'torch.package._digraph', 'torch.package._importlib', 'torch.package._mangling', 'torch.package._package_pickler', 'torch.package.importer', 'torch.package._stdlib', 'torch.package.find_file_dependencies', 'torch.package.glob_group', 'torch.package.analyze.trace_dependencies', 'torch.package.analyze.is_from_package', 'torch.package.file_structure_representation', 'torch.package.package_importer', 'torch.package._directory_reader', 'torch.package._package_unpickler', 'torch.nn._reduction', 'torch.nn.grad', 'torch.nn.modules.utils', 'torch.nn.init', 'torch.nn.modules.lazy', 'torch.nn.modules.conv', 'torch.nn.common_types', 'torch.nn.modules.activation', 'torch.nn.modules.loss', 'torch.nn.modules.distance', 'torch.nn.modules.container', 'torch.nn.modules.pooling', 'torch.nn.modules.batchnorm', 'torch.nn.modules._functions', 'torch.nn.modules.instancenorm', 'torch.nn.modules.normalization', 'torch.nn.modules.dropout', 'torch.nn.modules.padding', 'torch.nn.modules.sparse', 'torch.nn.modules.rnn', 'torch.nn.utils', 'torch.nn.utils.rnn', 'torch.nn.utils.clip_grad', 'torch.nn.utils.weight_norm', 'torch.nn.utils.convert_parameters', 'torch.nn.utils.spectral_norm', 'torch.nn.utils.fusion', 'torch.nn.utils.memory_format', 'torch.nn.utils.parametrizations', 'torch.nn.utils.parametrize', 'torch.nn.utils.init', 'torch.nn.utils.stateless', 'torch.nn.modules.pixelshuffle', 'torch.nn.modules.upsampling', 'torch.nn.modules.fold', 'torch.nn.modules.adaptive', 'torch.nn.modules.transformer', 'torch.nn.modules.flatten', 'torch.nn.modules.channelshuffle', 'torch.nn.parallel', 'torch.nn.parallel.parallel_apply', 'torch.nn.parallel.replicate', 'torch.nn.parallel.comm', 'torch.cuda.nccl', 'torch.nn.parallel.data_parallel', 'torch.nn.parallel.scatter_gather', 'torch.nn.parallel._functions', 'torch.nn.parallel.distributed', 'torch.distributed.algorithms', 'torch.distributed.algorithms.join', 'torch.distributed.utils', 'torch.nn.parallel._replicated_tensor_ddp_utils', 'torch._lowrank', 'torch._linalg_utils', 'torch.cpu', 'torch.cpu.amp', 'torch.cpu.amp.autocast_mode', 'torch.fft', 'torch.optim', 'torch.optim.adadelta', 'torch.optim.optimizer', 'torch.optim.adagrad', 'torch.optim.adam', 'torch.optim.adamw', 'torch.optim.sparse_adam', 'torch.optim._functional', 'torch.optim.adamax', 'torch.optim.asgd', 'torch.optim.nadam', 'torch.optim.radam', 'torch.optim.rmsprop', 'torch.optim.rprop', 'torch.optim.sgd', 'torch.optim.lbfgs', 'torch.optim.lr_scheduler', 'torch.optim.swa_utils', 'torch.optim._multi_tensor', 'torch.multiprocessing', 'torch.multiprocessing.reductions', 'multiprocessing', 'multiprocessing.context', 'multiprocessing.process', 'multiprocessing.reduction', '__mp_main__', 'multiprocessing.util', 'multiprocessing.resource_sharer', 'torch.multiprocessing.spawn', 'multiprocessing.connection', '_multiprocessing', 'torch.special', 'torch.utils.backcompat', 'torch.onnx', 'torch.jit', 'torch.jit._script', 'torch.jit._recursive', 'torch.jit.frontend', 'torch.jit._monkeytype_config', 'torch.jit.annotations', 'torch.jit._state', 'torch._ops', 'torch.jit._builtins', 'torch.backends', 'torch.backends.cudnn', 'torch.jit._check', 'torch.jit._fuser', 'torch.jit._serialization', 'torch._classes', 'torch.jit._trace', 'torch.jit._async', 'torch.jit._decomposition_utils', 'torch.jit._freeze', 'torch.jit._ir_utils', 'torch.linalg', 'torch.hub', 'tqdm', 'tqdm._monitor', 'tqdm._tqdm_pandas', 'tqdm.cli', 'tqdm.std', 'tqdm.utils', 'tqdm.version', 'tqdm._dist_ver', 'tqdm.gui', 'tqdm.auto', 'tqdm.autonotebook', 'tqdm.notebook', 'ipywidgets', 'ipywidgets._version', 'ipywidgets.widgets', 'ipywidgets.widgets.widget', 'ipython_genutils', 'ipython_genutils._version', 'ipython_genutils.py3compat', 'ipython_genutils.encoding', 'ipywidgets.widgets.domwidget', 'ipywidgets.widgets.trait_types', 'ipywidgets.widgets.util', 'ipywidgets.widgets.widget_layout', 'ipywidgets.widgets.widget_style', 'ipywidgets.widgets.valuewidget', 'ipywidgets.widgets.widget_core', 'ipywidgets.widgets.widget_bool', 'ipywidgets.widgets.widget_description', 'ipywidgets.widgets.widget_button', 'ipywidgets.widgets.widget_box', 'ipywidgets.widgets.docutils', 'ipywidgets.widgets.widget_float', 'ipywidgets.widgets.widget_int', 'ipywidgets.widgets.widget_color', 'ipywidgets.widgets.widget_date', 'ipywidgets.widgets.widget_output', 'ipywidgets.widgets.widget_selection', 'ipywidgets.widgets.widget_selectioncontainer', 'ipywidgets.widgets.widget_string', 'ipywidgets.widgets.widget_controller', 'ipywidgets.widgets.interaction', 'ipywidgets.widgets.widget_link', 'ipywidgets.widgets.widget_media', 'ipywidgets.widgets.widget_templates', 'ipywidgets.widgets.widget_upload', 'tqdm.asyncio', 'torch.distributions', 'torch.distributions.bernoulli', 'torch.distributions.constraints', 'torch.distributions.exp_family', 'torch.distributions.distribution', 'torch.distributions.utils', 'torch.distributions.beta', 'torch.distributions.dirichlet', 'torch.distributions.binomial', 'torch.distributions.categorical', 'torch.distributions.cauchy', 'torch.distributions.chi2', 'torch.distributions.gamma', 'torch.distributions.constraint_registry', 'torch.distributions.transforms', 'torch.distributions.continuous_bernoulli', 'torch.distributions.exponential', 'torch.distributions.fishersnedecor', 'torch.distributions.geometric', 'torch.distributions.gumbel', 'torch.distributions.uniform', 'torch.distributions.transformed_distribution', 'torch.distributions.independent', 'torch.distributions.half_cauchy', 'torch.distributions.half_normal', 'torch.distributions.normal', 'torch.distributions.kl', 'torch.distributions.laplace', 'torch.distributions.lowrank_multivariate_normal', 'torch.distributions.multivariate_normal', 'torch.distributions.one_hot_categorical', 'torch.distributions.pareto', 'torch.distributions.poisson', 'torch.distributions.kumaraswamy', 'torch.distributions.lkj_cholesky', 'torch.distributions.log_normal', 'torch.distributions.logistic_normal', 'torch.distributions.mixture_same_family', 'torch.distributions.multinomial', 'torch.distributions.negative_binomial', 'torch.distributions.relaxed_bernoulli', 'torch.distributions.relaxed_categorical', 'torch.distributions.studentT', 'torch.distributions.von_mises', 'torch.distributions.weibull', 'torch.distributions.wishart', 'torch.backends.cuda', 'torch.backends.mps', 'torch.backends.mkl', 'torch.backends.mkldnn', 'torch.backends.openmp', 'torch.backends.quantized', 'torch.utils.data', 'torch.utils.data.sampler', 'torch.utils.data.dataset', 'torch.utils.data.datapipes', 'torch.utils.data.datapipes.iter', 'torch.utils.data.datapipes.iter.utils', 'torch.utils.data.datapipes.datapipe', 'torch.utils.data.datapipes._typing', 'torch.utils.data.datapipes.utils', 'torch.utils.data.datapipes.utils.common', 'torch.utils.data._utils', 'torch.utils.data._utils.worker', 'torch.utils.data._utils.signal_handling', 'torch.utils.data._utils.pin_memory', 'torch.utils.data._utils.collate', 'torch.utils.data._utils.fetch', 'torch.utils.data._utils.serialization', 'torch.utils.data.datapipes.iter.callable', 'torch.utils.data.datapipes._decorator', 'torch.utils.data.datapipes.iter.combinatorics', 'torch.utils.data.datapipes.iter.combining', 'torch.utils.data.datapipes.iter.filelister', 'torch.utils.data.datapipes.iter.fileopener', 'torch.utils.data.datapipes.iter.grouping', 'torch.utils.data.datapipes.iter.routeddecoder', 'torch.utils.data.datapipes.utils.decoder', 'torch.utils.data.datapipes.iter.selecting', 'torch.utils.data.datapipes.dataframe', 'torch.utils.data.datapipes.dataframe.dataframes', 'torch.utils.data.datapipes.dataframe.structures', 'torch.utils.data.datapipes.dataframe.datapipes', 'torch.utils.data.datapipes.dataframe.dataframe_wrapper', 'torch.utils.data.datapipes.iter.streamreader', 'torch.utils.data.datapipes.map', 'torch.utils.data.datapipes.map.callable', 'torch.utils.data.datapipes.map.combinatorics', 'torch.utils.data.datapipes.map.combining', 'torch.utils.data.datapipes.map.grouping', 'torch.utils.data.datapipes.map.utils', 'torch.utils.data.dataloader', 'torch.utils.data.graph_settings', 'torch.utils.data.graph', 'torch.utils.data.distributed', 'torch.utils.data.dataloader_experimental', 'torch.utils.data.backward_compatibility', 'torch.utils.data.communication', 'torch.utils.data.communication.eventloop', 'torch.utils.data.communication.iter', 'torch.utils.data.communication.map', 'torch.utils.data.communication.messages', 'torch.utils.data.communication.protocol', 'torch.utils.data.communication.queue', 'torch.__config__', 'torch.__future__', 'torch.profiler', 'torch.profiler.profiler', 'torch.nn.intrinsic', 'torch.nn.intrinsic.modules', 'torch.nn.intrinsic.modules.fused', 'torch.nn.quantizable', 'torch.nn.quantizable.modules', 'torch.nn.quantizable.modules.activation', 'torch.nn.quantized', 'torch.nn.quantized.modules', 'torch.nn.quantized.modules.activation', 'torch.nn.quantized.functional', 'torch.nn.quantized.modules.utils', 'torch.nn.quantized.modules.dropout', 'torch.nn.quantized.modules.batchnorm', 'torch.nn.quantized.modules.normalization', 'torch.nn.quantized.modules.conv', 'torch.nn.intrinsic.qat', 'torch.nn.intrinsic.qat.modules', 'torch.nn.intrinsic.qat.modules.linear_relu', 'torch.nn.qat', 'torch.nn.qat.modules', 'torch.nn.qat.modules.linear', 'torch.nn.qat.modules.conv', 'torch.nn.qat.modules.embedding_ops', 'torch.nn.intrinsic.qat.modules.linear_fused', 'torch.nn.intrinsic.qat.modules.conv_fused', 'torch.nn.quantized.modules.linear', 'torch.nn.quantized.modules.embedding_ops', 'torch.nn.quantized.modules.functional_modules', 'torch.nn.quantizable.modules.rnn', 'torch.ao', 'torch._tensor_docs', 'torch._storage_docs', 'torch.quantization', 'torch.quantization.quantize', 'torch.ao.quantization', 'torch.ao.quantization.fake_quantize', 'torch.ao.quantization.observer', 'torch.ao.quantization.utils', 'torch.ao.quantization.quant_type', 'torch.ao.quantization.fuse_modules', 'torch.ao.quantization.fuser_method_mappings', 'torch.ao.quantization.qconfig', 'torch.ao.quantization.quantization_mappings', 'torch.nn.intrinsic.quantized', 'torch.nn.intrinsic.quantized.modules', 'torch.nn.intrinsic.quantized.modules.linear_relu', 'torch.nn.intrinsic.quantized.modules.conv_relu', 'torch.nn.intrinsic.quantized.modules.bn_relu', 'torch.nn.intrinsic.quantized.dynamic', 'torch.nn.intrinsic.quantized.dynamic.modules', 'torch.nn.intrinsic.quantized.dynamic.modules.linear_relu', 'torch.nn.quantized.dynamic', 'torch.nn.quantized.dynamic.modules', 'torch.nn.quantized.dynamic.modules.linear', 'torch.nn.quantized.dynamic.modules.rnn', 'torch.nn.quantized.dynamic.modules.conv', 'torch.nn.quantized._reference', 'torch.nn.quantized._reference.modules', 'torch.nn.quantized._reference.modules.linear', 'torch.nn.quantized._reference.modules.utils', 'torch.nn.quantized._reference.modules.conv', 'torch.nn.quantized._reference.modules.rnn', 'torch.nn.quantized._reference.modules.sparse', 'torch.nn.qat.dynamic', 'torch.nn.qat.dynamic.modules', 'torch.nn.qat.dynamic.modules.linear', 'torch.ao.nn', 'torch.ao.nn.sparse', 'torch.ao.nn.sparse.quantized', 'torch.ao.nn.sparse.quantized.dynamic', 'torch.ao.nn.sparse.quantized.dynamic.linear', 'torch.ao.nn.sparse.quantized.linear', 'torch.ao.nn.sparse.quantized.utils', 'torch.ao.quantization.stubs', 'torch.ao.quantization.quantize', 'torch.ao.quantization.quantize_jit', 'torch.quantization.observer', 'torch.quantization.qconfig', 'torch.quantization.fake_quantize', 'torch.quantization.fuse_modules', 'torch.quantization.fuser_method_mappings', 'torch.quantization.stubs', 'torch.quantization.quant_type', 'torch.quantization.quantize_jit', 'torch.quantization.quantization_mappings', 'torch.quasirandom', 'torch.multiprocessing._atfork', 'torch._lobpcg', 'torch.utils.dlpack', 'torch._masked', 'torch._masked._docs', 'torch.return_types', 'torch.library', 'torch._meta_registrations', 'torch._prims', 'torch._prims.utils', 'torchbeast', 'torchbeast.core', 'torchbeast.core.file_writer', 'csv', '_csv', 'torchbeast.core.prof', 'torchbeast.core.vtrace', 'torchbeast.core.environment', 'torchbeast.atari_wrappers', 'gym', 'gym.error', 'gym.version', 'gym.core', 'gym.utils', 'gym.utils.colorize', 'gym.utils.ezpickle', 'gym.utils.closer', 'gym.spaces', 'gym.spaces.space', 'gym.utils.seeding', 'gym.spaces.box', 'gym.logger', 'gym.spaces.discrete', 'gym.spaces.multi_discrete', 'gym.spaces.multi_binary', 'gym.spaces.tuple', 'gym.spaces.dict', 'gym.spaces.utils', 'gym.envs', 'gym.envs.registration', 'importlib_metadata', 'zipp', 'importlib_metadata._adapters', 'importlib_metadata._text', 'importlib_metadata._functools', 'importlib_metadata._meta', 'importlib_metadata._compat', 'importlib_metadata._collections', 'importlib_metadata._itertools', 'ale_py', 'ale_py._ale_py', 'ale_py.gym', 'ale_py.roms', 'ale_py.roms.utils', 'importlib_resources', 'importlib_resources._common', 'importlib_resources.abc', 'importlib_resources._compat', 'importlib_resources._legacy', 'importlib_resources._adapters', 'importlib_resources.readers', 'importlib_resources._itertools', 'AutoROM', 'AutoROM.AutoROM', 'requests', 'urllib3', 'urllib3.exceptions', 'urllib3.packages', 'urllib3.packages.six', 'urllib3.packages.six.moves', 'urllib3.packages.six.moves.http_client', 'urllib3._version', 'urllib3.connectionpool', 'urllib3.connection', 'urllib3.util', 'urllib3.util.connection', 'urllib3.contrib', 'urllib3.contrib._appengine_environ', 'urllib3.util.wait', 'urllib3.util.request', 'urllib3.util.response', 'urllib3.util.retry', 'urllib3.util.ssl_', 'urllib3.util.url', 'urllib3.util.ssltransport', 'urllib3.util.timeout', 'urllib3.util.proxy', 'urllib3._collections', 'urllib3.util.ssl_match_hostname', 'ipaddress', 'urllib3.request', 'urllib3.filepost', 'urllib3.fields', 'urllib3.packages.six.moves.urllib', 'urllib3.packages.six.moves.urllib.parse', 'urllib3.response', 'urllib3.util.queue', 'urllib3.poolmanager', 'requests.exceptions', 'requests.compat', 'chardet', 'chardet.universaldetector', 'chardet.charsetgroupprober', 'chardet.enums', 'chardet.charsetprober', 'chardet.escprober', 'chardet.codingstatemachine', 'chardet.escsm', 'chardet.latin1prober', 'chardet.mbcsgroupprober', 'chardet.utf8prober', 'chardet.mbcssm', 'chardet.sjisprober', 'chardet.mbcharsetprober', 'chardet.chardistribution', 'chardet.euctwfreq', 'chardet.euckrfreq', 'chardet.gb2312freq', 'chardet.big5freq', 'chardet.jisfreq', 'chardet.jpcntx', 'chardet.eucjpprober', 'chardet.gb2312prober', 'chardet.euckrprober', 'chardet.cp949prober', 'chardet.big5prober', 'chardet.euctwprober', 'chardet.sbcsgroupprober', 'chardet.hebrewprober', 'chardet.langbulgarianmodel', 'chardet.sbcharsetprober', 'chardet.langgreekmodel', 'chardet.langhebrewmodel', 'chardet.langrussianmodel', 'chardet.langthaimodel', 'chardet.langturkishmodel', 'chardet.version', 'http.cookiejar', 'http.cookies', 'charset_normalizer', 'charset_normalizer.api', 'charset_normalizer.constant', 'charset_normalizer.md', 'charset_normalizer.utils', '_multibytecodec', 'charset_normalizer.models', 'charset_normalizer.cd', 'charset_normalizer.assets', 'charset_normalizer.legacy', 'charset_normalizer.version', 'requests.packages', 'requests.packages.urllib3', 'requests.packages.urllib3.exceptions', 'requests.packages.urllib3.packages', 'requests.packages.urllib3.packages.six', 'requests.packages.urllib3.packages.six.moves', 'requests.packages.urllib3.packages.six.moves.http_client', 'requests.packages.urllib3._version', 'requests.packages.urllib3.connectionpool', 'requests.packages.urllib3.connection', 'requests.packages.urllib3.util', 'requests.packages.urllib3.util.connection', 'requests.packages.urllib3.contrib', 'requests.packages.urllib3.contrib._appengine_environ', 'requests.packages.urllib3.util.wait', 'requests.packages.urllib3.util.request', 'requests.packages.urllib3.util.response', 'requests.packages.urllib3.util.retry', 'requests.packages.urllib3.util.ssl_', 'requests.packages.urllib3.util.url', 'requests.packages.urllib3.util.ssltransport', 'requests.packages.urllib3.util.timeout', 'requests.packages.urllib3.util.proxy', 'requests.packages.urllib3._collections', 'requests.packages.urllib3.util.ssl_match_hostname', 'requests.packages.urllib3.request', 'requests.packages.urllib3.filepost', 'requests.packages.urllib3.fields', 'requests.packages.urllib3.packages.six.moves.urllib', 'requests.packages.urllib3.packages.six.moves.urllib.parse', 'requests.packages.urllib3.response', 'requests.packages.urllib3.util.queue', 'requests.packages.urllib3.poolmanager', 'idna', 'idna.package_data', 'idna.core', 'idna.idnadata', 'idna.intranges', 'requests.packages.idna', 'requests.packages.idna.package_data', 'requests.packages.idna.core', 'requests.packages.idna.idnadata', 'requests.packages.idna.intranges', 'requests.packages.chardet', 'requests.utils', 'requests.certs', 'certifi', 'certifi.core', 'requests.__version__', 'requests._internal_utils', 'requests.cookies', 'requests.structures', 'requests.api', 'requests.sessions', 'requests.adapters', 'requests.auth', 'requests.models', 'requests.hooks', 'requests.status_codes', 'urllib3.contrib.socks', 'socks', 'click', 'click.core', 'click.types', 'click._compat', 'click.exceptions', 'click.utils', 'click.globals', 'click._unicodefun', 'click.formatting', 'click.parser', 'click.termui', 'click.decorators', 'AutoROM.roms', 'atari_py', 'atari_py.ale_python_interface', 'atari_py.games', 'atari_py.atari_roms', 'gym.vector', 'gym.vector.async_vector_env', 'gym.vector.vector_env', 'gym.vector.utils', 'gym.vector.utils.misc', 'gym.vector.utils.numpy_utils', 'gym.vector.utils.spaces', 'gym.vector.utils.shared_memory', 'gym.vector.sync_vector_env', 'gym.wrappers', 'gym.wrappers.monitor', 'gym.wrappers.monitoring', 'gym.wrappers.monitoring.stats_recorder', 'gym.utils.atomic_write', 'gym.utils.json_utils', 'gym.wrappers.monitoring.video_recorder', 'setuptools', '_distutils_hack.override', 'setuptools._distutils', 'distutils', 'distutils.core', 'distutils.debug', 'distutils.errors', 'distutils.dist', 'distutils.fancy_getopt', 'distutils.util', 'distutils.dep_util', 'distutils.spawn', 'distutils.log', 'distutils.cmd', 'distutils.dir_util', 'distutils.file_util', 'distutils.archive_util', 'distutils.config', 'distutils.extension', 'setuptools._deprecation_warning', 'setuptools.version', 'setuptools.extension', 'setuptools.monkey', 'distutils.filelist', 'setuptools.dist', 'distutils.command', 'setuptools.extern', 'setuptools._vendor', 'setuptools._vendor.packaging', 'setuptools._vendor.packaging.__about__', 'setuptools.extern.packaging', 'setuptools._vendor.ordered_set', 'setuptools.extern.ordered_set', 'setuptools._vendor.more_itertools', 'setuptools._vendor.more_itertools.more', 'setuptools._vendor.more_itertools.recipes', 'setuptools.extern.more_itertools', 'setuptools._importlib', 'setuptools._vendor.importlib_metadata', 'setuptools._vendor.zipp', 'setuptools._vendor.importlib_metadata._adapters', 'setuptools._vendor.importlib_metadata._text', 'setuptools._vendor.importlib_metadata._functools', 'setuptools._vendor.importlib_metadata._meta', 'setuptools._vendor.importlib_metadata._compat', 'setuptools._vendor.typing_extensions', 'setuptools._vendor.importlib_metadata._collections', 'setuptools._vendor.importlib_metadata._itertools', 'setuptools.extern.importlib_metadata', 'setuptools._vendor.importlib_resources', 'setuptools._vendor.importlib_resources._common', 'setuptools._vendor.importlib_resources.abc', 'setuptools._vendor.importlib_resources._compat', 'setuptools._vendor.importlib_resources._legacy', 'setuptools.extern.importlib_resources', 'setuptools.command', 'distutils.command.bdist', 'setuptools.windows_support', 'setuptools.config', 'setuptools.config.setupcfg', 'setuptools.extern.packaging.version', 'setuptools.extern.packaging._structures', 'setuptools.extern.packaging.specifiers', 'setuptools.extern.packaging.utils', 'setuptools.extern.packaging.tags', 'setuptools._vendor.packaging._manylinux', 'setuptools._vendor.packaging._musllinux', 'setuptools.config.expand', 'setuptools.config.pyprojecttoml', 'setuptools.errors', 'setuptools.config._apply_pyprojecttoml', 'email.headerregistry', 'email._header_value_parser', 'setuptools.discovery', 'setuptools._reqs', 'setuptools._vendor.jaraco', 'setuptools.extern.jaraco', 'setuptools.extern.jaraco.text', 'setuptools.extern.jaraco.functools', 'setuptools.extern.jaraco.context', 'setuptools._entry_points', 'setuptools._itertools', 'setuptools.depends', 'setuptools._imp', 'setuptools.py34compat', 'setuptools.logging', 'setuptools.msvc', 'distutils.ccompiler', 'distutils.version', 'gym.wrappers.time_limit', 'gym.wrappers.filter_observation', 'gym.wrappers.atari_preprocessing', 'cv2', 'cv2.Error', 'cv2.cuda', 'cv2.detail', 'cv2.dnn', 'cv2.fisheye', 'cv2.flann', 'cv2.ipp', 'cv2.ml', 'cv2.ocl', 'cv2.ogl', 'cv2.samples', 'cv2.utils', 'cv2.videoio_registry', 'cv2.cv2', 'cv2.data', 'gym.wrappers.time_aware_observation', 'gym.wrappers.rescale_action', 'gym.wrappers.flatten_observation', 'gym.wrappers.gray_scale_observation', 'gym.wrappers.frame_stack', 'gym.wrappers.transform_observation', 'gym.wrappers.transform_reward', 'gym.wrappers.resize_observation', 'gym.wrappers.clip_action', 'gym.wrappers.record_episode_statistics', 'gym.wrappers.normalize', 'gym.wrappers.record_video', 'torchbeast.transformer_rnn', 'torchbeast.train', 'torchbeast.resnet', 'gym_sokoban', 'torchbeast.model', 'torchbeast.base', 'matplotlib', 'matplotlib._api', 'matplotlib._api.deprecation', 'matplotlib._version', 'matplotlib.cbook', 'matplotlib._c_internal_utils', 'matplotlib.docstring', 'matplotlib.rcsetup', 'matplotlib.colors', 'PIL', 'PIL._version', 'PIL.Image', 'defusedxml', 'defusedxml.common', 'defusedxml.ElementTree', 'xml.etree', 'xml.etree.ElementPath', '_elementtree', 'xml.etree.ElementTree', 'PIL.ImageMode', 'PIL.TiffTags', 'PIL._binary', 'PIL._deprecate', 'PIL._util', 'PIL._imaging', 'cffi', 'cffi.api', 'cffi.lock', 'cffi.error', 'cffi.model', 'PIL.PngImagePlugin', 'PIL.ImageChops', 'PIL.ImageFile', 'PIL.ImagePalette', 'PIL.GimpGradientFile', 'PIL.GimpPaletteFile', 'PIL.ImageColor', 'PIL.PaletteFile', 'PIL.ImageSequence', 'matplotlib.scale', 'matplotlib.ticker', 'matplotlib.transforms', 'matplotlib._path', 'matplotlib.path', 'matplotlib.bezier', 'matplotlib._color_data', 'matplotlib.fontconfig_pattern', 'pyparsing', 'pyparsing.util', 'pyparsing.exceptions', 'pyparsing.unicode', 'pyparsing.actions', 'pyparsing.core', 'pyparsing.results', 'pyparsing.helpers', 'pyparsing.testing', 'pyparsing.common', 'matplotlib._enums', 'cycler', 'matplotlib.ft2font', 'kiwisolver', 'kiwisolver._cext']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG:25924 __init__:277 2022-10-31 09:09:39,842] CACHEDIR=/home/schk/.cache/matplotlib\n",
      "[DEBUG:25924 font_manager:1439 2022-10-31 09:09:39,855] Using fontManager instance from /home/schk/.cache/matplotlib/fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core import file_writer\n",
    "from torchbeast.core import prof\n",
    "from torchbeast.core import vtrace\n",
    "from torchbeast.core.environment import Environment, Vec_Environment\n",
    "\n",
    "from torchbeast.atari_wrappers import *\n",
    "from torchbeast.transformer_rnn import *\n",
    "from torchbeast.train import *\n",
    "from torchbeast.model import Model\n",
    "from torchbeast.base import BaseNet\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "def get_param(net, name=None):\n",
    "    keys = []\n",
    "    for (k, v) in actor_wrapper.model.named_parameters(): \n",
    "        if name is None:\n",
    "            print(k)\n",
    "        else:\n",
    "            if name == k: return v\n",
    "        keys.append(k)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b0cae",
   "metadata": {},
   "source": [
    "<font size=\"5\">Agent Training Phase</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239d9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits, temperature, u=None, hard=True):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [bsz, n_class]\n",
    "    return: flatten --> [bsz, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    n_class = logits.shape[-1]\n",
    "    \n",
    "    if u is None:\n",
    "        u = torch.rand(logits.size()).to(logits.device)\n",
    "        u = -torch.log(-torch.log(u + eps) + eps)\n",
    "    y = logits + u\n",
    "    y = F.softmax(y / temperature, dim=-1)\n",
    "    \n",
    "    if not hard: return y.view(-1, n_class)\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "    return y_hard.view(-1, n_class), u\n",
    "\n",
    "class Actor_net(nn.Module):    \n",
    "    def __init__(self, obs_shape, num_actions, flags):\n",
    "\n",
    "        super(Actor_net, self).__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions  \n",
    "        \n",
    "        self.tran_t = flags.tran_t                   # number of recurrence of RNN\n",
    "        self.tran_mem_n = flags.tran_mem_n           # size of memory for the attn modules\n",
    "        self.tran_layer_n = flags.tran_layer_n       # number of layers\n",
    "        self.tran_lstm = flags.tran_lstm             # to use lstm or not\n",
    "        self.tran_lstm_no_attn = flags.tran_lstm_no_attn  # to use attention in lstm or not\n",
    "        self.tran_norm_first = flags.tran_norm_first # to use norm first in transformer (not on LSTM)\n",
    "        self.tran_ff_n = flags.tran_ff_n             # number of dim of ff in transformer (not on LSTM)        \n",
    "        self.tran_skip = flags.tran_skip             # whether to add skip connection\n",
    "        self.conv_out = flags.tran_dim               # size of transformer / LSTM embedding dim\n",
    "        self.ste = flags.ste\n",
    "        self.gb_ste = flags.gb_ste\n",
    "        \n",
    "        self.conv_out_hw = 1   \n",
    "        self.d_model = self.conv_out\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=self.obs_shape[0], out_channels=self.conv_out//2, kernel_size=1, stride=1)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv_out//2, out_channels=self.conv_out, kernel_size=1, stride=1)        \n",
    "        self.frame_conv = torch.nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU())\n",
    "        self.env_input_size = self.conv_out\n",
    "        d_in = self.env_input_size + self.d_model \n",
    "        \n",
    "        if self.tran_lstm:\n",
    "            self.core = ConvAttnLSTM(h=self.conv_out_hw, w=self.conv_out_hw,\n",
    "                                 input_dim=d_in-self.d_model, hidden_dim=self.d_model,\n",
    "                                 kernel_size=1, num_layers=self.tran_layer_n,\n",
    "                                 num_heads=8, mem_n=self.tran_mem_n, attn=not self.tran_lstm_no_attn)\n",
    "        else:            \n",
    "            self.core = ConvTransformerRNN(d_in=d_in,\n",
    "                                       h=self.conv_out_hw, w=self.conv_out_hw, d_model=self.d_model, \n",
    "                                       num_heads=8, dim_feedforward=self.tran_ff_n, \n",
    "                                       mem_n=self.tran_mem_n, norm_first=self.tran_norm_first,\n",
    "                                       num_layers=self.tran_layer_n, rpos=self.rpos, conv=False)   \n",
    "                         \n",
    "        \n",
    "        if self.tran_skip:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * (self.d_model + self.env_input_size)\n",
    "        else:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * self.d_model\n",
    "                \n",
    "        self.fc = nn.Linear(rnn_out_size, 256)        \n",
    "        \n",
    "        self.im_policy = nn.Linear(256, self.num_actions)        \n",
    "        self.policy = nn.Linear(256, self.num_actions)        \n",
    "        self.baseline = nn.Linear(256, 1)        \n",
    "        self.reset = nn.Linear(256, 1)        \n",
    "        \n",
    "        if self.gb_ste:\n",
    "            self.register_buffer('temp', torch.tensor(flags.gb_ste_temp_max, dtype=torch.float32))\n",
    "        \n",
    "        print(\"actor size: \", sum(p.numel() for p in self.parameters()))\n",
    "        #for k, v in self.named_parameters(): print(k, v.numel())   \n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        state = self.core.init_state(batch_size) + (torch.zeros(1, batch_size, \n",
    "               self.env_input_size, self.conv_out_hw, self.conv_out_hw),)\n",
    "        return state\n",
    "\n",
    "    def forward(self, x, done, core_state=(), u=None, debug=False):\n",
    "        # one-step forward for the actor\n",
    "        # input / done shape x: T x B x C x 1 x 1 / B x C x 1 x 1\n",
    "        # only supports T = 1 at the moment; all output does not have T dim.\n",
    "        \n",
    "        if len(x.shape) == 4: x = x.unsqueeze(0)\n",
    "        if len(done.shape) == 1: done = done.unsqueeze(0)  \n",
    "            \n",
    "        T, B, *_ = x.shape\n",
    "        x = torch.flatten(x, 0, 1)  # Merge time and batch.        \n",
    "        env_input = self.frame_conv(x)                \n",
    "        core_input = env_input.view(T, B, -1, self.conv_out_hw, self.conv_out_hw)\n",
    "        core_output_list = []\n",
    "        notdone = ~(done.bool())\n",
    "        \n",
    "        for n, (input, nd) in enumerate(zip(core_input.unbind(), notdone.unbind())):                \n",
    "            # Input shape: B, self.conv_out + self.num_actions + 1, H, W\n",
    "            for t in range(self.tran_t):                \n",
    "                if t > 0: nd = torch.ones(B).to(x.device).bool()                    \n",
    "                nd = nd.view(-1)      \n",
    "                output, core_state = self.core(input, core_state, nd, nd) # output shape: 1, B, core_output_size \n",
    "                \n",
    "            last_input = input   \n",
    "            core_output_list.append(output)\n",
    "                                   \n",
    "        core_output = torch.cat(core_output_list)  \n",
    "        if self.tran_skip: core_output = torch.concat([core_output, core_input], dim=-3)\n",
    "        core_output = torch.flatten(core_output, 0, 1)        \n",
    "        core_output = F.relu(self.fc(torch.flatten(core_output, start_dim=1)))   \n",
    "        \n",
    "        policy_logits = self.policy(core_output)\n",
    "        im_policy_logits = self.im_policy(core_output)        \n",
    "        reset_policy_logits_p = self.reset(core_output)\n",
    "        reset_policy_logits = torch.cat([reset_policy_logits_p, torch.zeros_like(reset_policy_logits_p)], dim=-1)   \n",
    "        baseline = self.baseline(core_output)\n",
    "        \n",
    "        if self.gb_ste:\n",
    "            if u is not None: \n",
    "                u_action = u[:, :-2]\n",
    "                u_reset = u[:, -2:]\n",
    "            else:\n",
    "                u_action = None\n",
    "                u_reset = None\n",
    "            im_action, u_action = gumbel_softmax(im_policy_logits, self.temp, u=u_action, hard=True)             \n",
    "            reset, u_reset = gumbel_softmax(reset_policy_logits, self.temp, u=u_reset, hard=True)\n",
    "            reset = reset[:, 0]\n",
    "            u = torch.cat([u_action, u_reset], dim=-1)\n",
    "        elif self.ste:\n",
    "            im_action_p = F.softmax(im_policy_logits, dim=-1)\n",
    "            im_action_h = torch.multinomial(im_action_p, num_samples=1)\n",
    "            im_action = (im_action_h - im_action_p).detach() + im_action_p\n",
    "          \n",
    "            reset_p = torch.sigmoid(reset_policy_logits_p)\n",
    "            reset_h = torch.bernoulli(reset_p)\n",
    "            reset = (reset_h - reset_p).detach() + reset_p\n",
    "        else:\n",
    "            im_action = F.softmax(im_policy_logits, dim=1)\n",
    "            reset = torch.sigmoid(reset_policy_logits_p)\n",
    "            \n",
    "        \n",
    "        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        \n",
    "        reg_loss = (1e-3 * torch.sum(policy_logits**2, dim=-1) / 2 + \n",
    "                    1e-5 * torch.sum(core_output**2, dim=-1) / 2)\n",
    "        reg_loss = reg_loss.view(T, B)\n",
    "        \n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        action = action.view(T, B)                \n",
    "        im_policy_logits = im_policy_logits.view(T, B, self.num_actions)        \n",
    "        im_action = im_action.view(T, B, self.num_actions)                \n",
    "        reset_policy_logits = reset_policy_logits.view(T, B, 2)\n",
    "        reset = reset.view(T, B)     \n",
    "        baseline = baseline.view(T, B)\n",
    "        \n",
    "        ret_dict = dict(policy_logits=policy_logits[0],                         \n",
    "                        action=action[0], \n",
    "                        im_policy_logits=im_policy_logits[0],                         \n",
    "                        im_action=im_action[0],                                                \n",
    "                        reset_policy_logits=reset_policy_logits[0], \n",
    "                        reset=reset[0],\n",
    "                        baseline=baseline[0], \n",
    "                        reg_loss=reg_loss[0], )\n",
    "        if self.gb_ste:\n",
    "            ret_dict['uniform'] = u\n",
    "        return (ret_dict, core_state)      \n",
    "    \n",
    "class Actor_Wrapper(nn.Module):    \n",
    "    def __init__(self, flags, model, actor=None, rec_t=5):\n",
    "        \n",
    "        super(Actor_Wrapper, self).__init__()   \n",
    "        self.model = model\n",
    "        self.num_actions = model.num_actions        \n",
    "        self.rec_t = rec_t\n",
    "        self.obs_shape = (3 * num_actions + 4 + self.rec_t, 1, 1)        \n",
    "        if actor is None:\n",
    "            self.actor = Actor_net(obs_shape=self.obs_shape, num_actions=self.num_actions, flags=flags)\n",
    "        else:\n",
    "            self.actor = actor   \n",
    "            \n",
    "    def initial_state(self, batch_size):\n",
    "        return self.actor.initial_state(batch_size)\n",
    "            \n",
    "    def forward(self, x, core_state=None):                      \n",
    "        # x is env_output object with:\n",
    "        # frame: T x B x C x H x W\n",
    "        # last_action: T x B\n",
    "        # reward: T x B\n",
    "        \n",
    "        tot_step, bsz, _, _, _ = x['frame'].shape\n",
    "        device = x['frame'].device        \n",
    "        self.model.train(False)\n",
    "        \n",
    "        for step in range(tot_step):\n",
    "        \n",
    "            state = x['frame'][step]        \n",
    "            action = F.one_hot(x['last_action'][step], self.num_actions)\n",
    "            reward = x['reward'][step]   \n",
    "            done = x['done'][step]\n",
    "            \n",
    "            rs, vs, logits, encodeds = self.model(state, action.unsqueeze(0), one_hot=True)\n",
    "            encoded_reset = encodeds[0].clone()\n",
    "            encoded = encodeds[0]\n",
    "\n",
    "            for t in range(self.rec_t): \n",
    "                \n",
    "                a = action if t == 0 else im_action  \n",
    "                r = (reward if t == 0 else rs[-1]).unsqueeze(-1)\n",
    "                v = vs[-1].unsqueeze(-1)\n",
    "                logit = logits[-1]                    \n",
    "                \n",
    "                if t == 0:\n",
    "                    r0 = r.clone()\n",
    "                    v0 = v.clone()\n",
    "                    logit0 = logit.clone()\n",
    "                    if self.actor.gb_ste: u_list = []\n",
    "                    im_logit_list = []\n",
    "                    reset_logit_list = []\n",
    "                \n",
    "                time = F.one_hot(torch.tensor([t], device=device).long(), self.rec_t).tile([bsz, 1])                        \n",
    "\n",
    "                actor_input = torch.concat([a, r, v, logit, r0, v0, logit0, time], dim=-1)     \n",
    "                actor_input = actor_input.unsqueeze(-1).unsqueeze(-1)\n",
    "                \n",
    "                if 'uniform' in x.keys():\n",
    "                    u = x['uniform'][step][:, t]\n",
    "                else:\n",
    "                    u = None                    \n",
    "                \n",
    "                actor_output, core_state = self.actor(actor_input, \n",
    "                                                      done=done, \n",
    "                                                      core_state=core_state,\n",
    "                                                      u=u)                \n",
    "                if self.actor.gb_ste: u_list.append(actor_output['uniform'].unsqueeze(1))\n",
    "                im_logit_list.append(actor_output['im_policy_logits'].unsqueeze(1))\n",
    "                reset_logit_list.append(actor_output['reset_policy_logits'].unsqueeze(1))\n",
    "                \n",
    "                if t == self.rec_t - 1:\n",
    "                    if self.actor.gb_ste:\n",
    "                        actor_output[\"uniform\"] = torch.concat(u_list, dim=1)\n",
    "                        actor_output['im_policy_logits'] = torch.concat(im_logit_list, dim=1)\n",
    "                        actor_output['reset_policy_logits'] = torch.concat(reset_logit_list, dim=1)\n",
    "                    if step == 0:\n",
    "                        all_actor_output = {k: [v.unsqueeze(0)] for k, v in actor_output.items()}\n",
    "                    else:\n",
    "                        for k, v in actor_output.items(): all_actor_output[k].append(v.unsqueeze(0))\n",
    "                \n",
    "                reset = actor_output[\"reset\"]\n",
    "                im_action = actor_output[\"im_action\"]\n",
    "\n",
    "                if t < self.rec_t - 1:                \n",
    "                    rs, vs, logits, encodeds = self.model.forward_encoded(encoded, im_action.unsqueeze(0), \n",
    "                                                                          one_hot=True)\n",
    "                    reset = reset.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "                    encoded = reset * encoded_reset + (1 - reset) * encodeds[-1]\n",
    "        \n",
    "        all_actor_output = {k: torch.concat(v) for k, v in all_actor_output.items()}\n",
    "        return all_actor_output, core_state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06dfbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_parser():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n",
    "\n",
    "    parser.add_argument(\"--env\", type=str, default=\"Sokoban-v0\",\n",
    "                        help=\"Gym environment.\")\n",
    "    parser.add_argument(\"--env_disable_noop\", action=\"store_true\",\n",
    "                        help=\"Disable noop in environment or not. (sokoban only)\")\n",
    "\n",
    "    parser.add_argument(\"--xpid\", default=None,\n",
    "                        help=\"Experiment id (default: None).\")\n",
    "\n",
    "    # Training settings.\n",
    "    parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n",
    "                        help=\"Disable saving checkpoint.\")\n",
    "    parser.add_argument(\"--savedir\", default=\"~/RS/thinker/logs/torchbeast\",\n",
    "                        help=\"Root dir where experiment data will be saved.\")\n",
    "    parser.add_argument(\"--num_actors\", default=48, type=int, metavar=\"N\",\n",
    "                        help=\"Number of actors (default: 48).\")\n",
    "    parser.add_argument(\"--total_steps\", default=100000000, type=int, metavar=\"T\",\n",
    "                        help=\"Total environment steps to train for.\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, metavar=\"B\",\n",
    "                        help=\"Learner batch size.\")\n",
    "    parser.add_argument(\"--unroll_length\", default=20, type=int, metavar=\"T\",\n",
    "                        help=\"The unroll length (time dimension).\")\n",
    "    parser.add_argument(\"--num_buffers\", default=None, type=int,\n",
    "                        metavar=\"N\", help=\"Number of shared-memory buffers.\")\n",
    "    parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=1, type=int,\n",
    "                        metavar=\"N\", help=\"Number learner threads.\")\n",
    "    parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n",
    "                        help=\"Disable CUDA.\")\n",
    "\n",
    "    # Architecture settings\n",
    "    parser.add_argument(\"--tran_dim\", default=64, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer hidden dim.\")\n",
    "    parser.add_argument(\"--tran_mem_n\", default=16, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer memory.\")\n",
    "    parser.add_argument(\"--tran_layer_n\", default=3, type=int, metavar=\"N\",\n",
    "                        help=\"Number of transformer layer.\")\n",
    "    parser.add_argument(\"--tran_t\", default=1, type=int, metavar=\"T\",\n",
    "                        help=\"Number of recurrent step for transformer.\")\n",
    "    parser.add_argument(\"--tran_ff_n\", default=256, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer ff .\")\n",
    "    parser.add_argument(\"--tran_skip\", action=\"store_true\",\n",
    "                        help=\"Whether to enable skip conn.\")\n",
    "    parser.add_argument(\"--tran_norm_first\", action=\"store_true\",\n",
    "                        help=\"Whether to use norm first in transformer.\")\n",
    "    parser.add_argument(\"--tran_rpos\", action=\"store_true\",\n",
    "                        help=\"Whether to use relative position in transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm\", action=\"store_true\",\n",
    "                        help=\"Whether to use LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm_no_attn\", action=\"store_true\",\n",
    "                        help=\"Whether to disable attention in LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_erasep\", action=\"store_true\",\n",
    "                        help=\"Whether to erase past memories if not planning.\")\n",
    "\n",
    "    parser.add_argument(\"--rec_t\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Number of planning steps.\")\n",
    "    parser.add_argument(\"--ste\", action=\"store_true\",\n",
    "                        help=\"Whether to use ste backprop.\")\n",
    "    parser.add_argument(\"--gb_ste\", action=\"store_true\",\n",
    "                        help=\"Whether to use gb-ste backprop.\")\n",
    "    parser.add_argument(\"--gb_ste_temp_max\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"Beginning temp. for gb-ste.\")\n",
    "    parser.add_argument(\"--gb_ste_temp_min\", default=0.5, type=int, metavar=\"N\",\n",
    "                        help=\"Ending temp. for gb-ste.\")    \n",
    "\n",
    "    # Loss settings.\n",
    "    parser.add_argument(\"--entropy_cost\", default=0.01,\n",
    "                        type=float, help=\"Entropy cost/multiplier.\")\n",
    "    parser.add_argument(\"--im_entropy_cost\", default=0.01,\n",
    "                        type=float, help=\"Entropy cost/multiplier.\")    \n",
    "    parser.add_argument(\"--baseline_cost\", default=0.5,\n",
    "                        type=float, help=\"Baseline cost/multiplier.\")\n",
    "    parser.add_argument(\"--reg_cost\", default=1,\n",
    "                        type=float, help=\"Reg cost/multiplier.\")\n",
    "    parser.add_argument(\"--discounting\", default=0.97,\n",
    "                        type=float, help=\"Discounting factor.\")\n",
    "    parser.add_argument(\"--lamb\", default=0.97,\n",
    "                        type=float, help=\"Lambda when computing trace.\")\n",
    "    parser.add_argument(\"--reward_clipping\", default=10, type=int, \n",
    "                        metavar=\"N\", help=\"Reward clipping.\")\n",
    "    parser.add_argument(\"--trun_bs\", action=\"store_true\",\n",
    "                        help=\"Whether to add baseline as reward when truncated.\")\n",
    "\n",
    "    # Optimizer settings.\n",
    "    parser.add_argument(\"--learning_rate\", default=0.0004,\n",
    "                        type=float, metavar=\"LR\", help=\"Learning rate.\")\n",
    "    parser.add_argument(\"--disable_adam\", action=\"store_true\",\n",
    "                        help=\"Use Aadm optimizer or not.\")\n",
    "    parser.add_argument(\"--alpha\", default=0.99, type=float,\n",
    "                        help=\"RMSProp smoothing constant.\")\n",
    "    parser.add_argument(\"--momentum\", default=0, type=float,\n",
    "                        help=\"RMSProp momentum.\")\n",
    "    parser.add_argument(\"--epsilon\", default=0.01, type=float,\n",
    "                        help=\"RMSProp epsilon.\")\n",
    "    parser.add_argument(\"--grad_norm_clipping\", default=0.0, type=float,\n",
    "                        help=\"Global gradient norm clip.\")\n",
    "    # yapf: enable\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.num_actors = 1\n",
    "flags.batch_size = 32\n",
    "flags.unroll_length = 20\n",
    "flags.learning_rate = 0.0004\n",
    "flags.entropy_cost = 0.001\n",
    "flags.im_entropy_cost = 0.\n",
    "flags.lamb = 1.\n",
    "\n",
    "flags.trun_bs = False\n",
    "flags.total_steps = 100000000\n",
    "flags.disable_adam = False\n",
    "\n",
    "flags.tran_t = 1\n",
    "flags.tran_mem_n = 16\n",
    "flags.tran_layer_n = 3\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = True\n",
    "flags.tran_norm_first = False\n",
    "flags.tran_ff_n = 256\n",
    "flags.tran_skip = True\n",
    "flags.tran_erasep = False\n",
    "flags.tran_dim = 64\n",
    "flags.tran_rpos = True\n",
    "flags.rec_t = 5\n",
    "flags.ste = False\n",
    "flags.gb_ste = True\n",
    "flags.gb_ste_temp_max = 1\n",
    "flags.gb_ste_temp_min = 0.5\n",
    "\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "model_learner = Model(flags, obs_shape, num_actions=num_actions)\n",
    "model_actor = Model(flags, obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model_learner.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "model_actor.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f342a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mp.set_sharing_strategy('file_system')\n",
    "\n",
    "if flags.xpid is None:\n",
    "    flags.xpid = \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "plogger = file_writer.FileWriter(\n",
    "    xpid=flags.xpid, xp_args=flags.__dict__, rootdir=flags.savedir\n",
    ")\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available(): \n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser(\"%s/%s/%s\" % (flags.savedir, flags.xpid, \"model.tar\"))\n",
    ")\n",
    "\n",
    "if flags.num_buffers is None:  # Set sensible default for num_buffers.\n",
    "    flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n",
    "if flags.num_actors >= flags.num_buffers:\n",
    "    raise ValueError(\"num_buffers should be larger than num_actors\")\n",
    "if flags.num_buffers < flags.batch_size:\n",
    "    raise ValueError(\"num_buffers should be larger than batch_size\")\n",
    "\n",
    "T = flags.unroll_length\n",
    "B = flags.batch_size\n",
    "\n",
    "env = create_env(flags)\n",
    "\n",
    "actor_net = Actor_Wrapper(flags, model_actor, actor=None, rec_t=flags.rec_t)\n",
    "buffers = create_buffers(flags, env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "actor_net.share_memory()\n",
    "\n",
    "# Add initial RNN state.\n",
    "initial_agent_state_buffers = []\n",
    "for _ in range(flags.num_buffers):\n",
    "    state = actor_net.initial_state(batch_size=1)\n",
    "    for t in state:\n",
    "        t.share_memory_()\n",
    "    initial_agent_state_buffers.append(state)\n",
    "\n",
    "actor_processes = []\n",
    "ctx = mp.get_context()\n",
    "free_queue = ctx.SimpleQueue()\n",
    "full_queue = ctx.SimpleQueue()\n",
    "\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=act, args=(flags, i, free_queue, full_queue,\n",
    "            actor_net, buffers, initial_agent_state_buffers,),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)\n",
    "\n",
    "learner_net = Actor_Wrapper(flags, model_learner, actor=None, \n",
    "                            rec_t=flags.rec_t).to(device=flags.device)\n",
    "\n",
    "if not flags.disable_adam:\n",
    "    print(\"Using Adam...\")        \n",
    "    optimizer = torch.optim.Adam(learner_net.actor.parameters(),lr=flags.learning_rate)\n",
    "else:\n",
    "    print(\"Using RMS Prop...\")\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        learner_net.actor.parameters(),\n",
    "        lr=flags.learning_rate,\n",
    "        momentum=flags.momentum,\n",
    "        eps=flags.epsilon,\n",
    "        alpha=flags.alpha,)\n",
    "print(\"All parameters: \")\n",
    "for k, v in learner_net.named_parameters(): print(k, v.numel())    \n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "logger = logging.getLogger(\"logfile\")\n",
    "stat_keys = [\"mean_episode_return\", \"episode_returns\", \"total_loss\",\n",
    "    \"pg_loss\", \"baseline_loss\", \"entropy_loss\", \"im_entropy_loss\"]\n",
    "logger.info(\"# Step\\t%s\", \"\\t\".join(stat_keys))\n",
    "\n",
    "step, stats, last_returns, tot_eps = 0, {}, deque(maxlen=400), 0\n",
    "\n",
    "def batch_and_learn(i, lock=threading.Lock()):\n",
    "    \"\"\"Thread target for the learning process.\"\"\"\n",
    "    #nonlocal step, stats, last_returns, tot_eps\n",
    "    global step, stats, last_returns, tot_eps\n",
    "    timings = prof.Timings()\n",
    "    while step < flags.total_steps:\n",
    "        timings.reset()\n",
    "        batch, agent_state = get_batch(flags, free_queue, full_queue, buffers,\n",
    "            initial_agent_state_buffers, timings,)\n",
    "        stats = learn(flags, actor_net, learner_net, batch, agent_state, optimizer, \n",
    "            scheduler)\n",
    "        if flags.gb_ste:\n",
    "            learner_net.actor.temp = torch.tensor(np.maximum(flags.gb_ste_temp_max * \n",
    "                    np.exp(-step/flags.total_steps), flags.gb_ste_temp_min).item(), \n",
    "                    dtype=torch.float32, device=learner_net.actor.temp.device)\n",
    "        last_returns.extend(stats[\"episode_returns\"])\n",
    "        tot_eps = tot_eps + len(stats[\"episode_returns\"])\n",
    "        timings.time(\"learn\")\n",
    "        with lock:\n",
    "            to_log = dict(step=step)\n",
    "            to_log.update({k: stats[k] for k in stat_keys})\n",
    "            to_log.update({\"trail_mean_episode_return\": np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "                           \"episode\": tot_eps})\n",
    "            plogger.log(to_log)\n",
    "            step += T * B\n",
    "\n",
    "    if i == 0:\n",
    "        logging.info(\"Batch and learn: %s\", timings.summary())\n",
    "\n",
    "for m in range(flags.num_buffers):\n",
    "    free_queue.put(m)\n",
    "\n",
    "threads = []\n",
    "for i in range(flags.num_learner_threads):\n",
    "    thread = threading.Thread(\n",
    "        target=batch_and_learn, name=\"batch-and-learn-%d\" % i, args=(i,)\n",
    "    )\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "def checkpoint():\n",
    "    if flags.disable_checkpoint:\n",
    "        return\n",
    "    logging.info(\"Saving checkpoint to %s\", checkpointpath)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": actor_net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"flags\": vars(flags),\n",
    "        },\n",
    "        checkpointpath,\n",
    "    )\n",
    "\n",
    "timer = timeit.default_timer\n",
    "try:\n",
    "    last_checkpoint_time = timer()\n",
    "    while step < flags.total_steps:\n",
    "        start_step = step\n",
    "        start_time = timer()\n",
    "        time.sleep(5)\n",
    "\n",
    "        if timer() - last_checkpoint_time > 10 * 60:  # Save every 10 min.\n",
    "            checkpoint()\n",
    "            last_checkpoint_time = timer()\n",
    "\n",
    "        sps = (step - start_step) / (timer() - start_time)\n",
    "        if stats.get(\"episode_returns\", None):\n",
    "            mean_return = (\n",
    "                \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n",
    "            )\n",
    "        else:\n",
    "            mean_return = \"\"\n",
    "        total_loss = stats.get(\"total_loss\", float(\"inf\"))\n",
    "\n",
    "        print_str =  \"Steps %i @ %.1f SPS. Eps %i. L400 Return %f. Loss %f\" % (step, sps, tot_eps, \n",
    "            np.average(last_returns) if len(last_returns) > 0 else 0., total_loss)\n",
    "\n",
    "        for s in [\"pg_loss\", \"baseline_loss\", \"entropy_loss\", \"im_entropy_loss\", \"reg_loss\"]:\n",
    "            if s in stats:\n",
    "                print_str += \" %s %f\" % (s, stats[s])\n",
    "\n",
    "        logging.info(print_str)\n",
    "except KeyboardInterrupt:\n",
    "    for thread in threads:\n",
    "        thread.join()        \n",
    "    # Try joining actors then quit.\n",
    "else:\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    logging.info(\"Learning finished after %d steps.\", step)\n",
    "finally:\n",
    "    for _ in range(flags.num_actors):\n",
    "        free_queue.put(None)\n",
    "    for actor in actor_processes:\n",
    "        actor.join(timeout=1)\n",
    "\n",
    "checkpoint()\n",
    "plogger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = flags.total_steps + 1\n",
    "for thread in threads:\n",
    "     thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(flags.num_actors):\n",
    "    free_queue.put(None)\n",
    "for actor in actor_processes:\n",
    "    actor.join(timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3413ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": actor_net.state_dict(),},\"base/actor_net_tmp_2.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a32f0b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  286316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Actor_Wrapper(\n",
       "  (model): Model(\n",
       "    (frameEncoder): FrameEncoder(\n",
       "      (conv1): Conv2d(8, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (res1): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (res2): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avg1): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      (res3): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avg2): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "    )\n",
       "    (dynamicModel): DynamicModel(\n",
       "      (res): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (conv1): Conv2d(133, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(133, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(133, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_rvpi): Output_rvpi(\n",
       "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (fc_r): Linear(in_features=800, out_features=1, bias=True)\n",
       "      (fc_v): Linear(in_features=800, out_features=1, bias=True)\n",
       "      (fc_logits): Linear(in_features=800, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (actor): Actor_net(\n",
       "    (conv1): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (frame_conv): Sequential(\n",
       "      (0): Conv2d(24, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (core): ConvAttnLSTM(\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvAttnLSTMCell(\n",
       "          (conv): Conv2d(256, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ConvAttnLSTMCell(\n",
       "          (conv): Conv2d(256, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ConvAttnLSTMCell(\n",
       "          (conv): Conv2d(256, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (proj_list): ModuleList(\n",
       "        (0): Conv2d(64, 64, kernel_size=(2, 1), stride=(1, 1), groups=64)\n",
       "        (1): Conv2d(64, 64, kernel_size=(2, 1), stride=(1, 1), groups=64)\n",
       "        (2): Conv2d(64, 64, kernel_size=(2, 1), stride=(1, 1), groups=64)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (im_policy): Linear(in_features=256, out_features=5, bias=True)\n",
       "    (policy): Linear(in_features=256, out_features=5, bias=True)\n",
       "    (baseline): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (reset): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debug trainned actor\n",
    "\n",
    "class Actor_Wrapper(nn.Module):    \n",
    "    def __init__(self, flags, model, actor=None, rec_t=5):\n",
    "        \n",
    "        super(Actor_Wrapper, self).__init__()   \n",
    "        self.model = model\n",
    "        self.num_actions = model.num_actions        \n",
    "        self.rec_t = rec_t\n",
    "        self.obs_shape = (3 * num_actions + 4 + self.rec_t, 1, 1)\n",
    "        if actor is None:\n",
    "            self.actor = Actor_net(obs_shape=self.obs_shape, num_actions=self.num_actions, flags=flags)\n",
    "        else:\n",
    "            self.actor = actor   \n",
    "            \n",
    "    def initial_state(self, batch_size):\n",
    "        return self.actor.initial_state(batch_size)\n",
    "            \n",
    "    def forward(self, x, core_state=None):                      \n",
    "        # x is env_output object with:\n",
    "        # frame: T x B x C x H x W\n",
    "        # last_action: T x B\n",
    "        # reward: T x B\n",
    "        \n",
    "        tot_step, bsz, _, _, _ = x['frame'].shape\n",
    "        device = x['frame'].device        \n",
    "        self.model.train(False)\n",
    "        \n",
    "        for step in range(tot_step):\n",
    "        \n",
    "            state = x['frame'][step]        \n",
    "            action = F.one_hot(x['last_action'][step], self.num_actions)\n",
    "            reward = x['reward'][step]   \n",
    "            done = x['done'][step]\n",
    "            \n",
    "            rs, vs, logits, encodeds = self.model(state, action.unsqueeze(0), one_hot=True)\n",
    "            encoded_reset = encodeds[0].clone()\n",
    "            encoded = encodeds[0]\n",
    "\n",
    "            for t in range(self.rec_t):\n",
    "                \n",
    "                a = action if t == 0 else im_action  \n",
    "                r = (reward if t == 0 else rs[-1]).unsqueeze(-1)\n",
    "                v = vs[-1].unsqueeze(-1)\n",
    "                logit = logits[-1]                    \n",
    "                \n",
    "                if t == 0:\n",
    "                    r0 = r.clone()\n",
    "                    v0 = v.clone()\n",
    "                    logit0 = logit.clone()\n",
    "                    #print(\"p0: \\n\", F.softmax(logit, dim=-1).cpu().detach().numpy())\n",
    "                \n",
    "                time = F.one_hot(torch.tensor([t], device=device).long(), self.rec_t).tile([bsz, 1])                        \n",
    "\n",
    "                actor_input = torch.concat([a, r, v, logit, r0, v0, logit0, time], dim=-1)     \n",
    "                actor_input = actor_input.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "                actor_output, core_state = self.actor(actor_input, done=done, core_state=core_state)\n",
    "                \n",
    "                \n",
    "                if t == self.rec_t - 1:\n",
    "                    if step == 0:\n",
    "                        all_actor_output = {k: [v.unsqueeze(0)] for k, v in actor_output.items()}\n",
    "                    else:\n",
    "                        for k, v in actor_output.items(): all_actor_output[k].append(v.unsqueeze(0))\n",
    "                reset = actor_output[\"reset\"]\n",
    "                im_action = actor_output[\"im_action\"]\n",
    "                \n",
    "                np.set_printoptions(suppress=True)   \n",
    "                print(\"===========STEP:%d============\" % t)\n",
    "                print(\"im_action: \\n\", actor_output[\"im_action\"].cpu().detach().numpy())\n",
    "                print(\"im_prob: \\n\", F.softmax(actor_output[\"im_policy_logits\"], dim=-1).cpu().detach().numpy())\n",
    "                print(\"reset_prob: \\n\", F.softmax(actor_output[\"reset_policy_logits\"], dim=-1).cpu().detach().numpy())\n",
    "                print(\"reset: \", reset.cpu().detach().numpy())\n",
    "\n",
    "                if t < self.rec_t - 1:                \n",
    "                    rs, vs, logits, encodeds = self.model.forward_encoded(encoded, im_action.unsqueeze(0), \n",
    "                                                                          one_hot=True)\n",
    "                    reset = reset.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "                    encoded = reset * encoded_reset + (1 - reset) * encodeds[-1]    \n",
    "        all_actor_output = {k: torch.concat(v) for k, v in all_actor_output.items()}        \n",
    "        print(\"action: \\n\", all_actor_output[\"action\"][-1].cpu().detach().numpy())\n",
    "        print(\"prob: \\n\", F.softmax(all_actor_output[\"policy_logits\"], dim=-1).cpu().detach().numpy())\n",
    "        return all_actor_output, core_state    \n",
    "\n",
    "    \n",
    "bsz = 2\n",
    "eps_n = 500\n",
    "flags.device = torch.device(\"cuda\")\n",
    "    \n",
    "actor_net = Actor_Wrapper(flags, model_actor, actor=None, rec_t=flags.rec_t)\n",
    "checkpoint = torch.load(\"../models/actor_net.tar\")\n",
    "actor_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "actor_net.train(False)    \n",
    "actor_net.to(flags.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5f74fdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========STEP:0============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.01303033 0.12202003 0.00011831 0.861277   0.00355438]\n",
      " [0.02005937 0.1563285  0.00020328 0.81897014 0.0044387 ]]\n",
      "reset_prob: \n",
      " [[0.66825104 0.33174896]\n",
      " [0.6270854  0.37291464]]\n",
      "reset:  [1. 0.]\n",
      "===========STEP:1============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00131315 0.00692749 0.00000532 0.99154586 0.00020826]\n",
      " [0.00367619 0.00946562 0.00000905 0.98676074 0.00008838]]\n",
      "reset_prob: \n",
      " [[0.26974186 0.7302581 ]\n",
      " [0.18466954 0.81533045]]\n",
      "reset:  [0. 0.]\n",
      "===========STEP:2============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00299733 0.02994021 0.00001489 0.966174   0.00087351]\n",
      " [0.00814811 0.02965293 0.00002791 0.9618866  0.0002845 ]]\n",
      "reset_prob: \n",
      " [[0.38081542 0.6191846 ]\n",
      " [0.24439421 0.7556058 ]]\n",
      "reset:  [0. 0.]\n",
      "===========STEP:3============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00719836 0.13608794 0.00004715 0.8526864  0.00398015]\n",
      " [0.02033918 0.1901869  0.00015018 0.7866306  0.0026932 ]]\n",
      "reset_prob: \n",
      " [[0.60847926 0.39152077]\n",
      " [0.47733212 0.5226679 ]]\n",
      "reset:  [1. 1.]\n",
      "===========STEP:4============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.0092778  0.2128586  0.00005861 0.7732341  0.00457086]\n",
      " [0.0286767  0.31080636 0.00021027 0.6566279  0.00367872]]\n",
      "reset_prob: \n",
      " [[0.5791115  0.42088848]\n",
      " [0.49340904 0.50659096]]\n",
      "reset:  [1. 1.]\n",
      "action: \n",
      " [2 4]\n",
      "prob: \n",
      " [[[0.02009038 0.02255619 0.04591284 0.01428151 0.897159  ]\n",
      "  [0.04612957 0.4500903  0.05764054 0.11851007 0.32762957]]]\n",
      "===========STEP:0============\n",
      "im_action: \n",
      " [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.01091958 0.16876675 0.00008257 0.8169795  0.00325161]\n",
      " [0.00313398 0.02422785 0.0000214  0.9720437  0.00057309]]\n",
      "reset_prob: \n",
      " [[0.6484993  0.3515007 ]\n",
      " [0.39037907 0.6096209 ]]\n",
      "reset:  [1. 0.]\n",
      "===========STEP:1============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00391402 0.01478347 0.0000127  0.9803585  0.00093137]\n",
      " [0.00016656 0.00037369 0.         0.9994598  0.00000003]]\n",
      "reset_prob: \n",
      " [[0.6683701  0.33162987]\n",
      " [0.00346701 0.9965329 ]]\n",
      "reset:  [1. 0.]\n",
      "===========STEP:2============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00077911 0.00549953 0.00000115 0.9935527  0.00016753]\n",
      " [0.0001266  0.0002625  0.         0.99961096 0.00000001]]\n",
      "reset_prob: \n",
      " [[0.5529144  0.44708562]\n",
      " [0.00179261 0.99820745]]\n",
      "reset:  [0. 0.]\n",
      "===========STEP:3============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00157926 0.03968272 0.00000446 0.95806265 0.00067092]\n",
      " [0.00059294 0.00174567 0.00000002 0.99766123 0.00000014]]\n",
      "reset_prob: \n",
      " [[0.7272687  0.27273133]\n",
      " [0.00726546 0.99273455]]\n",
      "reset:  [1. 0.]\n",
      "===========STEP:4============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00460477 0.10236283 0.00004381 0.8902868  0.00270184]\n",
      " [0.00340102 0.00882219 0.00000085 0.9877713  0.00000467]]\n",
      "reset_prob: \n",
      " [[0.8121357  0.18786435]\n",
      " [0.02337938 0.9766207 ]]\n",
      "reset:  [1. 0.]\n",
      "action: \n",
      " [2 3]\n",
      "prob: \n",
      " [[[0.00461387 0.00237423 0.9781858  0.00352907 0.01129704]\n",
      "  [0.0057618  0.00504803 0.00422001 0.9805917  0.00437843]]]\n",
      "===========STEP:0============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00707981 0.15751761 0.00006003 0.83347374 0.00186878]\n",
      " [0.00581069 0.02315886 0.00000505 0.9709829  0.00004248]]\n",
      "reset_prob: \n",
      " [[0.83732784 0.16267216]\n",
      " [0.12451383 0.87548614]]\n",
      "reset:  [1. 0.]\n",
      "===========STEP:1============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.0012997  0.01442965 0.00000306 0.98385113 0.00041635]\n",
      " [0.00394148 0.01318324 0.00000849 0.98281974 0.00004715]]\n",
      "reset_prob: \n",
      " [[0.60034317 0.39965686]\n",
      " [0.14350058 0.8564994 ]]\n",
      "reset:  [0. 1.]\n",
      "===========STEP:2============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "im_prob: \n",
      " [[0.00080408 0.015959   0.00000126 0.98290986 0.00032575]\n",
      " [0.01997085 0.05951058 0.0001016  0.9199307  0.0004862 ]]\n",
      "reset_prob: \n",
      " [[0.4765846  0.5234154 ]\n",
      " [0.34375584 0.65624416]]\n",
      "reset:  [1. 0.]\n",
      "===========STEP:3============\n",
      "im_action: \n",
      " [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00189568 0.05423372 0.00000384 0.94285774 0.00100904]\n",
      " [0.0421412  0.18909515 0.00019918 0.76715004 0.00141439]]\n",
      "reset_prob: \n",
      " [[0.609067  0.390933 ]\n",
      " [0.6073897 0.3926103]]\n",
      "reset:  [1. 0.]\n",
      "===========STEP:4============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00331412 0.0897872  0.00000676 0.90486753 0.00202431]\n",
      " [0.0623612  0.28043595 0.0002753  0.6550415  0.00188606]]\n",
      "reset_prob: \n",
      " [[0.6443042  0.35569578]\n",
      " [0.5779921  0.42200792]]\n",
      "reset:  [1. 1.]\n",
      "action: \n",
      " [4 1]\n",
      "prob: \n",
      " [[[0.0059123  0.0041139  0.00624433 0.0038583  0.9798712 ]\n",
      "  [0.01823643 0.8597765  0.08720398 0.02525038 0.00953274]]]\n",
      "===========STEP:0============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00075649 0.01091351 0.00000109 0.98806226 0.00026663]\n",
      " [0.03773765 0.12087614 0.00014643 0.83959866 0.00164115]]\n",
      "reset_prob: \n",
      " [[0.6335368  0.36646312]\n",
      " [0.713888   0.28611198]]\n",
      "reset:  [1. 0.]\n",
      "===========STEP:1============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00006151 0.00117401 0.00000001 0.9987525  0.0000119 ]\n",
      " [0.00460314 0.01002518 0.00001205 0.9852254  0.00013434]]\n",
      "reset_prob: \n",
      " [[0.3641775  0.63582253]\n",
      " [0.42019022 0.5798098 ]]\n",
      "reset:  [0. 1.]\n",
      "===========STEP:2============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00034847 0.00938015 0.00000024 0.99015605 0.00011503]\n",
      " [0.00388438 0.01165292 0.0000061  0.9843754  0.00008124]]\n",
      "reset_prob: \n",
      " [[0.49738777 0.50261223]\n",
      " [0.2653982  0.7346018 ]]\n",
      "reset:  [0. 0.]\n",
      "===========STEP:3============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00540662 0.05940758 0.00002965 0.9333877  0.00176844]\n",
      " [0.01380896 0.07220641 0.0001338  0.91201013 0.00184066]]\n",
      "reset_prob: \n",
      " [[0.7956775  0.2043225 ]\n",
      " [0.55612403 0.443876  ]]\n",
      "reset:  [1. 0.]\n",
      "===========STEP:4============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00887317 0.09328385 0.00008936 0.89441276 0.00334086]\n",
      " [0.01814665 0.14116646 0.0002748  0.8359542  0.00445789]]\n",
      "reset_prob: \n",
      " [[0.7987555  0.20124446]\n",
      " [0.74478334 0.2552167 ]]\n",
      "reset:  [0. 0.]\n",
      "action: \n",
      " [2 2]\n",
      "prob: \n",
      " [[[0.00760516 0.00609993 0.9671892  0.00360718 0.01549842]\n",
      "  [0.02175673 0.07613125 0.821521   0.05539382 0.02519718]]]\n",
      "===========STEP:0============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "im_prob: \n",
      " [[0.00741704 0.09247389 0.00003864 0.8959352  0.00413523]\n",
      " [0.02055236 0.21604685 0.00045864 0.7563224  0.00661985]]\n",
      "reset_prob: \n",
      " [[0.80828863 0.19171132]\n",
      " [0.7889455  0.21105447]]\n",
      "reset:  [1. 1.]\n",
      "===========STEP:1============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "im_prob: \n",
      " [[0.00044495 0.00522976 0.00000032 0.99417347 0.00015151]\n",
      " [0.08039879 0.12298501 0.00039113 0.79392713 0.00229793]]\n",
      "reset_prob: \n",
      " [[0.5294951  0.47050485]\n",
      " [0.49986404 0.500136  ]]\n",
      "reset:  [0. 1.]\n",
      "===========STEP:2============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "im_prob: \n",
      " [[0.00076048 0.01371269 0.00000058 0.9852379  0.00028837]\n",
      " [0.08514087 0.22705095 0.00030958 0.68659943 0.00089916]]\n",
      "reset_prob: \n",
      " [[0.5552188 0.4447812]\n",
      " [0.3390897 0.6609103]]\n",
      "reset:  [1. 1.]\n",
      "===========STEP:3============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.00736469 0.06827407 0.00002486 0.921845   0.00249128]\n",
      " [0.08842339 0.2667567  0.00011397 0.64428437 0.0004216 ]]\n",
      "reset_prob: \n",
      " [[0.67912304 0.320877  ]\n",
      " [0.30951464 0.69048536]]\n",
      "reset:  [1. 1.]\n",
      "===========STEP:4============\n",
      "im_action: \n",
      " [[0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "im_prob: \n",
      " [[0.01262786 0.12528907 0.00005045 0.85768557 0.00434706]\n",
      " [0.07770906 0.42807326 0.00016943 0.4935192  0.00052908]]\n",
      "reset_prob: \n",
      " [[0.7352539  0.2647461 ]\n",
      " [0.29643956 0.7035604 ]]\n",
      "reset:  [1. 0.]\n",
      "action: \n",
      " [4 1]\n",
      "prob: \n",
      " [[[0.03047786 0.05136207 0.16058396 0.01622253 0.7413536 ]\n",
      "  [0.02020199 0.928841   0.0163845  0.02029846 0.01427409]]]\n",
      "Finish 0 episode: avg. return: nan (+-nan) \n"
     ]
    }
   ],
   "source": [
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "core_state = actor_net.initial_state(bsz)\n",
    "core_state = tuple(v.to(flags.device) for v in core_state)\n",
    "returns = []\n",
    "obs = env.initial()\n",
    "\n",
    "#while(len(returns) <= eps_n):\n",
    "for t in range(5):\n",
    "    #print(\"t: %d\" %t)    \n",
    "    with torch.no_grad():\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}    \n",
    "        actor_out, core_state = actor_net(obs, core_state)\n",
    "        obs = env.step(actor_out[\"action\"][0])\n",
    "        #print(\"p_sel: \\n\", F.softmax(actor_out[\"policy_logits\"], dim=-1).detach().numpy())\n",
    "        #print(\"action: \", actor_out[\"action\"][0])\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "\n",
    "returns = returns[:eps_n]            \n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "      np.average(returns), np.std(returns) / np.sqrt(len(returns))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e82488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model to another one\n",
    "\n",
    "parser = define_parser()        \n",
    "flags = parser.parse_args(\"--env Sokoban-v0\".split())        \n",
    "flags.device = torch.device(\"cuda\")\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"base/model_r2.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "model.train(False)\n",
    "actor_net.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda48bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing actor wrapper with a one-step greedy actor\n",
    "\n",
    "class Actor_test(nn.Module):\n",
    "    # A one-step greedy actor (for testing only)\n",
    "    def __init__(self, obs_shape, num_actions, rec_t):\n",
    "        super(Actor_test, self).__init__()   \n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.rec_t = rec_t\n",
    "        self.rs = torch.zeros(num_actions,device=flags.device)\n",
    "        self.vs = []\n",
    "    \n",
    "    def forward(self, x, done, core_state=None):\n",
    "        x = x[:, :, 0, 0]        \n",
    "        #print(\"Input: \", x[0])\n",
    "        bsz = x.shape[0]\n",
    "        time = x[0, -self.rec_t:]\n",
    "        t = torch.argmax(time, dim=0)        \n",
    "        \n",
    "        if t == 0:\n",
    "            self.rs = torch.zeros(rec_t, bsz, device=x.device)\n",
    "            self.vs = torch.zeros(rec_t, bsz, device=x.device)\n",
    "        \n",
    "        self.rs[t] = x[:,self.num_actions]\n",
    "        self.vs[t] = x[:,self.num_actions+1]  \n",
    "        \n",
    "        im_action = t if t < self.num_actions else torch.zeros(1, device=x.device).long()\n",
    "        im_action = F.one_hot(im_action.unsqueeze(0), self.num_actions).tile([bsz, 1])\n",
    "        reset = torch.ones(bsz, device=x.device)\n",
    "        \n",
    "        policy_logits = (self.rs[1:1+self.num_actions] + flags.discounting * self.vs[1:1+self.num_actions]).transpose(0, 1) \n",
    "        policy_probs = F.softmax(20 * policy_logits, dim=1)\n",
    "        action = torch.multinomial(policy_probs, num_samples=1)\n",
    "        \n",
    "        ret = {\"im_action\": im_action, \n",
    "               \"reset\": reset, \n",
    "               \"policy_logits\": policy_logits, \n",
    "               \"policy_probs\": policy_probs, \n",
    "               \"action\": action,\n",
    "               \"baseline\": torch.zeros_like(reset),\n",
    "               \"reg_loss\": torch.zeros_like(reset)}\n",
    "        \n",
    "        return (ret, None)\n",
    "\n",
    "model = model_actor    \n",
    "model.train(False)    \n",
    "    \n",
    "rec_t = 6        \n",
    "actor = Actor_test(obs_shape=(model.num_actions + 2 + rec_t), num_actions=model.num_actions, rec_t=rec_t)        \n",
    "actor_wrapper = Actor_Wrapper(None, model, actor, rec_t = 6)\n",
    "        \n",
    "bsz = 2\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "obs = env.initial()\n",
    "state = obs['frame'][0].to(flags.device).clone()\n",
    "action = torch.zeros(bsz).long().to(flags.device)\n",
    "\n",
    "#obs = {k:v.to(flags.device).view((1, 10)+v.shape[2:]) for k, v in obs.items()}\n",
    "obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "actor_output, _ = actor_wrapper(obs)\n",
    "print(F.softmax(20 * actor_output['policy_logits'], dim=-1))\n",
    "action, prob, q_ret = n_step_greedy_model(state, action, model, 1, encoded=None, temp=20.)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201420df",
   "metadata": {},
   "source": [
    "<font size=\"5\">Misc.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch_m(flags, buffers)\n",
    "print(torch.max(batch[\"reward\"]), (torch.max(batch[\"reward\"]) == batch[\"reward\"]).nonzero())\n",
    "print(batch[\"done\"].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ce789",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"reward\"][:, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG LOSS\n",
    "\n",
    "#batch = get_batch_m(flags, buffers)\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "logits = logits[:-1]\n",
    "\n",
    "target_rewards = batch['reward'][1:]\n",
    "target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "target_vs = []\n",
    "target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0]    \n",
    "\n",
    "for t in range(vs.shape[0]-1, 0, -1):\n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))\n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "# if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "# rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "# vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "# logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "done_masks = []\n",
    "done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "for t in range(vs.shape[0]):\n",
    "    done = torch.logical_or(done, batch['done'][t])\n",
    "    done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "\n",
    "# compute final loss\n",
    "huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "rs_loss = torch.sum(huberloss(rs, target_rewards) * (~done_masks).float())\n",
    "#rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "vs_loss = torch.sum(huberloss(vs[:-1], target_vs) * (~done_masks).float())\n",
    "#vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "logits_loss = compute_cross_entropy_loss(logits, target_logits, done_masks)\n",
    "\n",
    "# debug\n",
    "ind = 21\n",
    "\n",
    "target_vs = []\n",
    "target_v = vs[-1]\n",
    "for t in range(vs.shape[0]-1, 0, -1):        \n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    print(t, \n",
    "          \"reward %2f\" % batch['reward'][t,ind].item(), \n",
    "          \"bootstrap %2f\" % (target_v * (~batch['done'][t]).float())[ind].item(), \n",
    "          \"truncated %2f\" % (vs[t-1] * (batch['truncated_done'][t]).float())[ind].item(),\n",
    "          \"vs[t-1] %2f\" % vs[t-1][ind].item(),\n",
    "          \"new_targ %2f\" % new_target_v[ind].item())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))    \n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)   \n",
    "print(\"done\", batch[\"done\"][:, ind])\n",
    "print(\"done_masks\", done_masks[:, ind])\n",
    "print(\"vs: \", vs[:, ind])\n",
    "print(\"target_vs: \", target_vs[:, ind])\n",
    "print(\"reward: \", rs[:, ind])\n",
    "print(\"target_reward: \", target_rewards[:, ind])\n",
    "print(\"logits: \", logits[:, ind])\n",
    "print(\"target_logits: \", target_logits[:, ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt. version of computing loss by treading terminal state as absorbing state (as in MuZero)\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_logits = batch['policy_logits'][1:].clone()\n",
    "    target_rewards = batch['reward'][1:].clone()\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "\n",
    "    c_logits = target_logits[0]\n",
    "    c_state = batch['frame'][0]\n",
    "    for t in range(vs.shape[0]-1):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        c_logits = torch.where(done.unsqueeze(-1), c_logits, target_logits[t])\n",
    "        target_logits[t] = c_logits\n",
    "        c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][t])  \n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "    done_masks = torch.concat(done_masks, dim=0)\n",
    "    done = torch.logical_or(done, batch['done'][-1])\n",
    "    c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][-1])\n",
    "    target_rewards = target_rewards * (~done_masks).float()\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(c_state, batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * target_v\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()))\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()))\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), None)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
