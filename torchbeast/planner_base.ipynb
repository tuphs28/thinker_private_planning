{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fc8a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core import file_writer\n",
    "from torchbeast.core import prof\n",
    "from torchbeast.core import vtrace\n",
    "from torchbeast.core.environment import Environment, Vec_Environment\n",
    "\n",
    "from torchbeast.atari_wrappers import *\n",
    "from torchbeast.transformer_rnn import *\n",
    "from torchbeast.train import *\n",
    "from torchbeast.model import Model\n",
    "from torchbeast.base import BaseNet\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "def get_param(net, name=None):\n",
    "    keys = []\n",
    "    for (k, v) in actor_wrapper.model.named_parameters(): \n",
    "        if name is None:\n",
    "            print(k)\n",
    "        else:\n",
    "            if name == k: return v\n",
    "        keys.append(k)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b0cae",
   "metadata": {},
   "source": [
    "<font size=\"5\">Agent Training Phase</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "239d9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits, temperature, u=None, hard=True):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [bsz, n_class]\n",
    "    return: flatten --> [bsz, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    n_class = logits.shape[-1]\n",
    "    \n",
    "    if u is None:\n",
    "        u = torch.rand(logits.size()).to(logits.device)\n",
    "        u = -torch.log(-torch.log(u + eps) + eps)\n",
    "    y = logits + u\n",
    "    y = F.softmax(y / temperature, dim=-1)\n",
    "    \n",
    "    if not hard: return y.view(-1, n_class)\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "    return y_hard.view(-1, n_class), u\n",
    "\n",
    "class Actor_Wrapper(nn.Module):    \n",
    "    def __init__(self, flags, model, actor=None):\n",
    "        \n",
    "        super(Actor_Wrapper, self).__init__()   \n",
    "        self.model = model\n",
    "        self.num_actions = model.num_actions        \n",
    "        self.rec_t = flags.rec_t\n",
    "        self.discounting = flags.discounting\n",
    "        self.aug_stat = flags.aug_stat\n",
    "        \n",
    "        obs_n = (7 + num_actions * 7 + self.rec_t if self.aug_stat else \n",
    "            5 + num_actions * 3 + self.rec_t)       \n",
    "        if actor is None:\n",
    "            self.actor = Actor_net(obs_shape=(obs_n, 1, 1), num_actions=self.num_actions, flags=flags)\n",
    "        else:\n",
    "            self.actor = actor   \n",
    "        self.use_model = self.use_model_aug if self.aug_stat else self.use_model_base        \n",
    "            \n",
    "    def initial_state(self, batch_size):\n",
    "        return self.actor.initial_state(batch_size)\n",
    "            \n",
    "    def forward(self, x, core_state=None):                      \n",
    "        # x is env_output object with:\n",
    "        # frame: T x B x C x H x W\n",
    "        # last_action: T x B\n",
    "        # reward: T x B\n",
    "        \n",
    "        tot_step, bsz, _, _, _ = x['frame'].shape\n",
    "        device = x['frame'].device        \n",
    "        self.model.train(False)\n",
    "        \n",
    "        for step in range(tot_step):\n",
    "        \n",
    "            state = x['frame'][step]        \n",
    "            action = F.one_hot(x['last_action'][step], self.num_actions)\n",
    "            reward = x['reward'][step]   \n",
    "            done = x['done'][step]\n",
    "            reset = torch.ones(bsz, device=device)\n",
    "            \n",
    "            u_list, im_logit_list, reset_logit_list = [], [], []                \n",
    "            for t in range(self.rec_t):                 \n",
    "                actor_input = self.use_model(state, reward, action, t, reset)                \n",
    "                reset_ex = reset.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "                self.encoded = reset_ex * self.encoded_reset + (1 - reset_ex) * self.encoded               \n",
    "                \n",
    "                if 'uniform' in x.keys():\n",
    "                    u = x['uniform'][step][:, t]\n",
    "                else:\n",
    "                    u = None                    \n",
    "                actor_output, core_state = self.actor(actor_input, \n",
    "                                                      done=done, \n",
    "                                                      core_state=core_state,\n",
    "                                                      u=u)                \n",
    "                if self.actor.gb_ste: u_list.append(actor_output['uniform'].unsqueeze(1))\n",
    "                im_logit_list.append(actor_output['im_policy_logits'].unsqueeze(1))\n",
    "                reset_logit_list.append(actor_output['reset_policy_logits'].unsqueeze(1))\n",
    "                \n",
    "                action = actor_output[\"im_action\"]\n",
    "                reset = actor_output[\"reset\"]\n",
    "        \n",
    "            if self.actor.gb_ste:\n",
    "                actor_output[\"uniform\"] = torch.concat(u_list, dim=1)\n",
    "                actor_output['im_policy_logits'] = torch.concat(im_logit_list, dim=1)\n",
    "                actor_output['reset_policy_logits'] = torch.concat(reset_logit_list, dim=1)\n",
    "            if step == 0:\n",
    "                all_actor_output = {k: [v.unsqueeze(0)] for k, v in actor_output.items()}\n",
    "            else:\n",
    "                for k, v in actor_output.items(): all_actor_output[k].append(v.unsqueeze(0))        \n",
    "        \n",
    "        all_actor_output = {k: torch.concat(v) for k, v in all_actor_output.items()}\n",
    "        return all_actor_output, core_state      \n",
    "    \n",
    "    def use_model_aug(self, x, r, a, cur_t, reset):\n",
    "        # input: \n",
    "        # r: reward - [B]; x: frame - [B, C, H, W]; a: action - [B, num_actions]\n",
    "        # cur_t: int; reset at cur_t == 0  \n",
    "        device = a.device\n",
    "        bsz = a.shape[0]\n",
    "        if cur_t == 0:\n",
    "            self.rollout_depth = torch.zeros(bsz, dtype=torch.float32, device=device)\n",
    "            self.re_action = a\n",
    "            _, vs, logits, encodeds = self.model(x, self.re_action.unsqueeze(0), one_hot=True)                \n",
    "            self.encoded = encodeds[-1]    \n",
    "            self.encoded_reset = self.encoded.clone()\n",
    "\n",
    "            self.re_reward = r.unsqueeze(-1)              \n",
    "            self.v0 = vs[-1].unsqueeze(-1).clone()\n",
    "            self.logit0 = logits[-1].clone()\n",
    "\n",
    "            self.im_action = torch.zeros(bsz, self.num_actions, dtype=torch.float32, device=device)\n",
    "            self.im_reset = torch.ones(bsz, 1, dtype=torch.float32, device=device)\n",
    "            self.im_reward = torch.zeros(bsz, 1, dtype=torch.float32, device=device)                              \n",
    "            self.v = vs[-1].unsqueeze(-1)\n",
    "            self.logit = logits[-1]\n",
    "            self.rollout_first_action = torch.zeros(bsz, self.num_actions, dtype=torch.float32, device=device)  \n",
    "            self.rollout_return_wo_v = torch.zeros(bsz, 1, dtype=torch.float32, device=device)     \n",
    "            self.rollout_return = torch.zeros(bsz, 1, dtype=torch.float32, device=device)     \n",
    "            self.q_s_a = torch.zeros(bsz, self.num_actions, dtype=torch.float32, device=device)      \n",
    "            self.n_s_a = torch.zeros(bsz, self.num_actions, dtype=torch.float32, device=device)                   \n",
    "        else:\n",
    "            self.rollout_depth = self.rollout_depth + 1                \n",
    "\n",
    "            self.im_action = a\n",
    "            rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "               self.im_action.unsqueeze(0), one_hot=True)\n",
    "            self.encoded = encodeds[-1]        \n",
    "\n",
    "            self.im_reward = rs[-1].unsqueeze(-1)\n",
    "            self.v = vs[-1].unsqueeze(-1)    \n",
    "            self.logit = logits[-1]     \n",
    "\n",
    "            self.rollout_first_action = (self.im_reset * self.im_action + (1 - self.im_reset) * \n",
    "                self.rollout_first_action)\n",
    "            self.rollout_return_wo_v = (self.im_reset * torch.zeros_like(self.rollout_return_wo_v) + \n",
    "                                        (1 - self.im_reset) * self.rollout_return_wo_v)\n",
    "            self.rollout_depth = (self.im_reset[:,0] * torch.ones_like(self.rollout_depth) + \n",
    "                                        (1 - self.im_reset)[:,0] * self.rollout_depth)\n",
    "\n",
    "            self.rollout_return_wo_v = (self.rollout_return_wo_v + (self.discounting ** (self.rollout_depth-1)\n",
    "                                                                   ).unsqueeze(-1) * self.im_reward)\n",
    "            self.rollout_return = self.rollout_return_wo_v + (\n",
    "                self.discounting ** (self.rollout_depth).unsqueeze(-1)) * self.v                    \n",
    "            \n",
    "            self.im_reset = reset.unsqueeze(-1)\n",
    "            \n",
    "            new_q_s_a = self.q_s_a * self.n_s_a / (self.n_s_a + 1) + self.rollout_return / (self.n_s_a + 1)\n",
    "            new_q_s_a = new_q_s_a * self.rollout_first_action + self.q_s_a * (1 - self.rollout_first_action)\n",
    "            self.q_s_a = (self.im_reset * new_q_s_a + (1 - self.im_reset) * self.q_s_a)\n",
    "            self.n_s_a = (self.im_reset * (self.n_s_a + self.rollout_first_action) + \n",
    "                (1 - self.im_reset) * self.n_s_a)\n",
    "            \n",
    "            \n",
    "        time = F.one_hot(torch.tensor([cur_t], device=device).long(), self.rec_t).tile([bsz, 1])\n",
    "        depc = self.discounting ** (self.rollout_depth-1).unsqueeze(-1)\n",
    "        ret_dict = {\"re_action\": self.re_action,\n",
    "                    \"re_reward\": self.re_reward,\n",
    "                    \"v0\": self.v0,\n",
    "                    \"logit0\": self.logit0,\n",
    "                    \"im_action\": self.im_action,\n",
    "                    \"im_reset\": self.im_reset,\n",
    "                    \"im_reward\": self.im_reward,\n",
    "                    \"v\": self.v,\n",
    "                    \"logit\": self.logit,\n",
    "                    \"rollout_first_action\": self.rollout_first_action,\n",
    "                    \"rollout_return\": self.rollout_return,\n",
    "                    \"n_s_a\": self.n_s_a,\n",
    "                    \"q_s_a\": self.q_s_a,\n",
    "                    \"time\": time,\n",
    "                    \"depc\": depc}\n",
    "        self.ret_dict = ret_dict\n",
    "        out = torch.concat(list(ret_dict.values()), dim=-1)   \n",
    "        out = out.unsqueeze(-1).unsqueeze(-1)  \n",
    "        return out          \n",
    "    \n",
    "    def use_model_base(self, x, r, a, cur_t, reset):\n",
    "        bsz = a.shape[0]\n",
    "        if cur_t == 0:\n",
    "            rs, vs, logits, encodeds = self.model(x, a.unsqueeze(0), one_hot=True)\n",
    "            self.encoded = encodeds[0] \n",
    "            self.encoded_reset = encodeds[0].clone()\n",
    "            self.r0 = r.unsqueeze(-1).clone()\n",
    "            self.v0 = vs[-1].unsqueeze(-1).clone()\n",
    "            self.logit0 = logits[-1].clone() \n",
    "            r = r.unsqueeze(-1)\n",
    "            v = vs[-1].unsqueeze(-1)\n",
    "            logit = logits[-1]     \n",
    "        else:\n",
    "            rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "                a.unsqueeze(0), one_hot=True)\n",
    "            self.encoded = encodeds[-1] \n",
    "            r = rs[-1].unsqueeze(-1)\n",
    "            v = vs[-1].unsqueeze(-1)\n",
    "            logit = logits[-1]  \n",
    "            \n",
    "        re = reset.unsqueeze(-1)\n",
    "        time = F.one_hot(torch.tensor([cur_t], device=a.device).long(), self.rec_t).tile([bsz, 1])                        \n",
    "\n",
    "        actor_input = torch.concat([re, a, r, v, logit, self.r0, self.v0, self.logit0, time], dim=-1)     \n",
    "        actor_input = actor_input.unsqueeze(-1).unsqueeze(-1)    \n",
    "        return actor_input\n",
    "\n",
    "class Actor_net(nn.Module):    \n",
    "    def __init__(self, obs_shape, num_actions, flags):\n",
    "\n",
    "        super(Actor_net, self).__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions  \n",
    "        \n",
    "        self.tran_t = flags.tran_t                   # number of recurrence of RNN\n",
    "        self.tran_mem_n = flags.tran_mem_n           # size of memory for the attn modules\n",
    "        self.tran_layer_n = flags.tran_layer_n       # number of layers\n",
    "        self.tran_lstm = flags.tran_lstm             # to use lstm or not\n",
    "        self.tran_lstm_no_attn = flags.tran_lstm_no_attn  # to use attention in lstm or not\n",
    "        self.tran_norm_first = flags.tran_norm_first # to use norm first in transformer (not on LSTM)\n",
    "        self.tran_ff_n = flags.tran_ff_n             # number of dim of ff in transformer (not on LSTM)        \n",
    "        self.tran_skip = flags.tran_skip             # whether to add skip connection\n",
    "        self.conv_out = flags.tran_dim               # size of transformer / LSTM embedding dim\n",
    "        self.ste = flags.ste\n",
    "        self.gb_ste = flags.gb_ste\n",
    "        \n",
    "        self.conv_out_hw = 1   \n",
    "        self.d_model = self.conv_out\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=self.obs_shape[0], out_channels=self.conv_out//2, kernel_size=1, stride=1)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv_out//2, out_channels=self.conv_out, kernel_size=1, stride=1)        \n",
    "        self.frame_conv = torch.nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU())\n",
    "        self.env_input_size = self.conv_out\n",
    "        d_in = self.env_input_size + self.d_model \n",
    "        \n",
    "        if self.tran_lstm:\n",
    "            self.core = ConvAttnLSTM(h=self.conv_out_hw, w=self.conv_out_hw,\n",
    "                                 input_dim=d_in-self.d_model, hidden_dim=self.d_model,\n",
    "                                 kernel_size=1, num_layers=self.tran_layer_n,\n",
    "                                 num_heads=8, mem_n=self.tran_mem_n, attn=not self.tran_lstm_no_attn)\n",
    "        else:            \n",
    "            self.core = ConvTransformerRNN(d_in=d_in,\n",
    "                                       h=self.conv_out_hw, w=self.conv_out_hw, d_model=self.d_model, \n",
    "                                       num_heads=8, dim_feedforward=self.tran_ff_n, \n",
    "                                       mem_n=self.tran_mem_n, norm_first=self.tran_norm_first,\n",
    "                                       num_layers=self.tran_layer_n, rpos=self.rpos, conv=False)   \n",
    "                         \n",
    "        \n",
    "        if self.tran_skip:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * (self.d_model + self.env_input_size)\n",
    "        else:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * self.d_model\n",
    "                \n",
    "        self.fc = nn.Linear(rnn_out_size, 256)        \n",
    "        \n",
    "        self.im_policy = nn.Linear(256, self.num_actions)        \n",
    "        self.policy = nn.Linear(256, self.num_actions)        \n",
    "        self.baseline = nn.Linear(256, 1)        \n",
    "        self.reset = nn.Linear(256, 1)        \n",
    "        \n",
    "        if self.gb_ste:\n",
    "            self.register_buffer('temp', torch.tensor(flags.gb_ste_temp_max, dtype=torch.float32))\n",
    "        \n",
    "        print(\"actor size: \", sum(p.numel() for p in self.parameters()))\n",
    "        #for k, v in self.named_parameters(): print(k, v.numel())   \n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        state = self.core.init_state(batch_size) + (torch.zeros(1, batch_size, \n",
    "               self.env_input_size, self.conv_out_hw, self.conv_out_hw),)\n",
    "        return state\n",
    "\n",
    "    def forward(self, x, done, core_state=(), u=None, debug=False):\n",
    "        # one-step forward for the actor\n",
    "        # input / done shape x: T x B x C x 1 x 1 / B x C x 1 x 1\n",
    "        # only supports T = 1 at the moment; all output does not have T dim.\n",
    "        \n",
    "        if len(x.shape) == 4: x = x.unsqueeze(0)\n",
    "        if len(done.shape) == 1: done = done.unsqueeze(0)  \n",
    "            \n",
    "        T, B, *_ = x.shape\n",
    "        x = torch.flatten(x, 0, 1)  # Merge time and batch.        \n",
    "        env_input = self.frame_conv(x)                \n",
    "        core_input = env_input.view(T, B, -1, self.conv_out_hw, self.conv_out_hw)\n",
    "        core_output_list = []\n",
    "        notdone = ~(done.bool())\n",
    "        \n",
    "        for n, (input, nd) in enumerate(zip(core_input.unbind(), notdone.unbind())):                \n",
    "            # Input shape: B, self.conv_out + self.num_actions + 1, H, W\n",
    "            for t in range(self.tran_t):                \n",
    "                if t > 0: nd = torch.ones(B).to(x.device).bool()                    \n",
    "                nd = nd.view(-1)      \n",
    "                output, core_state = self.core(input, core_state, nd, nd) # output shape: 1, B, core_output_size \n",
    "                \n",
    "            last_input = input   \n",
    "            core_output_list.append(output)\n",
    "                                   \n",
    "        core_output = torch.cat(core_output_list)  \n",
    "        if self.tran_skip: core_output = torch.concat([core_output, core_input], dim=-3)\n",
    "        core_output = torch.flatten(core_output, 0, 1)        \n",
    "        core_output = F.relu(self.fc(torch.flatten(core_output, start_dim=1)))   \n",
    "        \n",
    "        policy_logits = self.policy(core_output)\n",
    "        im_policy_logits = self.im_policy(core_output)        \n",
    "        reset_policy_logits_p = self.reset(core_output)\n",
    "        reset_policy_logits = torch.cat([reset_policy_logits_p, torch.zeros_like(reset_policy_logits_p)], dim=-1)   \n",
    "        baseline = self.baseline(core_output)\n",
    "        \n",
    "        if self.gb_ste:\n",
    "            if u is not None: \n",
    "                u_action = u[:, :-2]\n",
    "                u_reset = u[:, -2:]\n",
    "            else:\n",
    "                u_action = None\n",
    "                u_reset = None\n",
    "            im_action, u_action = gumbel_softmax(im_policy_logits, self.temp, u=u_action, hard=True)             \n",
    "            reset, u_reset = gumbel_softmax(reset_policy_logits, self.temp, u=u_reset, hard=True)\n",
    "            reset = reset[:, 0]\n",
    "            u = torch.cat([u_action, u_reset], dim=-1)\n",
    "        elif self.ste:\n",
    "            im_action_p = F.softmax(im_policy_logits, dim=-1)\n",
    "            im_action_h = torch.multinomial(im_action_p, num_samples=1)\n",
    "            im_action = (im_action_h - im_action_p).detach() + im_action_p\n",
    "          \n",
    "            reset_p = torch.sigmoid(reset_policy_logits_p)\n",
    "            reset_h = torch.bernoulli(reset_p)\n",
    "            reset = (reset_h - reset_p).detach() + reset_p\n",
    "        else:\n",
    "            im_action = F.softmax(im_policy_logits, dim=1)\n",
    "            reset = torch.sigmoid(reset_policy_logits_p)\n",
    "            \n",
    "        \n",
    "        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        \n",
    "        reg_loss = (1e-3 * torch.sum(policy_logits**2, dim=-1) / 2 + \n",
    "                    1e-5 * torch.sum(core_output**2, dim=-1) / 2)\n",
    "        reg_loss = reg_loss.view(T, B)\n",
    "        \n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        action = action.view(T, B)                \n",
    "        im_policy_logits = im_policy_logits.view(T, B, self.num_actions)        \n",
    "        im_action = im_action.view(T, B, self.num_actions)                \n",
    "        reset_policy_logits = reset_policy_logits.view(T, B, 2)\n",
    "        reset = reset.view(T, B)     \n",
    "        baseline = baseline.view(T, B)\n",
    "        \n",
    "        ret_dict = dict(policy_logits=policy_logits[0],                         \n",
    "                        action=action[0], \n",
    "                        im_policy_logits=im_policy_logits[0],                         \n",
    "                        im_action=im_action[0],                                                \n",
    "                        reset_policy_logits=reset_policy_logits[0], \n",
    "                        reset=reset[0],\n",
    "                        baseline=baseline[0], \n",
    "                        reg_loss=reg_loss[0], )\n",
    "        if self.gb_ste:\n",
    "            ret_dict['uniform'] = u\n",
    "        return (ret_dict, core_state)      \n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06dfbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_parser():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n",
    "\n",
    "    parser.add_argument(\"--env\", type=str, default=\"Sokoban-v0\",\n",
    "                        help=\"Gym environment.\")\n",
    "    parser.add_argument(\"--env_disable_noop\", action=\"store_true\",\n",
    "                        help=\"Disable noop in environment or not. (sokoban only)\")\n",
    "\n",
    "    parser.add_argument(\"--xpid\", default=None,\n",
    "                        help=\"Experiment id (default: None).\")\n",
    "\n",
    "    # Training settings.\n",
    "    parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n",
    "                        help=\"Disable saving checkpoint.\")\n",
    "    parser.add_argument(\"--savedir\", default=\"~/RS/thinker/logs/torchbeast\",\n",
    "                        help=\"Root dir where experiment data will be saved.\")\n",
    "    parser.add_argument(\"--num_actors\", default=48, type=int, metavar=\"N\",\n",
    "                        help=\"Number of actors (default: 48).\")\n",
    "    parser.add_argument(\"--total_steps\", default=100000000, type=int, metavar=\"T\",\n",
    "                        help=\"Total environment steps to train for.\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, metavar=\"B\",\n",
    "                        help=\"Learner batch size.\")\n",
    "    parser.add_argument(\"--unroll_length\", default=20, type=int, metavar=\"T\",\n",
    "                        help=\"The unroll length (time dimension).\")\n",
    "    parser.add_argument(\"--num_buffers\", default=None, type=int,\n",
    "                        metavar=\"N\", help=\"Number of shared-memory buffers.\")\n",
    "    parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=1, type=int,\n",
    "                        metavar=\"N\", help=\"Number learner threads.\")\n",
    "    parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n",
    "                        help=\"Disable CUDA.\")\n",
    "\n",
    "    # Architecture settings\n",
    "    parser.add_argument(\"--tran_dim\", default=64, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer hidden dim.\")\n",
    "    parser.add_argument(\"--tran_mem_n\", default=16, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer memory.\")\n",
    "    parser.add_argument(\"--tran_layer_n\", default=3, type=int, metavar=\"N\",\n",
    "                        help=\"Number of transformer layer.\")\n",
    "    parser.add_argument(\"--tran_t\", default=1, type=int, metavar=\"T\",\n",
    "                        help=\"Number of recurrent step for transformer.\")\n",
    "    parser.add_argument(\"--tran_ff_n\", default=256, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer ff .\")\n",
    "    parser.add_argument(\"--tran_skip\", action=\"store_true\",\n",
    "                        help=\"Whether to enable skip conn.\")\n",
    "    parser.add_argument(\"--tran_norm_first\", action=\"store_true\",\n",
    "                        help=\"Whether to use norm first in transformer.\")\n",
    "    parser.add_argument(\"--tran_rpos\", action=\"store_true\",\n",
    "                        help=\"Whether to use relative position in transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm\", action=\"store_true\",\n",
    "                        help=\"Whether to use LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm_no_attn\", action=\"store_true\",\n",
    "                        help=\"Whether to disable attention in LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_erasep\", action=\"store_true\",\n",
    "                        help=\"Whether to erase past memories if not planning.\")\n",
    "\n",
    "    parser.add_argument(\"--rec_t\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Number of planning steps.\")\n",
    "    parser.add_argument(\"--aug_stat\", action=\"store_true\",\n",
    "                        help=\"Whether to use augmented stat.\")      \n",
    "    \n",
    "    parser.add_argument(\"--ste\", action=\"store_true\",\n",
    "                        help=\"Whether to use ste backprop.\")\n",
    "    parser.add_argument(\"--gb_ste\", action=\"store_true\",\n",
    "                        help=\"Whether to use gb-ste backprop.\")\n",
    "    parser.add_argument(\"--gb_ste_temp_max\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"Beginning temp. for gb-ste.\")\n",
    "    parser.add_argument(\"--gb_ste_temp_min\", default=0.5, type=int, metavar=\"N\",\n",
    "                        help=\"Ending temp. for gb-ste.\")    \n",
    "\n",
    "    # Loss settings.\n",
    "    parser.add_argument(\"--entropy_cost\", default=0.01,\n",
    "                        type=float, help=\"Entropy cost/multiplier.\")\n",
    "    parser.add_argument(\"--im_entropy_cost\", default=0.01,\n",
    "                        type=float, help=\"Entropy cost/multiplier.\")    \n",
    "    parser.add_argument(\"--baseline_cost\", default=0.5,\n",
    "                        type=float, help=\"Baseline cost/multiplier.\")\n",
    "    parser.add_argument(\"--reg_cost\", default=1,\n",
    "                        type=float, help=\"Reg cost/multiplier.\")\n",
    "    parser.add_argument(\"--discounting\", default=0.97,\n",
    "                        type=float, help=\"Discounting factor.\")\n",
    "    parser.add_argument(\"--lamb\", default=0.97,\n",
    "                        type=float, help=\"Lambda when computing trace.\")\n",
    "    parser.add_argument(\"--reward_clipping\", default=10, type=int, \n",
    "                        metavar=\"N\", help=\"Reward clipping.\")\n",
    "    parser.add_argument(\"--trun_bs\", action=\"store_true\",\n",
    "                        help=\"Whether to add baseline as reward when truncated.\")\n",
    "\n",
    "    # Optimizer settings.\n",
    "    parser.add_argument(\"--learning_rate\", default=0.0004,\n",
    "                        type=float, metavar=\"LR\", help=\"Learning rate.\")\n",
    "    parser.add_argument(\"--disable_adam\", action=\"store_true\",\n",
    "                        help=\"Use Aadm optimizer or not.\")\n",
    "    parser.add_argument(\"--alpha\", default=0.99, type=float,\n",
    "                        help=\"RMSProp smoothing constant.\")\n",
    "    parser.add_argument(\"--momentum\", default=0, type=float,\n",
    "                        help=\"RMSProp momentum.\")\n",
    "    parser.add_argument(\"--epsilon\", default=0.01, type=float,\n",
    "                        help=\"RMSProp epsilon.\")\n",
    "    parser.add_argument(\"--grad_norm_clipping\", default=0.0, type=float,\n",
    "                        help=\"Global gradient norm clip.\")\n",
    "    # yapf: enable\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.num_actors = 1\n",
    "flags.batch_size = 32\n",
    "flags.unroll_length = 20\n",
    "flags.learning_rate = 0.0004\n",
    "flags.grad_norm_clipping = 40\n",
    "\n",
    "flags.entropy_cost = 0.001\n",
    "flags.im_entropy_cost = 0.\n",
    "flags.discounting = 0.97\n",
    "flags.lamb = 1.\n",
    "\n",
    "flags.trun_bs = False\n",
    "flags.total_steps = 100000000\n",
    "flags.disable_adam = False\n",
    "\n",
    "flags.tran_t = 1\n",
    "flags.tran_mem_n = 1\n",
    "flags.tran_layer_n = 3\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = False\n",
    "flags.tran_norm_first = False\n",
    "flags.tran_ff_n = 256\n",
    "flags.tran_skip = False\n",
    "flags.tran_erasep = False\n",
    "flags.tran_dim = 64\n",
    "flags.tran_rpos = True\n",
    "\n",
    "flags.rec_t = 1\n",
    "flags.aug_stat = True\n",
    "flags.ste = False\n",
    "flags.gb_ste = True\n",
    "flags.gb_ste_temp_max = 1\n",
    "flags.gb_ste_temp_min = 0.5\n",
    "\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "model_learner = Model(flags, obs_shape, num_actions=num_actions)\n",
    "model_actor = Model(flags, obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model_learner.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "model_actor.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f342a1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mp.set_sharing_strategy('file_system')\n",
    "\n",
    "if flags.xpid is None:\n",
    "    flags.xpid = \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "plogger = file_writer.FileWriter(\n",
    "    xpid=flags.xpid, xp_args=flags.__dict__, rootdir=flags.savedir\n",
    ")\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available(): \n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser(\"%s/%s/%s\" % (flags.savedir, flags.xpid, \"model.tar\"))\n",
    ")\n",
    "\n",
    "if flags.num_buffers is None:  # Set sensible default for num_buffers.\n",
    "    flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n",
    "if flags.num_actors >= flags.num_buffers:\n",
    "    raise ValueError(\"num_buffers should be larger than num_actors\")\n",
    "if flags.num_buffers < flags.batch_size:\n",
    "    raise ValueError(\"num_buffers should be larger than batch_size\")\n",
    "\n",
    "T = flags.unroll_length\n",
    "B = flags.batch_size\n",
    "\n",
    "env = create_env(flags)\n",
    "\n",
    "actor_net = Actor_Wrapper(flags, model_actor, actor=None)\n",
    "buffers = create_buffers(flags, env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "actor_net.share_memory()\n",
    "\n",
    "# Add initial RNN state.\n",
    "initial_agent_state_buffers = []\n",
    "for _ in range(flags.num_buffers):\n",
    "    state = actor_net.initial_state(batch_size=1)\n",
    "    for t in state:\n",
    "        t.share_memory_()\n",
    "    initial_agent_state_buffers.append(state)\n",
    "\n",
    "actor_processes = []\n",
    "ctx = mp.get_context()\n",
    "free_queue = ctx.SimpleQueue()\n",
    "full_queue = ctx.SimpleQueue()\n",
    "\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=act, args=(flags, i, free_queue, full_queue,\n",
    "            actor_net, buffers, initial_agent_state_buffers,),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)\n",
    "\n",
    "learner_net = Actor_Wrapper(flags, model_learner, actor=None, \n",
    "                            ).to(device=flags.device)\n",
    "\n",
    "if not flags.disable_adam:\n",
    "    print(\"Using Adam...\")        \n",
    "    optimizer = torch.optim.Adam(learner_net.actor.parameters(),lr=flags.learning_rate)\n",
    "else:\n",
    "    print(\"Using RMS Prop...\")\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        learner_net.actor.parameters(),\n",
    "        lr=flags.learning_rate,\n",
    "        momentum=flags.momentum,\n",
    "        eps=flags.epsilon,\n",
    "        alpha=flags.alpha,)\n",
    "print(\"All parameters: \")\n",
    "for k, v in learner_net.named_parameters(): print(k, v.numel())    \n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "logger = logging.getLogger(\"logfile\")\n",
    "stat_keys = [\"mean_episode_return\", \"episode_returns\", \"total_loss\",\n",
    "    \"pg_loss\", \"baseline_loss\", \"entropy_loss\", \"im_entropy_loss\", \"total_norm\"]\n",
    "logger.info(\"# Step\\t%s\", \"\\t\".join(stat_keys))\n",
    "\n",
    "step, stats, last_returns, tot_eps = 0, {}, deque(maxlen=400), 0\n",
    "\n",
    "def batch_and_learn(i, lock=threading.Lock()):\n",
    "    \"\"\"Thread target for the learning process.\"\"\"\n",
    "    #nonlocal step, stats, last_returns, tot_eps\n",
    "    global step, stats, last_returns, tot_eps\n",
    "    timings = prof.Timings()\n",
    "    while step < flags.total_steps:\n",
    "        timings.reset()\n",
    "        batch, agent_state = get_batch(flags, free_queue, full_queue, buffers,\n",
    "            initial_agent_state_buffers, timings,)\n",
    "        stats = learn(flags, actor_net, learner_net, batch, agent_state, optimizer, \n",
    "            scheduler)\n",
    "        if flags.gb_ste:\n",
    "            learner_net.actor.temp = torch.tensor(np.maximum(flags.gb_ste_temp_max * \n",
    "                    np.exp(-step/flags.total_steps), flags.gb_ste_temp_min).item(), \n",
    "                    dtype=torch.float32, device=learner_net.actor.temp.device)\n",
    "        last_returns.extend(stats[\"episode_returns\"])\n",
    "        tot_eps = tot_eps + len(stats[\"episode_returns\"])\n",
    "        timings.time(\"learn\")\n",
    "        with lock:\n",
    "            to_log = dict(step=step)\n",
    "            to_log.update({k: stats[k] for k in stat_keys})\n",
    "            to_log.update({\"trail_mean_episode_return\": np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "                           \"episode\": tot_eps})\n",
    "            plogger.log(to_log)\n",
    "            step += T * B\n",
    "\n",
    "    if i == 0:\n",
    "        logging.info(\"Batch and learn: %s\", timings.summary())\n",
    "\n",
    "for m in range(flags.num_buffers):\n",
    "    free_queue.put(m)\n",
    "\n",
    "threads = []\n",
    "for i in range(flags.num_learner_threads):\n",
    "    thread = threading.Thread(\n",
    "        target=batch_and_learn, name=\"batch-and-learn-%d\" % i, args=(i,)\n",
    "    )\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "def checkpoint():\n",
    "    if flags.disable_checkpoint:\n",
    "        return\n",
    "    logging.info(\"Saving checkpoint to %s\", checkpointpath)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": actor_net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"flags\": vars(flags),\n",
    "        },\n",
    "        checkpointpath,\n",
    "    )\n",
    "\n",
    "timer = timeit.default_timer\n",
    "try:\n",
    "    last_checkpoint_time = timer()\n",
    "    while step < flags.total_steps:\n",
    "        start_step = step\n",
    "        start_time = timer()\n",
    "        time.sleep(5)\n",
    "\n",
    "        if timer() - last_checkpoint_time > 10 * 60:  # Save every 10 min.\n",
    "            checkpoint()\n",
    "            last_checkpoint_time = timer()\n",
    "\n",
    "        sps = (step - start_step) / (timer() - start_time)\n",
    "        if stats.get(\"episode_returns\", None):\n",
    "            mean_return = (\n",
    "                \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n",
    "            )\n",
    "        else:\n",
    "            mean_return = \"\"\n",
    "        total_loss = stats.get(\"total_loss\", float(\"inf\"))\n",
    "\n",
    "        print_str =  \"Steps %i @ %.1f SPS. Eps %i. L400 Return %f. Loss %f\" % (step, sps, tot_eps, \n",
    "            np.average(last_returns) if len(last_returns) > 0 else 0., total_loss)\n",
    "\n",
    "        for s in [\"pg_loss\", \"baseline_loss\", \"entropy_loss\", \n",
    "                  \"im_entropy_loss\", \"reg_loss\", \"total_norm\"]:\n",
    "            if s in stats:\n",
    "                print_str += \" %s %f\" % (s, stats[s])\n",
    "\n",
    "        logging.info(print_str)\n",
    "except KeyboardInterrupt:\n",
    "    for thread in threads:\n",
    "        thread.join()        \n",
    "    # Try joining actors then quit.\n",
    "else:\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    logging.info(\"Learning finished after %d steps.\", step)\n",
    "finally:\n",
    "    for _ in range(flags.num_actors):\n",
    "        free_queue.put(None)\n",
    "    for actor in actor_processes:\n",
    "        actor.join(timeout=1)\n",
    "\n",
    "checkpoint()\n",
    "plogger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3809daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(0.).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = flags.total_steps + 1\n",
    "for thread in threads:\n",
    "     thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(flags.num_actors):\n",
    "    free_queue.put(None)\n",
    "for actor in actor_processes:\n",
    "    actor.join(timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3413ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": actor_net.state_dict(),},\"base/actor_net_tmp_2.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbc04556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  394788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Actor_Wrapper(\n",
       "  (model): Model(\n",
       "    (frameEncoder): FrameEncoder(\n",
       "      (conv1): Conv2d(8, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (res1): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (res2): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avg1): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      (res3): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (avg2): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "    )\n",
       "    (dynamicModel): DynamicModel(\n",
       "      (res): Sequential(\n",
       "        (0): ResBlock(\n",
       "          (conv1): Conv2d(133, 133, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(133, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(133, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(133, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (2): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (3): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (4): ResBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_rvpi): Output_rvpi(\n",
       "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (fc_r): Linear(in_features=800, out_features=1, bias=True)\n",
       "      (fc_v): Linear(in_features=800, out_features=1, bias=True)\n",
       "      (fc_logits): Linear(in_features=800, out_features=5, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (actor): Actor_net(\n",
       "    (conv1): Conv2d(43, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (frame_conv): Sequential(\n",
       "      (0): Conv2d(43, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (core): ConvAttnLSTM(\n",
       "      (layers): ModuleList(\n",
       "        (0): ConvAttnLSTMCell(\n",
       "          (conv): Conv2d(256, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (norm): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ConvAttnLSTMCell(\n",
       "          (conv): Conv2d(256, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (norm): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ConvAttnLSTMCell(\n",
       "          (conv): Conv2d(256, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (out): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "          (norm): LayerNorm((64, 1, 1), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (proj_list): ModuleList(\n",
       "        (0): Conv2d(64, 64, kernel_size=(2, 1), stride=(1, 1), groups=64)\n",
       "        (1): Conv2d(64, 64, kernel_size=(2, 1), stride=(1, 1), groups=64)\n",
       "        (2): Conv2d(64, 64, kernel_size=(2, 1), stride=(1, 1), groups=64)\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (im_policy): Linear(in_features=256, out_features=5, bias=True)\n",
       "    (policy): Linear(in_features=256, out_features=5, bias=True)\n",
       "    (baseline): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (reset): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debug trainned actor\n",
    "\n",
    "class Actor_Wrapper(nn.Module):    \n",
    "    def __init__(self, flags, model, actor=None):\n",
    "        \n",
    "        super(Actor_Wrapper, self).__init__()   \n",
    "        self.model = model\n",
    "        self.num_actions = model.num_actions        \n",
    "        self.rec_t = flags.rec_t\n",
    "        self.discounting = flags.discounting\n",
    "        self.aug_stat = flags.aug_stat\n",
    "        \n",
    "        obs_n = (7 + num_actions * 7 + self.rec_t if self.aug_stat else \n",
    "            5 + num_actions * 3 + self.rec_t)       \n",
    "        if actor is None:\n",
    "            self.actor = Actor_net(obs_shape=(obs_n, 1, 1), num_actions=self.num_actions, flags=flags)\n",
    "        else:\n",
    "            self.actor = actor   \n",
    "        self.use_model = self.use_model_aug if self.aug_stat else self.use_model_base        \n",
    "            \n",
    "    def initial_state(self, batch_size):\n",
    "        return self.actor.initial_state(batch_size)\n",
    "            \n",
    "    def forward(self, x, core_state=None, debug=False):                      \n",
    "        # x is env_output object with:\n",
    "        # frame: T x B x C x H x W\n",
    "        # last_action: T x B\n",
    "        # reward: T x B\n",
    "        \n",
    "        tot_step, bsz, _, _, _ = x['frame'].shape\n",
    "        device = x['frame'].device        \n",
    "        self.model.train(False)\n",
    "        \n",
    "        for step in range(tot_step):\n",
    "        \n",
    "            state = x['frame'][step]        \n",
    "            action = F.one_hot(x['last_action'][step], self.num_actions)\n",
    "            reward = x['reward'][step]   \n",
    "            done = x['done'][step]\n",
    "            reset = torch.ones(bsz, device=device)\n",
    "            \n",
    "            u_list, im_logit_list, reset_logit_list = [], [], []                \n",
    "            for t in range(self.rec_t):                 \n",
    "                actor_input = self.use_model(state, reward, action, t, reset)                \n",
    "                reset_ex = reset.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "                self.encoded = reset_ex * self.encoded_reset + (1 - reset_ex) * self.encoded               \n",
    "                \n",
    "                if 'uniform' in x.keys():\n",
    "                    u = x['uniform'][step][:, t]\n",
    "                else:\n",
    "                    u = None                    \n",
    "                actor_output, core_state = self.actor(actor_input, \n",
    "                                                      done=done, \n",
    "                                                      core_state=core_state,\n",
    "                                                      u=u)                \n",
    "                if self.actor.gb_ste: u_list.append(actor_output['uniform'].unsqueeze(1))\n",
    "                im_logit_list.append(actor_output['im_policy_logits'].unsqueeze(1))\n",
    "                reset_logit_list.append(actor_output['reset_policy_logits'].unsqueeze(1))\n",
    "                \n",
    "                action = actor_output[\"im_action\"]\n",
    "                reset = actor_output[\"reset\"]\n",
    "              \n",
    "                if debug:\n",
    "                  np.set_printoptions(suppress=True)   \n",
    "                  print(\"===========STEP:%d============\" % t)\n",
    "                  print(\"im_action: \\n\", actor_output[\"im_action\"].cpu().detach().numpy())\n",
    "                  print(\"im_prob: \\n\", F.softmax(actor_output[\"im_policy_logits\"], dim=-1).cpu().detach().numpy())\n",
    "                  print(\"reset_prob: \\n\", F.softmax(actor_output[\"reset_policy_logits\"], dim=-1).cpu().detach().numpy())\n",
    "                  print(\"reset: \", reset.cpu().detach().numpy())                \n",
    "            \n",
    "            if self.actor.gb_ste:\n",
    "                actor_output[\"uniform\"] = torch.concat(u_list, dim=1)\n",
    "                actor_output['im_policy_logits'] = torch.concat(im_logit_list, dim=1)\n",
    "                actor_output['reset_policy_logits'] = torch.concat(reset_logit_list, dim=1)\n",
    "            if step == 0:\n",
    "                all_actor_output = {k: [v.unsqueeze(0)] for k, v in actor_output.items()}\n",
    "            else:\n",
    "                for k, v in actor_output.items(): all_actor_output[k].append(v.unsqueeze(0))        \n",
    "            \n",
    "            if debug:\n",
    "              print(\"=========F=STEP:%d============\")\n",
    "              print(\"model prob: \\n\", F.softmax(self.logit0, dim=-1).cpu().detach().numpy())\n",
    "              print(\"prob: \\n\", F.softmax(all_actor_output[\"policy_logits\"][-1], dim=-1).cpu().detach().numpy())\n",
    "              print(\"action: \\n\", all_actor_output[\"action\"][-1].cpu().detach().numpy())        \n",
    "        \n",
    "        all_actor_output = {k: torch.concat(v) for k, v in all_actor_output.items()}\n",
    "        return all_actor_output, core_state      \n",
    "    \n",
    "    def use_model_aug(self, x, r, a, cur_t, reset):\n",
    "        # input: \n",
    "        # r: reward - [B]; x: frame - [B, C, H, W]; a: action - [B, num_actions]\n",
    "        # cur_t: int; reset at cur_t == 0  \n",
    "        device = a.device\n",
    "        bsz = a.shape[0]\n",
    "        if cur_t == 0:\n",
    "            self.rollout_depth = torch.zeros(bsz, dtype=torch.float32, device=device)\n",
    "            self.re_action = a\n",
    "            _, vs, logits, encodeds = self.model(x, self.re_action.unsqueeze(0), one_hot=True)                \n",
    "            self.encoded = encodeds[-1]    \n",
    "            self.encoded_reset = self.encoded.clone()\n",
    "\n",
    "            self.re_reward = r.unsqueeze(-1)              \n",
    "            self.v0 = vs[-1].unsqueeze(-1).clone()\n",
    "            self.logit0 = logits[-1].clone()\n",
    "\n",
    "            self.im_action = torch.zeros(bsz, self.num_actions, dtype=torch.float32, device=device)\n",
    "            self.im_reset = torch.ones(bsz, 1, dtype=torch.float32, device=device)\n",
    "            self.im_reward = torch.zeros(bsz, 1, dtype=torch.float32, device=device)                              \n",
    "            self.v = vs[-1].unsqueeze(-1)\n",
    "            self.logit = logits[-1]\n",
    "            self.rollout_first_action = torch.zeros(bsz, self.num_actions, dtype=torch.float32, device=device)  \n",
    "            self.rollout_return_wo_v = torch.zeros(bsz, 1, dtype=torch.float32, device=device)     \n",
    "            self.rollout_return = torch.zeros(bsz, 1, dtype=torch.float32, device=device)     \n",
    "            self.q_s_a = torch.zeros(bsz, self.num_actions, dtype=torch.float32, device=device)      \n",
    "            self.n_s_a = torch.zeros(bsz, self.num_actions, dtype=torch.float32, device=device)                   \n",
    "        else:\n",
    "            self.rollout_depth = self.rollout_depth + 1                \n",
    "\n",
    "            self.im_action = a\n",
    "            rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "               self.im_action.unsqueeze(0), one_hot=True)\n",
    "            self.encoded = encodeds[-1]        \n",
    "\n",
    "            self.im_reward = rs[-1].unsqueeze(-1)\n",
    "            self.v = vs[-1].unsqueeze(-1)    \n",
    "            self.logit = logits[-1]     \n",
    "\n",
    "            self.rollout_first_action = (self.im_reset * self.im_action + (1 - self.im_reset) * \n",
    "                self.rollout_first_action)\n",
    "            self.rollout_return_wo_v = (self.im_reset * torch.zeros_like(self.rollout_return_wo_v) + \n",
    "                                        (1 - self.im_reset) * self.rollout_return_wo_v)\n",
    "            self.rollout_depth = (self.im_reset[:,0] * torch.ones_like(self.rollout_depth) + \n",
    "                                        (1 - self.im_reset)[:,0] * self.rollout_depth)\n",
    "\n",
    "            self.rollout_return_wo_v = (self.rollout_return_wo_v + (self.discounting ** (self.rollout_depth-1)\n",
    "                                                                   ).unsqueeze(-1) * self.im_reward)\n",
    "            self.rollout_return = self.rollout_return_wo_v + (\n",
    "                self.discounting ** (self.rollout_depth).unsqueeze(-1)) * self.v                    \n",
    "            \n",
    "            self.im_reset = reset.unsqueeze(-1)\n",
    "            \n",
    "            new_q_s_a = self.q_s_a * self.n_s_a / (self.n_s_a + 1) + self.rollout_return / (self.n_s_a + 1)\n",
    "            new_q_s_a = new_q_s_a * self.rollout_first_action + self.q_s_a * (1 - self.rollout_first_action)\n",
    "            self.q_s_a = (self.im_reset * new_q_s_a + (1 - self.im_reset) * self.q_s_a)\n",
    "            self.n_s_a = (self.im_reset * (self.n_s_a + self.rollout_first_action) + \n",
    "                (1 - self.im_reset) * self.n_s_a)\n",
    "            \n",
    "        time = F.one_hot(torch.tensor([cur_t], device=device).long(), self.rec_t).tile([bsz, 1])\n",
    "        depc = self.discounting ** (self.rollout_depth-1).unsqueeze(-1)\n",
    "        ret_dict = {\"re_action\": self.re_action,\n",
    "                    \"re_reward\": self.re_reward,\n",
    "                    \"v0\": self.v0,\n",
    "                    \"logit0\": self.logit0,\n",
    "                    \"im_action\": self.im_action,\n",
    "                    \"im_reset\": self.im_reset,\n",
    "                    \"im_reward\": self.im_reward,\n",
    "                    \"v\": self.v,\n",
    "                    \"logit\": self.logit,\n",
    "                    \"rollout_first_action\": self.rollout_first_action,\n",
    "                    \"rollout_return\": self.rollout_return,\n",
    "                    \"n_s_a\": self.n_s_a,\n",
    "                    \"q_s_a\": self.q_s_a,\n",
    "                    \"time\": time,\n",
    "                    \"depc\": depc}\n",
    "        self.ret_dict = ret_dict\n",
    "        out = torch.concat(list(ret_dict.values()), dim=-1)   \n",
    "        out = out.unsqueeze(-1).unsqueeze(-1)  \n",
    "        return out          \n",
    "    \n",
    "    def use_model_base(self, x, r, a, cur_t, reset):\n",
    "        bsz = a.shape[0]\n",
    "        if cur_t == 0:\n",
    "            rs, vs, logits, encodeds = self.model(x, a.unsqueeze(0), one_hot=True)\n",
    "            self.encoded = encodeds[0] \n",
    "            self.encoded_reset = encodeds[0].clone()\n",
    "            self.r0 = r.unsqueeze(-1).clone()\n",
    "            self.v0 = vs[-1].unsqueeze(-1).clone()\n",
    "            self.logit0 = logits[-1].clone() \n",
    "            r = r.unsqueeze(-1)\n",
    "            v = vs[-1].unsqueeze(-1)\n",
    "            logit = logits[-1]     \n",
    "        else:\n",
    "            rs, vs, logits, encodeds = self.model.forward_encoded(self.encoded, \n",
    "                a.unsqueeze(0), one_hot=True)\n",
    "            self.encoded = encodeds[-1] \n",
    "            r = rs[-1].unsqueeze(-1)\n",
    "            v = vs[-1].unsqueeze(-1)\n",
    "            logit = logits[-1]  \n",
    "            \n",
    "        re = reset.unsqueeze(-1)\n",
    "        time = F.one_hot(torch.tensor([cur_t], device=a.device).long(), self.rec_t).tile([bsz, 1])                        \n",
    "\n",
    "        actor_input = torch.concat([re, a, r, v, logit, self.r0, self.v0, self.logit0, time], dim=-1)     \n",
    "        actor_input = actor_input.unsqueeze(-1).unsqueeze(-1)    \n",
    "        return actor_input\n",
    "      \n",
    "    \n",
    "bsz = 16\n",
    "eps_n = 500\n",
    "flags.device = torch.device(\"cuda\")\n",
    "    \n",
    "actor_net = Actor_Wrapper(flags, model_actor, actor=None)\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/test/alstm_3_1_rec_n_1_gbste_aug_clip.tar\")\n",
    "actor_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "actor_net.train(False)    \n",
    "actor_net.to(flags.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5f74fdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 500 episode: avg. return: 0.86 (+-0.09) \n"
     ]
    }
   ],
   "source": [
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "core_state = actor_net.initial_state(bsz)\n",
    "core_state = tuple(v.to(flags.device) for v in core_state)\n",
    "returns = []\n",
    "obs = env.initial()\n",
    "\n",
    "while(len(returns) <= eps_n):\n",
    "#for t in range(5):\n",
    "    #print(\"t: %d\" %t)    \n",
    "    with torch.no_grad():\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}    \n",
    "        actor_out, core_state = actor_net(obs, core_state, debug=False)\n",
    "        obs = env.step(actor_out[\"action\"][0])\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "\n",
    "returns = returns[:eps_n]            \n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "      np.average(returns), np.std(returns) / np.sqrt(len(returns))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1e82488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model to another one\n",
    "\n",
    "parser = define_parser()        \n",
    "flags = parser.parse_args(\"--env Sokoban-v0\".split())        \n",
    "flags.device = torch.device(\"cuda\")\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/model_2.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "model.train(False)\n",
    "actor_net.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda48bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing actor wrapper with a one-step greedy actor\n",
    "\n",
    "def n_step_greedy_model(state, action, model, n, encoded=None, temp=20.): \n",
    "    \n",
    "    # Either input state, action (S_t, A_{t-1}) or the encoded Z_t\n",
    "    # state / encoded in the shape of (B, C, H, W)\n",
    "    # action in the shape of (B)\n",
    "    \n",
    "    bsz = state.shape[0] if encoded is None else encoded.shape[0]\n",
    "    device = state.device if encoded is None else encoded.device\n",
    "    num_actions = model.num_actions    \n",
    "    \n",
    "    q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "    \n",
    "    for act in range(num_actions):        \n",
    "        new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "        if encoded is None:            \n",
    "            old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "            rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "        else:\n",
    "            rs, vs, logits, encodeds = model.forward_encoded(encoded, new_action.unsqueeze(0))\n",
    "        \n",
    "        if n > 1:\n",
    "            action, prob, sub_q_ret = n_step_greedy_model(state=None, action=None, \n",
    "                       model=model, n=n-1, encoded=encodeds[1])\n",
    "            ret = rs[0] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] \n",
    "        else:\n",
    "            ret = rs[0] + flags.discounting * vs[1]\n",
    "            #print(rs[0], vs[1])\n",
    "        q_ret[:, act] = ret\n",
    "    \n",
    "    prob = F.softmax(temp*q_ret, dim=1)\n",
    "    action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret        \n",
    "\n",
    "class Actor_test(nn.Module):\n",
    "    # A one-step greedy actor (for testing only)\n",
    "    def __init__(self, num_actions, rec_t):\n",
    "        super(Actor_test, self).__init__()  \n",
    "        self.num_actions = num_actions\n",
    "        self.rs = torch.zeros(num_actions)\n",
    "        self.vs = []\n",
    "        self.rec_t = rec_t\n",
    "        self.gb_ste = False\n",
    "        self.time = 0\n",
    "    \n",
    "    def forward(self, x, done, core_state=None, u=None):\n",
    "        \n",
    "        x = x[:, :, 0, 0]        \n",
    "        #print(\"Input: \", x[0])\n",
    "        bsz = x.shape[0]\n",
    "        rec_t = self.rec_t\n",
    "        t = self.time     \n",
    "        \n",
    "        if t == 0:\n",
    "            self.rs = torch.zeros(rec_t, bsz, device=x.device)\n",
    "            self.vs = torch.zeros(rec_t, bsz, device=x.device)\n",
    "        self.rs[t] = x[:, 18] #x[:,1+self.num_actions]        \n",
    "        self.vs[t] = x[:, 19] #x[:,2+self.num_actions]\n",
    "        print(t, x)\n",
    "        \n",
    "        im_action = torch.tensor(t) if t < self.num_actions else torch.zeros(1, device=x.device).long()\n",
    "        im_action = F.one_hot(im_action.unsqueeze(0), self.num_actions).tile([bsz, 1])\n",
    "        reset = torch.ones(bsz, device=x.device)\n",
    "        \n",
    "        policy_logits = (self.rs[1:1+self.num_actions] + flags.discounting * self.vs[1:1+self.num_actions]).transpose(0, 1) \n",
    "        policy_probs = F.softmax(20 * policy_logits, dim=1)\n",
    "        action = torch.multinomial(policy_probs, num_samples=1)\n",
    "        \n",
    "        ret = {\"im_policy_logits\": im_action, \n",
    "               \"im_action\": im_action, \n",
    "               \"reset_policy_logits\": reset, \n",
    "               \"reset\": reset,                \n",
    "               \"policy_logits\": policy_logits, \n",
    "               \"policy_probs\": policy_probs, \n",
    "               \"action\": action,\n",
    "               \"baseline\": torch.zeros_like(reset),\n",
    "               \"reg_loss\": torch.zeros_like(reset)}\n",
    "        \n",
    "        self.time += 1\n",
    "        return (ret, None)\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])     \n",
    "model.train(False)    \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "flags.discounting = 0.97\n",
    "flags.aug_stat = True\n",
    "flags.rec_t = 6\n",
    "\n",
    "actor = Actor_test(num_actions=model.num_actions,rec_t=flags.rec_t)        \n",
    "actor_wrapper = Actor_Wrapper(flags, model, actor)\n",
    "        \n",
    "bsz = 2\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "obs = env.initial()\n",
    "state = obs['frame'][0].clone()\n",
    "action = torch.zeros(bsz).long()\n",
    "\n",
    "#obs = {k:v.to(flags.device).view((1, 10)+v.shape[2:]) for k, v in obs.items()}\n",
    "#obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "actor_output, _ = actor_wrapper(obs)\n",
    "print(F.softmax(20 * actor_output['policy_logits'], dim=-1))\n",
    "action, prob, q_ret = n_step_greedy_model(state, action, model, 1, encoded=None, temp=20.)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201420df",
   "metadata": {},
   "source": [
    "<font size=\"5\">Misc.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch_m(flags, buffers)\n",
    "print(torch.max(batch[\"reward\"]), (torch.max(batch[\"reward\"]) == batch[\"reward\"]).nonzero())\n",
    "print(batch[\"done\"].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ce789",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"reward\"][:, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG LOSS\n",
    "\n",
    "#batch = get_batch_m(flags, buffers)\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "logits = logits[:-1]\n",
    "\n",
    "target_rewards = batch['reward'][1:]\n",
    "target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "target_vs = []\n",
    "target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0]    \n",
    "\n",
    "for t in range(vs.shape[0]-1, 0, -1):\n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))\n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "# if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "# rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "# vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "# logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "done_masks = []\n",
    "done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "for t in range(vs.shape[0]):\n",
    "    done = torch.logical_or(done, batch['done'][t])\n",
    "    done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "\n",
    "# compute final loss\n",
    "huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "rs_loss = torch.sum(huberloss(rs, target_rewards) * (~done_masks).float())\n",
    "#rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "vs_loss = torch.sum(huberloss(vs[:-1], target_vs) * (~done_masks).float())\n",
    "#vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "logits_loss = compute_cross_entropy_loss(logits, target_logits, done_masks)\n",
    "\n",
    "# debug\n",
    "ind = 21\n",
    "\n",
    "target_vs = []\n",
    "target_v = vs[-1]\n",
    "for t in range(vs.shape[0]-1, 0, -1):        \n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    print(t, \n",
    "          \"reward %2f\" % batch['reward'][t,ind].item(), \n",
    "          \"bootstrap %2f\" % (target_v * (~batch['done'][t]).float())[ind].item(), \n",
    "          \"truncated %2f\" % (vs[t-1] * (batch['truncated_done'][t]).float())[ind].item(),\n",
    "          \"vs[t-1] %2f\" % vs[t-1][ind].item(),\n",
    "          \"new_targ %2f\" % new_target_v[ind].item())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))    \n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)   \n",
    "print(\"done\", batch[\"done\"][:, ind])\n",
    "print(\"done_masks\", done_masks[:, ind])\n",
    "print(\"vs: \", vs[:, ind])\n",
    "print(\"target_vs: \", target_vs[:, ind])\n",
    "print(\"reward: \", rs[:, ind])\n",
    "print(\"target_reward: \", target_rewards[:, ind])\n",
    "print(\"logits: \", logits[:, ind])\n",
    "print(\"target_logits: \", target_logits[:, ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt. version of computing loss by treading terminal state as absorbing state (as in MuZero)\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_logits = batch['policy_logits'][1:].clone()\n",
    "    target_rewards = batch['reward'][1:].clone()\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "\n",
    "    c_logits = target_logits[0]\n",
    "    c_state = batch['frame'][0]\n",
    "    for t in range(vs.shape[0]-1):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        c_logits = torch.where(done.unsqueeze(-1), c_logits, target_logits[t])\n",
    "        target_logits[t] = c_logits\n",
    "        c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][t])  \n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "    done_masks = torch.concat(done_masks, dim=0)\n",
    "    done = torch.logical_or(done, batch['done'][-1])\n",
    "    c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][-1])\n",
    "    target_rewards = target_rewards * (~done_masks).float()\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(c_state, batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * target_v\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()))\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()))\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), None)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
