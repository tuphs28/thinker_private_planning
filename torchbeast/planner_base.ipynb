{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc8a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core import file_writer\n",
    "from torchbeast.core import prof\n",
    "from torchbeast.core import vtrace\n",
    "from torchbeast.core.environment import Environment, Vec_Environment\n",
    "\n",
    "from torchbeast.atari_wrappers import *\n",
    "from torchbeast.transformer_rnn import *\n",
    "from torchbeast.train import *\n",
    "from torchbeast.model import Model\n",
    "from torchbeast.base import BaseNet\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "def get_param(net, name=None):\n",
    "    keys = []\n",
    "    for (k, v) in actor_wrapper.model.named_parameters(): \n",
    "        if name is None:\n",
    "            print(k)\n",
    "        else:\n",
    "            if name == k: return v\n",
    "        keys.append(k)\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b0cae",
   "metadata": {},
   "source": [
    "<font size=\"5\">Agent Training Phase</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239d9e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits, temperature, u=None, hard=True):\n",
    "    \"\"\"\n",
    "    ST-gumple-softmax\n",
    "    input: [bsz, n_class]\n",
    "    return: flatten --> [bsz, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    eps = 1e-20\n",
    "    n_class = logits.shape[-1]\n",
    "    \n",
    "    if u is None:\n",
    "        u = torch.rand(logits.size()).to(logits.device)\n",
    "        u = -torch.log(-torch.log(u + eps) + eps)\n",
    "    y = logits + u\n",
    "    y = F.softmax(y / temperature, dim=-1)\n",
    "    \n",
    "    if not hard: return y.view(-1, n_class)\n",
    "\n",
    "    shape = y.size()\n",
    "    _, ind = y.max(dim=-1)\n",
    "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "    y_hard = y_hard.view(*shape)\n",
    "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
    "    y_hard = (y_hard - y).detach() + y\n",
    "    return y_hard.view(-1, n_class), u\n",
    "\n",
    "class Actor_net(nn.Module):    \n",
    "    def __init__(self, obs_shape, num_actions, flags):\n",
    "\n",
    "        super(Actor_net, self).__init__()\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions  \n",
    "        \n",
    "        self.tran_t = flags.tran_t                   # number of recurrence of RNN\n",
    "        self.tran_mem_n = flags.tran_mem_n           # size of memory for the attn modules\n",
    "        self.tran_layer_n = flags.tran_layer_n       # number of layers\n",
    "        self.tran_lstm = flags.tran_lstm             # to use lstm or not\n",
    "        self.tran_lstm_no_attn = flags.tran_lstm_no_attn  # to use attention in lstm or not\n",
    "        self.tran_norm_first = flags.tran_norm_first # to use norm first in transformer (not on LSTM)\n",
    "        self.tran_ff_n = flags.tran_ff_n             # number of dim of ff in transformer (not on LSTM)        \n",
    "        self.tran_skip = flags.tran_skip             # whether to add skip connection\n",
    "        self.conv_out = flags.tran_dim               # size of transformer / LSTM embedding dim\n",
    "        self.ste = flags.ste\n",
    "        self.gb_ste = flags.gb_ste\n",
    "        \n",
    "        self.conv_out_hw = 1   \n",
    "        self.d_model = self.conv_out\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=self.obs_shape[0], out_channels=self.conv_out//2, kernel_size=1, stride=1)        \n",
    "        self.conv2 = nn.Conv2d(in_channels=self.conv_out//2, out_channels=self.conv_out, kernel_size=1, stride=1)        \n",
    "        self.frame_conv = torch.nn.Sequential(self.conv1, nn.ReLU(), self.conv2, nn.ReLU())\n",
    "        self.env_input_size = self.conv_out\n",
    "        d_in = self.env_input_size + self.d_model \n",
    "        \n",
    "        if self.tran_lstm:\n",
    "            self.core = ConvAttnLSTM(h=self.conv_out_hw, w=self.conv_out_hw,\n",
    "                                 input_dim=d_in-self.d_model, hidden_dim=self.d_model,\n",
    "                                 kernel_size=1, num_layers=self.tran_layer_n,\n",
    "                                 num_heads=8, mem_n=self.tran_mem_n, attn=not self.tran_lstm_no_attn)\n",
    "        else:            \n",
    "            self.core = ConvTransformerRNN(d_in=d_in,\n",
    "                                       h=self.conv_out_hw, w=self.conv_out_hw, d_model=self.d_model, \n",
    "                                       num_heads=8, dim_feedforward=self.tran_ff_n, \n",
    "                                       mem_n=self.tran_mem_n, norm_first=self.tran_norm_first,\n",
    "                                       num_layers=self.tran_layer_n, rpos=self.rpos, conv=False)   \n",
    "                         \n",
    "        \n",
    "        if self.tran_skip:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * (self.d_model + self.env_input_size)\n",
    "        else:\n",
    "            rnn_out_size = self.conv_out_hw * self.conv_out_hw * self.d_model\n",
    "                \n",
    "        self.fc = nn.Linear(rnn_out_size, 256)        \n",
    "        \n",
    "        self.im_policy = nn.Linear(256, self.num_actions)        \n",
    "        self.policy = nn.Linear(256, self.num_actions)        \n",
    "        self.baseline = nn.Linear(256, 1)        \n",
    "        self.reset = nn.Linear(256, 1)        \n",
    "        \n",
    "        if self.gb_ste:\n",
    "            self.register_buffer('temp', torch.tensor(flags.gb_ste_temp_max, dtype=torch.float32))\n",
    "        \n",
    "        print(\"actor size: \", sum(p.numel() for p in self.parameters()))\n",
    "        #for k, v in self.named_parameters(): print(k, v.numel())   \n",
    "\n",
    "    def initial_state(self, batch_size):\n",
    "        state = self.core.init_state(batch_size) + (torch.zeros(1, batch_size, \n",
    "               self.env_input_size, self.conv_out_hw, self.conv_out_hw),)\n",
    "        return state\n",
    "\n",
    "    def forward(self, x, done, core_state=(), u=None, debug=False):\n",
    "        # one-step forward for the actor\n",
    "        # input / done shape x: T x B x C x 1 x 1 / B x C x 1 x 1\n",
    "        # only supports T = 1 at the moment; all output does not have T dim.\n",
    "        \n",
    "        if len(x.shape) == 4: x = x.unsqueeze(0)\n",
    "        if len(done.shape) == 1: done = done.unsqueeze(0)  \n",
    "            \n",
    "        T, B, *_ = x.shape\n",
    "        x = torch.flatten(x, 0, 1)  # Merge time and batch.        \n",
    "        env_input = self.frame_conv(x)                \n",
    "        core_input = env_input.view(T, B, -1, self.conv_out_hw, self.conv_out_hw)\n",
    "        core_output_list = []\n",
    "        notdone = ~(done.bool())\n",
    "        \n",
    "        for n, (input, nd) in enumerate(zip(core_input.unbind(), notdone.unbind())):                \n",
    "            # Input shape: B, self.conv_out + self.num_actions + 1, H, W\n",
    "            for t in range(self.tran_t):                \n",
    "                if t > 0: nd = torch.ones(B).to(x.device).bool()                    \n",
    "                nd = nd.view(-1)      \n",
    "                output, core_state = self.core(input, core_state, nd, nd) # output shape: 1, B, core_output_size \n",
    "                \n",
    "            last_input = input   \n",
    "            core_output_list.append(output)\n",
    "                                   \n",
    "        core_output = torch.cat(core_output_list)  \n",
    "        if self.tran_skip: core_output = torch.concat([core_output, core_input], dim=-3)\n",
    "        core_output = torch.flatten(core_output, 0, 1)        \n",
    "        core_output = F.relu(self.fc(torch.flatten(core_output, start_dim=1)))   \n",
    "        \n",
    "        policy_logits = self.policy(core_output)\n",
    "        im_policy_logits = self.im_policy(core_output)\n",
    "        baseline = self.baseline(core_output)\n",
    "        reset_logits = self.reset(core_output)\n",
    "        \n",
    "        if self.gb_ste:\n",
    "            if u is not None: \n",
    "                u_action = u[:, :-2]\n",
    "                u_reset = u[:, -2:]\n",
    "            else:\n",
    "                u_action = None\n",
    "                u_reset = None\n",
    "            im_action, u_action = gumbel_softmax(im_policy_logits, self.temp, u=u_action, hard=True)\n",
    "            n_reset_logits = torch.cat([reset_logits, torch.zeros_like(reset_logits)], dim=-1)    \n",
    "            reset, u_reset = gumbel_softmax(n_reset_logits, self.temp, u=u_reset, hard=True)\n",
    "            reset = reset[:, 0]\n",
    "            u = torch.cat([u_action, u_reset], dim=-1)\n",
    "        elif self.ste:\n",
    "            im_action_p = F.softmax(im_policy_logits, dim=-1)\n",
    "            im_action_h = torch.multinomial(im_action_p, num_samples=1)\n",
    "            im_action = (im_action_h - im_action_p).detach() + im_action_p\n",
    "          \n",
    "            reset_p = torch.sigmoid(reset_logits)\n",
    "            reset_h = torch.bernoulli(reset_p)\n",
    "            reset = (reset_h - reset_p).detach() + reset_p\n",
    "        else:\n",
    "            im_action = F.softmax(im_policy_logits, dim=1)\n",
    "            reset = torch.sigmoid(reset_logits)\n",
    "            \n",
    "        \n",
    "        action = torch.multinomial(F.softmax(policy_logits, dim=1), num_samples=1)\n",
    "        \n",
    "        reg_loss = (1e-3 * torch.sum(policy_logits**2, dim=-1) / 2 + \n",
    "                    1e-5 * torch.sum(core_output**2, dim=-1) / 2)\n",
    "        reg_loss = reg_loss.view(T, B)\n",
    "        \n",
    "        policy_logits = policy_logits.view(T, B, self.num_actions)\n",
    "        action = action.view(T, B)                \n",
    "        im_policy_logits = im_policy_logits.view(T, B, self.num_actions)\n",
    "        im_action = im_action.view(T, B, self.num_actions)        \n",
    "        baseline = baseline.view(T, B)\n",
    "        reset = reset.view(T, B)     \n",
    "        \n",
    "        ret_dict = dict(policy_logits=policy_logits[0],                         \n",
    "                        action=action[0], \n",
    "                        im_policy_logits=im_policy_logits[0],                         \n",
    "                        im_action=im_action[0],                        \n",
    "                        baseline=baseline[0], \n",
    "                        reset=reset[0],\n",
    "                        reg_loss=reg_loss[0], )\n",
    "        if self.gb_ste:\n",
    "            ret_dict['uniform'] = u\n",
    "        return (ret_dict, core_state)      \n",
    "    \n",
    "class Actor_Wrapper(nn.Module):    \n",
    "    def __init__(self, flags, model, actor=None, rec_t=5):\n",
    "        \n",
    "        super(Actor_Wrapper, self).__init__()   \n",
    "        self.model = model\n",
    "        self.num_actions = model.num_actions        \n",
    "        self.rec_t = rec_t\n",
    "        self.obs_shape = (3 * num_actions + 4 + self.rec_t, 1, 1)        \n",
    "        if actor is None:\n",
    "            self.actor = Actor_net(obs_shape=self.obs_shape, num_actions=self.num_actions, flags=flags)\n",
    "        else:\n",
    "            self.actor = actor   \n",
    "            \n",
    "    def initial_state(self, batch_size):\n",
    "        return self.actor.initial_state(batch_size)\n",
    "            \n",
    "    def forward(self, x, core_state=None):                      \n",
    "        # x is env_output object with:\n",
    "        # frame: T x B x C x H x W\n",
    "        # last_action: T x B\n",
    "        # reward: T x B\n",
    "        \n",
    "        tot_step, bsz, _, _, _ = x['frame'].shape\n",
    "        device = x['frame'].device        \n",
    "        self.model.train(False)\n",
    "        \n",
    "        for step in range(tot_step):\n",
    "        \n",
    "            state = x['frame'][step]        \n",
    "            action = F.one_hot(x['last_action'][step], self.num_actions)\n",
    "            reward = x['reward'][step]   \n",
    "            done = x['done'][step]\n",
    "            \n",
    "            rs, vs, logits, encodeds = self.model(state, action.unsqueeze(0), one_hot=True)\n",
    "            encoded_reset = encodeds[0].clone()\n",
    "            encoded = encodeds[0]\n",
    "\n",
    "            for t in range(self.rec_t): \n",
    "                \n",
    "                a = action if t == 0 else im_action  \n",
    "                r = (reward if t == 0 else rs[-1]).unsqueeze(-1)\n",
    "                v = vs[-1].unsqueeze(-1)\n",
    "                logit = logits[-1]                    \n",
    "                \n",
    "                if t == 0:\n",
    "                    r0 = r.clone()\n",
    "                    v0 = v.clone()\n",
    "                    logit0 = logit.clone()\n",
    "                    if self.actor.gb_ste: u_list = []\n",
    "                \n",
    "                time = F.one_hot(torch.tensor([t], device=device).long(), self.rec_t).tile([bsz, 1])                        \n",
    "\n",
    "                actor_input = torch.concat([a, r, v, logit, r0, v0, logit0, time], dim=-1)     \n",
    "                actor_input = actor_input.unsqueeze(-1).unsqueeze(-1)\n",
    "                \n",
    "                if 'uniform' in x.keys():\n",
    "                    u = x['uniform'][step][:, t]\n",
    "                else:\n",
    "                    u = None                    \n",
    "                \n",
    "                actor_output, core_state = self.actor(actor_input, done=done, core_state=core_state,\n",
    "                                                      u=u)                \n",
    "                if self.actor.gb_ste: u_list.append(actor_output['uniform'].unsqueeze(1))\n",
    "                \n",
    "                if t == self.rec_t - 1:\n",
    "                    if self.actor.gb_ste:\n",
    "                        actor_output[\"uniform\"] = torch.concat(u_list, dim=1)\n",
    "                    if step == 0:\n",
    "                        all_actor_output = {k: [v.unsqueeze(0)] for k, v in actor_output.items()}\n",
    "                    else:\n",
    "                        for k, v in actor_output.items(): all_actor_output[k].append(v.unsqueeze(0))\n",
    "                \n",
    "                reset = actor_output[\"reset\"]\n",
    "                im_action = actor_output[\"im_action\"]\n",
    "\n",
    "                if t < self.rec_t - 1:                \n",
    "                    rs, vs, logits, encodeds = self.model.forward_encoded(encoded, im_action.unsqueeze(0), \n",
    "                                                                          one_hot=True)\n",
    "                    reset = reset.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "                    encoded = reset * encoded_reset + (1 - reset) * encodeds[-1]\n",
    "        \n",
    "        all_actor_output = {k: torch.concat(v) for k, v in all_actor_output.items()}\n",
    "        return all_actor_output, core_state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06dfbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_parser():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Scalable Agent\")\n",
    "\n",
    "    parser.add_argument(\"--env\", type=str, default=\"Sokoban-v0\",\n",
    "                        help=\"Gym environment.\")\n",
    "    parser.add_argument(\"--env_disable_noop\", action=\"store_true\",\n",
    "                        help=\"Disable noop in environment or not. (sokoban only)\")\n",
    "\n",
    "    parser.add_argument(\"--xpid\", default=None,\n",
    "                        help=\"Experiment id (default: None).\")\n",
    "\n",
    "    # Training settings.\n",
    "    parser.add_argument(\"--disable_checkpoint\", action=\"store_true\",\n",
    "                        help=\"Disable saving checkpoint.\")\n",
    "    parser.add_argument(\"--savedir\", default=\"~/RS/thinker/logs/torchbeast\",\n",
    "                        help=\"Root dir where experiment data will be saved.\")\n",
    "    parser.add_argument(\"--num_actors\", default=48, type=int, metavar=\"N\",\n",
    "                        help=\"Number of actors (default: 48).\")\n",
    "    parser.add_argument(\"--total_steps\", default=100000000, type=int, metavar=\"T\",\n",
    "                        help=\"Total environment steps to train for.\")\n",
    "    parser.add_argument(\"--batch_size\", default=32, type=int, metavar=\"B\",\n",
    "                        help=\"Learner batch size.\")\n",
    "    parser.add_argument(\"--unroll_length\", default=20, type=int, metavar=\"T\",\n",
    "                        help=\"The unroll length (time dimension).\")\n",
    "    parser.add_argument(\"--num_buffers\", default=None, type=int,\n",
    "                        metavar=\"N\", help=\"Number of shared-memory buffers.\")\n",
    "    parser.add_argument(\"--num_learner_threads\", \"--num_threads\", default=1, type=int,\n",
    "                        metavar=\"N\", help=\"Number learner threads.\")\n",
    "    parser.add_argument(\"--disable_cuda\", action=\"store_true\",\n",
    "                        help=\"Disable CUDA.\")\n",
    "\n",
    "    # Architecture settings\n",
    "    parser.add_argument(\"--tran_dim\", default=64, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer hidden dim.\")\n",
    "    parser.add_argument(\"--tran_mem_n\", default=16, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer memory.\")\n",
    "    parser.add_argument(\"--tran_layer_n\", default=3, type=int, metavar=\"N\",\n",
    "                        help=\"Number of transformer layer.\")\n",
    "    parser.add_argument(\"--tran_t\", default=1, type=int, metavar=\"T\",\n",
    "                        help=\"Number of recurrent step for transformer.\")\n",
    "    parser.add_argument(\"--tran_ff_n\", default=256, type=int, metavar=\"N\",\n",
    "                        help=\"Size of transformer ff .\")\n",
    "    parser.add_argument(\"--tran_skip\", action=\"store_true\",\n",
    "                        help=\"Whether to enable skip conn.\")\n",
    "    parser.add_argument(\"--tran_norm_first\", action=\"store_true\",\n",
    "                        help=\"Whether to use norm first in transformer.\")\n",
    "    parser.add_argument(\"--tran_rpos\", action=\"store_true\",\n",
    "                        help=\"Whether to use relative position in transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm\", action=\"store_true\",\n",
    "                        help=\"Whether to use LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_lstm_no_attn\", action=\"store_true\",\n",
    "                        help=\"Whether to disable attention in LSTM-transformer.\")\n",
    "    parser.add_argument(\"--tran_erasep\", action=\"store_true\",\n",
    "                        help=\"Whether to erase past memories if not planning.\")\n",
    "\n",
    "    parser.add_argument(\"--rec_t\", default=5, type=int, metavar=\"N\",\n",
    "                        help=\"Number of planning steps.\")\n",
    "    parser.add_argument(\"--ste\", action=\"store_true\",\n",
    "                        help=\"Whether to use ste backprop.\")\n",
    "    parser.add_argument(\"--gb_ste\", action=\"store_true\",\n",
    "                        help=\"Whether to use gb-ste backprop.\")\n",
    "    parser.add_argument(\"--gb_ste_temp_max\", default=1, type=int, metavar=\"N\",\n",
    "                        help=\"Beginning temp. for gb-ste.\")\n",
    "    parser.add_argument(\"--gb_ste_temp_min\", default=0.5, type=int, metavar=\"N\",\n",
    "                        help=\"Ending temp. for gb-ste.\")    \n",
    "\n",
    "    # Loss settings.\n",
    "    parser.add_argument(\"--entropy_cost\", default=0.01,\n",
    "                        type=float, help=\"Entropy cost/multiplier.\")\n",
    "    parser.add_argument(\"--baseline_cost\", default=0.5,\n",
    "                        type=float, help=\"Baseline cost/multiplier.\")\n",
    "    parser.add_argument(\"--reg_cost\", default=1,\n",
    "                        type=float, help=\"Reg cost/multiplier.\")\n",
    "    parser.add_argument(\"--discounting\", default=0.97,\n",
    "                        type=float, help=\"Discounting factor.\")\n",
    "    parser.add_argument(\"--lamb\", default=0.97,\n",
    "                        type=float, help=\"Lambda when computing trace.\")\n",
    "    parser.add_argument(\"--reward_clipping\", default=10, type=int, \n",
    "                        metavar=\"N\", help=\"Reward clipping.\")\n",
    "    parser.add_argument(\"--trun_bs\", action=\"store_true\",\n",
    "                        help=\"Whether to add baseline as reward when truncated.\")\n",
    "\n",
    "    # Optimizer settings.\n",
    "    parser.add_argument(\"--learning_rate\", default=0.0004,\n",
    "                        type=float, metavar=\"LR\", help=\"Learning rate.\")\n",
    "    parser.add_argument(\"--disable_adam\", action=\"store_true\",\n",
    "                        help=\"Use Aadm optimizer or not.\")\n",
    "    parser.add_argument(\"--alpha\", default=0.99, type=float,\n",
    "                        help=\"RMSProp smoothing constant.\")\n",
    "    parser.add_argument(\"--momentum\", default=0, type=float,\n",
    "                        help=\"RMSProp momentum.\")\n",
    "    parser.add_argument(\"--epsilon\", default=0.01, type=float,\n",
    "                        help=\"RMSProp epsilon.\")\n",
    "    parser.add_argument(\"--grad_norm_clipping\", default=0.0, type=float,\n",
    "                        help=\"Global gradient norm clip.\")\n",
    "    # yapf: enable\n",
    "\n",
    "    return parser\n",
    "\n",
    "parser = define_parser()\n",
    "flags = parser.parse_args([])        \n",
    "\n",
    "flags.xpid = None\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.num_actors = 32\n",
    "flags.batch_size = 32\n",
    "flags.unroll_length = 20\n",
    "flags.learning_rate = 0.0004\n",
    "flags.entropy_cost = 0.001\n",
    "flags.lamb = 1.\n",
    "\n",
    "flags.trun_bs = False\n",
    "flags.total_steps = 100000000\n",
    "flags.disable_adam = False\n",
    "\n",
    "flags.tran_t = 1\n",
    "flags.tran_mem_n = 16\n",
    "flags.tran_layer_n = 3\n",
    "flags.tran_lstm = True\n",
    "flags.tran_lstm_no_attn = True\n",
    "flags.tran_norm_first = False\n",
    "flags.tran_ff_n = 256\n",
    "flags.tran_skip = True\n",
    "flags.tran_erasep = False\n",
    "flags.tran_dim = 64\n",
    "flags.tran_rpos = True\n",
    "flags.rec_t = 5\n",
    "flags.ste = True\n",
    "flags.gb_ste = False\n",
    "flags.gb_ste_temp_max = 1\n",
    "flags.gb_ste_temp_min = 0.5\n",
    "\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "model_learner = Model(flags, obs_shape, num_actions=num_actions)\n",
    "model_actor = Model(flags, obs_shape, num_actions=num_actions)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model_learner.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "model_actor.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f342a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating log directory: /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958\n",
      "[INFO:289857 file_writer:104 2022-10-30 20:09:58,382] Creating log directory: /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958\n",
      "Symlinked log directory: /home/sc/RS/thinker/logs/torchbeast/latest\n",
      "[INFO:289857 file_writer:117 2022-10-30 20:09:58,383] Symlinked log directory: /home/sc/RS/thinker/logs/torchbeast/latest\n",
      "Saving arguments to /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958/meta.json\n",
      "[INFO:289857 file_writer:129 2022-10-30 20:09:58,383] Saving arguments to /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958/meta.json\n",
      "Saving messages to /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958/out.log\n",
      "[INFO:289857 file_writer:137 2022-10-30 20:09:58,384] Saving messages to /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958/out.log\n",
      "Saving logs data to /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958/logs.csv\n",
      "[INFO:289857 file_writer:147 2022-10-30 20:09:58,384] Saving logs data to /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958/logs.csv\n",
      "Saving logs' fields to /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958/fields.csv\n",
      "[INFO:289857 file_writer:148 2022-10-30 20:09:58,384] Saving logs' fields to /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958/fields.csv\n",
      "[INFO:289857 4259067192:11 2022-10-30 20:09:58,385] Using CUDA.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  286316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:291828 train:191 2022-10-30 20:09:58,690] Actor 0 started.\n",
      "[INFO:289857 4259067192:82 2022-10-30 20:09:58,697] # Step\tmean_episode_return\tepisode_returns\ttotal_loss\tpg_loss\tbaseline_loss\tentropy_loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor size:  286316\n",
      "Using Adam...\n",
      "All parameters: \n",
      "model.frameEncoder.conv1.weight 4608\n",
      "model.frameEncoder.conv1.bias 64\n",
      "model.frameEncoder.res1.0.conv1.weight 36864\n",
      "model.frameEncoder.res1.0.bn1.weight 64\n",
      "model.frameEncoder.res1.0.bn1.bias 64\n",
      "model.frameEncoder.res1.0.conv2.weight 36864\n",
      "model.frameEncoder.res1.0.bn2.weight 64\n",
      "model.frameEncoder.res1.0.bn2.bias 64\n",
      "model.frameEncoder.conv2.weight 73728\n",
      "model.frameEncoder.conv2.bias 128\n",
      "model.frameEncoder.res2.0.conv1.weight 147456\n",
      "model.frameEncoder.res2.0.bn1.weight 128\n",
      "model.frameEncoder.res2.0.bn1.bias 128\n",
      "model.frameEncoder.res2.0.conv2.weight 147456\n",
      "model.frameEncoder.res2.0.bn2.weight 128\n",
      "model.frameEncoder.res2.0.bn2.bias 128\n",
      "model.frameEncoder.res3.0.conv1.weight 147456\n",
      "model.frameEncoder.res3.0.bn1.weight 128\n",
      "model.frameEncoder.res3.0.bn1.bias 128\n",
      "model.frameEncoder.res3.0.conv2.weight 147456\n",
      "model.frameEncoder.res3.0.bn2.weight 128\n",
      "model.frameEncoder.res3.0.bn2.bias 128\n",
      "model.dynamicModel.res.0.conv1.weight 159201\n",
      "model.dynamicModel.res.0.bn1.weight 133\n",
      "model.dynamicModel.res.0.bn1.bias 133\n",
      "model.dynamicModel.res.0.conv2.weight 153216\n",
      "model.dynamicModel.res.0.bn2.weight 128\n",
      "model.dynamicModel.res.0.bn2.bias 128\n",
      "model.dynamicModel.res.0.conv3.weight 17024\n",
      "model.dynamicModel.res.1.conv1.weight 147456\n",
      "model.dynamicModel.res.1.bn1.weight 128\n",
      "model.dynamicModel.res.1.bn1.bias 128\n",
      "model.dynamicModel.res.1.conv2.weight 147456\n",
      "model.dynamicModel.res.1.bn2.weight 128\n",
      "model.dynamicModel.res.1.bn2.bias 128\n",
      "model.dynamicModel.res.2.conv1.weight 147456\n",
      "model.dynamicModel.res.2.bn1.weight 128\n",
      "model.dynamicModel.res.2.bn1.bias 128\n",
      "model.dynamicModel.res.2.conv2.weight 147456\n",
      "model.dynamicModel.res.2.bn2.weight 128\n",
      "model.dynamicModel.res.2.bn2.bias 128\n",
      "model.dynamicModel.res.3.conv1.weight 147456\n",
      "model.dynamicModel.res.3.bn1.weight 128\n",
      "model.dynamicModel.res.3.bn1.bias 128\n",
      "model.dynamicModel.res.3.conv2.weight 147456\n",
      "model.dynamicModel.res.3.bn2.weight 128\n",
      "model.dynamicModel.res.3.bn2.bias 128\n",
      "model.dynamicModel.res.4.conv1.weight 147456\n",
      "model.dynamicModel.res.4.bn1.weight 128\n",
      "model.dynamicModel.res.4.bn1.bias 128\n",
      "model.dynamicModel.res.4.conv2.weight 147456\n",
      "model.dynamicModel.res.4.bn2.weight 128\n",
      "model.dynamicModel.res.4.bn2.bias 128\n",
      "model.output_rvpi.conv1.weight 73728\n",
      "model.output_rvpi.conv1.bias 64\n",
      "model.output_rvpi.conv2.weight 18432\n",
      "model.output_rvpi.conv2.bias 32\n",
      "model.output_rvpi.fc_r.weight 800\n",
      "model.output_rvpi.fc_r.bias 1\n",
      "model.output_rvpi.fc_v.weight 800\n",
      "model.output_rvpi.fc_v.bias 1\n",
      "model.output_rvpi.fc_logits.weight 4000\n",
      "model.output_rvpi.fc_logits.bias 5\n",
      "actor.conv1.weight 768\n",
      "actor.conv1.bias 32\n",
      "actor.conv2.weight 2048\n",
      "actor.conv2.bias 64\n",
      "actor.core.layers.0.conv.weight 81920\n",
      "actor.core.layers.0.conv.bias 320\n",
      "actor.core.layers.1.conv.weight 81920\n",
      "actor.core.layers.1.conv.bias 320\n",
      "actor.core.layers.2.conv.weight 81920\n",
      "actor.core.layers.2.conv.bias 320\n",
      "actor.core.proj_list.0.weight 128\n",
      "actor.core.proj_list.0.bias 64\n",
      "actor.core.proj_list.1.weight 128\n",
      "actor.core.proj_list.1.bias 64\n",
      "actor.core.proj_list.2.weight 128\n",
      "actor.core.proj_list.2.bias 64\n",
      "actor.fc.weight 32768\n",
      "actor.fc.bias 256\n",
      "actor.im_policy.weight 1280\n",
      "actor.im_policy.bias 5\n",
      "actor.policy.weight 1280\n",
      "actor.policy.bias 5\n",
      "actor.baseline.weight 256\n",
      "actor.baseline.bias 1\n",
      "actor.reset.weight 256\n",
      "actor.reset.bias 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:289857 4259067192:168 2022-10-30 20:10:03,703] Steps 0 @ 0.0 SPS. Eps 0. L400 Return 0.000000. Loss inf\n",
      "Updated log fields: ['_tick', '_time', 'step', 'mean_episode_return', 'episode_returns', 'total_loss', 'pg_loss', 'baseline_loss', 'entropy_loss', 'trail_mean_episode_return', 'episode']\n",
      "[INFO:289857 file_writer:189 2022-10-30 20:10:07,359] Updated log fields: ['_tick', '_time', 'step', 'mean_episode_return', 'episode_returns', 'total_loss', 'pg_loss', 'baseline_loss', 'entropy_loss', 'trail_mean_episode_return', 'episode']\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:08,708] Steps 640 @ 127.9 SPS. Eps 5. L400 Return -0.999999. Loss -33.505402 pg_loss -42.673157 baseline_loss 9.591001 entropy_loss -1.013017 reg_loss 0.589770\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:13,714] Steps 640 @ 0.0 SPS. Eps 5. L400 Return -0.999999. Loss -33.505402 pg_loss -42.673157 baseline_loss 9.591001 entropy_loss -1.013017 reg_loss 0.589770\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:18,719] Steps 1280 @ 127.9 SPS. Eps 10. L400 Return -0.899999. Loss -77.681503 pg_loss -97.100296 baseline_loss 19.701105 entropy_loss -1.012305 reg_loss 0.729996\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:23,725] Steps 1920 @ 127.9 SPS. Eps 16. L400 Return -0.824999. Loss -91.034882 pg_loss -113.716064 baseline_loss 23.385267 entropy_loss -1.016249 reg_loss 0.312163\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:28,730] Steps 1920 @ 0.0 SPS. Eps 16. L400 Return -0.824999. Loss -91.034882 pg_loss -113.716064 baseline_loss 23.385267 entropy_loss -1.016249 reg_loss 0.312163\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:33,736] Steps 2560 @ 127.9 SPS. Eps 21. L400 Return -0.771428. Loss -83.071220 pg_loss -102.686539 baseline_loss 20.406437 entropy_loss -1.022433 reg_loss 0.231313\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:38,739] Steps 2560 @ 0.0 SPS. Eps 21. L400 Return -0.771428. Loss -83.071220 pg_loss -102.686539 baseline_loss 20.406437 entropy_loss -1.022433 reg_loss 0.231313\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:43,744] Steps 3200 @ 127.9 SPS. Eps 26. L400 Return -0.853845. Loss -51.182446 pg_loss -59.605373 baseline_loss 9.152785 entropy_loss -1.022500 reg_loss 0.292642\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:48,750] Steps 3840 @ 127.9 SPS. Eps 32. L400 Return -0.918749. Loss -23.710722 pg_loss -26.310585 baseline_loss 3.559649 entropy_loss -1.026284 reg_loss 0.066499\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:53,755] Steps 3840 @ 0.0 SPS. Eps 32. L400 Return -0.918749. Loss -23.710722 pg_loss -26.310585 baseline_loss 3.559649 entropy_loss -1.026284 reg_loss 0.066499\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:10:58,760] Steps 4480 @ 127.9 SPS. Eps 37. L400 Return -0.956756. Loss 17.036818 pg_loss 16.216629 baseline_loss 1.721469 entropy_loss -1.026124 reg_loss 0.124842\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:03,763] Steps 5120 @ 128.0 SPS. Eps 42. L400 Return -0.938095. Loss 37.576374 pg_loss 27.079685 baseline_loss 11.060419 entropy_loss -1.020769 reg_loss 0.457034\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:08,768] Steps 5120 @ 0.0 SPS. Eps 42. L400 Return -0.938095. Loss 37.576374 pg_loss 27.079685 baseline_loss 11.060419 entropy_loss -1.020769 reg_loss 0.457034\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:13,774] Steps 5760 @ 127.9 SPS. Eps 48. L400 Return -0.887499. Loss 127.255211 pg_loss 106.959595 baseline_loss 19.864225 entropy_loss -1.012068 reg_loss 1.443455\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:18,779] Steps 6400 @ 127.9 SPS. Eps 53. L400 Return -0.841509. Loss 102.256966 pg_loss 82.792046 baseline_loss 18.468203 entropy_loss -1.014301 reg_loss 2.011021\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:23,785] Steps 6400 @ 0.0 SPS. Eps 53. L400 Return -0.841509. Loss 102.256966 pg_loss 82.792046 baseline_loss 18.468203 entropy_loss -1.014301 reg_loss 2.011021\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:28,790] Steps 7040 @ 127.9 SPS. Eps 58. L400 Return -0.837930. Loss 17.177994 pg_loss 8.548590 baseline_loss 9.550697 entropy_loss -1.025607 reg_loss 0.104313\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:33,796] Steps 7680 @ 127.9 SPS. Eps 64. L400 Return -0.824999. Loss 24.640688 pg_loss 13.542604 baseline_loss 11.318748 entropy_loss -1.017381 reg_loss 0.796717\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:38,801] Steps 7680 @ 0.0 SPS. Eps 64. L400 Return -0.824999. Loss 24.640688 pg_loss 13.542604 baseline_loss 11.318748 entropy_loss -1.017381 reg_loss 0.796717\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:43,807] Steps 8320 @ 127.9 SPS. Eps 69. L400 Return -0.823188. Loss 19.818388 pg_loss 12.365764 baseline_loss 8.297478 entropy_loss -1.025559 reg_loss 0.180705\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:48,811] Steps 8960 @ 127.9 SPS. Eps 74. L400 Return -0.835134. Loss -5.186342 pg_loss -8.214628 baseline_loss 3.905519 entropy_loss -1.024707 reg_loss 0.147474\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:53,816] Steps 8960 @ 0.0 SPS. Eps 74. L400 Return -0.835134. Loss -5.186342 pg_loss -8.214628 baseline_loss 3.905519 entropy_loss -1.024707 reg_loss 0.147474\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:11:58,822] Steps 9600 @ 127.9 SPS. Eps 80. L400 Return -0.837499. Loss -58.627480 pg_loss -93.772125 baseline_loss 35.479519 entropy_loss -1.020048 reg_loss 0.685177\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:03,827] Steps 9600 @ 0.0 SPS. Eps 80. L400 Return -0.837499. Loss -58.627480 pg_loss -93.772125 baseline_loss 35.479519 entropy_loss -1.020048 reg_loss 0.685177\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:08,832] Steps 10240 @ 127.9 SPS. Eps 85. L400 Return -0.835293. Loss 9.707390 pg_loss -3.007027 baseline_loss 13.635096 entropy_loss -1.026351 reg_loss 0.105673\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:13,835] Steps 10880 @ 128.0 SPS. Eps 90. L400 Return -0.833333. Loss -65.314331 pg_loss -86.997345 baseline_loss 22.219997 entropy_loss -1.019727 reg_loss 0.482740\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:18,839] Steps 10880 @ 0.0 SPS. Eps 90. L400 Return -0.833333. Loss -65.314331 pg_loss -86.997345 baseline_loss 22.219997 entropy_loss -1.019727 reg_loss 0.482740\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:23,844] Steps 11520 @ 127.9 SPS. Eps 96. L400 Return -0.824999. Loss -14.682935 pg_loss -27.073914 baseline_loss 13.195274 entropy_loss -1.026024 reg_loss 0.221728\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:28,850] Steps 12160 @ 127.9 SPS. Eps 101. L400 Return -0.843564. Loss -10.788115 pg_loss -11.153648 baseline_loss 1.346107 entropy_loss -1.028134 reg_loss 0.047562\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:33,855] Steps 12160 @ 0.0 SPS. Eps 101. L400 Return -0.843564. Loss -10.788115 pg_loss -11.153648 baseline_loss 1.346107 entropy_loss -1.028134 reg_loss 0.047562\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:38,860] Steps 12800 @ 127.9 SPS. Eps 106. L400 Return -0.850943. Loss 20.134817 pg_loss 17.695839 baseline_loss 3.337256 entropy_loss -1.027114 reg_loss 0.128836\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:43,866] Steps 13440 @ 127.9 SPS. Eps 112. L400 Return -0.842856. Loss 17.168209 pg_loss 5.361166 baseline_loss 12.274848 entropy_loss -1.021198 reg_loss 0.553393\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:48,871] Steps 13440 @ 0.0 SPS. Eps 112. L400 Return -0.842856. Loss 17.168209 pg_loss 5.361166 baseline_loss 12.274848 entropy_loss -1.021198 reg_loss 0.553393\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:53,875] Steps 14080 @ 127.9 SPS. Eps 117. L400 Return -0.841025. Loss 55.067894 pg_loss 45.722794 baseline_loss 10.227921 entropy_loss -1.026100 reg_loss 0.143279\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:12:58,880] Steps 14720 @ 127.9 SPS. Eps 122. L400 Return -0.839343. Loss 14.308016 pg_loss 11.231184 baseline_loss 3.998955 entropy_loss -1.026567 reg_loss 0.104444\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:03,886] Steps 14720 @ 0.0 SPS. Eps 122. L400 Return -0.839343. Loss 14.308016 pg_loss 11.231184 baseline_loss 3.998955 entropy_loss -1.026567 reg_loss 0.104444\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:08,891] Steps 15360 @ 127.9 SPS. Eps 128. L400 Return -0.840624. Loss 32.447445 pg_loss 24.366917 baseline_loss 8.950261 entropy_loss -1.025125 reg_loss 0.155393\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:13,897] Steps 15360 @ 0.0 SPS. Eps 128. L400 Return -0.840624. Loss 32.447445 pg_loss 24.366917 baseline_loss 8.950261 entropy_loss -1.025125 reg_loss 0.155393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:289857 4259067192:168 2022-10-30 20:13:18,903] Steps 16000 @ 127.9 SPS. Eps 133. L400 Return -0.846616. Loss 10.360522 pg_loss 1.805578 baseline_loss 9.115225 entropy_loss -1.018613 reg_loss 0.458333\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:23,908] Steps 16640 @ 127.9 SPS. Eps 138. L400 Return -0.844927. Loss 100.785751 pg_loss 86.309479 baseline_loss 15.141750 entropy_loss -1.019825 reg_loss 0.354341\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:28,914] Steps 16640 @ 0.0 SPS. Eps 138. L400 Return -0.844927. Loss 100.785751 pg_loss 86.309479 baseline_loss 15.141750 entropy_loss -1.019825 reg_loss 0.354341\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:33,919] Steps 17280 @ 127.9 SPS. Eps 144. L400 Return -0.845833. Loss 57.553970 pg_loss 51.052383 baseline_loss 7.359958 entropy_loss -1.023383 reg_loss 0.165014\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:38,925] Steps 17920 @ 127.9 SPS. Eps 149. L400 Return -0.857717. Loss -5.287713 pg_loss -8.284613 baseline_loss 3.961298 entropy_loss -1.025592 reg_loss 0.061194\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:43,930] Steps 17920 @ 0.0 SPS. Eps 149. L400 Return -0.857717. Loss -5.287713 pg_loss -8.284613 baseline_loss 3.961298 entropy_loss -1.025592 reg_loss 0.061194\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:48,936] Steps 18560 @ 127.9 SPS. Eps 154. L400 Return -0.862337. Loss 45.742203 pg_loss 36.125435 baseline_loss 10.469927 entropy_loss -1.021017 reg_loss 0.167862\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:53,941] Steps 19200 @ 127.9 SPS. Eps 160. L400 Return -0.868749. Loss -2.514037 pg_loss -4.756211 baseline_loss 3.210475 entropy_loss -1.025287 reg_loss 0.056986\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:13:58,947] Steps 19200 @ 0.0 SPS. Eps 160. L400 Return -0.868749. Loss -2.514037 pg_loss -4.756211 baseline_loss 3.210475 entropy_loss -1.025287 reg_loss 0.056986\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:03,953] Steps 19840 @ 127.9 SPS. Eps 165. L400 Return -0.872726. Loss 3.070197 pg_loss -1.039157 baseline_loss 4.855104 entropy_loss -1.019262 reg_loss 0.273512\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:08,958] Steps 20480 @ 127.9 SPS. Eps 170. L400 Return -0.852940. Loss 14.901694 pg_loss 10.383395 baseline_loss 5.412960 entropy_loss -1.022149 reg_loss 0.127489\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:13,964] Steps 20480 @ 0.0 SPS. Eps 170. L400 Return -0.852940. Loss 14.901694 pg_loss 10.383395 baseline_loss 5.412960 entropy_loss -1.022149 reg_loss 0.127489\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:18,967] Steps 21120 @ 127.9 SPS. Eps 176. L400 Return -0.842045. Loss 32.645985 pg_loss 27.430746 baseline_loss 6.108069 entropy_loss -1.020947 reg_loss 0.128118\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:23,972] Steps 21760 @ 127.9 SPS. Eps 181. L400 Return -0.835358. Loss 17.087841 pg_loss 11.852049 baseline_loss 5.789378 entropy_loss -1.015182 reg_loss 0.461597\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:28,978] Steps 21760 @ 0.0 SPS. Eps 181. L400 Return -0.835358. Loss 17.087841 pg_loss 11.852049 baseline_loss 5.789378 entropy_loss -1.015182 reg_loss 0.461597\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:33,984] Steps 22400 @ 127.9 SPS. Eps 186. L400 Return -0.845161. Loss -18.357693 pg_loss -20.406862 baseline_loss 2.336429 entropy_loss -1.006500 reg_loss 0.719241\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:38,989] Steps 22400 @ 0.0 SPS. Eps 186. L400 Return -0.845161. Loss -18.357693 pg_loss -20.406862 baseline_loss 2.336429 entropy_loss -1.006500 reg_loss 0.719241\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:43,995] Steps 23040 @ 127.9 SPS. Eps 192. L400 Return -0.856249. Loss -63.286167 pg_loss -71.253288 baseline_loss 8.894392 entropy_loss -1.021718 reg_loss 0.094444\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:49,001] Steps 23680 @ 127.9 SPS. Eps 197. L400 Return -0.859898. Loss -65.390114 pg_loss -77.480545 baseline_loss 12.487265 entropy_loss -1.012740 reg_loss 0.615904\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:54,006] Steps 23680 @ 0.0 SPS. Eps 197. L400 Return -0.859898. Loss -65.390114 pg_loss -77.480545 baseline_loss 12.487265 entropy_loss -1.012740 reg_loss 0.615904\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:14:59,012] Steps 24320 @ 127.9 SPS. Eps 202. L400 Return -0.858415. Loss -4.654961 pg_loss -13.902554 baseline_loss 9.865713 entropy_loss -1.013205 reg_loss 0.395085\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:04,018] Steps 24960 @ 127.9 SPS. Eps 208. L400 Return -0.868269. Loss -47.181564 pg_loss -48.228622 baseline_loss 1.746291 entropy_loss -1.013636 reg_loss 0.314402\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:09,023] Steps 24960 @ 0.0 SPS. Eps 208. L400 Return -0.868269. Loss -47.181564 pg_loss -48.228622 baseline_loss 1.746291 entropy_loss -1.013636 reg_loss 0.314402\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:14,028] Steps 25600 @ 127.9 SPS. Eps 213. L400 Return -0.852581. Loss -11.290969 pg_loss -20.585087 baseline_loss 9.849213 entropy_loss -1.014024 reg_loss 0.458929\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:19,034] Steps 26240 @ 127.9 SPS. Eps 218. L400 Return -0.851375. Loss -21.927006 pg_loss -32.061508 baseline_loss 10.849469 entropy_loss -1.010181 reg_loss 0.295216\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:24,039] Steps 26240 @ 0.0 SPS. Eps 218. L400 Return -0.851375. Loss -21.927006 pg_loss -32.061508 baseline_loss 10.849469 entropy_loss -1.010181 reg_loss 0.295216\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:29,043] Steps 26880 @ 127.9 SPS. Eps 224. L400 Return -0.842856. Loss 6.836389 pg_loss 0.017027 baseline_loss 7.347257 entropy_loss -1.017170 reg_loss 0.489275\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:34,047] Steps 27520 @ 127.9 SPS. Eps 229. L400 Return -0.846287. Loss -20.179771 pg_loss -24.111488 baseline_loss 4.855426 entropy_loss -1.024821 reg_loss 0.101112\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:39,052] Steps 27520 @ 0.0 SPS. Eps 229. L400 Return -0.846287. Loss -20.179771 pg_loss -24.111488 baseline_loss 4.855426 entropy_loss -1.024821 reg_loss 0.101112\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:44,058] Steps 28160 @ 127.9 SPS. Eps 234. L400 Return -0.853845. Loss -12.101690 pg_loss -14.194924 baseline_loss 2.944324 entropy_loss -1.022843 reg_loss 0.171753\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:49,063] Steps 28800 @ 127.9 SPS. Eps 240. L400 Return -0.854166. Loss -29.318220 pg_loss -32.250702 baseline_loss 3.330210 entropy_loss -1.010592 reg_loss 0.612864\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:54,069] Steps 28800 @ 0.0 SPS. Eps 240. L400 Return -0.854166. Loss -29.318220 pg_loss -32.250702 baseline_loss 3.330210 entropy_loss -1.010592 reg_loss 0.612864\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:15:59,074] Steps 29440 @ 127.9 SPS. Eps 245. L400 Return -0.857142. Loss -11.851573 pg_loss -16.428734 baseline_loss 5.355266 entropy_loss -1.013777 reg_loss 0.235671\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:04,080] Steps 29440 @ 0.0 SPS. Eps 245. L400 Return -0.857142. Loss -11.851573 pg_loss -16.428734 baseline_loss 5.355266 entropy_loss -1.013777 reg_loss 0.235671\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:09,086] Steps 30080 @ 127.9 SPS. Eps 250. L400 Return -0.859999. Loss -23.167694 pg_loss -25.734303 baseline_loss 3.221382 entropy_loss -1.015043 reg_loss 0.360270\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:14,091] Steps 30720 @ 127.9 SPS. Eps 256. L400 Return -0.860155. Loss 11.890078 pg_loss 8.404783 baseline_loss 3.997979 entropy_loss -1.013511 reg_loss 0.500827\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:19,097] Steps 30720 @ 0.0 SPS. Eps 256. L400 Return -0.860155. Loss 11.890078 pg_loss 8.404783 baseline_loss 3.997979 entropy_loss -1.013511 reg_loss 0.500827\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:24,101] Steps 31360 @ 127.9 SPS. Eps 261. L400 Return -0.866666. Loss -5.327096 pg_loss -7.966504 baseline_loss 3.614683 entropy_loss -1.024642 reg_loss 0.049367\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:29,107] Steps 32000 @ 127.9 SPS. Eps 266. L400 Return -0.869172. Loss 14.461401 pg_loss 13.676516 baseline_loss 1.113699 entropy_loss -0.993334 reg_loss 0.664521\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:34,112] Steps 32000 @ 0.0 SPS. Eps 266. L400 Return -0.869172. Loss 14.461401 pg_loss 13.676516 baseline_loss 1.113699 entropy_loss -0.993334 reg_loss 0.664521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:289857 4259067192:168 2022-10-30 20:16:39,118] Steps 32640 @ 127.9 SPS. Eps 272. L400 Return -0.858088. Loss 64.069046 pg_loss 58.211369 baseline_loss 6.309720 entropy_loss -1.007984 reg_loss 0.555939\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:44,124] Steps 33280 @ 127.9 SPS. Eps 277. L400 Return -0.857039. Loss 52.003502 pg_loss 47.394684 baseline_loss 5.235051 entropy_loss -1.012238 reg_loss 0.386005\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:49,130] Steps 33280 @ 0.0 SPS. Eps 277. L400 Return -0.857039. Loss 52.003502 pg_loss 47.394684 baseline_loss 5.235051 entropy_loss -1.012238 reg_loss 0.386005\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:54,135] Steps 33920 @ 127.9 SPS. Eps 282. L400 Return -0.856028. Loss 54.116577 pg_loss 48.284386 baseline_loss 6.429394 entropy_loss -1.016277 reg_loss 0.419076\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:16:59,140] Steps 34560 @ 127.9 SPS. Eps 288. L400 Return -0.849305. Loss 47.060219 pg_loss 39.956230 baseline_loss 7.602670 entropy_loss -1.008356 reg_loss 0.509674\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:04,146] Steps 34560 @ 0.0 SPS. Eps 288. L400 Return -0.849305. Loss 47.060219 pg_loss 39.956230 baseline_loss 7.602670 entropy_loss -1.008356 reg_loss 0.509674\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:09,151] Steps 35200 @ 127.9 SPS. Eps 293. L400 Return -0.851876. Loss 41.840908 pg_loss 37.170700 baseline_loss 5.035212 entropy_loss -1.014608 reg_loss 0.649605\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:14,156] Steps 35840 @ 127.9 SPS. Eps 298. L400 Return -0.851006. Loss 3.500189 pg_loss -5.322792 baseline_loss 9.825556 entropy_loss -1.027389 reg_loss 0.024815\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:19,162] Steps 35840 @ 0.0 SPS. Eps 298. L400 Return -0.851006. Loss 3.500189 pg_loss -5.322792 baseline_loss 9.825556 entropy_loss -1.027389 reg_loss 0.024815\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:24,168] Steps 36480 @ 127.9 SPS. Eps 304. L400 Return -0.851315. Loss 16.318531 pg_loss 14.179935 baseline_loss 3.002879 entropy_loss -1.021454 reg_loss 0.157171\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:29,171] Steps 36480 @ 0.0 SPS. Eps 304. L400 Return -0.851315. Loss 16.318531 pg_loss 14.179935 baseline_loss 3.002879 entropy_loss -1.021454 reg_loss 0.157171\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:34,176] Steps 37120 @ 127.9 SPS. Eps 309. L400 Return -0.856957. Loss -11.856740 pg_loss -16.967686 baseline_loss 5.972814 entropy_loss -1.019984 reg_loss 0.158115\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:39,182] Steps 37760 @ 127.9 SPS. Eps 314. L400 Return -0.856050. Loss -0.663590 pg_loss -3.846395 baseline_loss 3.747576 entropy_loss -1.014515 reg_loss 0.449743\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:44,188] Steps 37760 @ 0.0 SPS. Eps 314. L400 Return -0.856050. Loss -0.663590 pg_loss -3.846395 baseline_loss 3.747576 entropy_loss -1.014515 reg_loss 0.449743\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:49,191] Steps 38400 @ 127.9 SPS. Eps 320. L400 Return -0.856249. Loss 40.569477 pg_loss 31.681108 baseline_loss 9.891941 entropy_loss -1.027695 reg_loss 0.024122\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:54,196] Steps 39040 @ 127.9 SPS. Eps 325. L400 Return -0.855384. Loss 16.148926 pg_loss 10.010054 baseline_loss 7.019708 entropy_loss -1.023715 reg_loss 0.142880\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:17:59,202] Steps 39040 @ 0.0 SPS. Eps 325. L400 Return -0.855384. Loss 16.148926 pg_loss 10.010054 baseline_loss 7.019708 entropy_loss -1.023715 reg_loss 0.142880\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:04,207] Steps 39680 @ 127.9 SPS. Eps 330. L400 Return -0.860605. Loss -10.228533 pg_loss -11.710054 baseline_loss 2.264692 entropy_loss -1.010742 reg_loss 0.227571\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:09,211] Steps 40320 @ 127.9 SPS. Eps 336. L400 Return -0.851785. Loss 33.926105 pg_loss 29.022139 baseline_loss 5.670977 entropy_loss -1.019694 reg_loss 0.252685\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:14,215] Steps 40320 @ 0.0 SPS. Eps 336. L400 Return -0.851785. Loss 33.926105 pg_loss 29.022139 baseline_loss 5.670977 entropy_loss -1.019694 reg_loss 0.252685\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:19,219] Steps 40960 @ 127.9 SPS. Eps 341. L400 Return -0.848093. Loss -0.087823 pg_loss -5.668242 baseline_loss 6.308942 entropy_loss -1.015955 reg_loss 0.287433\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:24,220] Steps 41600 @ 128.0 SPS. Eps 346. L400 Return -0.850288. Loss -20.365164 pg_loss -23.983822 baseline_loss 4.564250 entropy_loss -1.023583 reg_loss 0.077990\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:29,226] Steps 41600 @ 0.0 SPS. Eps 346. L400 Return -0.850288. Loss -20.365164 pg_loss -23.983822 baseline_loss 4.564250 entropy_loss -1.023583 reg_loss 0.077990\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:34,229] Steps 42240 @ 128.0 SPS. Eps 352. L400 Return -0.853408. Loss -33.542004 pg_loss -36.173607 baseline_loss 3.297844 entropy_loss -1.017620 reg_loss 0.351381\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:39,234] Steps 42240 @ 0.0 SPS. Eps 352. L400 Return -0.853408. Loss -33.542004 pg_loss -36.173607 baseline_loss 3.297844 entropy_loss -1.017620 reg_loss 0.351381\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:44,240] Steps 42880 @ 127.9 SPS. Eps 357. L400 Return -0.847058. Loss 9.896136 pg_loss 0.987740 baseline_loss 9.844116 entropy_loss -1.023803 reg_loss 0.088084\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:49,246] Steps 43520 @ 127.9 SPS. Eps 362. L400 Return -0.849171. Loss -10.770394 pg_loss -14.086454 baseline_loss 4.271349 entropy_loss -1.025056 reg_loss 0.069768\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:54,250] Steps 43520 @ 0.0 SPS. Eps 362. L400 Return -0.849171. Loss -10.770394 pg_loss -14.086454 baseline_loss 4.271349 entropy_loss -1.025056 reg_loss 0.069768\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:18:59,256] Steps 44160 @ 127.9 SPS. Eps 368. L400 Return -0.849456. Loss -44.089611 pg_loss -52.857948 baseline_loss 9.648859 entropy_loss -1.018801 reg_loss 0.138281\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:04,262] Steps 44800 @ 127.9 SPS. Eps 373. L400 Return -0.851474. Loss -29.676863 pg_loss -31.349926 baseline_loss 2.515140 entropy_loss -1.016604 reg_loss 0.174525\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:09,267] Steps 44800 @ 0.0 SPS. Eps 373. L400 Return -0.851474. Loss -29.676863 pg_loss -31.349926 baseline_loss 2.515140 entropy_loss -1.016604 reg_loss 0.174525\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:14,272] Steps 45440 @ 127.9 SPS. Eps 378. L400 Return -0.856084. Loss -35.254364 pg_loss -36.056934 baseline_loss 1.734100 entropy_loss -1.018751 reg_loss 0.087222\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:19,278] Steps 46080 @ 127.9 SPS. Eps 384. L400 Return -0.856249. Loss -43.750835 pg_loss -55.162071 baseline_loss 12.058686 entropy_loss -1.013376 reg_loss 0.365930\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:24,283] Steps 46080 @ 0.0 SPS. Eps 384. L400 Return -0.856249. Loss -43.750835 pg_loss -55.162071 baseline_loss 12.058686 entropy_loss -1.013376 reg_loss 0.365930\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:29,289] Steps 46720 @ 127.9 SPS. Eps 389. L400 Return -0.858097. Loss -16.614004 pg_loss -19.113884 baseline_loss 3.120312 entropy_loss -1.016686 reg_loss 0.396253\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:34,291] Steps 47360 @ 128.0 SPS. Eps 394. L400 Return -0.859898. Loss -9.312565 pg_loss -13.798652 baseline_loss 4.961243 entropy_loss -1.009431 reg_loss 0.534275\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:39,295] Steps 47360 @ 0.0 SPS. Eps 394. L400 Return -0.859898. Loss -9.312565 pg_loss -13.798652 baseline_loss 4.961243 entropy_loss -1.009431 reg_loss 0.534275\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:44,300] Steps 48000 @ 127.9 SPS. Eps 400. L400 Return -0.859999. Loss -13.454972 pg_loss -17.653688 baseline_loss 5.128255 entropy_loss -1.020525 reg_loss 0.090986\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:49,303] Steps 48000 @ 0.0 SPS. Eps 400. L400 Return -0.859999. Loss -13.454972 pg_loss -17.653688 baseline_loss 5.128255 entropy_loss -1.020525 reg_loss 0.090986\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:54,307] Steps 48640 @ 127.9 SPS. Eps 405. L400 Return -0.857499. Loss 27.354378 pg_loss 21.491171 baseline_loss 6.746939 entropy_loss -1.020806 reg_loss 0.137073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:289857 4259067192:129 2022-10-30 20:19:59,312] Saving checkpoint to /home/sc/RS/thinker/logs/torchbeast/torchbeast-20221030-200958/model.tar\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:19:59,323] Steps 49280 @ 127.6 SPS. Eps 410. L400 Return -0.859999. Loss -12.523324 pg_loss -13.647356 baseline_loss 1.952675 entropy_loss -1.017601 reg_loss 0.188958\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:04,329] Steps 49280 @ 0.0 SPS. Eps 410. L400 Return -0.859999. Loss -12.523324 pg_loss -13.647356 baseline_loss 1.952675 entropy_loss -1.017601 reg_loss 0.188958\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:09,334] Steps 49920 @ 127.9 SPS. Eps 416. L400 Return -0.859999. Loss 3.253895 pg_loss -0.860927 baseline_loss 5.045345 entropy_loss -1.021411 reg_loss 0.090888\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:14,340] Steps 50560 @ 127.9 SPS. Eps 421. L400 Return -0.824299. Loss 152.498505 pg_loss 102.970818 baseline_loss 50.416328 entropy_loss -1.020107 reg_loss 0.131465\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:19,346] Steps 50560 @ 0.0 SPS. Eps 421. L400 Return -0.824299. Loss 152.498505 pg_loss 102.970818 baseline_loss 50.416328 entropy_loss -1.020107 reg_loss 0.131465\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:24,351] Steps 51200 @ 127.9 SPS. Eps 426. L400 Return -0.814299. Loss 7.631362 pg_loss 2.955578 baseline_loss 5.510418 entropy_loss -1.020447 reg_loss 0.185813\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:29,355] Steps 51840 @ 127.9 SPS. Eps 432. L400 Return -0.804299. Loss 37.759037 pg_loss 32.222313 baseline_loss 6.488757 entropy_loss -1.021860 reg_loss 0.069825\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:34,360] Steps 51840 @ 0.0 SPS. Eps 432. L400 Return -0.804299. Loss 37.759037 pg_loss 32.222313 baseline_loss 6.488757 entropy_loss -1.021860 reg_loss 0.069825\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:39,366] Steps 52480 @ 127.9 SPS. Eps 437. L400 Return -0.794299. Loss 41.034229 pg_loss 36.676586 baseline_loss 5.301219 entropy_loss -1.021044 reg_loss 0.077465\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:44,371] Steps 53120 @ 127.9 SPS. Eps 442. L400 Return -0.796799. Loss 22.893423 pg_loss 16.619875 baseline_loss 6.879827 entropy_loss -1.018971 reg_loss 0.412692\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:49,376] Steps 53120 @ 0.0 SPS. Eps 442. L400 Return -0.796799. Loss 22.893423 pg_loss 16.619875 baseline_loss 6.879827 entropy_loss -1.018971 reg_loss 0.412692\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:54,379] Steps 53760 @ 128.0 SPS. Eps 448. L400 Return -0.796799. Loss 77.426323 pg_loss 65.278236 baseline_loss 12.601042 entropy_loss -1.009264 reg_loss 0.556308\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:20:59,383] Steps 53760 @ 0.0 SPS. Eps 448. L400 Return -0.796799. Loss 77.426323 pg_loss 65.278236 baseline_loss 12.601042 entropy_loss -1.009264 reg_loss 0.556308\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:04,388] Steps 54400 @ 127.9 SPS. Eps 453. L400 Return -0.801799. Loss 9.027239 pg_loss 4.962562 baseline_loss 4.936876 entropy_loss -1.021518 reg_loss 0.149321\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:09,391] Steps 55040 @ 128.0 SPS. Eps 458. L400 Return -0.796799. Loss 25.520369 pg_loss 19.017221 baseline_loss 7.437948 entropy_loss -1.021896 reg_loss 0.087095\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:14,396] Steps 55040 @ 0.0 SPS. Eps 458. L400 Return -0.796799. Loss 25.520369 pg_loss 19.017221 baseline_loss 7.437948 entropy_loss -1.021896 reg_loss 0.087095\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:19,402] Steps 55680 @ 127.9 SPS. Eps 464. L400 Return -0.794299. Loss 19.216438 pg_loss 10.330467 baseline_loss 9.795757 entropy_loss -1.019884 reg_loss 0.110098\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:24,408] Steps 56320 @ 127.9 SPS. Eps 469. L400 Return -0.791799. Loss 10.437438 pg_loss 6.980256 baseline_loss 4.192323 entropy_loss -1.016797 reg_loss 0.281656\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:29,413] Steps 56320 @ 0.0 SPS. Eps 469. L400 Return -0.791799. Loss 10.437438 pg_loss 6.980256 baseline_loss 4.192323 entropy_loss -1.016797 reg_loss 0.281656\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:34,419] Steps 56960 @ 127.9 SPS. Eps 474. L400 Return -0.781799. Loss 21.064131 pg_loss 17.070030 baseline_loss 4.734071 entropy_loss -1.015288 reg_loss 0.275316\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:39,424] Steps 57600 @ 127.9 SPS. Eps 480. L400 Return -0.781799. Loss 3.093465 pg_loss -1.336849 baseline_loss 5.412776 entropy_loss -1.025378 reg_loss 0.042916\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:44,430] Steps 57600 @ 0.0 SPS. Eps 480. L400 Return -0.781799. Loss 3.093465 pg_loss -1.336849 baseline_loss 5.412776 entropy_loss -1.025378 reg_loss 0.042916\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:49,435] Steps 58240 @ 127.9 SPS. Eps 485. L400 Return -0.774299. Loss 6.681773 pg_loss 1.728024 baseline_loss 5.746025 entropy_loss -1.022327 reg_loss 0.230051\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:54,439] Steps 58880 @ 127.9 SPS. Eps 490. L400 Return -0.774299. Loss -15.926696 pg_loss -21.803831 baseline_loss 6.850280 entropy_loss -1.024808 reg_loss 0.051663\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:21:59,444] Steps 58880 @ 0.0 SPS. Eps 490. L400 Return -0.774299. Loss -15.926696 pg_loss -21.803831 baseline_loss 6.850280 entropy_loss -1.024808 reg_loss 0.051663\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:04,450] Steps 59520 @ 127.9 SPS. Eps 496. L400 Return -0.776799. Loss -40.727528 pg_loss -43.182518 baseline_loss 3.299485 entropy_loss -1.020518 reg_loss 0.176026\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:09,456] Steps 60160 @ 127.9 SPS. Eps 501. L400 Return -0.776799. Loss -23.303185 pg_loss -29.660208 baseline_loss 6.949826 entropy_loss -1.012863 reg_loss 0.420060\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:14,459] Steps 60160 @ 0.0 SPS. Eps 501. L400 Return -0.776799. Loss -23.303185 pg_loss -29.660208 baseline_loss 6.949826 entropy_loss -1.012863 reg_loss 0.420060\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:19,464] Steps 60800 @ 127.9 SPS. Eps 506. L400 Return -0.776799. Loss -24.184917 pg_loss -28.785961 baseline_loss 5.419155 entropy_loss -1.018351 reg_loss 0.200239\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:24,470] Steps 60800 @ 0.0 SPS. Eps 506. L400 Return -0.776799. Loss -24.184917 pg_loss -28.785961 baseline_loss 5.419155 entropy_loss -1.018351 reg_loss 0.200239\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:29,475] Steps 61440 @ 127.9 SPS. Eps 512. L400 Return -0.766799. Loss 19.736053 pg_loss 7.426834 baseline_loss 13.175159 entropy_loss -1.020996 reg_loss 0.155056\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:34,481] Steps 62080 @ 127.9 SPS. Eps 517. L400 Return -0.771799. Loss -50.087143 pg_loss -50.406685 baseline_loss 1.307593 entropy_loss -1.025610 reg_loss 0.037560\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:39,487] Steps 62080 @ 0.0 SPS. Eps 517. L400 Return -0.771799. Loss -50.087143 pg_loss -50.406685 baseline_loss 1.307593 entropy_loss -1.025610 reg_loss 0.037560\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:44,492] Steps 62720 @ 127.9 SPS. Eps 522. L400 Return -0.771799. Loss -26.433384 pg_loss -36.197403 baseline_loss 10.677715 entropy_loss -1.022671 reg_loss 0.108974\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:49,495] Steps 63360 @ 127.9 SPS. Eps 528. L400 Return -0.771799. Loss -10.328542 pg_loss -16.263279 baseline_loss 6.877376 entropy_loss -1.021913 reg_loss 0.079274\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:54,500] Steps 63360 @ 0.0 SPS. Eps 528. L400 Return -0.771799. Loss -10.328542 pg_loss -16.263279 baseline_loss 6.877376 entropy_loss -1.021913 reg_loss 0.079274\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:22:59,506] Steps 64000 @ 127.9 SPS. Eps 533. L400 Return -0.769299. Loss -27.760406 pg_loss -30.399904 baseline_loss 3.610828 entropy_loss -1.024547 reg_loss 0.053217\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:23:04,512] Steps 64640 @ 127.9 SPS. Eps 538. L400 Return -0.764299. Loss 0.780909 pg_loss -11.063283 baseline_loss 12.612442 entropy_loss -1.015188 reg_loss 0.246938\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:23:09,515] Steps 64640 @ 0.0 SPS. Eps 538. L400 Return -0.764299. Loss 0.780909 pg_loss -11.063283 baseline_loss 12.612442 entropy_loss -1.015188 reg_loss 0.246938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:289857 4259067192:168 2022-10-30 20:23:14,520] Steps 65280 @ 127.9 SPS. Eps 544. L400 Return -0.754299. Loss 2.859816 pg_loss -3.571741 baseline_loss 7.306978 entropy_loss -1.019160 reg_loss 0.143739\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:23:19,523] Steps 65280 @ 0.0 SPS. Eps 544. L400 Return -0.754299. Loss 2.859816 pg_loss -3.571741 baseline_loss 7.306978 entropy_loss -1.019160 reg_loss 0.143739\n",
      "[INFO:289857 4259067192:168 2022-10-30 20:23:24,527] Steps 65920 @ 127.9 SPS. Eps 549. L400 Return -0.749299. Loss 31.629723 pg_loss 21.296101 baseline_loss 11.223036 entropy_loss -1.020484 reg_loss 0.131072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_289857/4259067192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_289857/4259067192.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0;31m# Try joining actors then quit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mp.set_sharing_strategy('file_system')\n",
    "\n",
    "if flags.xpid is None:\n",
    "    flags.xpid = \"torchbeast-%s\" % time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "plogger = file_writer.FileWriter(\n",
    "    xpid=flags.xpid, xp_args=flags.__dict__, rootdir=flags.savedir\n",
    ")\n",
    "\n",
    "flags.device = None\n",
    "if not flags.disable_cuda and torch.cuda.is_available(): \n",
    "    logging.info(\"Using CUDA.\")\n",
    "    flags.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logging.info(\"Not using CUDA.\")\n",
    "    flags.device = torch.device(\"cpu\")\n",
    "\n",
    "checkpointpath = os.path.expandvars(\n",
    "    os.path.expanduser(\"%s/%s/%s\" % (flags.savedir, flags.xpid, \"model.tar\"))\n",
    ")\n",
    "\n",
    "if flags.num_buffers is None:  # Set sensible default for num_buffers.\n",
    "    flags.num_buffers = max(2 * flags.num_actors, flags.batch_size)\n",
    "if flags.num_actors >= flags.num_buffers:\n",
    "    raise ValueError(\"num_buffers should be larger than num_actors\")\n",
    "if flags.num_buffers < flags.batch_size:\n",
    "    raise ValueError(\"num_buffers should be larger than batch_size\")\n",
    "\n",
    "T = flags.unroll_length\n",
    "B = flags.batch_size\n",
    "\n",
    "env = create_env(flags)\n",
    "\n",
    "actor_net = Actor_Wrapper(flags, model_actor, actor=None, rec_t=flags.rec_t)\n",
    "buffers = create_buffers(flags, env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "actor_net.share_memory()\n",
    "\n",
    "# Add initial RNN state.\n",
    "initial_agent_state_buffers = []\n",
    "for _ in range(flags.num_buffers):\n",
    "    state = actor_net.initial_state(batch_size=1)\n",
    "    for t in state:\n",
    "        t.share_memory_()\n",
    "    initial_agent_state_buffers.append(state)\n",
    "\n",
    "actor_processes = []\n",
    "ctx = mp.get_context()\n",
    "free_queue = ctx.SimpleQueue()\n",
    "full_queue = ctx.SimpleQueue()\n",
    "\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=act, args=(flags, i, free_queue, full_queue,\n",
    "            actor_net, buffers, initial_agent_state_buffers,),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)\n",
    "\n",
    "learner_net = Actor_Wrapper(flags, model_learner, actor=None, \n",
    "                            rec_t=flags.rec_t).to(device=flags.device)\n",
    "\n",
    "if not flags.disable_adam:\n",
    "    print(\"Using Adam...\")        \n",
    "    optimizer = torch.optim.Adam(learner_net.actor.parameters(),lr=flags.learning_rate)\n",
    "else:\n",
    "    print(\"Using RMS Prop...\")\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        learner_net.actor.parameters(),\n",
    "        lr=flags.learning_rate,\n",
    "        momentum=flags.momentum,\n",
    "        eps=flags.epsilon,\n",
    "        alpha=flags.alpha,)\n",
    "print(\"All parameters: \")\n",
    "for k, v in learner_net.named_parameters(): print(k, v.numel())    \n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    return 1 - min(epoch * T * B, flags.total_steps) / flags.total_steps\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "logger = logging.getLogger(\"logfile\")\n",
    "stat_keys = [\"mean_episode_return\", \"episode_returns\", \"total_loss\",\n",
    "    \"pg_loss\", \"baseline_loss\", \"entropy_loss\"]\n",
    "logger.info(\"# Step\\t%s\", \"\\t\".join(stat_keys))\n",
    "\n",
    "step, stats, last_returns, tot_eps = 0, {}, deque(maxlen=400), 0\n",
    "\n",
    "def batch_and_learn(i, lock=threading.Lock()):\n",
    "    \"\"\"Thread target for the learning process.\"\"\"\n",
    "    #nonlocal step, stats, last_returns, tot_eps\n",
    "    global step, stats, last_returns, tot_eps\n",
    "    timings = prof.Timings()\n",
    "    while step < flags.total_steps:\n",
    "        timings.reset()\n",
    "        batch, agent_state = get_batch(flags, free_queue, full_queue, buffers,\n",
    "            initial_agent_state_buffers, timings,)\n",
    "        stats = learn(flags, actor_net, learner_net, batch, agent_state, optimizer, \n",
    "            scheduler)\n",
    "        if flags.gb_ste:\n",
    "            learner_net.actor.temp = torch.tensor(np.maximum(flags.gb_ste_temp_max * \n",
    "                    np.exp(-step/flags.total_steps), flags.gb_ste_temp_min).item(), \n",
    "                    dtype=torch.float32, device=learner_net.actor.temp.device)\n",
    "        last_returns.extend(stats[\"episode_returns\"])\n",
    "        tot_eps = tot_eps + len(stats[\"episode_returns\"])\n",
    "        timings.time(\"learn\")\n",
    "        with lock:\n",
    "            to_log = dict(step=step)\n",
    "            to_log.update({k: stats[k] for k in stat_keys})\n",
    "            to_log.update({\"trail_mean_episode_return\": np.average(last_returns) if len(last_returns) > 0 else 0.,\n",
    "                           \"episode\": tot_eps})\n",
    "            plogger.log(to_log)\n",
    "            step += T * B\n",
    "\n",
    "    if i == 0:\n",
    "        logging.info(\"Batch and learn: %s\", timings.summary())\n",
    "\n",
    "for m in range(flags.num_buffers):\n",
    "    free_queue.put(m)\n",
    "\n",
    "threads = []\n",
    "for i in range(flags.num_learner_threads):\n",
    "    thread = threading.Thread(\n",
    "        target=batch_and_learn, name=\"batch-and-learn-%d\" % i, args=(i,)\n",
    "    )\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "def checkpoint():\n",
    "    if flags.disable_checkpoint:\n",
    "        return\n",
    "    logging.info(\"Saving checkpoint to %s\", checkpointpath)\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": actor_net.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"flags\": vars(flags),\n",
    "        },\n",
    "        checkpointpath,\n",
    "    )\n",
    "\n",
    "timer = timeit.default_timer\n",
    "try:\n",
    "    last_checkpoint_time = timer()\n",
    "    while step < flags.total_steps:\n",
    "        start_step = step\n",
    "        start_time = timer()\n",
    "        time.sleep(5)\n",
    "\n",
    "        if timer() - last_checkpoint_time > 10 * 60:  # Save every 10 min.\n",
    "            checkpoint()\n",
    "            last_checkpoint_time = timer()\n",
    "\n",
    "        sps = (step - start_step) / (timer() - start_time)\n",
    "        if stats.get(\"episode_returns\", None):\n",
    "            mean_return = (\n",
    "                \"Return per episode: %.1f. \" % stats[\"mean_episode_return\"]\n",
    "            )\n",
    "        else:\n",
    "            mean_return = \"\"\n",
    "        total_loss = stats.get(\"total_loss\", float(\"inf\"))\n",
    "\n",
    "        print_str =  \"Steps %i @ %.1f SPS. Eps %i. L400 Return %f. Loss %f\" % (step, sps, tot_eps, \n",
    "            np.average(last_returns) if len(last_returns) > 0 else 0., total_loss)\n",
    "\n",
    "        for s in [\"pg_loss\", \"baseline_loss\", \"entropy_loss\", \"reg_loss\"]:\n",
    "            if s in stats:\n",
    "                print_str += \" %s %f\" % (s, stats[s])\n",
    "\n",
    "        logging.info(print_str)\n",
    "except KeyboardInterrupt:\n",
    "    for thread in threads:\n",
    "        thread.join()        \n",
    "    # Try joining actors then quit.\n",
    "else:\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    logging.info(\"Learning finished after %d steps.\", step)\n",
    "finally:\n",
    "    for _ in range(flags.num_actors):\n",
    "        free_queue.put(None)\n",
    "    for actor in actor_processes:\n",
    "        actor.join(timeout=1)\n",
    "\n",
    "checkpoint()\n",
    "plogger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a399b",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = flags.total_steps + 1\n",
    "for thread in threads:\n",
    "     thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(flags.num_actors):\n",
    "    free_queue.put(None)\n",
    "for actor in actor_processes:\n",
    "    actor.join(timeout=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3413ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": actor_net.state_dict(),},\"base/actor_net_tmp_2.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug trainned actor\n",
    "\n",
    "class Actor_Wrapper(nn.Module):    \n",
    "    def __init__(self, flags, model, actor=None, rec_t=5):\n",
    "        \n",
    "        super(Actor_Wrapper, self).__init__()   \n",
    "        self.model = model\n",
    "        self.num_actions = model.num_actions        \n",
    "        self.rec_t = rec_t\n",
    "        self.obs_shape = (3 * num_actions + 4 + self.rec_t, 1, 1)\n",
    "        if actor is None:\n",
    "            self.actor = Actor_net(obs_shape=self.obs_shape, num_actions=self.num_actions, flags=flags)\n",
    "        else:\n",
    "            self.actor = actor   \n",
    "            \n",
    "    def initial_state(self, batch_size):\n",
    "        return self.actor.initial_state(batch_size)\n",
    "            \n",
    "    def forward(self, x, core_state=None):                      \n",
    "        # x is env_output object with:\n",
    "        # frame: T x B x C x H x W\n",
    "        # last_action: T x B\n",
    "        # reward: T x B\n",
    "        \n",
    "        tot_step, bsz, _, _, _ = x['frame'].shape\n",
    "        device = x['frame'].device        \n",
    "        self.model.train(False)\n",
    "        \n",
    "        for step in range(tot_step):\n",
    "        \n",
    "            state = x['frame'][step]        \n",
    "            action = F.one_hot(x['last_action'][step], self.num_actions)\n",
    "            reward = x['reward'][step]   \n",
    "            done = x['done'][step]\n",
    "            \n",
    "            rs, vs, logits, encodeds = self.model(state, action.unsqueeze(0), one_hot=True)\n",
    "            encoded_reset = encodeds[0].clone()\n",
    "            encoded = encodeds[0]\n",
    "\n",
    "            for t in range(self.rec_t):\n",
    "                \n",
    "                a = action if t == 0 else im_action  \n",
    "                r = (reward if t == 0 else rs[-1]).unsqueeze(-1)\n",
    "                v = vs[-1].unsqueeze(-1)\n",
    "                logit = logits[-1]                    \n",
    "                \n",
    "                if t == 0:\n",
    "                    r0 = r.clone()\n",
    "                    v0 = v.clone()\n",
    "                    logit0 = logit.clone()\n",
    "                    #print(\"p0: \\n\", F.softmax(logit, dim=-1).cpu().detach().numpy())\n",
    "                \n",
    "                time = F.one_hot(torch.tensor([t], device=device).long(), self.rec_t).tile([bsz, 1])                        \n",
    "\n",
    "                actor_input = torch.concat([a, r, v, logit, r0, v0, logit0, time], dim=-1)     \n",
    "                actor_input = actor_input.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "                actor_output, core_state = self.actor(actor_input, done=done, core_state=core_state)\n",
    "                \n",
    "                if t == self.rec_t - 1:\n",
    "                    if step == 0:\n",
    "                        all_actor_output = {k: [v.unsqueeze(0)] for k, v in actor_output.items()}\n",
    "                    else:\n",
    "                        for k, v in actor_output.items(): all_actor_output[k].append(v.unsqueeze(0))\n",
    "                \n",
    "                reset = actor_output[\"reset\"]\n",
    "                im_action = actor_output[\"im_action\"]\n",
    "                \n",
    "                np.set_printoptions(suppress=True)    \n",
    "                #print(\"im_action: \\n\", actor_output[\"im_action\"].cpu().detach().numpy())\n",
    "                #print(\"reset: \", reset.cpu().detach().numpy())\n",
    "\n",
    "                if t < self.rec_t - 1:                \n",
    "                    rs, vs, logits, encodeds = self.model.forward_encoded(encoded, im_action.unsqueeze(0), \n",
    "                                                                          one_hot=True)\n",
    "                    reset = reset.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "                    encoded = reset * encoded_reset + (1 - reset) * encodeds[-1]\n",
    "        \n",
    "        all_actor_output = {k: torch.concat(v) for k, v in all_actor_output.items()}\n",
    "        #ret_keys = [\"policy_logits\", \"baseline\", \"action\", \"model_loss\", \"reg_loss\"]\n",
    "        #all_actor_output = {k: all_actor_output[k] for k in ret_keys}\n",
    "        return all_actor_output, core_state    \n",
    "\n",
    "    \n",
    "bsz = 16\n",
    "eps_n = 500\n",
    "    \n",
    "actor_net = Actor_Wrapper(flags, model_actor, actor=None, rec_t=flags.rec_t)\n",
    "checkpoint = torch.load(\"base/actor_net_tmp.tar\")\n",
    "actor_net.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "actor_net.train(False)    \n",
    "actor_net.to(flags.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f74fdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "core_state = actor_net.initial_state(bsz)\n",
    "core_state = tuple(v.to(flags.device) for v in core_state)\n",
    "returns = []\n",
    "obs = env.initial()\n",
    "\n",
    "while(len(returns) <= eps_n):\n",
    "#for t in range(5):\n",
    "    #print(\"t: %d\" %t)    \n",
    "    with torch.no_grad():\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}    \n",
    "        actor_out, core_state = actor_net(obs, core_state)\n",
    "        obs = env.step(actor_out[\"action\"][0])\n",
    "        #print(\"p_sel: \\n\", F.softmax(actor_out[\"policy_logits\"], dim=-1).detach().numpy())\n",
    "        #print(\"action: \", actor_out[\"action\"][0])\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "\n",
    "returns = returns[:eps_n]            \n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "      np.average(returns), np.std(returns) / np.sqrt(len(returns))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e82488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model to another one\n",
    "\n",
    "parser = define_parser()        \n",
    "flags = parser.parse_args(\"--env Sokoban-v0\".split())        \n",
    "flags.device = torch.device(\"cuda\")\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"base/model_r2.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "model.train(False)\n",
    "actor_net.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda48bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing actor wrapper with a one-step greedy actor\n",
    "\n",
    "class Actor_test(nn.Module):\n",
    "    # A one-step greedy actor (for testing only)\n",
    "    def __init__(self, obs_shape, num_actions, rec_t):\n",
    "        super(Actor_test, self).__init__()   \n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions\n",
    "        self.rec_t = rec_t\n",
    "        self.rs = torch.zeros(num_actions,device=flags.device)\n",
    "        self.vs = []\n",
    "    \n",
    "    def forward(self, x, done, core_state=None):\n",
    "        x = x[:, :, 0, 0]        \n",
    "        #print(\"Input: \", x[0])\n",
    "        bsz = x.shape[0]\n",
    "        time = x[0, -self.rec_t:]\n",
    "        t = torch.argmax(time, dim=0)        \n",
    "        \n",
    "        if t == 0:\n",
    "            self.rs = torch.zeros(rec_t, bsz, device=x.device)\n",
    "            self.vs = torch.zeros(rec_t, bsz, device=x.device)\n",
    "        \n",
    "        self.rs[t] = x[:,self.num_actions]\n",
    "        self.vs[t] = x[:,self.num_actions+1]  \n",
    "        \n",
    "        im_action = t if t < self.num_actions else torch.zeros(1, device=x.device).long()\n",
    "        im_action = F.one_hot(im_action.unsqueeze(0), self.num_actions).tile([bsz, 1])\n",
    "        reset = torch.ones(bsz, device=x.device)\n",
    "        \n",
    "        policy_logits = (self.rs[1:1+self.num_actions] + flags.discounting * self.vs[1:1+self.num_actions]).transpose(0, 1) \n",
    "        policy_probs = F.softmax(20 * policy_logits, dim=1)\n",
    "        action = torch.multinomial(policy_probs, num_samples=1)\n",
    "        \n",
    "        ret = {\"im_action\": im_action, \n",
    "               \"reset\": reset, \n",
    "               \"policy_logits\": policy_logits, \n",
    "               \"policy_probs\": policy_probs, \n",
    "               \"action\": action,\n",
    "               \"baseline\": torch.zeros_like(reset),\n",
    "               \"reg_loss\": torch.zeros_like(reset)}\n",
    "        \n",
    "        return (ret, None)\n",
    "\n",
    "model = model_actor    \n",
    "model.train(False)    \n",
    "    \n",
    "rec_t = 6        \n",
    "actor = Actor_test(obs_shape=(model.num_actions + 2 + rec_t), num_actions=model.num_actions, rec_t=rec_t)        \n",
    "actor_wrapper = Actor_Wrapper(None, model, actor, rec_t = 6)\n",
    "        \n",
    "bsz = 2\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "obs = env.initial()\n",
    "state = obs['frame'][0].to(flags.device).clone()\n",
    "action = torch.zeros(bsz).long().to(flags.device)\n",
    "\n",
    "#obs = {k:v.to(flags.device).view((1, 10)+v.shape[2:]) for k, v in obs.items()}\n",
    "obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "actor_output, _ = actor_wrapper(obs)\n",
    "print(F.softmax(20 * actor_output['policy_logits'], dim=-1))\n",
    "action, prob, q_ret = n_step_greedy_model(state, action, model, 1, encoded=None, temp=20.)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201420df",
   "metadata": {},
   "source": [
    "<font size=\"5\">Misc.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch_m(flags, buffers)\n",
    "print(torch.max(batch[\"reward\"]), (torch.max(batch[\"reward\"]) == batch[\"reward\"]).nonzero())\n",
    "print(batch[\"done\"].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ce789",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"reward\"][:, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG LOSS\n",
    "\n",
    "#batch = get_batch_m(flags, buffers)\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "logits = logits[:-1]\n",
    "\n",
    "target_rewards = batch['reward'][1:]\n",
    "target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "target_vs = []\n",
    "target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0]    \n",
    "\n",
    "for t in range(vs.shape[0]-1, 0, -1):\n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))\n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "# if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "# rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "# vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "# logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "done_masks = []\n",
    "done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "for t in range(vs.shape[0]):\n",
    "    done = torch.logical_or(done, batch['done'][t])\n",
    "    done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "\n",
    "# compute final loss\n",
    "huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "rs_loss = torch.sum(huberloss(rs, target_rewards) * (~done_masks).float())\n",
    "#rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "vs_loss = torch.sum(huberloss(vs[:-1], target_vs) * (~done_masks).float())\n",
    "#vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "logits_loss = compute_cross_entropy_loss(logits, target_logits, done_masks)\n",
    "\n",
    "# debug\n",
    "ind = 21\n",
    "\n",
    "target_vs = []\n",
    "target_v = vs[-1]\n",
    "for t in range(vs.shape[0]-1, 0, -1):        \n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    print(t, \n",
    "          \"reward %2f\" % batch['reward'][t,ind].item(), \n",
    "          \"bootstrap %2f\" % (target_v * (~batch['done'][t]).float())[ind].item(), \n",
    "          \"truncated %2f\" % (vs[t-1] * (batch['truncated_done'][t]).float())[ind].item(),\n",
    "          \"vs[t-1] %2f\" % vs[t-1][ind].item(),\n",
    "          \"new_targ %2f\" % new_target_v[ind].item())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))    \n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)   \n",
    "print(\"done\", batch[\"done\"][:, ind])\n",
    "print(\"done_masks\", done_masks[:, ind])\n",
    "print(\"vs: \", vs[:, ind])\n",
    "print(\"target_vs: \", target_vs[:, ind])\n",
    "print(\"reward: \", rs[:, ind])\n",
    "print(\"target_reward: \", target_rewards[:, ind])\n",
    "print(\"logits: \", logits[:, ind])\n",
    "print(\"target_logits: \", target_logits[:, ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt. version of computing loss by treading terminal state as absorbing state (as in MuZero)\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_logits = batch['policy_logits'][1:].clone()\n",
    "    target_rewards = batch['reward'][1:].clone()\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "\n",
    "    c_logits = target_logits[0]\n",
    "    c_state = batch['frame'][0]\n",
    "    for t in range(vs.shape[0]-1):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        c_logits = torch.where(done.unsqueeze(-1), c_logits, target_logits[t])\n",
    "        target_logits[t] = c_logits\n",
    "        c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][t])  \n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "    done_masks = torch.concat(done_masks, dim=0)\n",
    "    done = torch.logical_or(done, batch['done'][-1])\n",
    "    c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][-1])\n",
    "    target_rewards = target_rewards * (~done_masks).float()\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(c_state, batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * target_v\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()))\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()))\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), None)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
