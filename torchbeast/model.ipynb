{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc8a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core.environment import Environment, Vec_Environment\n",
    "from torchbeast.atari_wrappers import SokobanWrapper\n",
    "from torchbeast.base import BaseNet\n",
    "from torchbeast.train import create_env\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "def get_param(net, name=None):\n",
    "    keys = []\n",
    "    for (k, v) in actor_wrapper.model.named_parameters(): \n",
    "        if name is None:\n",
    "            print(k)\n",
    "        else:\n",
    "            if name == k: return v\n",
    "        keys.append(k)\n",
    "    return keys        \n",
    "\n",
    "def n_step_greedy(env, net, n, temp=10.):    \n",
    "    if isinstance(env, Vec_Environment):\n",
    "        num_actions = env.gym_env.action_space[0].n\n",
    "        bsz = len(env.gym_env.envs)\n",
    "    else:\n",
    "        num_actions = env.gym_env.action_space.n\n",
    "        bsz = 1\n",
    "\n",
    "    q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "    state = env.clone_state()\n",
    "\n",
    "    for act in range(num_actions):\n",
    "        obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "        \n",
    "        if n > 1:\n",
    "            action, prob, sub_q_ret = n_step_greedy(env, net, n-1)\n",
    "            ret = obs['reward'] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] * (~obs['done']).float()\n",
    "        else:\n",
    "            ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "\n",
    "        q_ret[:, act] = ret\n",
    "        env.restore_state(state)\n",
    "    \n",
    "    prob = F.softmax(temp*q_ret, dim=1)\n",
    "    action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893df32e",
   "metadata": {},
   "source": [
    "<font size=\"5\">Testing planning algo. for perfect model with bootstrapped values</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51bf8c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1095814\n",
      "Testing 2 step planning\n",
      "Finish 16 episode: avg. return: 0.36 (+-0.22) \n",
      "Finish 32 episode: avg. return: 0.46 (+-0.19) \n",
      "Finish 48 episode: avg. return: 0.59 (+-0.14) \n",
      "Finish 64 episode: avg. return: 0.61 (+-0.11) \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25704/3626975367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mall_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_n_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time required for %d step planning: %f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/3626975367.py\u001b[0m in \u001b[0;36mtest_n_step\u001b[0;34m(n, net, env, temp)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_step_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/706465574.py\u001b[0m in \u001b[0;36mn_step_greedy\u001b[0;34m(env, net, n, temp)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_q_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_step_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscounting\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_q_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/706465574.py\u001b[0m in \u001b[0;36mn_step_greedy\u001b[0;34m(env, net, n, temp)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscounting\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_q_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscounting\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'baseline'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mq_ret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RS/thinker/torchbeast/torchbeast/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, core_state, debug)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0menv_input_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_conv_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mcore_output_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_input_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore_output_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Synchronous version of testing \n",
    "\n",
    "def test_n_step(n, net, env, temp=10.):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    returns = []\n",
    "    obs = env.initial()\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}\n",
    "        net_out, core_state = net(obs)            \n",
    "        if n == 0:\n",
    "            action = net_out[\"action\"][0]\n",
    "        else:\n",
    "            action, _, _ = n_step_greedy(env, net, n, temp)\n",
    "        obs = env.step(action)\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns\n",
    "\n",
    "bsz = 16    \n",
    "eps_n = 500\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# create environments\n",
    "\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "num_actions = env.gym_env.action_space[0].n\n",
    "\n",
    "# import the net\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "flags.discounting = 0.97\n",
    "temp = 5\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=num_actions, flags=flags)  \n",
    "net = net.to(device)\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_2.tar\", map_location=\"cuda\")\n",
    "#checkpoint = torch.load(\"/home/schk/RS/thinker/logs/base/torchbeast-20221105-033530/model.tar\", map_location=\"cuda\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "# initialize net\n",
    "\n",
    "core_state = net.initial_state(batch_size=bsz)\n",
    "core_state = tuple(v.to(device) for v in core_state)\n",
    "net.train(False)\n",
    "\n",
    "all_returns = {}\n",
    "for n in range(2,3):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step(n, net, env, temp)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13ffb0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1095814\n",
      "Testing 0 step planning\n",
      "Finish 500 episode: avg. return: 0.27 (+-0.04)\n",
      "Time required for 0 step planning: 12.629324\n",
      "Testing 1 step planning\n",
      "Finish 502 episode: avg. return: 0.51 (+-0.04)\n",
      "Time required for 1 step planning: 74.194364\n",
      "Testing 2 step planning\n",
      "Finish 500 episode: avg. return: 0.74 (+-0.04)\n",
      "Time required for 2 step planning: 339.732901\n",
      "Testing 3 step planning\n",
      "Finish 500 episode: avg. return: 0.76 (+-0.04)\n",
      "Time required for 3 step planning: 1695.472523\n"
     ]
    }
   ],
   "source": [
    "# Asynchronous version of testing \n",
    "\n",
    "def act_m(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    returns: Manager().list,\n",
    "    eps_n: int,\n",
    "    n: int,\n",
    "    temp: float,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        net_out, unused_state = net(env_output, agent_state)      \n",
    "        while True:            \n",
    "            if len(returns) >= eps_n: break\n",
    "            with torch.no_grad():\n",
    "                net_out, agent_state = net(env_output, agent_state)                            \n",
    "            if n == 0:\n",
    "                action = net_out[\"action\"]\n",
    "            else:\n",
    "                action, _, _ = n_step_greedy(env, net, n, temp)            \n",
    "            env_output = env.step(action)           \n",
    "            if env_output['done']: returns.append(ret)\n",
    "            ret = env_output['episode_return'].item()        \n",
    "        #logging.info(\"Actor %i end\", actor_index)\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "def asy_test_n_step(n, net, flags, temp):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    mp.set_sharing_strategy('file_system')\n",
    "    net.share_memory()\n",
    "    ctx = mp.get_context()        \n",
    "    returns = Manager().list()\n",
    "\n",
    "    actor_processes = []\n",
    "    for i in range(flags.num_actors):\n",
    "        actor = ctx.Process(target=act_m, args=(flags, i, net, returns, eps_n, n, temp),)\n",
    "        actor.start()\n",
    "        actor_processes.append(actor)    \n",
    "\n",
    "    for actor in actor_processes:\n",
    "        actor.join()    \n",
    "\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f)\" % (len(returns),\n",
    "                    np.average(returns), np.std(returns) / np.sqrt(len(returns)),))        \n",
    "    return returns        \n",
    "        \n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.discounting = 0.97     \n",
    "flags.num_actors = 32\n",
    "bsz = 1\n",
    "eps_n = 500\n",
    "temp = 5\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "net = net.to(\"cpu\")\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_2.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "all_returns = {}\n",
    "for n in range(4):\n",
    "    t = time.time()\n",
    "    all_returns[n] = asy_test_n_step(n, net, flags, temp)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abac9a",
   "metadata": {},
   "source": [
    "Results (base_1.tar):\n",
    "    \n",
    "Testing 0 step planning <br>\n",
    "Finish 512 episode: avg. return: 0.12 (+-0.06) <br>\n",
    "Testing 1 step planning <br>\n",
    "Finish 502 episode: avg. return: 0.61 (+-0.04) <br>\n",
    "Testing 2 step planning <br>\n",
    "Finish 501 episode: avg. return: 0.92 (+-0.04) <br>\n",
    "Testing 3 step planning <br>\n",
    "Finish 501 episode: avg. return: 1.01 (+-0.04) <br>\n",
    "\n",
    "Results (base_2.tar):\n",
    "Testing 0 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.27 (+-0.04) <br>\n",
    "Time required for 0 step planning: 12.629324 <br>\n",
    "Testing 1 step planning <br>\n",
    "Finish 502 episode: avg. return: 0.51 (+-0.04) <br>\n",
    "Time required for 1 step planning: 74.194364 <br>\n",
    "Testing 2 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.74 (+-0.04) <br>\n",
    "Time required for 2 step planning: 339.732901 <br>\n",
    "Testing 3 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.76 (+-0.04) <br>\n",
    "Time required for 3 step planning: 1695.472523 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097bd18",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model Training Phase</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "968043ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data for learning model [RUN]\n",
    "\n",
    "Buffers = typing.Dict[str, typing.List[torch.Tensor]]\n",
    "\n",
    "def create_buffers_m(flags, obs_shape, num_actions) -> Buffers:\n",
    "    \n",
    "    seq_len = flags.seq_len\n",
    "    seq_n = flags.seq_n\n",
    "    specs = dict(\n",
    "        frame=dict(size=(seq_len + 1, *obs_shape), dtype=torch.uint8),\n",
    "        reward=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        truncated_done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        episode_return=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        episode_step=dict(size=(seq_len + 1,), dtype=torch.int32),\n",
    "        policy_logits=dict(size=(seq_len + 1, num_actions), dtype=torch.float32),\n",
    "        baseline=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        last_action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        reg_loss=dict(size=(seq_len + 1,), dtype=torch.float32)\n",
    "    )\n",
    "    buffers: Buffers = {key: [] for key in specs}\n",
    "    for _ in range(seq_n):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "            \n",
    "    return buffers\n",
    "\n",
    "def gen_data(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    buffers: Buffers,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = net(env_output, agent_state)     \n",
    "        \n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break         \n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:\n",
    "                buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                buffers[key][index][0, ...] = agent_output[key]\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.seq_len):\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = net(env_output, agent_state)\n",
    "                env_output = env.step(agent_output[\"action\"])\n",
    "                for key in env_output:\n",
    "                    buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "        \n",
    "\n",
    "# Models\n",
    "\n",
    "DOWNSCALE_C = 2\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation,\n",
    "        groups=groups, bias=False, dilation=dilation,)\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, inplanes, outplanes=None):\n",
    "        super().__init__()\n",
    "        if outplanes is None: outplanes = inplanes \n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        self.conv1 = conv3x3(inplanes, inplanes)\n",
    "        self.bn1 = norm_layer(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(inplanes, outplanes)\n",
    "        self.bn2 = norm_layer(outplanes)\n",
    "        self.skip_conv = (outplanes != inplanes)\n",
    "        if outplanes != inplanes:\n",
    "            self.conv3 = conv1x1(inplanes, outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.skip_conv:\n",
    "            out += self.conv3(identity)\n",
    "        else:\n",
    "            out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class FrameEncoder(nn.Module):    \n",
    "    def __init__(self, num_actions, frame_channels=3):\n",
    "        self.num_actions = num_actions\n",
    "        super(FrameEncoder, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=frame_channels+num_actions, out_channels=128//DOWNSCALE_C, kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=128//DOWNSCALE_C) for i in range(1)]) # Deep: 2 blocks here\n",
    "        self.res1 = torch.nn.Sequential(*res)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128//DOWNSCALE_C, out_channels=256//DOWNSCALE_C, \n",
    "                               kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(1)]) # Deep: 3 blocks here\n",
    "        self.res2 = torch.nn.Sequential(*res)\n",
    "        self.avg1 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(1)]) # Deep: 3 blocks here\n",
    "        self.res3 = torch.nn.Sequential(*res)\n",
    "        self.avg2 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x, actions):        \n",
    "        # input shape: B, C, H, W        \n",
    "        # action shape: B \n",
    "        \n",
    "        x = x.float() / 255.0    \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "        x = torch.concat([x, actions], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.res1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.res2(x)\n",
    "        x = self.avg1(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.avg2(x)\n",
    "        return x\n",
    "    \n",
    "class DynamicModel(nn.Module):\n",
    "    def __init__(self, num_actions, inplanes=256):        \n",
    "        super(DynamicModel, self).__init__()\n",
    "        res = nn.ModuleList([ResBlock(inplanes=inplanes+num_actions, outplanes=inplanes)] + [\n",
    "            ResBlock(inplanes=inplanes) for i in range(4)]) # Deep: 15 blocks here\n",
    "        self.res = torch.nn.Sequential(*res)\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def forward(self, x, actions):      \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "        x = torch.concat([x, actions], dim=1)\n",
    "        return self.res(x)\n",
    "    \n",
    "class Output_rvpi(nn.Module):   \n",
    "    def __init__(self, num_actions, input_shape):         \n",
    "        super(Output_rvpi, self).__init__()        \n",
    "        c, h, w = input_shape\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=c//2, kernel_size=3, padding='same') \n",
    "        self.conv2 = nn.Conv2d(in_channels=c//2, out_channels=c//4, kernel_size=3, padding='same') \n",
    "        fc_in = h * w * (c // 4)\n",
    "        self.fc_r = nn.Linear(fc_in, 1) \n",
    "        self.fc_v = nn.Linear(fc_in, 1) \n",
    "        self.fc_logits = nn.Linear(fc_in, num_actions)         \n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        r, v, logits = self.fc_r(x), self.fc_v(x), self.fc_logits(x)\n",
    "        return r, v, logits\n",
    "\n",
    "class Model(nn.Module):    \n",
    "    def __init__(self, flags, obs_shape, num_actions):\n",
    "        super(Model, self).__init__()      \n",
    "        self.flags = flags\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions          \n",
    "        self.frameEncoder = FrameEncoder(num_actions=num_actions, frame_channels=obs_shape[0])\n",
    "        self.dynamicModel = DynamicModel(num_actions=num_actions, inplanes=256//DOWNSCALE_C)\n",
    "        self.output_rvpi = Output_rvpi(num_actions=num_actions, input_shape=(256//DOWNSCALE_C, \n",
    "                      obs_shape[1]//16, obs_shape[1]//16))\n",
    "        \n",
    "    def forward(self, x, actions, one_hot=False):\n",
    "        # Input\n",
    "        # x: frames with shape (B, C, H, W), in the form of s_t\n",
    "        # actions: action (int64) with shape (k+1, B), in the form of a_{t-1}, a_{t}, a_{t+1}, .. a_{t+k-1}\n",
    "        # Output\n",
    "        # reward: predicted reward with shape (k, B), in the form of r_{t+1}, r_{t+2}, ..., r_{t+k}\n",
    "        # value: predicted value with shape (k+1, B), in the form of v_{t}, v_{t+1}, v_{t+2}, ..., v_{t+k}\n",
    "        # policy: predicted policy with shape (k+1, B), in the form of pi_{t}, pi_{t+1}, pi_{t+2}, ..., pi_{t+k}\n",
    "        # encoded: encoded states with shape (k+1, B), in the form of z_t, z_{t+1}, z_{t+2}, ..., z_{t+k}\n",
    "        # Recall the transition notation: s_t, a_t, r_{t+1}, s_{t+1}, ...\n",
    "        \n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        encoded = self.frameEncoder(x, actions[0])\n",
    "        return self.forward_encoded(encoded, actions[1:], one_hot=True)\n",
    "    \n",
    "    def forward_encoded(self, encoded, actions, one_hot=False):\n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        \n",
    "        r, v, logits = self.output_rvpi(encoded)\n",
    "        r_list, v_list, logits_list = [], [v.squeeze(-1).unsqueeze(0)], [logits.unsqueeze(0)]\n",
    "        encoded_list = [encoded.unsqueeze(0)]\n",
    "        \n",
    "        for k in range(actions.shape[0]):            \n",
    "            encoded = self.dynamicModel(encoded, actions[k])\n",
    "            r, v, logits = self.output_rvpi(encoded)\n",
    "            r_list.append(r.squeeze(-1).unsqueeze(0))\n",
    "            v_list.append(v.squeeze(-1).unsqueeze(0))\n",
    "            logits_list.append(logits.unsqueeze(0))\n",
    "            encoded_list.append(encoded.unsqueeze(0))        \n",
    "        \n",
    "        if len(r_list) > 0:\n",
    "            rs = torch.concat(r_list, dim=0)\n",
    "        else:\n",
    "            rs = None\n",
    "            \n",
    "        vs = torch.concat(v_list, dim=0)\n",
    "        logits = torch.concat(logits_list, dim=0)\n",
    "        encodeds = torch.concat(encoded_list, dim=0)        \n",
    "        \n",
    "        return rs, vs, logits, encodeds\n",
    "\n",
    "#model = Model(flags, (3, 80, 80), num_actions=5)\n",
    "#rs, vs, logits = model(torch.rand(16, 3, 80, 80), torch.ones(8, 16).long())\n",
    "\n",
    "# functions for training models\n",
    "\n",
    "def get_batch_m(flags, buffers: Buffers):\n",
    "    batch_indices = np.random.randint(flags.seq_n, size=flags.bsz)\n",
    "    time_indices = np.random.randint(flags.seq_len - flags.unroll_len, size=flags.bsz)\n",
    "    batch = {key: torch.stack([buffers[key][m][time_indices[n]:time_indices[n]+flags.unroll_len+1] \n",
    "                          for n, m in enumerate(batch_indices)], dim=1) for key in buffers}\n",
    "    batch = {k: t.to(device=flags.device, non_blocking=True) for k, t in batch.items()}\n",
    "    return batch\n",
    "\n",
    "def compute_cross_entropy_loss(logits, target_logits, mask):\n",
    "    target_policy = F.softmax(target_logits, dim=-1)\n",
    "    log_policy = F.log_softmax(logits, dim=-1)\n",
    "    return -torch.sum(target_policy * log_policy * (~mask).float().unsqueeze(-1))\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_rewards = batch['reward'][1:]\n",
    "    target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float())# +\n",
    "                           #vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "    # if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "    # rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "    # vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "    # logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "    for t in range(vs.shape[0]):\n",
    "        done = torch.logical_or(done, batch['done'][t])\n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "    done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()) * (~done_masks).float())\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()) * (~done_masks).float())\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), done_masks)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss\n",
    "\n",
    "# n_step_greedy for testing\n",
    "\n",
    "def n_step_greedy_model(state, action, model, n, encoded=None, temp=20.): \n",
    "    \n",
    "    # Either input state, action (S_t, A_{t-1}) or the encoded Z_t\n",
    "    # state / encoded in the shape of (B, C, H, W)\n",
    "    # action in the shape of (B)    \n",
    "    with torch.no_grad():    \n",
    "      bsz = state.shape[0] if encoded is None else encoded.shape[0]\n",
    "      device = state.device if encoded is None else encoded.device\n",
    "      num_actions = model.num_actions    \n",
    "\n",
    "      q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "\n",
    "      for act in range(num_actions):        \n",
    "          new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "          if encoded is None:            \n",
    "              old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "              rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "          else:\n",
    "              rs, vs, logits, encodeds = model.forward_encoded(encoded, new_action.unsqueeze(0))\n",
    "\n",
    "          if n > 1:\n",
    "              action, prob, sub_q_ret = n_step_greedy_model(state=None, action=None, \n",
    "                         model=model, n=n-1, encoded=encodeds[1])\n",
    "              ret = rs[0] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] \n",
    "          else:\n",
    "              ret = rs[0] + flags.discounting * vs[1]\n",
    "          q_ret[:, act] = ret\n",
    "\n",
    "      prob = F.softmax(temp*q_ret, dim=1)\n",
    "      action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret        \n",
    "   \n",
    "#n_step_greedy_model(batch['frame'][0], batch['action'][0], model, 4)  \n",
    "\n",
    "def test_n_step_model(n, model, flags, eps_n=100, temp=20.):    \n",
    "    \n",
    "    print(\"Testing %d step planning\" % n) \n",
    "    \n",
    "    bsz = 100\n",
    "    env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "    env = Vec_Environment(env, bsz)\n",
    "    num_actions = env.gym_env.action_space[0].n\n",
    "    \n",
    "    model.train(False)\n",
    "    returns = []\n",
    "    \n",
    "    obs = env.initial()\n",
    "    action = torch.zeros(bsz).long().to(flags.device)\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "        new_action, _, _ = n_step_greedy_model(obs['frame'][0], action, model, n, None, temp)        \n",
    "        obs = env.step(new_action)\n",
    "        action = new_action\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            #print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            #    np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    returns = returns[:eps_n]\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6fa3769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer created successfully.\n",
      "model size:  2352882\n",
      "frameEncoder.conv1.weight 4608\n",
      "frameEncoder.conv1.bias 64\n",
      "frameEncoder.res1.0.conv1.weight 36864\n",
      "frameEncoder.res1.0.bn1.weight 64\n",
      "frameEncoder.res1.0.bn1.bias 64\n",
      "frameEncoder.res1.0.conv2.weight 36864\n",
      "frameEncoder.res1.0.bn2.weight 64\n",
      "frameEncoder.res1.0.bn2.bias 64\n",
      "frameEncoder.conv2.weight 73728\n",
      "frameEncoder.conv2.bias 128\n",
      "frameEncoder.res2.0.conv1.weight 147456\n",
      "frameEncoder.res2.0.bn1.weight 128\n",
      "frameEncoder.res2.0.bn1.bias 128\n",
      "frameEncoder.res2.0.conv2.weight 147456\n",
      "frameEncoder.res2.0.bn2.weight 128\n",
      "frameEncoder.res2.0.bn2.bias 128\n",
      "frameEncoder.res3.0.conv1.weight 147456\n",
      "frameEncoder.res3.0.bn1.weight 128\n",
      "frameEncoder.res3.0.bn1.bias 128\n",
      "frameEncoder.res3.0.conv2.weight 147456\n",
      "frameEncoder.res3.0.bn2.weight 128\n",
      "frameEncoder.res3.0.bn2.bias 128\n",
      "dynamicModel.res.0.conv1.weight 159201\n",
      "dynamicModel.res.0.bn1.weight 133\n",
      "dynamicModel.res.0.bn1.bias 133\n",
      "dynamicModel.res.0.conv2.weight 153216\n",
      "dynamicModel.res.0.bn2.weight 128\n",
      "dynamicModel.res.0.bn2.bias 128\n",
      "dynamicModel.res.0.conv3.weight 17024\n",
      "dynamicModel.res.1.conv1.weight 147456\n",
      "dynamicModel.res.1.bn1.weight 128\n",
      "dynamicModel.res.1.bn1.bias 128\n",
      "dynamicModel.res.1.conv2.weight 147456\n",
      "dynamicModel.res.1.bn2.weight 128\n",
      "dynamicModel.res.1.bn2.bias 128\n",
      "dynamicModel.res.2.conv1.weight 147456\n",
      "dynamicModel.res.2.bn1.weight 128\n",
      "dynamicModel.res.2.bn1.bias 128\n",
      "dynamicModel.res.2.conv2.weight 147456\n",
      "dynamicModel.res.2.bn2.weight 128\n",
      "dynamicModel.res.2.bn2.bias 128\n",
      "dynamicModel.res.3.conv1.weight 147456\n",
      "dynamicModel.res.3.bn1.weight 128\n",
      "dynamicModel.res.3.bn1.bias 128\n",
      "dynamicModel.res.3.conv2.weight 147456\n",
      "dynamicModel.res.3.bn2.weight 128\n",
      "dynamicModel.res.3.bn2.bias 128\n",
      "dynamicModel.res.4.conv1.weight 147456\n",
      "dynamicModel.res.4.bn1.weight 128\n",
      "dynamicModel.res.4.bn1.bias 128\n",
      "dynamicModel.res.4.conv2.weight 147456\n",
      "dynamicModel.res.4.bn2.weight 128\n",
      "dynamicModel.res.4.bn2.bias 128\n",
      "output_rvpi.conv1.weight 73728\n",
      "output_rvpi.conv1.bias 64\n",
      "output_rvpi.conv2.weight 18432\n",
      "output_rvpi.conv2.bias 32\n",
      "output_rvpi.fc_r.weight 800\n",
      "output_rvpi.fc_r.bias 1\n",
      "output_rvpi.fc_v.weight 800\n",
      "output_rvpi.fc_v.bias 1\n",
      "output_rvpi.fc_logits.weight 4000\n",
      "output_rvpi.fc_logits.bias 5\n"
     ]
    }
   ],
   "source": [
    "# Start training models\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())       \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.bsz = 32\n",
    "flags.unroll_len = 3\n",
    "flags.num_actors = 32\n",
    "flags.seq_n = 1000\n",
    "flags.seq_len = 200\n",
    "flags.learning_rate = 0.0002\n",
    "flags.loop_batch_n = 3\n",
    "flags.discounting = 0.97\n",
    "flags.tot_epoch = 10000\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "# Create buffer for actors to write\n",
    "\n",
    "mp.set_sharing_strategy('file_system')\n",
    "ctx = mp.get_context()        \n",
    "\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "buffers = create_buffers_m(flags, obs_shape, num_actions)\n",
    "print(\"Buffer created successfully.\")\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "\n",
    "env = create_env(flags)\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=flags.learning_rate)\n",
    "\n",
    "print(\"model size: \", sum(p.numel() for p in model.parameters()))\n",
    "for k, v in model.named_parameters(): print(k, v.numel())    \n",
    "    \n",
    "tot_step = int(flags.loop_batch_n * flags.seq_n * flags.seq_len / flags.bsz / flags.unroll_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "301892ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1095814\n",
      "Batch [0] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.85 (+-0.06) \n",
      "[0:0] F: 0 \t tot_loss 31.412720 rs_loss 12.127158 vs_loss 11.798643 logits_loss 149.738403\n",
      "[0:100] F: 9600 \t tot_loss 10.489563 rs_loss 1.093408 vs_loss 1.963170 logits_loss 148.659694\n",
      "[0:200] F: 19200 \t tot_loss 10.237407 rs_loss 1.001062 vs_loss 1.835907 logits_loss 148.008782\n",
      "[0:300] F: 28800 \t tot_loss 10.119629 rs_loss 0.958913 vs_loss 1.761552 logits_loss 147.983300\n",
      "[0:400] F: 38400 \t tot_loss 10.025569 rs_loss 0.916511 vs_loss 1.713362 logits_loss 147.913907\n",
      "[0:500] F: 48000 \t tot_loss 9.768834 rs_loss 0.831759 vs_loss 1.553641 logits_loss 147.668655\n",
      "[0:600] F: 57600 \t tot_loss 9.554952 rs_loss 0.757879 vs_loss 1.417541 logits_loss 147.590648\n",
      "[0:700] F: 67200 \t tot_loss 9.429279 rs_loss 0.721470 vs_loss 1.341246 logits_loss 147.331269\n",
      "[0:800] F: 76800 \t tot_loss 9.247243 rs_loss 0.663996 vs_loss 1.232054 logits_loss 147.023865\n",
      "[0:900] F: 86400 \t tot_loss 9.276123 rs_loss 0.684279 vs_loss 1.247438 logits_loss 146.888123\n",
      "[0:1000] F: 96000 \t tot_loss 9.305076 rs_loss 0.697538 vs_loss 1.270312 logits_loss 146.744517\n",
      "[0:1100] F: 105600 \t tot_loss 9.320096 rs_loss 0.704057 vs_loss 1.285106 logits_loss 146.618646\n",
      "[0:1200] F: 115200 \t tot_loss 9.286809 rs_loss 0.690345 vs_loss 1.263866 logits_loss 146.651936\n",
      "[0:1300] F: 124800 \t tot_loss 9.313898 rs_loss 0.696776 vs_loss 1.291329 logits_loss 146.515863\n",
      "[0:1400] F: 134400 \t tot_loss 9.232035 rs_loss 0.665933 vs_loss 1.235141 logits_loss 146.619218\n",
      "[0:1500] F: 144000 \t tot_loss 9.068019 rs_loss 0.605545 vs_loss 1.129605 logits_loss 146.657383\n",
      "[0:1600] F: 153600 \t tot_loss 9.143722 rs_loss 0.651352 vs_loss 1.167370 logits_loss 146.500000\n",
      "[0:1700] F: 163200 \t tot_loss 9.086733 rs_loss 0.635754 vs_loss 1.128021 logits_loss 146.459156\n",
      "[0:1800] F: 172800 \t tot_loss 9.143855 rs_loss 0.673825 vs_loss 1.164822 logits_loss 146.104147\n",
      "[0:1900] F: 182400 \t tot_loss 9.289544 rs_loss 0.742290 vs_loss 1.254002 logits_loss 145.865029\n",
      "[0:2000] F: 192000 \t tot_loss 9.346441 rs_loss 0.729429 vs_loss 1.327805 logits_loss 145.784123\n",
      "[0:2100] F: 201600 \t tot_loss 9.355191 rs_loss 0.729233 vs_loss 1.346670 logits_loss 145.585760\n",
      "[0:2200] F: 211200 \t tot_loss 9.267953 rs_loss 0.687967 vs_loss 1.302040 logits_loss 145.558926\n",
      "[0:2300] F: 220800 \t tot_loss 9.167097 rs_loss 0.649005 vs_loss 1.259964 logits_loss 145.162541\n",
      "[0:2400] F: 230400 \t tot_loss 9.122629 rs_loss 0.651239 vs_loss 1.223845 logits_loss 144.950891\n",
      "[0:2500] F: 240000 \t tot_loss 9.156827 rs_loss 0.704170 vs_loss 1.227235 logits_loss 144.508434\n",
      "[0:2600] F: 249600 \t tot_loss 9.130815 rs_loss 0.718826 vs_loss 1.214692 logits_loss 143.945940\n",
      "[0:2700] F: 259200 \t tot_loss 9.168331 rs_loss 0.754729 vs_loss 1.233415 logits_loss 143.603727\n",
      "[0:2800] F: 268800 \t tot_loss 9.173161 rs_loss 0.777906 vs_loss 1.253911 logits_loss 142.826880\n",
      "[0:2900] F: 278400 \t tot_loss 9.215645 rs_loss 0.773537 vs_loss 1.339592 logits_loss 142.050316\n",
      "[0:3000] F: 288000 \t tot_loss 9.345313 rs_loss 0.814757 vs_loss 1.455484 logits_loss 141.501442\n",
      "[0:3100] F: 297600 \t tot_loss 9.220888 rs_loss 0.774516 vs_loss 1.405828 logits_loss 140.810877\n",
      "[0:3200] F: 307200 \t tot_loss 9.182082 rs_loss 0.780678 vs_loss 1.403313 logits_loss 139.961809\n",
      "[0:3300] F: 316800 \t tot_loss 8.969095 rs_loss 0.719206 vs_loss 1.273566 logits_loss 139.526462\n",
      "[0:3400] F: 326400 \t tot_loss 8.843617 rs_loss 0.695036 vs_loss 1.199374 logits_loss 138.984140\n",
      "[0:3500] F: 336000 \t tot_loss 9.041156 rs_loss 0.765224 vs_loss 1.339904 logits_loss 138.720545\n",
      "[0:3600] F: 345600 \t tot_loss 8.951258 rs_loss 0.728889 vs_loss 1.292032 logits_loss 138.606749\n",
      "[0:3700] F: 355200 \t tot_loss 8.975444 rs_loss 0.751969 vs_loss 1.322008 logits_loss 138.029358\n",
      "[0:3800] F: 364800 \t tot_loss 8.896725 rs_loss 0.722226 vs_loss 1.286154 logits_loss 137.766903\n",
      "[0:3900] F: 374400 \t tot_loss 8.650745 rs_loss 0.650010 vs_loss 1.133225 logits_loss 137.350203\n",
      "[0:4000] F: 384000 \t tot_loss 8.684139 rs_loss 0.661091 vs_loss 1.181719 logits_loss 136.826575\n",
      "[0:4100] F: 393600 \t tot_loss 8.527140 rs_loss 0.606043 vs_loss 1.081120 logits_loss 136.799537\n",
      "[0:4200] F: 403200 \t tot_loss 8.539725 rs_loss 0.644054 vs_loss 1.084026 logits_loss 136.232893\n",
      "[0:4300] F: 412800 \t tot_loss 8.602604 rs_loss 0.677330 vs_loss 1.131780 logits_loss 135.869870\n",
      "[0:4400] F: 422400 \t tot_loss 8.712596 rs_loss 0.702813 vs_loss 1.195538 logits_loss 136.284889\n",
      "[0:4500] F: 432000 \t tot_loss 8.713390 rs_loss 0.711162 vs_loss 1.204176 logits_loss 135.961040\n",
      "[0:4600] F: 441600 \t tot_loss 8.710755 rs_loss 0.705270 vs_loss 1.207771 logits_loss 135.954281\n",
      "[0:4700] F: 451200 \t tot_loss 8.806672 rs_loss 0.723265 vs_loss 1.296085 logits_loss 135.746434\n",
      "[0:4800] F: 460800 \t tot_loss 8.644916 rs_loss 0.702340 vs_loss 1.171498 logits_loss 135.421552\n",
      "[0:4900] F: 470400 \t tot_loss 8.800028 rs_loss 0.761000 vs_loss 1.271976 logits_loss 135.341031\n",
      "[0:5000] F: 480000 \t tot_loss 8.765248 rs_loss 0.748716 vs_loss 1.260923 logits_loss 135.112156\n",
      "[0:5100] F: 489600 \t tot_loss 8.726640 rs_loss 0.732174 vs_loss 1.250044 logits_loss 134.888445\n",
      "[0:5200] F: 499200 \t tot_loss 8.580817 rs_loss 0.691655 vs_loss 1.172964 logits_loss 134.323955\n",
      "[0:5300] F: 508800 \t tot_loss 8.554785 rs_loss 0.670727 vs_loss 1.176769 logits_loss 134.145786\n",
      "[0:5400] F: 518400 \t tot_loss 8.707963 rs_loss 0.715624 vs_loss 1.297175 logits_loss 133.903286\n",
      "[0:5500] F: 528000 \t tot_loss 8.783934 rs_loss 0.721707 vs_loss 1.369780 logits_loss 133.848938\n",
      "[0:5600] F: 537600 \t tot_loss 8.866790 rs_loss 0.721775 vs_loss 1.440528 logits_loss 134.089747\n",
      "[0:5700] F: 547200 \t tot_loss 8.766023 rs_loss 0.690290 vs_loss 1.373913 logits_loss 134.036375\n",
      "[0:5800] F: 556800 \t tot_loss 8.578735 rs_loss 0.611777 vs_loss 1.241274 logits_loss 134.513672\n",
      "[0:5900] F: 566400 \t tot_loss 8.315476 rs_loss 0.552461 vs_loss 1.032880 logits_loss 134.602704\n",
      "[0:6000] F: 576000 \t tot_loss 8.359961 rs_loss 0.578231 vs_loss 1.074182 logits_loss 134.150956\n",
      "[0:6100] F: 585600 \t tot_loss 8.469889 rs_loss 0.625582 vs_loss 1.127818 logits_loss 134.329769\n",
      "[0:6200] F: 595200 \t tot_loss 8.587822 rs_loss 0.698437 vs_loss 1.197803 logits_loss 133.831647\n",
      "Batch [1] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.81 (+-0.06) \n",
      "[1:0] F: 600000 \t tot_loss 8.677883 rs_loss 0.712270 vs_loss 1.281763 logits_loss 133.676991\n",
      "[1:100] F: 609600 \t tot_loss 8.837038 rs_loss 0.776911 vs_loss 1.375968 logits_loss 133.683178\n",
      "[1:200] F: 619200 \t tot_loss 8.907538 rs_loss 0.799972 vs_loss 1.427080 logits_loss 133.609727\n",
      "[1:300] F: 628800 \t tot_loss 8.860551 rs_loss 0.782581 vs_loss 1.417970 logits_loss 133.199980\n",
      "[1:400] F: 638400 \t tot_loss 8.784812 rs_loss 0.779959 vs_loss 1.363133 logits_loss 132.834391\n",
      "[1:500] F: 648000 \t tot_loss 8.724905 rs_loss 0.751651 vs_loss 1.335415 logits_loss 132.756787\n",
      "[1:600] F: 657600 \t tot_loss 8.668935 rs_loss 0.729738 vs_loss 1.304557 logits_loss 132.692794\n",
      "[1:700] F: 667200 \t tot_loss 8.607599 rs_loss 0.686263 vs_loss 1.282839 logits_loss 132.769935\n",
      "[1:800] F: 676800 \t tot_loss 8.940229 rs_loss 0.787081 vs_loss 1.489828 logits_loss 133.266415\n",
      "[1:900] F: 686400 \t tot_loss 8.935471 rs_loss 0.786535 vs_loss 1.492333 logits_loss 133.132060\n",
      "[1:1000] F: 696000 \t tot_loss 8.834391 rs_loss 0.763026 vs_loss 1.429831 logits_loss 132.830664\n",
      "[1:1100] F: 705600 \t tot_loss 8.742643 rs_loss 0.747541 vs_loss 1.359019 logits_loss 132.721653\n",
      "[1:1200] F: 715200 \t tot_loss 8.372779 rs_loss 0.621626 vs_loss 1.127736 logits_loss 132.468337\n",
      "[1:1300] F: 724800 \t tot_loss 8.185005 rs_loss 0.556894 vs_loss 1.018729 logits_loss 132.187627\n",
      "[1:1400] F: 734400 \t tot_loss 8.383211 rs_loss 0.611800 vs_loss 1.162747 logits_loss 132.173287\n",
      "[1:1500] F: 744000 \t tot_loss 8.477410 rs_loss 0.643948 vs_loss 1.221861 logits_loss 132.232026\n",
      "[1:1600] F: 753600 \t tot_loss 8.723545 rs_loss 0.708866 vs_loss 1.408994 logits_loss 132.113700\n",
      "[1:1700] F: 763200 \t tot_loss 8.864111 rs_loss 0.765498 vs_loss 1.497893 logits_loss 132.014384\n",
      "[1:1800] F: 772800 \t tot_loss 8.702035 rs_loss 0.717174 vs_loss 1.383217 logits_loss 132.032877\n",
      "[1:1900] F: 782400 \t tot_loss 8.698270 rs_loss 0.694461 vs_loss 1.412422 logits_loss 131.827734\n",
      "[1:2000] F: 792000 \t tot_loss 8.538089 rs_loss 0.650739 vs_loss 1.308690 logits_loss 131.573188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1:2100] F: 801600 \t tot_loss 8.496630 rs_loss 0.617685 vs_loss 1.296238 logits_loss 131.654140\n",
      "[1:2200] F: 811200 \t tot_loss 8.499312 rs_loss 0.620892 vs_loss 1.302410 logits_loss 131.520200\n",
      "[1:2300] F: 820800 \t tot_loss 8.453572 rs_loss 0.618202 vs_loss 1.270241 logits_loss 131.302579\n",
      "[1:2400] F: 830400 \t tot_loss 8.334602 rs_loss 0.577469 vs_loss 1.190457 logits_loss 131.333533\n",
      "[1:2500] F: 840000 \t tot_loss 8.206642 rs_loss 0.546674 vs_loss 1.090678 logits_loss 131.385819\n",
      "[1:2600] F: 849600 \t tot_loss 8.182158 rs_loss 0.523237 vs_loss 1.094809 logits_loss 131.282252\n",
      "[1:2700] F: 859200 \t tot_loss 8.173522 rs_loss 0.510099 vs_loss 1.087494 logits_loss 131.518565\n",
      "[1:2800] F: 868800 \t tot_loss 8.335642 rs_loss 0.566286 vs_loss 1.206568 logits_loss 131.255739\n",
      "[1:2900] F: 878400 \t tot_loss 8.625595 rs_loss 0.665342 vs_loss 1.403503 logits_loss 131.134995\n",
      "[1:3000] F: 888000 \t tot_loss 8.768502 rs_loss 0.736567 vs_loss 1.482653 logits_loss 130.985628\n",
      "[1:3100] F: 897600 \t tot_loss 8.966936 rs_loss 0.785121 vs_loss 1.641693 logits_loss 130.802447\n",
      "[1:3200] F: 907200 \t tot_loss 8.847667 rs_loss 0.752387 vs_loss 1.549332 logits_loss 130.918962\n",
      "[1:3300] F: 916800 \t tot_loss 8.816931 rs_loss 0.711891 vs_loss 1.580047 logits_loss 130.499854\n",
      "[1:3400] F: 926400 \t tot_loss 8.777531 rs_loss 0.663881 vs_loss 1.582104 logits_loss 130.630932\n",
      "[1:3500] F: 936000 \t tot_loss 8.678814 rs_loss 0.646031 vs_loss 1.506529 logits_loss 130.525076\n",
      "[1:3600] F: 945600 \t tot_loss 8.644861 rs_loss 0.639513 vs_loss 1.483379 logits_loss 130.439386\n",
      "[1:3700] F: 955200 \t tot_loss 8.519523 rs_loss 0.619769 vs_loss 1.366097 logits_loss 130.673127\n",
      "[1:3800] F: 964800 \t tot_loss 8.514581 rs_loss 0.625510 vs_loss 1.364163 logits_loss 130.498157\n",
      "[1:3900] F: 974400 \t tot_loss 8.500177 rs_loss 0.633344 vs_loss 1.349406 logits_loss 130.348540\n",
      "[1:4000] F: 984000 \t tot_loss 8.622475 rs_loss 0.647932 vs_loss 1.457675 logits_loss 130.337332\n",
      "[1:4100] F: 993600 \t tot_loss 8.483016 rs_loss 0.606309 vs_loss 1.370077 logits_loss 130.132593\n",
      "[1:4200] F: 1003200 \t tot_loss 8.487491 rs_loss 0.627001 vs_loss 1.351053 logits_loss 130.188723\n",
      "[1:4300] F: 1012800 \t tot_loss 8.256660 rs_loss 0.520877 vs_loss 1.214806 logits_loss 130.419552\n",
      "[1:4400] F: 1022400 \t tot_loss 8.104457 rs_loss 0.476008 vs_loss 1.106552 logits_loss 130.437937\n",
      "[1:4500] F: 1032000 \t tot_loss 8.127997 rs_loss 0.479283 vs_loss 1.123682 logits_loss 130.500641\n",
      "[1:4600] F: 1041600 \t tot_loss 8.114746 rs_loss 0.465078 vs_loss 1.132722 logits_loss 130.338923\n",
      "[1:4700] F: 1051200 \t tot_loss 8.031921 rs_loss 0.448524 vs_loss 1.075170 logits_loss 130.164523\n",
      "[1:4800] F: 1060800 \t tot_loss 8.121886 rs_loss 0.493066 vs_loss 1.118373 logits_loss 130.208945\n",
      "[1:4900] F: 1070400 \t tot_loss 8.316296 rs_loss 0.542721 vs_loss 1.272988 logits_loss 130.011734\n",
      "[1:5000] F: 1080000 \t tot_loss 8.227585 rs_loss 0.510838 vs_loss 1.221322 logits_loss 129.908513\n",
      "[1:5100] F: 1089600 \t tot_loss 8.386512 rs_loss 0.579193 vs_loss 1.323782 logits_loss 129.670743\n",
      "[1:5200] F: 1099200 \t tot_loss 8.255318 rs_loss 0.522623 vs_loss 1.269956 logits_loss 129.254797\n",
      "[1:5300] F: 1108800 \t tot_loss 8.182006 rs_loss 0.492853 vs_loss 1.219024 logits_loss 129.402561\n",
      "[1:5400] F: 1118400 \t tot_loss 8.332540 rs_loss 0.534286 vs_loss 1.319226 logits_loss 129.580559\n",
      "[1:5500] F: 1128000 \t tot_loss 8.240354 rs_loss 0.487305 vs_loss 1.266171 logits_loss 129.737551\n",
      "[1:5600] F: 1137600 \t tot_loss 8.460681 rs_loss 0.569210 vs_loss 1.400218 logits_loss 129.825059\n",
      "[1:5700] F: 1147200 \t tot_loss 8.442131 rs_loss 0.565508 vs_loss 1.394318 logits_loss 129.646113\n",
      "[1:5800] F: 1156800 \t tot_loss 8.551274 rs_loss 0.568868 vs_loss 1.509425 logits_loss 129.459611\n",
      "[1:5900] F: 1166400 \t tot_loss 8.709182 rs_loss 0.618576 vs_loss 1.627464 logits_loss 129.262847\n",
      "[1:6000] F: 1176000 \t tot_loss 8.513490 rs_loss 0.535470 vs_loss 1.511602 logits_loss 129.328364\n",
      "[1:6100] F: 1185600 \t tot_loss 8.432271 rs_loss 0.515475 vs_loss 1.450458 logits_loss 129.326749\n",
      "[1:6200] F: 1195200 \t tot_loss 8.168963 rs_loss 0.464151 vs_loss 1.243834 logits_loss 129.219566\n",
      "Batch [2] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.57 (+-0.07) \n",
      "[2:0] F: 1200000 \t tot_loss 8.089886 rs_loss 0.432224 vs_loss 1.196061 logits_loss 129.232011\n",
      "[2:100] F: 1209600 \t tot_loss 8.056938 rs_loss 0.431826 vs_loss 1.157417 logits_loss 129.353916\n",
      "[2:200] F: 1219200 \t tot_loss 8.205685 rs_loss 0.481635 vs_loss 1.251057 logits_loss 129.459853\n",
      "[2:300] F: 1228800 \t tot_loss 8.374934 rs_loss 0.542316 vs_loss 1.345897 logits_loss 129.734413\n",
      "[2:400] F: 1238400 \t tot_loss 8.518454 rs_loss 0.554867 vs_loss 1.448463 logits_loss 130.302477\n",
      "[2:500] F: 1248000 \t tot_loss 8.636376 rs_loss 0.571704 vs_loss 1.550752 logits_loss 130.278414\n",
      "[2:600] F: 1257600 \t tot_loss 8.455895 rs_loss 0.518224 vs_loss 1.422097 logits_loss 130.311469\n",
      "[2:700] F: 1267200 \t tot_loss 8.291597 rs_loss 0.470307 vs_loss 1.317758 logits_loss 130.070648\n",
      "[2:800] F: 1276800 \t tot_loss 8.287439 rs_loss 0.479203 vs_loss 1.320860 logits_loss 129.747510\n",
      "[2:900] F: 1286400 \t tot_loss 8.046688 rs_loss 0.419156 vs_loss 1.148117 logits_loss 129.588307\n",
      "[2:1000] F: 1296000 \t tot_loss 8.035479 rs_loss 0.420754 vs_loss 1.137976 logits_loss 129.534978\n",
      "[2:1100] F: 1305600 \t tot_loss 7.960866 rs_loss 0.397666 vs_loss 1.095574 logits_loss 129.352512\n",
      "[2:1200] F: 1315200 \t tot_loss 7.820361 rs_loss 0.353687 vs_loss 0.997492 logits_loss 129.383626\n",
      "[2:1300] F: 1324800 \t tot_loss 7.835559 rs_loss 0.350229 vs_loss 1.005712 logits_loss 129.592372\n",
      "[2:1400] F: 1334400 \t tot_loss 7.837380 rs_loss 0.332711 vs_loss 1.017517 logits_loss 129.743033\n",
      "[2:1500] F: 1344000 \t tot_loss 7.797906 rs_loss 0.314089 vs_loss 1.009014 logits_loss 129.496074\n",
      "[2:1600] F: 1353600 \t tot_loss 7.962989 rs_loss 0.385631 vs_loss 1.122923 logits_loss 129.088717\n",
      "[2:1700] F: 1363200 \t tot_loss 7.958647 rs_loss 0.389580 vs_loss 1.119266 logits_loss 128.996014\n",
      "[2:1800] F: 1372800 \t tot_loss 7.931153 rs_loss 0.400439 vs_loss 1.084198 logits_loss 128.930328\n",
      "[2:1900] F: 1382400 \t tot_loss 7.913752 rs_loss 0.400623 vs_loss 1.059044 logits_loss 129.081718\n",
      "[2:2000] F: 1392000 \t tot_loss 7.717491 rs_loss 0.317630 vs_loss 0.942061 logits_loss 129.156014\n",
      "[2:2100] F: 1401600 \t tot_loss 7.747722 rs_loss 0.324701 vs_loss 0.977781 logits_loss 128.904823\n",
      "[2:2200] F: 1411200 \t tot_loss 7.812720 rs_loss 0.334704 vs_loss 1.039356 logits_loss 128.773188\n",
      "[2:2300] F: 1420800 \t tot_loss 7.765253 rs_loss 0.314061 vs_loss 0.997567 logits_loss 129.072487\n",
      "[2:2400] F: 1430400 \t tot_loss 7.770702 rs_loss 0.331695 vs_loss 0.976775 logits_loss 129.244648\n",
      "[2:2500] F: 1440000 \t tot_loss 7.678737 rs_loss 0.310320 vs_loss 0.914016 logits_loss 129.088011\n",
      "[2:2600] F: 1449600 \t tot_loss 7.551858 rs_loss 0.270404 vs_loss 0.843830 logits_loss 128.752476\n",
      "[2:2700] F: 1459200 \t tot_loss 7.512932 rs_loss 0.255327 vs_loss 0.831053 logits_loss 128.531041\n",
      "[2:2800] F: 1468800 \t tot_loss 7.447106 rs_loss 0.227788 vs_loss 0.790687 logits_loss 128.572619\n",
      "[2:2900] F: 1478400 \t tot_loss 7.621654 rs_loss 0.300622 vs_loss 0.895141 logits_loss 128.517828\n",
      "[2:3000] F: 1488000 \t tot_loss 7.771093 rs_loss 0.359250 vs_loss 0.973850 logits_loss 128.759854\n",
      "[2:3100] F: 1497600 \t tot_loss 7.810825 rs_loss 0.365336 vs_loss 1.012760 logits_loss 128.654570\n",
      "[2:3200] F: 1507200 \t tot_loss 7.820310 rs_loss 0.372730 vs_loss 1.026871 logits_loss 128.414194\n",
      "[2:3300] F: 1516800 \t tot_loss 7.777254 rs_loss 0.351218 vs_loss 1.006063 logits_loss 128.399455\n",
      "[2:3400] F: 1526400 \t tot_loss 7.658573 rs_loss 0.303929 vs_loss 0.945739 logits_loss 128.178103\n",
      "[2:3500] F: 1536000 \t tot_loss 7.691569 rs_loss 0.330224 vs_loss 0.952426 logits_loss 128.178392\n",
      "[2:3600] F: 1545600 \t tot_loss 7.852442 rs_loss 0.363761 vs_loss 1.085217 logits_loss 128.069279\n",
      "[2:3700] F: 1555200 \t tot_loss 7.864745 rs_loss 0.349136 vs_loss 1.115647 logits_loss 127.999246\n",
      "[2:3800] F: 1564800 \t tot_loss 7.972411 rs_loss 0.361744 vs_loss 1.200473 logits_loss 128.203872\n",
      "[2:3900] F: 1574400 \t tot_loss 8.016798 rs_loss 0.369483 vs_loss 1.240154 logits_loss 128.143227\n",
      "[2:4000] F: 1584000 \t tot_loss 7.876885 rs_loss 0.335148 vs_loss 1.138817 logits_loss 128.058377\n",
      "[2:4100] F: 1593600 \t tot_loss 7.776340 rs_loss 0.313729 vs_loss 1.055007 logits_loss 128.152096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2:4200] F: 1603200 \t tot_loss 7.860138 rs_loss 0.365131 vs_loss 1.090903 logits_loss 128.082064\n",
      "[2:4300] F: 1612800 \t tot_loss 7.711450 rs_loss 0.315041 vs_loss 0.989141 logits_loss 128.145339\n",
      "[2:4400] F: 1622400 \t tot_loss 7.787088 rs_loss 0.335345 vs_loss 1.036070 logits_loss 128.313465\n",
      "[2:4500] F: 1632000 \t tot_loss 7.764541 rs_loss 0.330410 vs_loss 1.017992 logits_loss 128.322770\n",
      "[2:4600] F: 1641600 \t tot_loss 7.802283 rs_loss 0.330317 vs_loss 1.064813 logits_loss 128.143051\n",
      "[2:4700] F: 1651200 \t tot_loss 8.006921 rs_loss 0.414604 vs_loss 1.196729 logits_loss 127.911762\n",
      "[2:4800] F: 1660800 \t tot_loss 7.997725 rs_loss 0.420250 vs_loss 1.190842 logits_loss 127.732654\n",
      "[2:4900] F: 1670400 \t tot_loss 8.133379 rs_loss 0.464213 vs_loss 1.276212 logits_loss 127.859061\n",
      "[2:5000] F: 1680000 \t tot_loss 8.004659 rs_loss 0.431745 vs_loss 1.184935 logits_loss 127.759580\n",
      "[2:5100] F: 1689600 \t tot_loss 7.968961 rs_loss 0.402816 vs_loss 1.177624 logits_loss 127.770404\n",
      "[2:5200] F: 1699200 \t tot_loss 7.841659 rs_loss 0.367169 vs_loss 1.092742 logits_loss 127.634951\n",
      "[2:5300] F: 1708800 \t tot_loss 7.758894 rs_loss 0.337787 vs_loss 1.053004 logits_loss 127.362057\n",
      "[2:5400] F: 1718400 \t tot_loss 7.741173 rs_loss 0.340177 vs_loss 1.038715 logits_loss 127.245595\n",
      "[2:5500] F: 1728000 \t tot_loss 7.664125 rs_loss 0.309693 vs_loss 0.982720 logits_loss 127.434234\n",
      "[2:5600] F: 1737600 \t tot_loss 7.735433 rs_loss 0.325949 vs_loss 1.026686 logits_loss 127.655965\n",
      "[2:5700] F: 1747200 \t tot_loss 7.814526 rs_loss 0.345354 vs_loss 1.088402 logits_loss 127.615399\n",
      "[2:5800] F: 1756800 \t tot_loss 7.750211 rs_loss 0.337629 vs_loss 1.032444 logits_loss 127.602759\n",
      "[2:5900] F: 1766400 \t tot_loss 7.859302 rs_loss 0.376850 vs_loss 1.100536 logits_loss 127.638311\n",
      "[2:6000] F: 1776000 \t tot_loss 7.771344 rs_loss 0.349418 vs_loss 1.054058 logits_loss 127.357358\n",
      "[2:6100] F: 1785600 \t tot_loss 7.635262 rs_loss 0.309172 vs_loss 0.949704 logits_loss 127.527695\n",
      "[2:6200] F: 1795200 \t tot_loss 7.744918 rs_loss 0.315592 vs_loss 1.043875 logits_loss 127.708995\n",
      "Batch [3] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.20 (+-0.16) \n",
      "[3:0] F: 1800000 \t tot_loss 7.684296 rs_loss 0.309061 vs_loss 1.001520 logits_loss 127.474306\n",
      "[3:100] F: 1809600 \t tot_loss 7.672059 rs_loss 0.300914 vs_loss 0.988753 logits_loss 127.647842\n",
      "[3:200] F: 1819200 \t tot_loss 7.688685 rs_loss 0.296218 vs_loss 1.003584 logits_loss 127.777648\n",
      "[3:300] F: 1828800 \t tot_loss 7.539593 rs_loss 0.252208 vs_loss 0.903420 logits_loss 127.679298\n",
      "[3:400] F: 1838400 \t tot_loss 7.561192 rs_loss 0.237737 vs_loss 0.927198 logits_loss 127.925139\n",
      "[3:500] F: 1848000 \t tot_loss 7.527855 rs_loss 0.233556 vs_loss 0.922187 logits_loss 127.442219\n",
      "[3:600] F: 1857600 \t tot_loss 7.517839 rs_loss 0.226759 vs_loss 0.920332 logits_loss 127.414958\n",
      "[3:700] F: 1867200 \t tot_loss 7.467946 rs_loss 0.227251 vs_loss 0.866961 logits_loss 127.474692\n",
      "[3:800] F: 1876800 \t tot_loss 7.409416 rs_loss 0.224761 vs_loss 0.818855 logits_loss 127.316009\n",
      "[3:900] F: 1886400 \t tot_loss 7.392642 rs_loss 0.207184 vs_loss 0.800051 logits_loss 127.708132\n",
      "[3:1000] F: 1896000 \t tot_loss 7.457535 rs_loss 0.235988 vs_loss 0.846812 logits_loss 127.494708\n",
      "[3:1100] F: 1905600 \t tot_loss 7.491201 rs_loss 0.248036 vs_loss 0.879703 logits_loss 127.269247\n",
      "[3:1200] F: 1915200 \t tot_loss 7.516258 rs_loss 0.255896 vs_loss 0.901363 logits_loss 127.179970\n",
      "[3:1300] F: 1924800 \t tot_loss 7.551625 rs_loss 0.272956 vs_loss 0.928480 logits_loss 127.003792\n",
      "[3:1400] F: 1934400 \t tot_loss 7.456219 rs_loss 0.254424 vs_loss 0.854666 logits_loss 126.942583\n",
      "[3:1500] F: 1944000 \t tot_loss 7.368197 rs_loss 0.220283 vs_loss 0.798717 logits_loss 126.983949\n",
      "[3:1600] F: 1953600 \t tot_loss 7.464915 rs_loss 0.270205 vs_loss 0.853652 logits_loss 126.821157\n",
      "[3:1700] F: 1963200 \t tot_loss 7.687592 rs_loss 0.351506 vs_loss 0.996684 logits_loss 126.788038\n",
      "[3:1800] F: 1972800 \t tot_loss 7.680328 rs_loss 0.340501 vs_loss 0.997618 logits_loss 126.844178\n",
      "[3:1900] F: 1982400 \t tot_loss 7.810481 rs_loss 0.376621 vs_loss 1.087247 logits_loss 126.932267\n",
      "[3:2000] F: 1992000 \t tot_loss 7.665278 rs_loss 0.301960 vs_loss 1.007947 logits_loss 127.107402\n",
      "[3:2100] F: 2001600 \t tot_loss 7.487602 rs_loss 0.242250 vs_loss 0.906897 logits_loss 126.769086\n",
      "[3:2200] F: 2011200 \t tot_loss 7.519191 rs_loss 0.259757 vs_loss 0.917850 logits_loss 126.831683\n",
      "[3:2300] F: 2020800 \t tot_loss 7.426423 rs_loss 0.234281 vs_loss 0.853276 logits_loss 126.777329\n",
      "[3:2400] F: 2030400 \t tot_loss 7.541301 rs_loss 0.280449 vs_loss 0.920080 logits_loss 126.815428\n",
      "[3:2500] F: 2040000 \t tot_loss 7.494186 rs_loss 0.269212 vs_loss 0.870406 logits_loss 127.091340\n",
      "[3:2600] F: 2049600 \t tot_loss 7.457029 rs_loss 0.251975 vs_loss 0.857576 logits_loss 126.949561\n",
      "[3:2700] F: 2059200 \t tot_loss 7.763052 rs_loss 0.368149 vs_loss 1.063746 logits_loss 126.623140\n",
      "[3:2800] F: 2068800 \t tot_loss 7.793944 rs_loss 0.372315 vs_loss 1.090163 logits_loss 126.629316\n",
      "[3:2900] F: 2078400 \t tot_loss 7.819480 rs_loss 0.396347 vs_loss 1.093442 logits_loss 126.593794\n",
      "[3:3000] F: 2088000 \t tot_loss 7.922578 rs_loss 0.422396 vs_loss 1.171355 logits_loss 126.576531\n",
      "[3:3100] F: 2097600 \t tot_loss 7.710288 rs_loss 0.330795 vs_loss 1.047941 logits_loss 126.631052\n",
      "[3:3200] F: 2107200 \t tot_loss 7.522310 rs_loss 0.271341 vs_loss 0.926612 logits_loss 126.487134\n",
      "[3:3300] F: 2116800 \t tot_loss 7.564170 rs_loss 0.261045 vs_loss 0.981976 logits_loss 126.422991\n",
      "[3:3400] F: 2126400 \t tot_loss 7.502793 rs_loss 0.255875 vs_loss 0.925309 logits_loss 126.432183\n",
      "[3:3500] F: 2136000 \t tot_loss 7.434501 rs_loss 0.233618 vs_loss 0.878637 logits_loss 126.444909\n",
      "[3:3600] F: 2145600 \t tot_loss 7.559978 rs_loss 0.273182 vs_loss 0.950558 logits_loss 126.724733\n",
      "[3:3700] F: 2155200 \t tot_loss 7.452132 rs_loss 0.232854 vs_loss 0.878985 logits_loss 126.805850\n",
      "[3:3800] F: 2164800 \t tot_loss 7.492840 rs_loss 0.234242 vs_loss 0.930636 logits_loss 126.559240\n",
      "[3:3900] F: 2174400 \t tot_loss 7.511631 rs_loss 0.250778 vs_loss 0.921329 logits_loss 126.790494\n",
      "[3:4000] F: 2184000 \t tot_loss 7.470930 rs_loss 0.234726 vs_loss 0.923260 logits_loss 126.258876\n",
      "[3:4100] F: 2193600 \t tot_loss 7.503050 rs_loss 0.263278 vs_loss 0.937420 logits_loss 126.047031\n",
      "[3:4200] F: 2203200 \t tot_loss 7.443424 rs_loss 0.262990 vs_loss 0.866670 logits_loss 126.275290\n",
      "[3:4300] F: 2212800 \t tot_loss 7.389928 rs_loss 0.243133 vs_loss 0.839471 logits_loss 126.146485\n",
      "[3:4400] F: 2222400 \t tot_loss 7.413156 rs_loss 0.258068 vs_loss 0.848152 logits_loss 126.138712\n",
      "[3:4500] F: 2232000 \t tot_loss 7.430703 rs_loss 0.267482 vs_loss 0.863189 logits_loss 126.000624\n",
      "[3:4600] F: 2241600 \t tot_loss 7.410172 rs_loss 0.238404 vs_loss 0.878221 logits_loss 125.870945\n",
      "[3:4700] F: 2251200 \t tot_loss 7.526455 rs_loss 0.279409 vs_loss 0.953818 logits_loss 125.864577\n",
      "[3:4800] F: 2260800 \t tot_loss 7.504733 rs_loss 0.282734 vs_loss 0.926272 logits_loss 125.914531\n",
      "[3:4900] F: 2270400 \t tot_loss 7.528082 rs_loss 0.294779 vs_loss 0.929091 logits_loss 126.084235\n",
      "[3:5000] F: 2280000 \t tot_loss 7.603463 rs_loss 0.323120 vs_loss 0.975302 logits_loss 126.100818\n",
      "[3:5100] F: 2289600 \t tot_loss 7.553388 rs_loss 0.289099 vs_loss 0.958008 logits_loss 126.125610\n",
      "[3:5200] F: 2299200 \t tot_loss 7.629046 rs_loss 0.315458 vs_loss 1.013002 logits_loss 126.011718\n",
      "[3:5300] F: 2308800 \t tot_loss 7.600023 rs_loss 0.286876 vs_loss 1.014989 logits_loss 125.963147\n",
      "[3:5400] F: 2318400 \t tot_loss 7.564843 rs_loss 0.291271 vs_loss 0.982889 logits_loss 125.813650\n",
      "[3:5500] F: 2328000 \t tot_loss 7.511874 rs_loss 0.292937 vs_loss 0.942513 logits_loss 125.528493\n",
      "[3:5600] F: 2337600 \t tot_loss 7.381495 rs_loss 0.249180 vs_loss 0.848696 logits_loss 125.672366\n",
      "[3:5700] F: 2347200 \t tot_loss 7.444213 rs_loss 0.263007 vs_loss 0.900492 logits_loss 125.614278\n",
      "[3:5800] F: 2356800 \t tot_loss 7.459923 rs_loss 0.253280 vs_loss 0.921827 logits_loss 125.696305\n",
      "[3:5900] F: 2366400 \t tot_loss 7.495700 rs_loss 0.254756 vs_loss 0.950667 logits_loss 125.805541\n",
      "[3:6000] F: 2376000 \t tot_loss 7.504333 rs_loss 0.255400 vs_loss 0.964798 logits_loss 125.682702\n",
      "[3:6100] F: 2385600 \t tot_loss 7.498862 rs_loss 0.278788 vs_loss 0.936044 logits_loss 125.680619\n",
      "[3:6200] F: 2395200 \t tot_loss 7.603676 rs_loss 0.299523 vs_loss 1.024716 logits_loss 125.588738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [4] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.36 (+-0.08) \n",
      "[4:0] F: 2400000 \t tot_loss 7.640826 rs_loss 0.305346 vs_loss 1.049919 logits_loss 125.711223\n",
      "[4:100] F: 2409600 \t tot_loss 7.670875 rs_loss 0.308049 vs_loss 1.067206 logits_loss 125.912408\n",
      "[4:200] F: 2419200 \t tot_loss 7.602986 rs_loss 0.266957 vs_loss 1.029124 logits_loss 126.138119\n",
      "[4:300] F: 2428800 \t tot_loss 7.522819 rs_loss 0.236336 vs_loss 0.981851 logits_loss 126.092653\n",
      "[4:400] F: 2438400 \t tot_loss 7.438647 rs_loss 0.234159 vs_loss 0.897039 logits_loss 126.148985\n",
      "[4:500] F: 2448000 \t tot_loss 7.276044 rs_loss 0.188018 vs_loss 0.785447 logits_loss 126.051584\n",
      "[4:600] F: 2457600 \t tot_loss 7.281548 rs_loss 0.188191 vs_loss 0.791988 logits_loss 126.027388\n",
      "[4:700] F: 2467200 \t tot_loss 7.415973 rs_loss 0.255639 vs_loss 0.855477 logits_loss 126.097141\n",
      "[4:800] F: 2476800 \t tot_loss 7.370998 rs_loss 0.235366 vs_loss 0.830191 logits_loss 126.108801\n",
      "[4:900] F: 2486400 \t tot_loss 7.380445 rs_loss 0.231737 vs_loss 0.836330 logits_loss 126.247564\n",
      "[4:1000] F: 2496000 \t tot_loss 7.518755 rs_loss 0.276074 vs_loss 0.933363 logits_loss 126.186352\n",
      "[4:1100] F: 2505600 \t tot_loss 7.464671 rs_loss 0.222356 vs_loss 0.921593 logits_loss 126.414444\n",
      "[4:1200] F: 2515200 \t tot_loss 7.473552 rs_loss 0.225110 vs_loss 0.928396 logits_loss 126.400908\n",
      "[4:1300] F: 2524800 \t tot_loss 7.579709 rs_loss 0.253091 vs_loss 1.006741 logits_loss 126.397526\n",
      "[4:1400] F: 2534400 \t tot_loss 7.610189 rs_loss 0.260217 vs_loss 1.026839 logits_loss 126.462650\n",
      "[4:1500] F: 2544000 \t tot_loss 7.509909 rs_loss 0.235650 vs_loss 0.958857 logits_loss 126.308028\n",
      "[4:1600] F: 2553600 \t tot_loss 7.509282 rs_loss 0.243143 vs_loss 0.970402 logits_loss 125.914736\n",
      "[4:1700] F: 2563200 \t tot_loss 7.460191 rs_loss 0.230142 vs_loss 0.938817 logits_loss 125.824636\n",
      "[4:1800] F: 2572800 \t tot_loss 7.440409 rs_loss 0.243108 vs_loss 0.905671 logits_loss 125.832602\n",
      "[4:1900] F: 2582400 \t tot_loss 7.385274 rs_loss 0.240158 vs_loss 0.871522 logits_loss 125.471879\n",
      "[4:2000] F: 2592000 \t tot_loss 7.586450 rs_loss 0.299017 vs_loss 0.998683 logits_loss 125.774992\n",
      "[4:2100] F: 2601600 \t tot_loss 7.621827 rs_loss 0.310769 vs_loss 1.022705 logits_loss 125.767074\n",
      "[4:2200] F: 2611200 \t tot_loss 7.667209 rs_loss 0.317005 vs_loss 1.079432 logits_loss 125.415429\n",
      "[4:2300] F: 2620800 \t tot_loss 7.779321 rs_loss 0.344557 vs_loss 1.155550 logits_loss 125.584278\n",
      "[4:2400] F: 2630400 \t tot_loss 7.661967 rs_loss 0.319712 vs_loss 1.074263 logits_loss 125.359849\n",
      "[4:2500] F: 2640000 \t tot_loss 7.648525 rs_loss 0.328384 vs_loss 1.077986 logits_loss 124.843118\n",
      "[4:2600] F: 2649600 \t tot_loss 7.466488 rs_loss 0.276695 vs_loss 0.949213 logits_loss 124.811572\n",
      "[4:2700] F: 2659200 \t tot_loss 7.548443 rs_loss 0.297229 vs_loss 1.015591 logits_loss 124.712475\n",
      "[4:2800] F: 2668800 \t tot_loss 7.418547 rs_loss 0.244357 vs_loss 0.934015 logits_loss 124.803496\n",
      "[4:2900] F: 2678400 \t tot_loss 7.386531 rs_loss 0.233265 vs_loss 0.893549 logits_loss 125.194328\n",
      "[4:3000] F: 2688000 \t tot_loss 7.442810 rs_loss 0.244432 vs_loss 0.929510 logits_loss 125.377349\n",
      "[4:3100] F: 2697600 \t tot_loss 7.270227 rs_loss 0.196751 vs_loss 0.801035 logits_loss 125.448826\n",
      "[4:3200] F: 2707200 \t tot_loss 7.267281 rs_loss 0.198697 vs_loss 0.795814 logits_loss 125.455408\n",
      "[4:3300] F: 2716800 \t tot_loss 7.274131 rs_loss 0.183915 vs_loss 0.815196 logits_loss 125.500395\n",
      "[4:3400] F: 2726400 \t tot_loss 7.139429 rs_loss 0.144444 vs_loss 0.740414 logits_loss 125.091426\n",
      "[4:3500] F: 2736000 \t tot_loss 7.321564 rs_loss 0.212044 vs_loss 0.858259 logits_loss 125.025213\n",
      "[4:3600] F: 2745600 \t tot_loss 7.545492 rs_loss 0.267992 vs_loss 1.025659 logits_loss 125.036839\n",
      "[4:3700] F: 2755200 \t tot_loss 7.441949 rs_loss 0.254874 vs_loss 0.961546 logits_loss 124.510573\n",
      "[4:3800] F: 2764800 \t tot_loss 7.639076 rs_loss 0.295963 vs_loss 1.095244 logits_loss 124.957360\n",
      "[4:3900] F: 2774400 \t tot_loss 7.615227 rs_loss 0.292710 vs_loss 1.062221 logits_loss 125.205934\n",
      "[4:4000] F: 2784000 \t tot_loss 7.524490 rs_loss 0.262942 vs_loss 1.006161 logits_loss 125.107745\n",
      "[4:4100] F: 2793600 \t tot_loss 7.502602 rs_loss 0.256824 vs_loss 0.984842 logits_loss 125.218715\n",
      "[4:4200] F: 2803200 \t tot_loss 7.339018 rs_loss 0.234072 vs_loss 0.850058 logits_loss 125.097770\n",
      "[4:4300] F: 2812800 \t tot_loss 7.169655 rs_loss 0.163972 vs_loss 0.759305 logits_loss 124.927571\n",
      "[4:4400] F: 2822400 \t tot_loss 7.120155 rs_loss 0.161411 vs_loss 0.715603 logits_loss 124.862824\n",
      "[4:4500] F: 2832000 \t tot_loss 7.159288 rs_loss 0.163429 vs_loss 0.734094 logits_loss 125.235299\n",
      "[4:4600] F: 2841600 \t tot_loss 7.164690 rs_loss 0.155668 vs_loss 0.756269 logits_loss 125.055067\n",
      "[4:4700] F: 2851200 \t tot_loss 7.176445 rs_loss 0.154515 vs_loss 0.762016 logits_loss 125.198263\n",
      "[4:4800] F: 2860800 \t tot_loss 7.109476 rs_loss 0.135938 vs_loss 0.713320 logits_loss 125.204356\n",
      "[4:4900] F: 2870400 \t tot_loss 7.081831 rs_loss 0.132952 vs_loss 0.712235 logits_loss 124.732873\n",
      "[4:5000] F: 2880000 \t tot_loss 7.133425 rs_loss 0.150575 vs_loss 0.736742 logits_loss 124.922146\n",
      "[4:5100] F: 2889600 \t tot_loss 7.102398 rs_loss 0.146211 vs_loss 0.721702 logits_loss 124.689703\n",
      "[4:5200] F: 2899200 \t tot_loss 7.249849 rs_loss 0.193078 vs_loss 0.823018 logits_loss 124.675059\n",
      "[4:5300] F: 2908800 \t tot_loss 7.352331 rs_loss 0.212399 vs_loss 0.905617 logits_loss 124.686294\n",
      "[4:5400] F: 2918400 \t tot_loss 7.354564 rs_loss 0.213486 vs_loss 0.921709 logits_loss 124.387373\n",
      "[4:5500] F: 2928000 \t tot_loss 7.536271 rs_loss 0.273973 vs_loss 1.040970 logits_loss 124.426562\n",
      "[4:5600] F: 2937600 \t tot_loss 7.428143 rs_loss 0.245026 vs_loss 0.963431 logits_loss 124.393715\n",
      "[4:5700] F: 2947200 \t tot_loss 7.532434 rs_loss 0.272942 vs_loss 1.029755 logits_loss 124.594717\n",
      "[4:5800] F: 2956800 \t tot_loss 7.497197 rs_loss 0.261583 vs_loss 0.994793 logits_loss 124.816413\n",
      "[4:5900] F: 2966400 \t tot_loss 7.301148 rs_loss 0.204009 vs_loss 0.865042 logits_loss 124.641930\n",
      "[4:6000] F: 2976000 \t tot_loss 7.248039 rs_loss 0.186167 vs_loss 0.829378 logits_loss 124.649896\n",
      "[4:6100] F: 2985600 \t tot_loss 7.117309 rs_loss 0.165219 vs_loss 0.718427 logits_loss 124.673269\n",
      "[4:6200] F: 2995200 \t tot_loss 7.318169 rs_loss 0.217381 vs_loss 0.877040 logits_loss 124.474970\n",
      "Batch [5] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.29 (+-0.08) \n",
      "[5:0] F: 3000000 \t tot_loss 7.440189 rs_loss 0.241364 vs_loss 0.969032 logits_loss 124.595868\n",
      "[5:100] F: 3009600 \t tot_loss 7.511579 rs_loss 0.238175 vs_loss 1.010246 logits_loss 125.263151\n",
      "[5:200] F: 3019200 \t tot_loss 7.541841 rs_loss 0.236534 vs_loss 1.024869 logits_loss 125.608759\n",
      "[5:300] F: 3028800 \t tot_loss 7.295161 rs_loss 0.149705 vs_loss 0.834618 logits_loss 126.216741\n",
      "[5:400] F: 3038400 \t tot_loss 7.176811 rs_loss 0.128061 vs_loss 0.728576 logits_loss 126.403479\n",
      "[5:500] F: 3048000 \t tot_loss 7.288161 rs_loss 0.181826 vs_loss 0.801102 logits_loss 126.104663\n",
      "[5:600] F: 3057600 \t tot_loss 7.338084 rs_loss 0.207656 vs_loss 0.835409 logits_loss 125.900372\n",
      "[5:700] F: 3067200 \t tot_loss 7.474404 rs_loss 0.259739 vs_loss 0.919026 logits_loss 125.912787\n",
      "[5:800] F: 3076800 \t tot_loss 7.496891 rs_loss 0.275109 vs_loss 0.930146 logits_loss 125.832703\n",
      "[5:900] F: 3086400 \t tot_loss 7.481658 rs_loss 0.268335 vs_loss 0.923794 logits_loss 125.790575\n",
      "[5:1000] F: 3096000 \t tot_loss 7.409422 rs_loss 0.237914 vs_loss 0.877059 logits_loss 125.888983\n",
      "[5:1100] F: 3105600 \t tot_loss 7.252419 rs_loss 0.179666 vs_loss 0.789665 logits_loss 125.661753\n",
      "[5:1200] F: 3115200 \t tot_loss 7.401533 rs_loss 0.220670 vs_loss 0.889203 logits_loss 125.833221\n",
      "[5:1300] F: 3124800 \t tot_loss 7.286164 rs_loss 0.177445 vs_loss 0.826282 logits_loss 125.648724\n",
      "[5:1400] F: 3134400 \t tot_loss 7.384250 rs_loss 0.199375 vs_loss 0.905469 logits_loss 125.588118\n",
      "[5:1500] F: 3144000 \t tot_loss 7.476420 rs_loss 0.223927 vs_loss 0.978504 logits_loss 125.479766\n",
      "[5:1600] F: 3153600 \t tot_loss 7.460443 rs_loss 0.220766 vs_loss 0.976627 logits_loss 125.260989\n",
      "[5:1700] F: 3163200 \t tot_loss 7.424810 rs_loss 0.213614 vs_loss 0.948791 logits_loss 125.248109\n",
      "[5:1800] F: 3172800 \t tot_loss 7.317129 rs_loss 0.185678 vs_loss 0.864046 logits_loss 125.348084\n",
      "[5:1900] F: 3182400 \t tot_loss 7.316794 rs_loss 0.192460 vs_loss 0.855698 logits_loss 125.372739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5:2000] F: 3192000 \t tot_loss 7.169198 rs_loss 0.149763 vs_loss 0.758761 logits_loss 125.213470\n",
      "[5:2100] F: 3201600 \t tot_loss 7.174175 rs_loss 0.156444 vs_loss 0.755232 logits_loss 125.249986\n",
      "[5:2200] F: 3211200 \t tot_loss 7.239128 rs_loss 0.186655 vs_loss 0.800325 logits_loss 125.042963\n",
      "[5:2300] F: 3220800 \t tot_loss 7.236779 rs_loss 0.181366 vs_loss 0.802262 logits_loss 125.063013\n",
      "[5:2400] F: 3230400 \t tot_loss 7.333910 rs_loss 0.198863 vs_loss 0.886926 logits_loss 124.962425\n",
      "[5:2500] F: 3240000 \t tot_loss 7.325291 rs_loss 0.190798 vs_loss 0.889218 logits_loss 124.905507\n",
      "[5:2600] F: 3249600 \t tot_loss 7.237264 rs_loss 0.159542 vs_loss 0.842983 logits_loss 124.694789\n",
      "[5:2700] F: 3259200 \t tot_loss 7.206619 rs_loss 0.164486 vs_loss 0.813441 logits_loss 124.573835\n",
      "[5:2800] F: 3268800 \t tot_loss 7.132605 rs_loss 0.146322 vs_loss 0.747578 logits_loss 124.774078\n",
      "[5:2900] F: 3278400 \t tot_loss 7.142328 rs_loss 0.150585 vs_loss 0.751718 logits_loss 124.800497\n",
      "[5:3000] F: 3288000 \t tot_loss 7.172103 rs_loss 0.155515 vs_loss 0.762321 logits_loss 125.085353\n",
      "[5:3100] F: 3297600 \t tot_loss 7.228903 rs_loss 0.185542 vs_loss 0.785492 logits_loss 125.157373\n",
      "[5:3200] F: 3307200 \t tot_loss 7.179597 rs_loss 0.175874 vs_loss 0.755815 logits_loss 124.958162\n",
      "[5:3300] F: 3316800 \t tot_loss 7.310122 rs_loss 0.220958 vs_loss 0.842911 logits_loss 124.925043\n",
      "[5:3400] F: 3326400 \t tot_loss 7.433664 rs_loss 0.252343 vs_loss 0.944533 logits_loss 124.735758\n",
      "[5:3500] F: 3336000 \t tot_loss 7.410526 rs_loss 0.218527 vs_loss 0.962296 logits_loss 124.594062\n",
      "[5:3600] F: 3345600 \t tot_loss 7.581514 rs_loss 0.276710 vs_loss 1.077515 logits_loss 124.545790\n",
      "[5:3700] F: 3355200 \t tot_loss 7.706350 rs_loss 0.304017 vs_loss 1.176172 logits_loss 124.523227\n",
      "[5:3800] F: 3364800 \t tot_loss 7.558014 rs_loss 0.266583 vs_loss 1.057838 logits_loss 124.671864\n",
      "[5:3900] F: 3374400 \t tot_loss 7.680293 rs_loss 0.317586 vs_loss 1.123382 logits_loss 124.786487\n",
      "[5:4000] F: 3384000 \t tot_loss 7.532613 rs_loss 0.266778 vs_loss 1.014670 logits_loss 125.023312\n",
      "[5:4100] F: 3393600 \t tot_loss 7.233054 rs_loss 0.183441 vs_loss 0.807514 logits_loss 124.841959\n",
      "[5:4200] F: 3403200 \t tot_loss 7.208637 rs_loss 0.182870 vs_loss 0.785083 logits_loss 124.813681\n",
      "[5:4300] F: 3412800 \t tot_loss 7.048457 rs_loss 0.130867 vs_loss 0.674409 logits_loss 124.863601\n",
      "[5:4400] F: 3422400 \t tot_loss 6.994644 rs_loss 0.115103 vs_loss 0.641781 logits_loss 124.755206\n",
      "[5:4500] F: 3432000 \t tot_loss 7.122490 rs_loss 0.147366 vs_loss 0.726725 logits_loss 124.967984\n",
      "[5:4600] F: 3441600 \t tot_loss 7.142264 rs_loss 0.156065 vs_loss 0.748535 logits_loss 124.753286\n",
      "[5:4700] F: 3451200 \t tot_loss 7.175806 rs_loss 0.160989 vs_loss 0.787579 logits_loss 124.544747\n",
      "[5:4800] F: 3460800 \t tot_loss 7.174725 rs_loss 0.168820 vs_loss 0.791225 logits_loss 124.293612\n",
      "[5:4900] F: 3470400 \t tot_loss 7.306769 rs_loss 0.206840 vs_loss 0.888124 logits_loss 124.236095\n",
      "[5:5000] F: 3480000 \t tot_loss 7.332279 rs_loss 0.200438 vs_loss 0.915707 logits_loss 124.322677\n",
      "[5:5100] F: 3489600 \t tot_loss 7.232256 rs_loss 0.172360 vs_loss 0.841284 logits_loss 124.372236\n",
      "[5:5200] F: 3499200 \t tot_loss 7.384687 rs_loss 0.220183 vs_loss 0.937126 logits_loss 124.547546\n",
      "[5:5300] F: 3508800 \t tot_loss 7.243004 rs_loss 0.179076 vs_loss 0.842524 logits_loss 124.428079\n",
      "[5:5400] F: 3518400 \t tot_loss 7.287809 rs_loss 0.197653 vs_loss 0.868134 logits_loss 124.440431\n",
      "[5:5500] F: 3528000 \t tot_loss 7.291997 rs_loss 0.205479 vs_loss 0.866150 logits_loss 124.407350\n",
      "[5:5600] F: 3537600 \t tot_loss 7.391016 rs_loss 0.228551 vs_loss 0.940064 logits_loss 124.448017\n",
      "[5:5700] F: 3547200 \t tot_loss 7.306197 rs_loss 0.208186 vs_loss 0.877709 logits_loss 124.406045\n",
      "[5:5800] F: 3556800 \t tot_loss 7.311469 rs_loss 0.206123 vs_loss 0.885015 logits_loss 124.406624\n",
      "[5:5900] F: 3566400 \t tot_loss 7.378978 rs_loss 0.225124 vs_loss 0.927758 logits_loss 124.521932\n",
      "[5:6000] F: 3576000 \t tot_loss 7.122301 rs_loss 0.153086 vs_loss 0.753353 logits_loss 124.317230\n",
      "[5:6100] F: 3585600 \t tot_loss 7.071764 rs_loss 0.148395 vs_loss 0.713034 logits_loss 124.206682\n",
      "[5:6200] F: 3595200 \t tot_loss 6.987921 rs_loss 0.128806 vs_loss 0.651638 logits_loss 124.149526\n",
      "Batch [6] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.04 (+-0.15) \n",
      "[6:0] F: 3600000 \t tot_loss 6.910951 rs_loss 0.105958 vs_loss 0.601585 logits_loss 124.068153\n",
      "[6:100] F: 3609600 \t tot_loss 7.246951 rs_loss 0.225471 vs_loss 0.822697 logits_loss 123.975651\n",
      "[6:200] F: 3619200 \t tot_loss 7.470835 rs_loss 0.303835 vs_loss 0.963554 logits_loss 124.068911\n",
      "[6:300] F: 3628800 \t tot_loss 7.905266 rs_loss 0.421194 vs_loss 1.273696 logits_loss 124.207516\n",
      "[6:400] F: 3638400 \t tot_loss 8.007709 rs_loss 0.447117 vs_loss 1.342630 logits_loss 124.359241\n",
      "[6:500] F: 3648000 \t tot_loss 7.778436 rs_loss 0.342257 vs_loss 1.207282 logits_loss 124.577937\n",
      "[6:600] F: 3657600 \t tot_loss 7.658039 rs_loss 0.289341 vs_loss 1.142792 logits_loss 124.518103\n",
      "[6:700] F: 3667200 \t tot_loss 7.233964 rs_loss 0.169033 vs_loss 0.837777 logits_loss 124.543080\n",
      "[6:800] F: 3676800 \t tot_loss 7.173958 rs_loss 0.165809 vs_loss 0.800586 logits_loss 124.151262\n",
      "[6:900] F: 3686400 \t tot_loss 7.135508 rs_loss 0.175739 vs_loss 0.751022 logits_loss 124.174931\n",
      "[6:1000] F: 3696000 \t tot_loss 7.168155 rs_loss 0.182145 vs_loss 0.770870 logits_loss 124.302815\n",
      "[6:1100] F: 3705600 \t tot_loss 7.357139 rs_loss 0.225039 vs_loss 0.914632 logits_loss 124.349348\n",
      "[6:1200] F: 3715200 \t tot_loss 7.612444 rs_loss 0.299004 vs_loss 1.078069 logits_loss 124.707416\n",
      "[6:1300] F: 3724800 \t tot_loss 7.747880 rs_loss 0.314543 vs_loss 1.203664 logits_loss 124.593444\n",
      "[6:1400] F: 3734400 \t tot_loss 7.767791 rs_loss 0.319010 vs_loss 1.222669 logits_loss 124.522250\n",
      "[6:1500] F: 3744000 \t tot_loss 7.626638 rs_loss 0.295130 vs_loss 1.098828 logits_loss 124.653597\n",
      "[6:1600] F: 3753600 \t tot_loss 7.478226 rs_loss 0.251952 vs_loss 0.996728 logits_loss 124.590906\n",
      "[6:1700] F: 3763200 \t tot_loss 7.246965 rs_loss 0.197419 vs_loss 0.823050 logits_loss 124.529916\n",
      "[6:1800] F: 3772800 \t tot_loss 7.123509 rs_loss 0.168190 vs_loss 0.732229 logits_loss 124.461810\n",
      "[6:1900] F: 3782400 \t tot_loss 7.193768 rs_loss 0.182487 vs_loss 0.793180 logits_loss 124.362015\n",
      "[6:2000] F: 3792000 \t tot_loss 7.173049 rs_loss 0.161278 vs_loss 0.808946 logits_loss 124.056514\n",
      "[6:2100] F: 3801600 \t tot_loss 7.267447 rs_loss 0.193594 vs_loss 0.877156 logits_loss 123.933931\n",
      "[6:2200] F: 3811200 \t tot_loss 7.267669 rs_loss 0.180051 vs_loss 0.897754 logits_loss 123.797291\n",
      "[6:2300] F: 3820800 \t tot_loss 7.295235 rs_loss 0.196031 vs_loss 0.908340 logits_loss 123.817283\n",
      "[6:2400] F: 3830400 \t tot_loss 7.324579 rs_loss 0.218135 vs_loss 0.905644 logits_loss 124.016008\n",
      "[6:2500] F: 3840000 \t tot_loss 7.310709 rs_loss 0.208955 vs_loss 0.900055 logits_loss 124.033978\n",
      "[6:2600] F: 3849600 \t tot_loss 7.445110 rs_loss 0.259519 vs_loss 0.991081 logits_loss 123.890187\n",
      "[6:2700] F: 3859200 \t tot_loss 7.441330 rs_loss 0.262073 vs_loss 0.982741 logits_loss 123.930320\n",
      "[6:2800] F: 3868800 \t tot_loss 7.312999 rs_loss 0.222553 vs_loss 0.905394 logits_loss 123.701036\n",
      "[6:2900] F: 3878400 \t tot_loss 7.274914 rs_loss 0.216613 vs_loss 0.882125 logits_loss 123.523521\n",
      "[6:3000] F: 3888000 \t tot_loss 7.217325 rs_loss 0.196459 vs_loss 0.831131 logits_loss 123.794707\n",
      "[6:3100] F: 3897600 \t tot_loss 7.228582 rs_loss 0.201327 vs_loss 0.850897 logits_loss 123.527154\n",
      "[6:3200] F: 3907200 \t tot_loss 7.368108 rs_loss 0.247337 vs_loss 0.945396 logits_loss 123.507490\n",
      "[6:3300] F: 3916800 \t tot_loss 7.352007 rs_loss 0.251662 vs_loss 0.924103 logits_loss 123.524848\n",
      "[6:3400] F: 3926400 \t tot_loss 7.337358 rs_loss 0.245628 vs_loss 0.919117 logits_loss 123.452257\n",
      "[6:3500] F: 3936000 \t tot_loss 7.185238 rs_loss 0.186781 vs_loss 0.833699 logits_loss 123.295181\n",
      "[6:3600] F: 3945600 \t tot_loss 7.028661 rs_loss 0.124811 vs_loss 0.734150 logits_loss 123.393997\n",
      "[6:3700] F: 3955200 \t tot_loss 6.981585 rs_loss 0.110164 vs_loss 0.703589 logits_loss 123.356644\n",
      "[6:3800] F: 3964800 \t tot_loss 6.884730 rs_loss 0.092786 vs_loss 0.630166 logits_loss 123.235546\n",
      "[6:3900] F: 3974400 \t tot_loss 6.977583 rs_loss 0.121690 vs_loss 0.682442 logits_loss 123.469022\n",
      "[6:4000] F: 3984000 \t tot_loss 7.074582 rs_loss 0.151734 vs_loss 0.757715 logits_loss 123.302658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6:4100] F: 3993600 \t tot_loss 7.198776 rs_loss 0.195494 vs_loss 0.840142 logits_loss 123.262803\n",
      "[6:4200] F: 4003200 \t tot_loss 7.378460 rs_loss 0.243231 vs_loss 0.964504 logits_loss 123.414518\n",
      "[6:4300] F: 4012800 \t tot_loss 7.287929 rs_loss 0.216203 vs_loss 0.908336 logits_loss 123.267797\n",
      "[6:4400] F: 4022400 \t tot_loss 7.189839 rs_loss 0.185614 vs_loss 0.831593 logits_loss 123.452624\n",
      "[6:4500] F: 4032000 \t tot_loss 7.068335 rs_loss 0.128475 vs_loss 0.753923 logits_loss 123.718745\n",
      "[6:4600] F: 4041600 \t tot_loss 6.914824 rs_loss 0.084850 vs_loss 0.647475 logits_loss 123.649967\n",
      "[6:4700] F: 4051200 \t tot_loss 6.984845 rs_loss 0.121739 vs_loss 0.672938 logits_loss 123.803369\n",
      "[6:4800] F: 4060800 \t tot_loss 7.016823 rs_loss 0.130798 vs_loss 0.690339 logits_loss 123.913723\n",
      "[6:4900] F: 4070400 \t tot_loss 7.223955 rs_loss 0.205894 vs_loss 0.831984 logits_loss 123.721539\n",
      "[6:5000] F: 4080000 \t tot_loss 7.292978 rs_loss 0.227489 vs_loss 0.879399 logits_loss 123.721812\n",
      "[6:5100] F: 4089600 \t tot_loss 7.348946 rs_loss 0.241617 vs_loss 0.926507 logits_loss 123.616428\n",
      "[6:5200] F: 4099200 \t tot_loss 7.469906 rs_loss 0.293256 vs_loss 1.000129 logits_loss 123.530416\n",
      "[6:5300] F: 4108800 \t tot_loss 7.435825 rs_loss 0.278139 vs_loss 0.981147 logits_loss 123.530781\n",
      "[6:5400] F: 4118400 \t tot_loss 7.340858 rs_loss 0.246741 vs_loss 0.908977 logits_loss 123.702806\n",
      "[6:5500] F: 4128000 \t tot_loss 7.329762 rs_loss 0.245217 vs_loss 0.905765 logits_loss 123.575609\n",
      "[6:5600] F: 4137600 \t tot_loss 7.244539 rs_loss 0.215078 vs_loss 0.857351 logits_loss 123.442192\n",
      "[6:5700] F: 4147200 \t tot_loss 7.108102 rs_loss 0.178869 vs_loss 0.766116 logits_loss 123.262337\n",
      "[6:5800] F: 4156800 \t tot_loss 7.086844 rs_loss 0.182556 vs_loss 0.776457 logits_loss 122.556617\n",
      "[6:5900] F: 4166400 \t tot_loss 7.036648 rs_loss 0.166954 vs_loss 0.749328 logits_loss 122.407305\n",
      "[6:6000] F: 4176000 \t tot_loss 6.960904 rs_loss 0.140869 vs_loss 0.705817 logits_loss 122.284346\n",
      "[6:6100] F: 4185600 \t tot_loss 7.061550 rs_loss 0.176592 vs_loss 0.767479 logits_loss 122.349583\n",
      "[6:6200] F: 4195200 \t tot_loss 7.127695 rs_loss 0.206099 vs_loss 0.788970 logits_loss 122.652502\n",
      "Batch [7] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.07 (+-0.16) \n",
      "[7:0] F: 4200000 \t tot_loss 7.135491 rs_loss 0.201273 vs_loss 0.804407 logits_loss 122.596220\n",
      "[7:100] F: 4209600 \t tot_loss 7.073738 rs_loss 0.185310 vs_loss 0.743224 logits_loss 122.904057\n",
      "[7:200] F: 4219200 \t tot_loss 7.152488 rs_loss 0.202839 vs_loss 0.775718 logits_loss 123.478617\n",
      "[7:300] F: 4228800 \t tot_loss 6.983311 rs_loss 0.117696 vs_loss 0.662820 logits_loss 124.055900\n",
      "[7:400] F: 4238400 \t tot_loss 7.002198 rs_loss 0.124187 vs_loss 0.666979 logits_loss 124.220646\n",
      "[7:500] F: 4248000 \t tot_loss 7.007221 rs_loss 0.115180 vs_loss 0.680380 logits_loss 124.233214\n",
      "[7:600] F: 4257600 \t tot_loss 6.970907 rs_loss 0.104145 vs_loss 0.656901 logits_loss 124.197221\n",
      "[7:700] F: 4267200 \t tot_loss 7.010797 rs_loss 0.113156 vs_loss 0.701979 logits_loss 123.913253\n",
      "[7:800] F: 4276800 \t tot_loss 7.119821 rs_loss 0.135274 vs_loss 0.783128 logits_loss 124.028383\n",
      "[7:900] F: 4286400 \t tot_loss 7.118733 rs_loss 0.139799 vs_loss 0.773402 logits_loss 124.110622\n",
      "[7:1000] F: 4296000 \t tot_loss 7.174668 rs_loss 0.159506 vs_loss 0.817107 logits_loss 123.961095\n",
      "[7:1100] F: 4305600 \t tot_loss 7.203014 rs_loss 0.177812 vs_loss 0.836439 logits_loss 123.775252\n",
      "[7:1200] F: 4315200 \t tot_loss 7.105602 rs_loss 0.152472 vs_loss 0.764453 logits_loss 123.773534\n",
      "[7:1300] F: 4324800 \t tot_loss 7.122548 rs_loss 0.154948 vs_loss 0.787811 logits_loss 123.595783\n",
      "[7:1400] F: 4334400 \t tot_loss 7.177305 rs_loss 0.172392 vs_loss 0.832280 logits_loss 123.452671\n",
      "[7:1500] F: 4344000 \t tot_loss 7.241782 rs_loss 0.190162 vs_loss 0.877777 logits_loss 123.476868\n",
      "[7:1600] F: 4353600 \t tot_loss 7.230912 rs_loss 0.186367 vs_loss 0.868491 logits_loss 123.521065\n",
      "[7:1700] F: 4363200 \t tot_loss 7.245538 rs_loss 0.202325 vs_loss 0.870575 logits_loss 123.452749\n",
      "[7:1800] F: 4372800 \t tot_loss 7.189562 rs_loss 0.184248 vs_loss 0.832156 logits_loss 123.463170\n",
      "[7:1900] F: 4382400 \t tot_loss 7.044546 rs_loss 0.143323 vs_loss 0.716869 logits_loss 123.687070\n",
      "[7:2000] F: 4392000 \t tot_loss 7.117611 rs_loss 0.162087 vs_loss 0.786283 logits_loss 123.384816\n",
      "[7:2100] F: 4401600 \t tot_loss 7.068840 rs_loss 0.138727 vs_loss 0.755791 logits_loss 123.486435\n",
      "[7:2200] F: 4411200 \t tot_loss 7.080885 rs_loss 0.137856 vs_loss 0.770689 logits_loss 123.446804\n",
      "[7:2300] F: 4420800 \t tot_loss 7.165781 rs_loss 0.157446 vs_loss 0.850245 logits_loss 123.161809\n",
      "[7:2400] F: 4430400 \t tot_loss 7.331939 rs_loss 0.219029 vs_loss 0.944087 logits_loss 123.376436\n",
      "[7:2500] F: 4440000 \t tot_loss 7.316018 rs_loss 0.214501 vs_loss 0.937992 logits_loss 123.270490\n",
      "[7:2600] F: 4449600 \t tot_loss 7.262325 rs_loss 0.211580 vs_loss 0.889701 logits_loss 123.220869\n",
      "[7:2700] F: 4459200 \t tot_loss 7.145584 rs_loss 0.185501 vs_loss 0.794068 logits_loss 123.320308\n",
      "[7:2800] F: 4468800 \t tot_loss 6.972626 rs_loss 0.151025 vs_loss 0.667384 logits_loss 123.084328\n",
      "[7:2900] F: 4478400 \t tot_loss 7.134895 rs_loss 0.199550 vs_loss 0.777610 logits_loss 123.154697\n",
      "[7:3000] F: 4488000 \t tot_loss 7.223923 rs_loss 0.223505 vs_loss 0.835665 logits_loss 123.295045\n",
      "[7:3100] F: 4497600 \t tot_loss 7.244581 rs_loss 0.225710 vs_loss 0.852718 logits_loss 123.323073\n",
      "[7:3200] F: 4507200 \t tot_loss 7.170360 rs_loss 0.175567 vs_loss 0.824737 logits_loss 123.401121\n",
      "[7:3300] F: 4516800 \t tot_loss 7.070783 rs_loss 0.146786 vs_loss 0.759577 logits_loss 123.288400\n",
      "[7:3400] F: 4526400 \t tot_loss 7.108237 rs_loss 0.150784 vs_loss 0.813486 logits_loss 122.879353\n",
      "[7:3500] F: 4536000 \t tot_loss 7.214843 rs_loss 0.177094 vs_loss 0.892578 logits_loss 122.903415\n",
      "[7:3600] F: 4545600 \t tot_loss 7.282141 rs_loss 0.232240 vs_loss 0.916587 logits_loss 122.666289\n",
      "[7:3700] F: 4555200 \t tot_loss 7.313020 rs_loss 0.237237 vs_loss 0.933417 logits_loss 122.847303\n",
      "[7:3800] F: 4564800 \t tot_loss 7.173012 rs_loss 0.206022 vs_loss 0.820585 logits_loss 122.928103\n",
      "[7:3900] F: 4574400 \t tot_loss 7.300462 rs_loss 0.229617 vs_loss 0.936416 logits_loss 122.688580\n",
      "[7:4000] F: 4584000 \t tot_loss 7.309917 rs_loss 0.207262 vs_loss 0.950142 logits_loss 123.050266\n",
      "[7:4100] F: 4593600 \t tot_loss 7.305201 rs_loss 0.208149 vs_loss 0.948217 logits_loss 122.976707\n",
      "[7:4200] F: 4603200 \t tot_loss 7.276602 rs_loss 0.186317 vs_loss 0.937618 logits_loss 123.053335\n",
      "[7:4300] F: 4612800 \t tot_loss 7.018259 rs_loss 0.130485 vs_loss 0.741302 logits_loss 122.929425\n",
      "[7:4400] F: 4622400 \t tot_loss 6.914834 rs_loss 0.101815 vs_loss 0.679092 logits_loss 122.678531\n",
      "[7:4500] F: 4632000 \t tot_loss 6.909321 rs_loss 0.100513 vs_loss 0.672784 logits_loss 122.720496\n",
      "[7:4600] F: 4641600 \t tot_loss 6.892005 rs_loss 0.105268 vs_loss 0.649665 logits_loss 122.741444\n",
      "[7:4700] F: 4651200 \t tot_loss 6.971630 rs_loss 0.131733 vs_loss 0.699070 logits_loss 122.816528\n",
      "[7:4800] F: 4660800 \t tot_loss 6.977535 rs_loss 0.131659 vs_loss 0.717832 logits_loss 122.560880\n",
      "[7:4900] F: 4670400 \t tot_loss 6.950033 rs_loss 0.139743 vs_loss 0.685670 logits_loss 122.492394\n",
      "[7:5000] F: 4680000 \t tot_loss 7.071351 rs_loss 0.179440 vs_loss 0.769036 logits_loss 122.457503\n",
      "[7:5100] F: 4689600 \t tot_loss 6.997320 rs_loss 0.155429 vs_loss 0.718573 logits_loss 122.466369\n",
      "[7:5200] F: 4699200 \t tot_loss 7.021455 rs_loss 0.158528 vs_loss 0.727092 logits_loss 122.716695\n",
      "[7:5300] F: 4708800 \t tot_loss 7.025790 rs_loss 0.153801 vs_loss 0.736949 logits_loss 122.700800\n",
      "[7:5400] F: 4718400 \t tot_loss 6.890927 rs_loss 0.108766 vs_loss 0.650771 logits_loss 122.627787\n",
      "[7:5500] F: 4728000 \t tot_loss 6.886653 rs_loss 0.107113 vs_loss 0.644247 logits_loss 122.705855\n",
      "[7:5600] F: 4737600 \t tot_loss 6.946819 rs_loss 0.133819 vs_loss 0.668293 logits_loss 122.894135\n",
      "[7:5700] F: 4747200 \t tot_loss 6.899416 rs_loss 0.113450 vs_loss 0.653417 logits_loss 122.650986\n",
      "[7:5800] F: 4756800 \t tot_loss 6.896126 rs_loss 0.116783 vs_loss 0.646414 logits_loss 122.658576\n",
      "[7:5900] F: 4766400 \t tot_loss 7.001580 rs_loss 0.140035 vs_loss 0.729965 logits_loss 122.631611\n",
      "[7:6000] F: 4776000 \t tot_loss 7.024279 rs_loss 0.137877 vs_loss 0.770362 logits_loss 122.320783\n",
      "[7:6100] F: 4785600 \t tot_loss 7.046839 rs_loss 0.156754 vs_loss 0.773720 logits_loss 122.327308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7:6200] F: 4795200 \t tot_loss 7.070941 rs_loss 0.158685 vs_loss 0.806590 logits_loss 122.113327\n",
      "Batch [8] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.12 (+-0.09) \n",
      "[8:0] F: 4800000 \t tot_loss 7.161080 rs_loss 0.181902 vs_loss 0.871898 logits_loss 122.145608\n",
      "[8:100] F: 4809600 \t tot_loss 7.092304 rs_loss 0.164018 vs_loss 0.811476 logits_loss 122.336191\n",
      "[8:200] F: 4819200 \t tot_loss 7.186691 rs_loss 0.191345 vs_loss 0.869865 logits_loss 122.509607\n",
      "[8:300] F: 4828800 \t tot_loss 7.330579 rs_loss 0.241249 vs_loss 0.945317 logits_loss 122.880259\n",
      "[8:400] F: 4838400 \t tot_loss 7.333165 rs_loss 0.239998 vs_loss 0.928985 logits_loss 123.283633\n",
      "[8:500] F: 4848000 \t tot_loss 7.325083 rs_loss 0.239113 vs_loss 0.930305 logits_loss 123.113310\n",
      "[8:600] F: 4857600 \t tot_loss 7.283378 rs_loss 0.212837 vs_loss 0.901006 logits_loss 123.390698\n",
      "[8:700] F: 4867200 \t tot_loss 7.255071 rs_loss 0.176699 vs_loss 0.908876 logits_loss 123.389938\n",
      "[8:800] F: 4876800 \t tot_loss 7.381644 rs_loss 0.219894 vs_loss 0.988922 logits_loss 123.456541\n",
      "[8:900] F: 4886400 \t tot_loss 7.330814 rs_loss 0.218720 vs_loss 0.944682 logits_loss 123.348224\n",
      "[8:1000] F: 4896000 \t tot_loss 7.273563 rs_loss 0.209327 vs_loss 0.919861 logits_loss 122.887490\n",
      "[8:1100] F: 4905600 \t tot_loss 7.259750 rs_loss 0.208204 vs_loss 0.916293 logits_loss 122.705055\n",
      "[8:1200] F: 4915200 \t tot_loss 7.055809 rs_loss 0.143126 vs_loss 0.784672 logits_loss 122.560221\n",
      "[8:1300] F: 4924800 \t tot_loss 7.086469 rs_loss 0.139797 vs_loss 0.817455 logits_loss 122.584334\n",
      "[8:1400] F: 4934400 \t tot_loss 7.231877 rs_loss 0.177297 vs_loss 0.908359 logits_loss 122.924418\n",
      "[8:1500] F: 4944000 \t tot_loss 7.137730 rs_loss 0.159629 vs_loss 0.832764 logits_loss 122.906747\n",
      "[8:1600] F: 4953600 \t tot_loss 7.231743 rs_loss 0.190780 vs_loss 0.904017 logits_loss 122.738908\n",
      "[8:1700] F: 4963200 \t tot_loss 7.183657 rs_loss 0.187333 vs_loss 0.858761 logits_loss 122.751253\n",
      "[8:1800] F: 4972800 \t tot_loss 7.000831 rs_loss 0.132345 vs_loss 0.748033 logits_loss 122.409048\n",
      "[8:1900] F: 4982400 \t tot_loss 7.089458 rs_loss 0.173466 vs_loss 0.790918 logits_loss 122.501471\n",
      "[8:2000] F: 4992000 \t tot_loss 7.029181 rs_loss 0.144946 vs_loss 0.759191 logits_loss 122.500878\n",
      "[8:2100] F: 5001600 \t tot_loss 7.175320 rs_loss 0.187469 vs_loss 0.859664 logits_loss 122.563729\n",
      "[8:2200] F: 5011200 \t tot_loss 7.254702 rs_loss 0.206039 vs_loss 0.914134 logits_loss 122.690580\n",
      "[8:2300] F: 5020800 \t tot_loss 7.149097 rs_loss 0.156640 vs_loss 0.871954 logits_loss 122.410060\n",
      "[8:2400] F: 5030400 \t tot_loss 7.360966 rs_loss 0.234002 vs_loss 1.004653 logits_loss 122.446220\n",
      "[8:2500] F: 5040000 \t tot_loss 7.142088 rs_loss 0.163947 vs_loss 0.859386 logits_loss 122.375095\n",
      "[8:2600] F: 5049600 \t tot_loss 7.222816 rs_loss 0.192618 vs_loss 0.913130 logits_loss 122.341353\n",
      "[8:2700] F: 5059200 \t tot_loss 7.460095 rs_loss 0.254548 vs_loss 1.070355 logits_loss 122.703835\n",
      "[8:2800] F: 5068800 \t tot_loss 7.338661 rs_loss 0.209397 vs_loss 0.980084 logits_loss 122.983603\n",
      "[8:2900] F: 5078400 \t tot_loss 7.482996 rs_loss 0.242597 vs_loss 1.098180 logits_loss 122.844374\n",
      "[8:3000] F: 5088000 \t tot_loss 7.493444 rs_loss 0.240680 vs_loss 1.118997 logits_loss 122.675332\n",
      "[8:3100] F: 5097600 \t tot_loss 7.518649 rs_loss 0.248465 vs_loss 1.145998 logits_loss 122.483721\n",
      "[8:3200] F: 5107200 \t tot_loss 7.559629 rs_loss 0.268537 vs_loss 1.182551 logits_loss 122.170819\n",
      "[8:3300] F: 5116800 \t tot_loss 7.428938 rs_loss 0.243232 vs_loss 1.073984 logits_loss 122.234425\n",
      "[8:3400] F: 5126400 \t tot_loss 7.259996 rs_loss 0.195016 vs_loss 0.931639 logits_loss 122.666815\n",
      "[8:3500] F: 5136000 \t tot_loss 7.070867 rs_loss 0.146516 vs_loss 0.786450 logits_loss 122.758018\n",
      "[8:3600] F: 5145600 \t tot_loss 7.237519 rs_loss 0.188805 vs_loss 0.902310 logits_loss 122.928085\n",
      "[8:3700] F: 5155200 \t tot_loss 7.428886 rs_loss 0.233201 vs_loss 1.043207 logits_loss 123.049564\n",
      "[8:3800] F: 5164800 \t tot_loss 7.484421 rs_loss 0.262014 vs_loss 1.084359 logits_loss 122.760945\n",
      "[8:3900] F: 5174400 \t tot_loss 7.483920 rs_loss 0.263878 vs_loss 1.093997 logits_loss 122.520900\n",
      "[8:4000] F: 5184000 \t tot_loss 7.323111 rs_loss 0.225578 vs_loss 0.984892 logits_loss 122.252812\n",
      "[8:4100] F: 5193600 \t tot_loss 7.199703 rs_loss 0.204261 vs_loss 0.888341 logits_loss 122.142028\n",
      "[8:4200] F: 5203200 \t tot_loss 7.267574 rs_loss 0.216741 vs_loss 0.945412 logits_loss 122.108437\n",
      "[8:4300] F: 5212800 \t tot_loss 7.300483 rs_loss 0.223881 vs_loss 0.956745 logits_loss 122.397139\n",
      "[8:4400] F: 5222400 \t tot_loss 7.214548 rs_loss 0.198125 vs_loss 0.891514 logits_loss 122.498186\n",
      "[8:4500] F: 5232000 \t tot_loss 7.299695 rs_loss 0.221991 vs_loss 0.952268 logits_loss 122.508729\n",
      "[8:4600] F: 5241600 \t tot_loss 7.130905 rs_loss 0.179741 vs_loss 0.840242 logits_loss 122.218430\n",
      "[8:4700] F: 5251200 \t tot_loss 7.177577 rs_loss 0.198265 vs_loss 0.878577 logits_loss 122.014697\n",
      "[8:4800] F: 5260800 \t tot_loss 7.219616 rs_loss 0.202961 vs_loss 0.915785 logits_loss 122.017392\n",
      "[8:4900] F: 5270400 \t tot_loss 7.253046 rs_loss 0.218361 vs_loss 0.952263 logits_loss 121.648428\n",
      "[8:5000] F: 5280000 \t tot_loss 7.402467 rs_loss 0.252251 vs_loss 1.059908 logits_loss 121.806158\n",
      "[8:5100] F: 5289600 \t tot_loss 7.314232 rs_loss 0.222011 vs_loss 0.992304 logits_loss 121.998351\n",
      "[8:5200] F: 5299200 \t tot_loss 7.202515 rs_loss 0.193423 vs_loss 0.911795 logits_loss 121.945927\n",
      "[8:5300] F: 5308800 \t tot_loss 7.168713 rs_loss 0.177822 vs_loss 0.884479 logits_loss 122.128224\n",
      "[8:5400] F: 5318400 \t tot_loss 7.147718 rs_loss 0.166426 vs_loss 0.871991 logits_loss 122.186031\n",
      "[8:5500] F: 5328000 \t tot_loss 7.056356 rs_loss 0.145194 vs_loss 0.815011 logits_loss 121.923031\n",
      "[8:5600] F: 5337600 \t tot_loss 7.152107 rs_loss 0.165028 vs_loss 0.887442 logits_loss 121.992751\n",
      "[8:5700] F: 5347200 \t tot_loss 6.979192 rs_loss 0.111190 vs_loss 0.765965 logits_loss 122.040728\n",
      "[8:5800] F: 5356800 \t tot_loss 7.071580 rs_loss 0.156629 vs_loss 0.819700 logits_loss 121.905008\n",
      "[8:5900] F: 5366400 \t tot_loss 7.283833 rs_loss 0.211492 vs_loss 0.983091 logits_loss 121.784991\n",
      "[8:6000] F: 5376000 \t tot_loss 7.279662 rs_loss 0.214105 vs_loss 0.976065 logits_loss 121.789861\n",
      "[8:6100] F: 5385600 \t tot_loss 7.294437 rs_loss 0.215057 vs_loss 0.981644 logits_loss 121.954715\n",
      "[8:6200] F: 5395200 \t tot_loss 7.174634 rs_loss 0.176434 vs_loss 0.897426 logits_loss 122.015468\n",
      "Batch [9] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.01 (+-0.16) \n",
      "[9:0] F: 5400000 \t tot_loss 7.061526 rs_loss 0.149462 vs_loss 0.794745 logits_loss 122.346391\n",
      "[9:100] F: 5409600 \t tot_loss 7.081659 rs_loss 0.158402 vs_loss 0.788628 logits_loss 122.692593\n",
      "[9:200] F: 5419200 \t tot_loss 7.226744 rs_loss 0.198012 vs_loss 0.880388 logits_loss 122.966877\n",
      "[9:300] F: 5428800 \t tot_loss 7.408301 rs_loss 0.239938 vs_loss 0.999006 logits_loss 123.387129\n",
      "[9:400] F: 5438400 \t tot_loss 7.363405 rs_loss 0.224093 vs_loss 0.950808 logits_loss 123.770075\n",
      "[9:500] F: 5448000 \t tot_loss 7.256620 rs_loss 0.188803 vs_loss 0.875532 logits_loss 123.845704\n",
      "[9:600] F: 5457600 \t tot_loss 7.316847 rs_loss 0.216358 vs_loss 0.904877 logits_loss 123.912221\n",
      "[9:700] F: 5467200 \t tot_loss 7.136838 rs_loss 0.170315 vs_loss 0.776694 logits_loss 123.796562\n",
      "[9:800] F: 5476800 \t tot_loss 7.207099 rs_loss 0.200374 vs_loss 0.819889 logits_loss 123.736702\n",
      "[9:900] F: 5486400 \t tot_loss 7.190221 rs_loss 0.202414 vs_loss 0.805446 logits_loss 123.647231\n",
      "[9:1000] F: 5496000 \t tot_loss 7.112037 rs_loss 0.165882 vs_loss 0.759149 logits_loss 123.740112\n",
      "[9:1100] F: 5505600 \t tot_loss 7.170624 rs_loss 0.194010 vs_loss 0.793282 logits_loss 123.666637\n",
      "[9:1200] F: 5515200 \t tot_loss 7.092074 rs_loss 0.165502 vs_loss 0.739533 logits_loss 123.740769\n",
      "[9:1300] F: 5524800 \t tot_loss 7.107045 rs_loss 0.162008 vs_loss 0.768909 logits_loss 123.522555\n",
      "[9:1400] F: 5534400 \t tot_loss 7.128229 rs_loss 0.175137 vs_loss 0.794007 logits_loss 123.181700\n",
      "[9:1500] F: 5544000 \t tot_loss 7.067433 rs_loss 0.143812 vs_loss 0.753872 logits_loss 123.394966\n",
      "[9:1600] F: 5553600 \t tot_loss 7.044995 rs_loss 0.131174 vs_loss 0.752179 logits_loss 123.232835\n",
      "[9:1700] F: 5563200 \t tot_loss 7.190760 rs_loss 0.185911 vs_loss 0.831565 logits_loss 123.465678\n",
      "[9:1800] F: 5572800 \t tot_loss 7.161141 rs_loss 0.189497 vs_loss 0.791115 logits_loss 123.610576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9:1900] F: 5582400 \t tot_loss 7.175905 rs_loss 0.202118 vs_loss 0.811403 logits_loss 123.247679\n",
      "[9:2000] F: 5592000 \t tot_loss 7.401554 rs_loss 0.269498 vs_loss 0.970911 logits_loss 123.222901\n",
      "[9:2100] F: 5601600 \t tot_loss 7.373969 rs_loss 0.248766 vs_loss 0.974654 logits_loss 123.010986\n",
      "[9:2200] F: 5611200 \t tot_loss 7.366872 rs_loss 0.225682 vs_loss 0.993682 logits_loss 122.950174\n",
      "[9:2300] F: 5620800 \t tot_loss 7.368877 rs_loss 0.229395 vs_loss 0.987393 logits_loss 123.041778\n",
      "[9:2400] F: 5630400 \t tot_loss 7.172896 rs_loss 0.167212 vs_loss 0.843049 logits_loss 123.252700\n",
      "[9:2500] F: 5640000 \t tot_loss 7.153120 rs_loss 0.161567 vs_loss 0.829832 logits_loss 123.234410\n",
      "[9:2600] F: 5649600 \t tot_loss 7.309112 rs_loss 0.210961 vs_loss 0.938368 logits_loss 123.195667\n",
      "[9:2700] F: 5659200 \t tot_loss 7.312305 rs_loss 0.216097 vs_loss 0.943264 logits_loss 123.058882\n",
      "[9:2800] F: 5668800 \t tot_loss 7.299077 rs_loss 0.218778 vs_loss 0.944237 logits_loss 122.721248\n",
      "[9:2900] F: 5678400 \t tot_loss 7.240669 rs_loss 0.191363 vs_loss 0.906995 logits_loss 122.846238\n",
      "[9:3000] F: 5688000 \t tot_loss 7.068323 rs_loss 0.142921 vs_loss 0.778769 logits_loss 122.932648\n",
      "[9:3100] F: 5697600 \t tot_loss 7.038514 rs_loss 0.130057 vs_loss 0.750829 logits_loss 123.152547\n",
      "[9:3200] F: 5707200 \t tot_loss 7.095361 rs_loss 0.151824 vs_loss 0.778920 logits_loss 123.292349\n",
      "[9:3300] F: 5716800 \t tot_loss 7.026629 rs_loss 0.148094 vs_loss 0.724132 logits_loss 123.088061\n",
      "[9:3400] F: 5726400 \t tot_loss 6.963641 rs_loss 0.140743 vs_loss 0.671369 logits_loss 123.030583\n",
      "[9:3500] F: 5736000 \t tot_loss 7.124349 rs_loss 0.187874 vs_loss 0.795574 logits_loss 122.818025\n",
      "[9:3600] F: 5745600 \t tot_loss 7.060404 rs_loss 0.165660 vs_loss 0.766762 logits_loss 122.559659\n",
      "[9:3700] F: 5755200 \t tot_loss 6.977367 rs_loss 0.143125 vs_loss 0.710253 logits_loss 122.479773\n",
      "[9:3800] F: 5764800 \t tot_loss 7.000474 rs_loss 0.144465 vs_loss 0.737169 logits_loss 122.376797\n",
      "[9:3900] F: 5774400 \t tot_loss 7.045014 rs_loss 0.167918 vs_loss 0.751404 logits_loss 122.513840\n",
      "[9:4000] F: 5784000 \t tot_loss 7.052890 rs_loss 0.165824 vs_loss 0.755982 logits_loss 122.621666\n",
      "[9:4100] F: 5793600 \t tot_loss 7.201930 rs_loss 0.214611 vs_loss 0.844904 logits_loss 122.848305\n",
      "[9:4200] F: 5803200 \t tot_loss 7.270616 rs_loss 0.240919 vs_loss 0.885789 logits_loss 122.878150\n",
      "[9:4300] F: 5812800 \t tot_loss 7.112277 rs_loss 0.192257 vs_loss 0.777492 logits_loss 122.850554\n",
      "[9:4400] F: 5822400 \t tot_loss 7.171680 rs_loss 0.210316 vs_loss 0.819814 logits_loss 122.830988\n",
      "[9:4500] F: 5832000 \t tot_loss 7.120333 rs_loss 0.184105 vs_loss 0.794717 logits_loss 122.830216\n",
      "[9:4600] F: 5841600 \t tot_loss 7.001461 rs_loss 0.143192 vs_loss 0.717827 logits_loss 122.808823\n",
      "[9:4700] F: 5851200 \t tot_loss 7.036122 rs_loss 0.140460 vs_loss 0.753728 logits_loss 122.838662\n",
      "[9:4800] F: 5860800 \t tot_loss 7.023781 rs_loss 0.150193 vs_loss 0.732343 logits_loss 122.824878\n",
      "[9:4900] F: 5870400 \t tot_loss 6.952426 rs_loss 0.127216 vs_loss 0.685776 logits_loss 122.788668\n",
      "[9:5000] F: 5880000 \t tot_loss 7.018466 rs_loss 0.136732 vs_loss 0.745601 logits_loss 122.722661\n",
      "[9:5100] F: 5889600 \t tot_loss 6.948201 rs_loss 0.112209 vs_loss 0.694511 logits_loss 122.829613\n",
      "[9:5200] F: 5899200 \t tot_loss 6.980885 rs_loss 0.108275 vs_loss 0.727817 logits_loss 122.895864\n",
      "[9:5300] F: 5908800 \t tot_loss 6.983306 rs_loss 0.103557 vs_loss 0.736253 logits_loss 122.869928\n",
      "[9:5400] F: 5918400 \t tot_loss 6.905627 rs_loss 0.083772 vs_loss 0.683431 logits_loss 122.768467\n",
      "[9:5500] F: 5928000 \t tot_loss 6.953581 rs_loss 0.108167 vs_loss 0.714156 logits_loss 122.625150\n",
      "[9:5600] F: 5937600 \t tot_loss 6.843591 rs_loss 0.084550 vs_loss 0.632524 logits_loss 122.530343\n",
      "[9:5700] F: 5947200 \t tot_loss 6.849385 rs_loss 0.100893 vs_loss 0.618226 logits_loss 122.605333\n",
      "[9:5800] F: 5956800 \t tot_loss 6.919595 rs_loss 0.131133 vs_loss 0.649574 logits_loss 122.777767\n",
      "[9:5900] F: 5966400 \t tot_loss 6.909808 rs_loss 0.127802 vs_loss 0.646437 logits_loss 122.711375\n",
      "[9:6000] F: 5976000 \t tot_loss 6.934585 rs_loss 0.140303 vs_loss 0.649803 logits_loss 122.889566\n",
      "[9:6100] F: 5985600 \t tot_loss 7.089957 rs_loss 0.180602 vs_loss 0.772579 logits_loss 122.735516\n",
      "[9:6200] F: 5995200 \t tot_loss 7.154080 rs_loss 0.204607 vs_loss 0.826050 logits_loss 122.468464\n",
      "Batch [10] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.03 (+-0.16) \n",
      "[10:0] F: 6000000 \t tot_loss 7.120214 rs_loss 0.180732 vs_loss 0.807670 logits_loss 122.636250\n",
      "[10:100] F: 6009600 \t tot_loss 7.408948 rs_loss 0.251446 vs_loss 1.020708 logits_loss 122.735878\n",
      "[10:200] F: 6019200 \t tot_loss 7.382214 rs_loss 0.230497 vs_loss 1.007335 logits_loss 122.887645\n",
      "[10:300] F: 6028800 \t tot_loss 7.416335 rs_loss 0.222400 vs_loss 1.034148 logits_loss 123.195746\n",
      "[10:400] F: 6038400 \t tot_loss 7.250254 rs_loss 0.179865 vs_loss 0.904872 logits_loss 123.310358\n",
      "[10:500] F: 6048000 \t tot_loss 7.292717 rs_loss 0.198301 vs_loss 0.937265 logits_loss 123.143019\n",
      "[10:600] F: 6057600 \t tot_loss 7.307806 rs_loss 0.219764 vs_loss 0.924472 logits_loss 123.271411\n",
      "[10:700] F: 6067200 \t tot_loss 7.297153 rs_loss 0.220772 vs_loss 0.919240 logits_loss 123.142806\n",
      "[10:800] F: 6076800 \t tot_loss 7.299395 rs_loss 0.212737 vs_loss 0.940341 logits_loss 122.926344\n",
      "[10:900] F: 6086400 \t tot_loss 7.075638 rs_loss 0.136477 vs_loss 0.794634 logits_loss 122.890522\n",
      "[10:1000] F: 6096000 \t tot_loss 7.170917 rs_loss 0.171208 vs_loss 0.870276 logits_loss 122.588661\n",
      "[10:1100] F: 6105600 \t tot_loss 7.126650 rs_loss 0.169502 vs_loss 0.829651 logits_loss 122.549936\n",
      "[10:1200] F: 6115200 \t tot_loss 7.177746 rs_loss 0.195762 vs_loss 0.860434 logits_loss 122.430998\n",
      "[10:1300] F: 6124800 \t tot_loss 7.287249 rs_loss 0.237793 vs_loss 0.927998 logits_loss 122.429161\n",
      "[10:1400] F: 6134400 \t tot_loss 7.231817 rs_loss 0.207037 vs_loss 0.900012 logits_loss 122.495357\n",
      "[10:1500] F: 6144000 \t tot_loss 7.301705 rs_loss 0.213756 vs_loss 0.961172 logits_loss 122.535537\n",
      "[10:1600] F: 6153600 \t tot_loss 7.246355 rs_loss 0.186949 vs_loss 0.922939 logits_loss 122.729317\n",
      "[10:1700] F: 6163200 \t tot_loss 7.071862 rs_loss 0.146756 vs_loss 0.795086 logits_loss 122.600412\n",
      "[10:1800] F: 6172800 \t tot_loss 6.971728 rs_loss 0.120907 vs_loss 0.715450 logits_loss 122.707428\n",
      "[10:1900] F: 6182400 \t tot_loss 7.029295 rs_loss 0.159692 vs_loss 0.752899 logits_loss 122.334082\n",
      "[10:2000] F: 6192000 \t tot_loss 7.260229 rs_loss 0.237264 vs_loss 0.910271 logits_loss 122.253873\n",
      "[10:2100] F: 6201600 \t tot_loss 7.250539 rs_loss 0.221681 vs_loss 0.911382 logits_loss 122.349510\n",
      "[10:2200] F: 6211200 \t tot_loss 7.292389 rs_loss 0.243843 vs_loss 0.937353 logits_loss 122.223854\n",
      "[10:2300] F: 6220800 \t tot_loss 7.425306 rs_loss 0.269929 vs_loss 1.039624 logits_loss 122.315050\n",
      "[10:2400] F: 6230400 \t tot_loss 7.370590 rs_loss 0.246989 vs_loss 1.007340 logits_loss 122.325220\n",
      "[10:2500] F: 6240000 \t tot_loss 7.482744 rs_loss 0.285110 vs_loss 1.085822 logits_loss 122.236252\n",
      "[10:2600] F: 6249600 \t tot_loss 7.462052 rs_loss 0.265533 vs_loss 1.085604 logits_loss 122.218308\n",
      "[10:2700] F: 6259200 \t tot_loss 7.231219 rs_loss 0.198421 vs_loss 0.913732 logits_loss 122.381335\n",
      "[10:2800] F: 6268800 \t tot_loss 7.139235 rs_loss 0.183826 vs_loss 0.844378 logits_loss 122.220602\n",
      "[10:2900] F: 6278400 \t tot_loss 7.109235 rs_loss 0.179501 vs_loss 0.819797 logits_loss 122.198719\n",
      "[10:3000] F: 6288000 \t tot_loss 7.153790 rs_loss 0.200607 vs_loss 0.849331 logits_loss 122.077042\n",
      "[10:3100] F: 6297600 \t tot_loss 7.234870 rs_loss 0.217790 vs_loss 0.921595 logits_loss 121.909707\n",
      "[10:3200] F: 6307200 \t tot_loss 7.220396 rs_loss 0.204995 vs_loss 0.913070 logits_loss 122.046631\n",
      "[10:3300] F: 6316800 \t tot_loss 7.180834 rs_loss 0.183342 vs_loss 0.896994 logits_loss 122.009943\n",
      "[10:3400] F: 6326400 \t tot_loss 7.152727 rs_loss 0.177222 vs_loss 0.876616 logits_loss 121.977777\n",
      "[10:3500] F: 6336000 \t tot_loss 7.209502 rs_loss 0.201501 vs_loss 0.911872 logits_loss 121.922573\n",
      "[10:3600] F: 6345600 \t tot_loss 7.249371 rs_loss 0.225910 vs_loss 0.941278 logits_loss 121.643653\n",
      "[10:3700] F: 6355200 \t tot_loss 7.240068 rs_loss 0.232796 vs_loss 0.930999 logits_loss 121.525471\n",
      "[10:3800] F: 6364800 \t tot_loss 7.189182 rs_loss 0.213790 vs_loss 0.903310 logits_loss 121.441619\n",
      "[10:3900] F: 6374400 \t tot_loss 7.024505 rs_loss 0.164456 vs_loss 0.777847 logits_loss 121.644038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:4000] F: 6384000 \t tot_loss 7.059137 rs_loss 0.162546 vs_loss 0.805284 logits_loss 121.826153\n",
      "[10:4100] F: 6393600 \t tot_loss 7.115749 rs_loss 0.184430 vs_loss 0.837044 logits_loss 121.885504\n",
      "[10:4200] F: 6403200 \t tot_loss 7.438186 rs_loss 0.276719 vs_loss 1.055649 logits_loss 122.116351\n",
      "[10:4300] F: 6412800 \t tot_loss 7.366944 rs_loss 0.254203 vs_loss 1.013955 logits_loss 121.975707\n",
      "[10:4400] F: 6422400 \t tot_loss 7.229116 rs_loss 0.212907 vs_loss 0.922303 logits_loss 121.878119\n",
      "[10:4500] F: 6432000 \t tot_loss 7.268736 rs_loss 0.209089 vs_loss 0.952836 logits_loss 122.136235\n",
      "[10:4600] F: 6441600 \t tot_loss 7.006340 rs_loss 0.134057 vs_loss 0.766162 logits_loss 122.122420\n",
      "[10:4700] F: 6451200 \t tot_loss 7.016163 rs_loss 0.130069 vs_loss 0.779638 logits_loss 122.129124\n",
      "[10:4800] F: 6460800 \t tot_loss 7.139304 rs_loss 0.171816 vs_loss 0.856307 logits_loss 122.223618\n",
      "[10:4900] F: 6470400 \t tot_loss 7.133431 rs_loss 0.180736 vs_loss 0.856840 logits_loss 121.917112\n",
      "[10:5000] F: 6480000 \t tot_loss 7.087758 rs_loss 0.162249 vs_loss 0.834260 logits_loss 121.824989\n",
      "[10:5100] F: 6489600 \t tot_loss 7.315718 rs_loss 0.262881 vs_loss 0.972670 logits_loss 121.603336\n",
      "[10:5200] F: 6499200 \t tot_loss 7.388527 rs_loss 0.261372 vs_loss 1.046094 logits_loss 121.621215\n",
      "[10:5300] F: 6508800 \t tot_loss 7.347334 rs_loss 0.250005 vs_loss 1.005117 logits_loss 121.844228\n",
      "[10:5400] F: 6518400 \t tot_loss 7.551968 rs_loss 0.316229 vs_loss 1.136524 logits_loss 121.984303\n",
      "[10:5500] F: 6528000 \t tot_loss 7.497452 rs_loss 0.295543 vs_loss 1.105668 logits_loss 121.924811\n",
      "[10:5600] F: 6537600 \t tot_loss 7.283434 rs_loss 0.246805 vs_loss 0.948181 logits_loss 121.768957\n",
      "[10:5700] F: 6547200 \t tot_loss 7.160707 rs_loss 0.206017 vs_loss 0.872718 logits_loss 121.639426\n",
      "[10:5800] F: 6556800 \t tot_loss 7.261345 rs_loss 0.229965 vs_loss 0.968049 logits_loss 121.266609\n",
      "[10:5900] F: 6566400 \t tot_loss 7.131018 rs_loss 0.182947 vs_loss 0.869640 logits_loss 121.568605\n",
      "[10:6000] F: 6576000 \t tot_loss 7.143445 rs_loss 0.184869 vs_loss 0.884335 logits_loss 121.484816\n",
      "[10:6100] F: 6585600 \t tot_loss 7.288963 rs_loss 0.223318 vs_loss 0.988996 logits_loss 121.532960\n",
      "[10:6200] F: 6595200 \t tot_loss 6.928750 rs_loss 0.125399 vs_loss 0.726742 logits_loss 121.532165\n",
      "Batch [11] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.08 (+-0.09) \n",
      "[11:0] F: 6600000 \t tot_loss 6.989325 rs_loss 0.150678 vs_loss 0.763040 logits_loss 121.512128\n",
      "[11:100] F: 6609600 \t tot_loss 7.063673 rs_loss 0.162521 vs_loss 0.800946 logits_loss 122.004128\n",
      "[11:200] F: 6619200 \t tot_loss 7.303746 rs_loss 0.220843 vs_loss 0.968224 logits_loss 122.293586\n",
      "[11:300] F: 6628800 \t tot_loss 7.362007 rs_loss 0.237334 vs_loss 1.001087 logits_loss 122.471724\n",
      "[11:400] F: 6638400 \t tot_loss 7.477800 rs_loss 0.260927 vs_loss 1.079369 logits_loss 122.750082\n",
      "[11:500] F: 6648000 \t tot_loss 7.505431 rs_loss 0.267331 vs_loss 1.124962 logits_loss 122.262743\n",
      "[11:600] F: 6657600 \t tot_loss 7.333150 rs_loss 0.237020 vs_loss 0.991830 logits_loss 122.085996\n",
      "[11:700] F: 6667200 \t tot_loss 7.209127 rs_loss 0.212498 vs_loss 0.909063 logits_loss 121.751319\n",
      "[11:800] F: 6676800 \t tot_loss 7.189199 rs_loss 0.220766 vs_loss 0.894652 logits_loss 121.475614\n",
      "[11:900] F: 6686400 \t tot_loss 7.058554 rs_loss 0.191101 vs_loss 0.795357 logits_loss 121.441920\n",
      "[11:1000] F: 6696000 \t tot_loss 6.942017 rs_loss 0.154427 vs_loss 0.720584 logits_loss 121.340109\n",
      "[11:1100] F: 6705600 \t tot_loss 7.070658 rs_loss 0.192080 vs_loss 0.815031 logits_loss 121.270929\n",
      "[11:1200] F: 6715200 \t tot_loss 7.023060 rs_loss 0.160195 vs_loss 0.804933 logits_loss 121.158656\n",
      "[11:1300] F: 6724800 \t tot_loss 7.003328 rs_loss 0.149215 vs_loss 0.789778 logits_loss 121.286718\n",
      "[11:1400] F: 6734400 \t tot_loss 7.084828 rs_loss 0.165521 vs_loss 0.854828 logits_loss 121.289580\n",
      "[11:1500] F: 6744000 \t tot_loss 6.953220 rs_loss 0.120517 vs_loss 0.774333 logits_loss 121.167372\n",
      "[11:1600] F: 6753600 \t tot_loss 7.075637 rs_loss 0.154253 vs_loss 0.858432 logits_loss 121.259036\n",
      "[11:1700] F: 6763200 \t tot_loss 7.075236 rs_loss 0.160906 vs_loss 0.875416 logits_loss 120.778301\n",
      "[11:1800] F: 6772800 \t tot_loss 7.058901 rs_loss 0.152136 vs_loss 0.871162 logits_loss 120.712049\n",
      "[11:1900] F: 6782400 \t tot_loss 7.161343 rs_loss 0.176931 vs_loss 0.943979 logits_loss 120.808656\n",
      "[11:2000] F: 6792000 \t tot_loss 7.034835 rs_loss 0.146194 vs_loss 0.863563 logits_loss 120.501553\n",
      "[11:2100] F: 6801600 \t tot_loss 7.120637 rs_loss 0.173704 vs_loss 0.914273 logits_loss 120.653221\n",
      "[11:2200] F: 6811200 \t tot_loss 7.015337 rs_loss 0.151026 vs_loss 0.845140 logits_loss 120.383411\n",
      "[11:2300] F: 6820800 \t tot_loss 6.896770 rs_loss 0.124491 vs_loss 0.754846 logits_loss 120.348673\n",
      "[11:2400] F: 6830400 \t tot_loss 6.836025 rs_loss 0.106319 vs_loss 0.695595 logits_loss 120.682215\n",
      "[11:2500] F: 6840000 \t tot_loss 6.970556 rs_loss 0.140139 vs_loss 0.784581 logits_loss 120.916714\n",
      "[11:2600] F: 6849600 \t tot_loss 6.971013 rs_loss 0.142980 vs_loss 0.759118 logits_loss 121.378295\n",
      "[11:2700] F: 6859200 \t tot_loss 7.009941 rs_loss 0.165184 vs_loss 0.782769 logits_loss 121.239754\n",
      "[11:2800] F: 6868800 \t tot_loss 7.033722 rs_loss 0.182645 vs_loss 0.786094 logits_loss 121.299644\n",
      "[11:2900] F: 6878400 \t tot_loss 6.803766 rs_loss 0.110141 vs_loss 0.638407 logits_loss 121.104354\n",
      "[11:3000] F: 6888000 \t tot_loss 6.796184 rs_loss 0.114263 vs_loss 0.653493 logits_loss 120.568541\n",
      "[11:3100] F: 6897600 \t tot_loss 6.949813 rs_loss 0.138662 vs_loss 0.770348 logits_loss 120.816072\n",
      "[11:3200] F: 6907200 \t tot_loss 6.887111 rs_loss 0.113885 vs_loss 0.744156 logits_loss 120.581405\n",
      "[11:3300] F: 6916800 \t tot_loss 6.874053 rs_loss 0.121181 vs_loss 0.734576 logits_loss 120.365938\n",
      "[11:3400] F: 6926400 \t tot_loss 6.937570 rs_loss 0.140366 vs_loss 0.768252 logits_loss 120.579028\n",
      "[11:3500] F: 6936000 \t tot_loss 6.852978 rs_loss 0.118373 vs_loss 0.711724 logits_loss 120.457605\n",
      "[11:3600] F: 6945600 \t tot_loss 6.963400 rs_loss 0.144074 vs_loss 0.795065 logits_loss 120.485219\n",
      "[11:3700] F: 6955200 \t tot_loss 6.945340 rs_loss 0.135395 vs_loss 0.775881 logits_loss 120.681278\n",
      "[11:3800] F: 6964800 \t tot_loss 6.910739 rs_loss 0.123283 vs_loss 0.751347 logits_loss 120.722170\n",
      "[11:3900] F: 6974400 \t tot_loss 6.976098 rs_loss 0.144499 vs_loss 0.802733 logits_loss 120.577322\n",
      "[11:4000] F: 6984000 \t tot_loss 7.013961 rs_loss 0.161635 vs_loss 0.819871 logits_loss 120.649109\n",
      "[11:4100] F: 6993600 \t tot_loss 7.218546 rs_loss 0.218411 vs_loss 0.967345 logits_loss 120.655777\n",
      "[11:4200] F: 7003200 \t tot_loss 7.274983 rs_loss 0.232251 vs_loss 1.007616 logits_loss 120.702334\n",
      "[11:4300] F: 7012800 \t tot_loss 7.203673 rs_loss 0.212378 vs_loss 0.951559 logits_loss 120.794726\n",
      "[11:4400] F: 7022400 \t tot_loss 7.330260 rs_loss 0.241921 vs_loss 1.054067 logits_loss 120.685446\n",
      "[11:4500] F: 7032000 \t tot_loss 7.282728 rs_loss 0.239100 vs_loss 1.005916 logits_loss 120.754242\n",
      "[11:4600] F: 7041600 \t tot_loss 7.253546 rs_loss 0.242423 vs_loss 0.971102 logits_loss 120.800421\n",
      "[11:4700] F: 7051200 \t tot_loss 7.137630 rs_loss 0.214574 vs_loss 0.885729 logits_loss 120.746533\n",
      "[11:4800] F: 7060800 \t tot_loss 6.867669 rs_loss 0.146414 vs_loss 0.670300 logits_loss 121.019094\n",
      "[11:4900] F: 7070400 \t tot_loss 6.717410 rs_loss 0.098318 vs_loss 0.581884 logits_loss 120.744148\n",
      "[11:5000] F: 7080000 \t tot_loss 6.822984 rs_loss 0.120419 vs_loss 0.672061 logits_loss 120.610086\n",
      "[11:5100] F: 7089600 \t tot_loss 6.907448 rs_loss 0.144139 vs_loss 0.718900 logits_loss 120.888192\n",
      "[11:5200] F: 7099200 \t tot_loss 6.929132 rs_loss 0.142412 vs_loss 0.749513 logits_loss 120.744124\n",
      "[11:5300] F: 7108800 \t tot_loss 7.022549 rs_loss 0.189831 vs_loss 0.789422 logits_loss 120.865921\n",
      "[11:5400] F: 7118400 \t tot_loss 6.955917 rs_loss 0.158426 vs_loss 0.746913 logits_loss 121.011565\n",
      "[11:5500] F: 7128000 \t tot_loss 6.866179 rs_loss 0.134785 vs_loss 0.695043 logits_loss 120.727014\n",
      "[11:5600] F: 7137600 \t tot_loss 6.898022 rs_loss 0.148727 vs_loss 0.715754 logits_loss 120.670828\n",
      "[11:5700] F: 7147200 \t tot_loss 6.873323 rs_loss 0.127766 vs_loss 0.715722 logits_loss 120.596683\n",
      "[11:5800] F: 7156800 \t tot_loss 6.855134 rs_loss 0.129816 vs_loss 0.696430 logits_loss 120.577756\n",
      "[11:5900] F: 7166400 \t tot_loss 6.933748 rs_loss 0.157779 vs_loss 0.751440 logits_loss 120.490594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:6000] F: 7176000 \t tot_loss 6.992450 rs_loss 0.180848 vs_loss 0.801298 logits_loss 120.206086\n",
      "[11:6100] F: 7185600 \t tot_loss 7.023801 rs_loss 0.184646 vs_loss 0.834697 logits_loss 120.089159\n",
      "[11:6200] F: 7195200 \t tot_loss 6.931111 rs_loss 0.157828 vs_loss 0.773013 logits_loss 120.005392\n",
      "Batch [12] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.05 (+-0.08) \n",
      "[12:0] F: 7200000 \t tot_loss 6.904033 rs_loss 0.158284 vs_loss 0.749492 logits_loss 119.925121\n",
      "[12:100] F: 7209600 \t tot_loss 7.149747 rs_loss 0.210022 vs_loss 0.908172 logits_loss 120.631054\n",
      "[12:200] F: 7219200 \t tot_loss 7.013691 rs_loss 0.168456 vs_loss 0.774054 logits_loss 121.423635\n",
      "[12:300] F: 7228800 \t tot_loss 7.141307 rs_loss 0.194706 vs_loss 0.847630 logits_loss 121.979413\n",
      "[12:400] F: 7238400 \t tot_loss 7.260878 rs_loss 0.199575 vs_loss 0.942479 logits_loss 122.376476\n",
      "[12:500] F: 7248000 \t tot_loss 7.084602 rs_loss 0.149018 vs_loss 0.816040 logits_loss 122.390891\n",
      "[12:600] F: 7257600 \t tot_loss 6.977349 rs_loss 0.115721 vs_loss 0.748754 logits_loss 122.257495\n",
      "[12:700] F: 7267200 \t tot_loss 7.097799 rs_loss 0.138518 vs_loss 0.857171 logits_loss 122.042204\n",
      "[12:800] F: 7276800 \t tot_loss 7.156190 rs_loss 0.187391 vs_loss 0.864208 logits_loss 122.091804\n",
      "[12:900] F: 7286400 \t tot_loss 7.344498 rs_loss 0.239448 vs_loss 1.005333 logits_loss 121.994341\n",
      "[12:1000] F: 7296000 \t tot_loss 7.438210 rs_loss 0.268795 vs_loss 1.073298 logits_loss 121.922349\n",
      "[12:1100] F: 7305600 \t tot_loss 7.295257 rs_loss 0.244569 vs_loss 0.968991 logits_loss 121.633933\n",
      "[12:1200] F: 7315200 \t tot_loss 7.147896 rs_loss 0.199319 vs_loss 0.873295 logits_loss 121.505646\n",
      "[12:1300] F: 7324800 \t tot_loss 7.003553 rs_loss 0.173485 vs_loss 0.752459 logits_loss 121.552182\n",
      "[12:1400] F: 7334400 \t tot_loss 7.037048 rs_loss 0.169585 vs_loss 0.775950 logits_loss 121.830279\n",
      "[12:1500] F: 7344000 \t tot_loss 6.962908 rs_loss 0.142687 vs_loss 0.704923 logits_loss 122.305962\n",
      "[12:1600] F: 7353600 \t tot_loss 6.903466 rs_loss 0.109414 vs_loss 0.687139 logits_loss 122.138238\n",
      "[12:1700] F: 7363200 \t tot_loss 6.937693 rs_loss 0.122242 vs_loss 0.722405 logits_loss 121.860918\n",
      "[12:1800] F: 7372800 \t tot_loss 6.917548 rs_loss 0.121830 vs_loss 0.720136 logits_loss 121.511635\n",
      "[12:1900] F: 7382400 \t tot_loss 6.994353 rs_loss 0.153661 vs_loss 0.770980 logits_loss 121.394256\n",
      "[12:2000] F: 7392000 \t tot_loss 7.157338 rs_loss 0.215897 vs_loss 0.868849 logits_loss 121.451828\n",
      "[12:2100] F: 7401600 \t tot_loss 7.037869 rs_loss 0.174177 vs_loss 0.783625 logits_loss 121.601342\n",
      "[12:2200] F: 7411200 \t tot_loss 7.124268 rs_loss 0.203963 vs_loss 0.838317 logits_loss 121.639748\n",
      "[12:2300] F: 7420800 \t tot_loss 7.225693 rs_loss 0.235329 vs_loss 0.925269 logits_loss 121.301919\n",
      "[12:2400] F: 7430400 \t tot_loss 7.314603 rs_loss 0.250195 vs_loss 0.996046 logits_loss 121.367232\n",
      "[12:2500] F: 7440000 \t tot_loss 7.386326 rs_loss 0.271789 vs_loss 1.053401 logits_loss 121.222730\n",
      "[12:2600] F: 7449600 \t tot_loss 7.276658 rs_loss 0.244844 vs_loss 0.972940 logits_loss 121.177482\n",
      "[12:2700] F: 7459200 \t tot_loss 7.138398 rs_loss 0.200579 vs_loss 0.877955 logits_loss 121.197274\n",
      "[12:2800] F: 7468800 \t tot_loss 6.882926 rs_loss 0.132785 vs_loss 0.702870 logits_loss 120.945418\n",
      "[12:2900] F: 7478400 \t tot_loss 6.896640 rs_loss 0.136826 vs_loss 0.706754 logits_loss 121.061191\n",
      "[12:3000] F: 7488000 \t tot_loss 6.966124 rs_loss 0.159720 vs_loss 0.745622 logits_loss 121.215661\n",
      "[12:3100] F: 7497600 \t tot_loss 7.167770 rs_loss 0.208672 vs_loss 0.892053 logits_loss 121.340887\n",
      "[12:3200] F: 7507200 \t tot_loss 7.188477 rs_loss 0.223850 vs_loss 0.889297 logits_loss 121.506588\n",
      "[12:3300] F: 7516800 \t tot_loss 7.157421 rs_loss 0.219724 vs_loss 0.876825 logits_loss 121.217438\n",
      "[12:3400] F: 7526400 \t tot_loss 6.989655 rs_loss 0.174221 vs_loss 0.769745 logits_loss 120.913781\n",
      "[12:3500] F: 7536000 \t tot_loss 6.808948 rs_loss 0.124969 vs_loss 0.647425 logits_loss 120.731082\n",
      "[12:3600] F: 7545600 \t tot_loss 7.005159 rs_loss 0.171498 vs_loss 0.807157 logits_loss 120.530083\n",
      "[12:3700] F: 7555200 \t tot_loss 7.081492 rs_loss 0.174841 vs_loss 0.876211 logits_loss 120.608798\n",
      "[12:3800] F: 7564800 \t tot_loss 7.220887 rs_loss 0.199593 vs_loss 0.974811 logits_loss 120.929656\n",
      "[12:3900] F: 7574400 \t tot_loss 7.259427 rs_loss 0.204477 vs_loss 1.001252 logits_loss 121.073959\n",
      "[12:4000] F: 7584000 \t tot_loss 7.160162 rs_loss 0.186825 vs_loss 0.919154 logits_loss 121.083653\n",
      "[12:4100] F: 7593600 \t tot_loss 7.091183 rs_loss 0.176988 vs_loss 0.853524 logits_loss 121.213435\n",
      "[12:4200] F: 7603200 \t tot_loss 7.006748 rs_loss 0.174011 vs_loss 0.787660 logits_loss 120.901542\n",
      "[12:4300] F: 7612800 \t tot_loss 6.980449 rs_loss 0.176262 vs_loss 0.756269 logits_loss 120.958372\n",
      "[12:4400] F: 7622400 \t tot_loss 7.030534 rs_loss 0.175134 vs_loss 0.801532 logits_loss 121.077353\n",
      "[12:4500] F: 7632000 \t tot_loss 6.972377 rs_loss 0.155869 vs_loss 0.759048 logits_loss 121.149202\n",
      "[12:4600] F: 7641600 \t tot_loss 6.992663 rs_loss 0.147189 vs_loss 0.788979 logits_loss 121.129896\n",
      "[12:4700] F: 7651200 \t tot_loss 7.178415 rs_loss 0.190160 vs_loss 0.936866 logits_loss 121.027776\n",
      "[12:4800] F: 7660800 \t tot_loss 7.086113 rs_loss 0.184797 vs_loss 0.855073 logits_loss 120.924859\n",
      "[12:4900] F: 7670400 \t tot_loss 7.201561 rs_loss 0.238433 vs_loss 0.922937 logits_loss 120.803818\n",
      "[12:5000] F: 7680000 \t tot_loss 7.131149 rs_loss 0.226148 vs_loss 0.859471 logits_loss 120.910606\n",
      "[12:5100] F: 7689600 \t tot_loss 7.005481 rs_loss 0.203243 vs_loss 0.755658 logits_loss 120.931608\n",
      "[12:5200] F: 7699200 \t tot_loss 6.976021 rs_loss 0.186035 vs_loss 0.738414 logits_loss 121.031439\n",
      "[12:5300] F: 7708800 \t tot_loss 6.864885 rs_loss 0.132383 vs_loss 0.680550 logits_loss 121.039034\n",
      "[12:5400] F: 7718400 \t tot_loss 7.011866 rs_loss 0.176426 vs_loss 0.773553 logits_loss 121.237759\n",
      "[12:5500] F: 7728000 \t tot_loss 7.057926 rs_loss 0.186349 vs_loss 0.805797 logits_loss 121.315609\n",
      "[12:5600] F: 7737600 \t tot_loss 7.138344 rs_loss 0.212127 vs_loss 0.862400 logits_loss 121.276332\n",
      "[12:5700] F: 7747200 \t tot_loss 7.246426 rs_loss 0.244818 vs_loss 0.941385 logits_loss 121.204463\n",
      "[12:5800] F: 7756800 \t tot_loss 7.127143 rs_loss 0.221584 vs_loss 0.858079 logits_loss 120.949602\n",
      "[12:5900] F: 7766400 \t tot_loss 6.920113 rs_loss 0.162307 vs_loss 0.719685 logits_loss 120.762416\n",
      "[12:6000] F: 7776000 \t tot_loss 6.846834 rs_loss 0.129584 vs_loss 0.687828 logits_loss 120.588442\n",
      "[12:6100] F: 7785600 \t tot_loss 6.812670 rs_loss 0.130975 vs_loss 0.635738 logits_loss 120.919137\n",
      "[12:6200] F: 7795200 \t tot_loss 6.916564 rs_loss 0.130720 vs_loss 0.731434 logits_loss 121.088197\n",
      "Batch [13] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.18 (+-0.09) \n",
      "[13:0] F: 7800000 \t tot_loss 7.012448 rs_loss 0.158846 vs_loss 0.796508 logits_loss 121.141886\n",
      "[13:100] F: 7809600 \t tot_loss 7.329535 rs_loss 0.237126 vs_loss 1.006802 logits_loss 121.712126\n",
      "[13:200] F: 7819200 \t tot_loss 7.529725 rs_loss 0.294657 vs_loss 1.133129 logits_loss 122.038760\n",
      "[13:300] F: 7828800 \t tot_loss 7.566089 rs_loss 0.298957 vs_loss 1.153729 logits_loss 122.268059\n",
      "[13:400] F: 7838400 \t tot_loss 7.547309 rs_loss 0.277293 vs_loss 1.150325 logits_loss 122.393833\n",
      "[13:500] F: 7848000 \t tot_loss 7.449928 rs_loss 0.257241 vs_loss 1.078846 logits_loss 122.276817\n",
      "[13:600] F: 7857600 \t tot_loss 7.371320 rs_loss 0.239540 vs_loss 1.033962 logits_loss 121.956375\n",
      "[13:700] F: 7867200 \t tot_loss 7.322421 rs_loss 0.232626 vs_loss 0.995912 logits_loss 121.877675\n",
      "[13:800] F: 7876800 \t tot_loss 7.405890 rs_loss 0.255250 vs_loss 1.057048 logits_loss 121.871845\n",
      "[13:900] F: 7886400 \t tot_loss 7.202624 rs_loss 0.182545 vs_loss 0.913347 logits_loss 122.134633\n",
      "[13:1000] F: 7896000 \t tot_loss 7.233014 rs_loss 0.197006 vs_loss 0.924303 logits_loss 122.234096\n",
      "[13:1100] F: 7905600 \t tot_loss 7.206401 rs_loss 0.168204 vs_loss 0.919631 logits_loss 122.371312\n",
      "[13:1200] F: 7915200 \t tot_loss 7.115902 rs_loss 0.159065 vs_loss 0.849009 logits_loss 122.156566\n",
      "[13:1300] F: 7924800 \t tot_loss 7.131778 rs_loss 0.179423 vs_loss 0.859175 logits_loss 121.863606\n",
      "[13:1400] F: 7934400 \t tot_loss 7.081304 rs_loss 0.143850 vs_loss 0.840519 logits_loss 121.938709\n",
      "[13:1500] F: 7944000 \t tot_loss 7.231222 rs_loss 0.196299 vs_loss 0.941913 logits_loss 121.860210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:1600] F: 7953600 \t tot_loss 7.211788 rs_loss 0.174825 vs_loss 0.932021 logits_loss 122.098834\n",
      "[13:1700] F: 7963200 \t tot_loss 7.249443 rs_loss 0.178482 vs_loss 0.958294 logits_loss 122.253344\n",
      "[13:1800] F: 7972800 \t tot_loss 7.232434 rs_loss 0.180629 vs_loss 0.960758 logits_loss 121.820931\n",
      "[13:1900] F: 7982400 \t tot_loss 7.234903 rs_loss 0.206382 vs_loss 0.940815 logits_loss 121.754116\n",
      "[13:2000] F: 7992000 \t tot_loss 7.226429 rs_loss 0.217009 vs_loss 0.924921 logits_loss 121.689976\n",
      "[13:2100] F: 8001600 \t tot_loss 7.392270 rs_loss 0.259883 vs_loss 1.055879 logits_loss 121.530152\n",
      "[13:2200] F: 8011200 \t tot_loss 7.309073 rs_loss 0.239590 vs_loss 0.983901 logits_loss 121.711630\n",
      "[13:2300] F: 8020800 \t tot_loss 7.111607 rs_loss 0.164319 vs_loss 0.860622 logits_loss 121.733314\n",
      "[13:2400] F: 8030400 \t tot_loss 7.091045 rs_loss 0.154780 vs_loss 0.844186 logits_loss 121.841585\n",
      "[13:2500] F: 8040000 \t tot_loss 7.006890 rs_loss 0.155515 vs_loss 0.753700 logits_loss 121.953505\n",
      "[13:2600] F: 8049600 \t tot_loss 7.002926 rs_loss 0.155115 vs_loss 0.742086 logits_loss 122.114509\n",
      "[13:2700] F: 8059200 \t tot_loss 7.022202 rs_loss 0.154265 vs_loss 0.769240 logits_loss 121.973929\n",
      "[13:2800] F: 8068800 \t tot_loss 6.999137 rs_loss 0.149340 vs_loss 0.765777 logits_loss 121.680389\n",
      "[13:2900] F: 8078400 \t tot_loss 6.994864 rs_loss 0.131640 vs_loss 0.790820 logits_loss 121.448088\n",
      "[13:3000] F: 8088000 \t tot_loss 7.054507 rs_loss 0.150149 vs_loss 0.827050 logits_loss 121.546149\n",
      "[13:3100] F: 8097600 \t tot_loss 7.112979 rs_loss 0.178647 vs_loss 0.849812 logits_loss 121.690401\n",
      "[13:3200] F: 8107200 \t tot_loss 7.137645 rs_loss 0.193860 vs_loss 0.867098 logits_loss 121.533733\n",
      "[13:3300] F: 8116800 \t tot_loss 7.069994 rs_loss 0.181627 vs_loss 0.809141 logits_loss 121.584515\n",
      "[13:3400] F: 8126400 \t tot_loss 7.084889 rs_loss 0.187208 vs_loss 0.822920 logits_loss 121.495210\n",
      "[13:3500] F: 8136000 \t tot_loss 7.083552 rs_loss 0.208707 vs_loss 0.807620 logits_loss 121.344501\n",
      "[13:3600] F: 8145600 \t tot_loss 7.184637 rs_loss 0.237464 vs_loss 0.863407 logits_loss 121.675313\n",
      "[13:3700] F: 8155200 \t tot_loss 7.073836 rs_loss 0.203845 vs_loss 0.787717 logits_loss 121.645484\n",
      "[13:3800] F: 8164800 \t tot_loss 7.000965 rs_loss 0.178543 vs_loss 0.741131 logits_loss 121.625820\n",
      "[13:3900] F: 8174400 \t tot_loss 6.911073 rs_loss 0.132755 vs_loss 0.691580 logits_loss 121.734760\n",
      "[13:4000] F: 8184000 \t tot_loss 6.969746 rs_loss 0.143477 vs_loss 0.744313 logits_loss 121.639117\n",
      "[13:4100] F: 8193600 \t tot_loss 6.965156 rs_loss 0.140719 vs_loss 0.741152 logits_loss 121.665691\n",
      "[13:4200] F: 8203200 \t tot_loss 6.937286 rs_loss 0.137144 vs_loss 0.721190 logits_loss 121.579027\n",
      "[13:4300] F: 8212800 \t tot_loss 6.850919 rs_loss 0.107300 vs_loss 0.670991 logits_loss 121.452549\n",
      "[13:4400] F: 8222400 \t tot_loss 6.726546 rs_loss 0.079493 vs_loss 0.570855 logits_loss 121.523954\n",
      "[13:4500] F: 8232000 \t tot_loss 6.697195 rs_loss 0.075387 vs_loss 0.556197 logits_loss 121.312228\n",
      "[13:4600] F: 8241600 \t tot_loss 6.824028 rs_loss 0.101803 vs_loss 0.654781 logits_loss 121.348886\n",
      "[13:4700] F: 8251200 \t tot_loss 6.875450 rs_loss 0.128861 vs_loss 0.676524 logits_loss 121.401297\n",
      "[13:4800] F: 8260800 \t tot_loss 6.975854 rs_loss 0.154471 vs_loss 0.762002 logits_loss 121.187606\n",
      "[13:4900] F: 8270400 \t tot_loss 7.013879 rs_loss 0.154733 vs_loss 0.789205 logits_loss 121.398818\n",
      "[13:5000] F: 8280000 \t tot_loss 6.939135 rs_loss 0.146372 vs_loss 0.719865 logits_loss 121.457977\n",
      "[13:5100] F: 8289600 \t tot_loss 7.080230 rs_loss 0.168850 vs_loss 0.831290 logits_loss 121.601792\n",
      "[13:5200] F: 8299200 \t tot_loss 7.122236 rs_loss 0.174068 vs_loss 0.854779 logits_loss 121.867766\n",
      "[13:5300] F: 8308800 \t tot_loss 7.113522 rs_loss 0.171484 vs_loss 0.860576 logits_loss 121.629238\n",
      "[13:5400] F: 8318400 \t tot_loss 7.160687 rs_loss 0.176491 vs_loss 0.910560 logits_loss 121.472723\n",
      "[13:5500] F: 8328000 \t tot_loss 6.970787 rs_loss 0.129886 vs_loss 0.770385 logits_loss 121.410298\n",
      "[13:5600] F: 8337600 \t tot_loss 6.821016 rs_loss 0.095386 vs_loss 0.667185 logits_loss 121.168885\n",
      "[13:5700] F: 8347200 \t tot_loss 7.131975 rs_loss 0.196672 vs_loss 0.876551 logits_loss 121.175042\n",
      "[13:5800] F: 8356800 \t tot_loss 7.213771 rs_loss 0.230115 vs_loss 0.919494 logits_loss 121.283244\n",
      "[13:5900] F: 8366400 \t tot_loss 7.339004 rs_loss 0.259231 vs_loss 1.029731 logits_loss 121.000840\n",
      "[13:6000] F: 8376000 \t tot_loss 7.294391 rs_loss 0.246158 vs_loss 0.999594 logits_loss 120.972790\n",
      "[13:6100] F: 8385600 \t tot_loss 6.957169 rs_loss 0.144660 vs_loss 0.755255 logits_loss 121.145058\n",
      "[13:6200] F: 8395200 \t tot_loss 6.827650 rs_loss 0.109756 vs_loss 0.669348 logits_loss 120.970904\n",
      "Batch [14] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.12 (+-0.16) \n",
      "[14:0] F: 8400000 \t tot_loss 6.817558 rs_loss 0.110116 vs_loss 0.650606 logits_loss 121.136716\n",
      "[14:100] F: 8409600 \t tot_loss 6.937539 rs_loss 0.136710 vs_loss 0.725666 logits_loss 121.503259\n",
      "[14:200] F: 8419200 \t tot_loss 6.981970 rs_loss 0.142782 vs_loss 0.747935 logits_loss 121.825071\n",
      "[14:300] F: 8428800 \t tot_loss 7.141604 rs_loss 0.201025 vs_loss 0.827229 logits_loss 122.267003\n",
      "[14:400] F: 8438400 \t tot_loss 7.524859 rs_loss 0.296719 vs_loss 1.084797 logits_loss 122.866875\n",
      "[14:500] F: 8448000 \t tot_loss 7.434641 rs_loss 0.273479 vs_loss 1.034510 logits_loss 122.533048\n",
      "[14:600] F: 8457600 \t tot_loss 7.631095 rs_loss 0.320446 vs_loss 1.171505 logits_loss 122.782870\n",
      "[14:700] F: 8467200 \t tot_loss 7.661343 rs_loss 0.306889 vs_loss 1.227407 logits_loss 122.540933\n",
      "[14:800] F: 8476800 \t tot_loss 7.496208 rs_loss 0.238931 vs_loss 1.141031 logits_loss 122.324927\n",
      "[14:900] F: 8486400 \t tot_loss 7.600201 rs_loss 0.274322 vs_loss 1.197527 logits_loss 122.567053\n",
      "[14:1000] F: 8496000 \t tot_loss 7.447099 rs_loss 0.241017 vs_loss 1.088813 logits_loss 122.345353\n",
      "[14:1100] F: 8505600 \t tot_loss 7.526252 rs_loss 0.278229 vs_loss 1.132992 logits_loss 122.300619\n",
      "[14:1200] F: 8515200 \t tot_loss 7.520990 rs_loss 0.289057 vs_loss 1.118494 logits_loss 122.268791\n",
      "[14:1300] F: 8524800 \t tot_loss 7.451735 rs_loss 0.270399 vs_loss 1.066898 logits_loss 122.288755\n",
      "[14:1400] F: 8534400 \t tot_loss 7.540401 rs_loss 0.298428 vs_loss 1.126045 logits_loss 122.318580\n",
      "[14:1500] F: 8544000 \t tot_loss 7.331403 rs_loss 0.227646 vs_loss 0.991193 logits_loss 122.251290\n",
      "[14:1600] F: 8553600 \t tot_loss 7.298850 rs_loss 0.225446 vs_loss 0.959881 logits_loss 122.270460\n",
      "[14:1700] F: 8563200 \t tot_loss 7.200752 rs_loss 0.201028 vs_loss 0.894734 logits_loss 122.099811\n",
      "[14:1800] F: 8572800 \t tot_loss 7.125195 rs_loss 0.173985 vs_loss 0.850459 logits_loss 122.015002\n",
      "[14:1900] F: 8582400 \t tot_loss 7.068121 rs_loss 0.164344 vs_loss 0.801694 logits_loss 122.041652\n",
      "[14:2000] F: 8592000 \t tot_loss 7.001974 rs_loss 0.154455 vs_loss 0.750882 logits_loss 121.932740\n",
      "[14:2100] F: 8601600 \t tot_loss 7.099783 rs_loss 0.185671 vs_loss 0.824682 logits_loss 121.788585\n",
      "[14:2200] F: 8611200 \t tot_loss 7.120430 rs_loss 0.205842 vs_loss 0.840022 logits_loss 121.491325\n",
      "[14:2300] F: 8620800 \t tot_loss 7.252782 rs_loss 0.253848 vs_loss 0.934572 logits_loss 121.287229\n",
      "[14:2400] F: 8630400 \t tot_loss 7.258900 rs_loss 0.249130 vs_loss 0.944925 logits_loss 121.296908\n",
      "[14:2500] F: 8640000 \t tot_loss 7.076142 rs_loss 0.188811 vs_loss 0.818077 logits_loss 121.385099\n",
      "[14:2600] F: 8649600 \t tot_loss 7.134591 rs_loss 0.195720 vs_loss 0.846694 logits_loss 121.843539\n",
      "[14:2700] F: 8659200 \t tot_loss 7.089233 rs_loss 0.168535 vs_loss 0.818549 logits_loss 122.042979\n",
      "[14:2800] F: 8668800 \t tot_loss 6.990651 rs_loss 0.138204 vs_loss 0.747991 logits_loss 122.089108\n",
      "[14:2900] F: 8678400 \t tot_loss 7.133135 rs_loss 0.177902 vs_loss 0.856972 logits_loss 121.965225\n",
      "[14:3000] F: 8688000 \t tot_loss 7.108916 rs_loss 0.183741 vs_loss 0.851696 logits_loss 121.469563\n",
      "[14:3100] F: 8697600 \t tot_loss 7.063507 rs_loss 0.168317 vs_loss 0.818876 logits_loss 121.526293\n",
      "[14:3200] F: 8707200 \t tot_loss 7.243907 rs_loss 0.221818 vs_loss 0.955217 logits_loss 121.337423\n",
      "[14:3300] F: 8716800 \t tot_loss 7.120918 rs_loss 0.186146 vs_loss 0.856778 logits_loss 121.559880\n",
      "[14:3400] F: 8726400 \t tot_loss 7.023287 rs_loss 0.152562 vs_loss 0.797886 logits_loss 121.456780\n",
      "[14:3500] F: 8736000 \t tot_loss 6.996208 rs_loss 0.145439 vs_loss 0.790411 logits_loss 121.207160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:3600] F: 8745600 \t tot_loss 6.865317 rs_loss 0.103079 vs_loss 0.707172 logits_loss 121.101316\n",
      "[14:3700] F: 8755200 \t tot_loss 6.964078 rs_loss 0.129054 vs_loss 0.787604 logits_loss 120.948394\n",
      "[14:3800] F: 8764800 \t tot_loss 6.917771 rs_loss 0.107016 vs_loss 0.752532 logits_loss 121.164445\n",
      "[14:3900] F: 8774400 \t tot_loss 7.161548 rs_loss 0.184911 vs_loss 0.910046 logits_loss 121.331795\n",
      "[14:4000] F: 8784000 \t tot_loss 7.234307 rs_loss 0.208254 vs_loss 0.954507 logits_loss 121.430914\n",
      "[14:4100] F: 8793600 \t tot_loss 7.200720 rs_loss 0.212740 vs_loss 0.909315 logits_loss 121.573294\n",
      "[14:4200] F: 8803200 \t tot_loss 7.306668 rs_loss 0.240278 vs_loss 0.988958 logits_loss 121.548638\n",
      "[14:4300] F: 8812800 \t tot_loss 7.222762 rs_loss 0.215805 vs_loss 0.947382 logits_loss 121.191487\n",
      "[14:4400] F: 8822400 \t tot_loss 7.164262 rs_loss 0.193003 vs_loss 0.907459 logits_loss 121.276011\n",
      "[14:4500] F: 8832000 \t tot_loss 7.107821 rs_loss 0.162956 vs_loss 0.889895 logits_loss 121.099401\n",
      "[14:4600] F: 8841600 \t tot_loss 7.098188 rs_loss 0.184155 vs_loss 0.867153 logits_loss 120.937605\n",
      "[14:4700] F: 8851200 \t tot_loss 6.934696 rs_loss 0.134331 vs_loss 0.732073 logits_loss 121.365832\n",
      "[14:4800] F: 8860800 \t tot_loss 6.805794 rs_loss 0.101806 vs_loss 0.650769 logits_loss 121.064376\n",
      "[14:4900] F: 8870400 \t tot_loss 6.961606 rs_loss 0.163234 vs_loss 0.754013 logits_loss 120.887174\n",
      "[14:5000] F: 8880000 \t tot_loss 6.945011 rs_loss 0.144526 vs_loss 0.743070 logits_loss 121.148298\n",
      "[14:5100] F: 8889600 \t tot_loss 7.005783 rs_loss 0.164406 vs_loss 0.778327 logits_loss 121.260995\n",
      "[14:5200] F: 8899200 \t tot_loss 7.019381 rs_loss 0.167988 vs_loss 0.783615 logits_loss 121.355570\n",
      "[14:5300] F: 8908800 \t tot_loss 6.976887 rs_loss 0.154345 vs_loss 0.739891 logits_loss 121.653016\n",
      "[14:5400] F: 8918400 \t tot_loss 6.974044 rs_loss 0.145540 vs_loss 0.741307 logits_loss 121.743930\n",
      "[14:5500] F: 8928000 \t tot_loss 6.995863 rs_loss 0.151589 vs_loss 0.785020 logits_loss 121.185074\n",
      "[14:5600] F: 8937600 \t tot_loss 7.289727 rs_loss 0.238083 vs_loss 0.989976 logits_loss 121.233366\n",
      "[14:5700] F: 8947200 \t tot_loss 7.546702 rs_loss 0.296190 vs_loss 1.185303 logits_loss 121.304186\n",
      "[14:5800] F: 8956800 \t tot_loss 7.495149 rs_loss 0.286346 vs_loss 1.153900 logits_loss 121.098038\n",
      "[14:5900] F: 8966400 \t tot_loss 7.452341 rs_loss 0.283642 vs_loss 1.095664 logits_loss 121.460695\n",
      "[14:6000] F: 8976000 \t tot_loss 7.177715 rs_loss 0.206400 vs_loss 0.886682 logits_loss 121.692658\n",
      "[14:6100] F: 8985600 \t tot_loss 6.783846 rs_loss 0.102703 vs_loss 0.600599 logits_loss 121.610865\n",
      "[14:6200] F: 8995200 \t tot_loss 6.844423 rs_loss 0.117450 vs_loss 0.640010 logits_loss 121.739253\n",
      "Batch [15] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.25 (+-0.21) \n",
      "[15:0] F: 9000000 \t tot_loss 6.793173 rs_loss 0.095874 vs_loss 0.620547 logits_loss 121.535053\n",
      "[15:100] F: 9009600 \t tot_loss 6.826877 rs_loss 0.098616 vs_loss 0.641420 logits_loss 121.736833\n",
      "[15:200] F: 9019200 \t tot_loss 7.135838 rs_loss 0.176870 vs_loss 0.874956 logits_loss 121.680228\n",
      "[15:300] F: 9028800 \t tot_loss 7.136964 rs_loss 0.171146 vs_loss 0.881401 logits_loss 121.688341\n",
      "[15:400] F: 9038400 \t tot_loss 7.072557 rs_loss 0.146923 vs_loss 0.826616 logits_loss 121.980347\n",
      "[15:500] F: 9048000 \t tot_loss 7.043701 rs_loss 0.140791 vs_loss 0.806856 logits_loss 121.921076\n",
      "[15:600] F: 9057600 \t tot_loss 6.771785 rs_loss 0.065555 vs_loss 0.610827 logits_loss 121.908065\n",
      "[15:700] F: 9067200 \t tot_loss 6.975002 rs_loss 0.115679 vs_loss 0.762796 logits_loss 121.930528\n",
      "[15:800] F: 9076800 \t tot_loss 7.061693 rs_loss 0.144737 vs_loss 0.833706 logits_loss 121.664993\n",
      "[15:900] F: 9086400 \t tot_loss 7.059236 rs_loss 0.149826 vs_loss 0.840304 logits_loss 121.382106\n",
      "[15:1000] F: 9096000 \t tot_loss 7.170431 rs_loss 0.171000 vs_loss 0.930050 logits_loss 121.387631\n",
      "[15:1100] F: 9105600 \t tot_loss 7.032656 rs_loss 0.147649 vs_loss 0.813651 logits_loss 121.427118\n",
      "[15:1200] F: 9115200 \t tot_loss 7.120661 rs_loss 0.189227 vs_loss 0.853373 logits_loss 121.561234\n",
      "[15:1300] F: 9124800 \t tot_loss 7.134435 rs_loss 0.188423 vs_loss 0.865918 logits_loss 121.601883\n",
      "[15:1400] F: 9134400 \t tot_loss 6.982898 rs_loss 0.160040 vs_loss 0.750100 logits_loss 121.455155\n",
      "[15:1500] F: 9144000 \t tot_loss 6.914969 rs_loss 0.133069 vs_loss 0.716077 logits_loss 121.316456\n",
      "[15:1600] F: 9153600 \t tot_loss 6.804246 rs_loss 0.084298 vs_loss 0.668617 logits_loss 121.026619\n",
      "[15:1700] F: 9163200 \t tot_loss 6.791516 rs_loss 0.083874 vs_loss 0.652286 logits_loss 121.107109\n",
      "[15:1800] F: 9172800 \t tot_loss 6.876080 rs_loss 0.110210 vs_loss 0.695561 logits_loss 121.406190\n",
      "[15:1900] F: 9182400 \t tot_loss 6.926346 rs_loss 0.137414 vs_loss 0.721820 logits_loss 121.342241\n",
      "[15:2000] F: 9192000 \t tot_loss 6.913358 rs_loss 0.134425 vs_loss 0.698884 logits_loss 121.600967\n",
      "[15:2100] F: 9201600 \t tot_loss 6.921575 rs_loss 0.134690 vs_loss 0.711764 logits_loss 121.502437\n",
      "[15:2200] F: 9211200 \t tot_loss 6.821141 rs_loss 0.096594 vs_loss 0.656856 logits_loss 121.353830\n",
      "[15:2300] F: 9220800 \t tot_loss 6.779511 rs_loss 0.085857 vs_loss 0.631022 logits_loss 121.252639\n",
      "[15:2400] F: 9230400 \t tot_loss 6.704549 rs_loss 0.067377 vs_loss 0.584088 logits_loss 121.061668\n",
      "[15:2500] F: 9240000 \t tot_loss 6.746048 rs_loss 0.089826 vs_loss 0.590542 logits_loss 121.313603\n",
      "[15:2600] F: 9249600 \t tot_loss 6.804142 rs_loss 0.123231 vs_loss 0.630737 logits_loss 121.003466\n",
      "[15:2700] F: 9259200 \t tot_loss 6.815275 rs_loss 0.130484 vs_loss 0.631271 logits_loss 121.070397\n",
      "[15:2800] F: 9268800 \t tot_loss 7.004007 rs_loss 0.172800 vs_loss 0.769098 logits_loss 121.242182\n",
      "[15:2900] F: 9278400 \t tot_loss 6.943704 rs_loss 0.148475 vs_loss 0.748899 logits_loss 120.926583\n",
      "[15:3000] F: 9288000 \t tot_loss 6.971940 rs_loss 0.148570 vs_loss 0.774194 logits_loss 120.983515\n",
      "[15:3100] F: 9297600 \t tot_loss 7.015760 rs_loss 0.155745 vs_loss 0.817152 logits_loss 120.857270\n",
      "[15:3200] F: 9307200 \t tot_loss 6.939925 rs_loss 0.134724 vs_loss 0.768313 logits_loss 120.737766\n",
      "[15:3300] F: 9316800 \t tot_loss 7.136222 rs_loss 0.202421 vs_loss 0.899306 logits_loss 120.689910\n",
      "[15:3400] F: 9326400 \t tot_loss 7.136105 rs_loss 0.199462 vs_loss 0.907526 logits_loss 120.582338\n",
      "[15:3500] F: 9336000 \t tot_loss 7.135647 rs_loss 0.184624 vs_loss 0.912318 logits_loss 120.774094\n",
      "[15:3600] F: 9345600 \t tot_loss 7.035766 rs_loss 0.162668 vs_loss 0.830679 logits_loss 120.848369\n",
      "[15:3700] F: 9355200 \t tot_loss 6.827571 rs_loss 0.095669 vs_loss 0.682282 logits_loss 120.992402\n",
      "[15:3800] F: 9364800 \t tot_loss 6.787304 rs_loss 0.100296 vs_loss 0.628612 logits_loss 121.167934\n",
      "[15:3900] F: 9374400 \t tot_loss 6.964740 rs_loss 0.179772 vs_loss 0.733972 logits_loss 121.019934\n",
      "[15:4000] F: 9384000 \t tot_loss 7.099337 rs_loss 0.222496 vs_loss 0.829560 logits_loss 120.945629\n",
      "[15:4100] F: 9393600 \t tot_loss 7.164152 rs_loss 0.246522 vs_loss 0.875861 logits_loss 120.835375\n",
      "[15:4200] F: 9403200 \t tot_loss 7.127594 rs_loss 0.222362 vs_loss 0.860355 logits_loss 120.897536\n",
      "[15:4300] F: 9412800 \t tot_loss 6.852689 rs_loss 0.127839 vs_loss 0.692081 logits_loss 120.655365\n",
      "[15:4400] F: 9422400 \t tot_loss 6.788318 rs_loss 0.107171 vs_loss 0.651307 logits_loss 120.596794\n",
      "[15:4500] F: 9432000 \t tot_loss 7.098423 rs_loss 0.178185 vs_loss 0.903926 logits_loss 120.326247\n",
      "[15:4600] F: 9441600 \t tot_loss 7.169062 rs_loss 0.198872 vs_loss 0.963439 logits_loss 120.135022\n",
      "[15:4700] F: 9451200 \t tot_loss 7.233518 rs_loss 0.226011 vs_loss 0.995179 logits_loss 120.246541\n",
      "[15:4800] F: 9460800 \t tot_loss 7.220748 rs_loss 0.228376 vs_loss 0.984134 logits_loss 120.164752\n",
      "[15:4900] F: 9470400 \t tot_loss 7.108180 rs_loss 0.210009 vs_loss 0.878888 logits_loss 120.385650\n",
      "[15:5000] F: 9480000 \t tot_loss 7.022160 rs_loss 0.184603 vs_loss 0.817535 logits_loss 120.400436\n",
      "[15:5100] F: 9489600 \t tot_loss 7.010174 rs_loss 0.176032 vs_loss 0.804820 logits_loss 120.586448\n",
      "[15:5200] F: 9499200 \t tot_loss 6.923106 rs_loss 0.149587 vs_loss 0.735831 logits_loss 120.753765\n",
      "[15:5300] F: 9508800 \t tot_loss 6.640069 rs_loss 0.070373 vs_loss 0.540808 logits_loss 120.577761\n",
      "[15:5400] F: 9518400 \t tot_loss 6.912952 rs_loss 0.143502 vs_loss 0.728048 logits_loss 120.828053\n",
      "[15:5500] F: 9528000 \t tot_loss 6.961063 rs_loss 0.153526 vs_loss 0.766304 logits_loss 120.824673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:5600] F: 9537600 \t tot_loss 7.028932 rs_loss 0.174008 vs_loss 0.827109 logits_loss 120.556304\n",
      "[15:5700] F: 9547200 \t tot_loss 7.061898 rs_loss 0.180494 vs_loss 0.851094 logits_loss 120.606203\n",
      "[15:5800] F: 9556800 \t tot_loss 6.942697 rs_loss 0.161803 vs_loss 0.766547 logits_loss 120.286952\n",
      "[15:5900] F: 9566400 \t tot_loss 6.840465 rs_loss 0.130172 vs_loss 0.695334 logits_loss 120.299194\n",
      "[15:6000] F: 9576000 \t tot_loss 7.010235 rs_loss 0.170231 vs_loss 0.815356 logits_loss 120.492945\n",
      "[15:6100] F: 9585600 \t tot_loss 7.073951 rs_loss 0.192858 vs_loss 0.846124 logits_loss 120.699371\n",
      "[15:6200] F: 9595200 \t tot_loss 7.018986 rs_loss 0.164674 vs_loss 0.808806 logits_loss 120.910118\n",
      "Batch [16] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.11 (+-0.08) \n",
      "[16:0] F: 9600000 \t tot_loss 7.023078 rs_loss 0.175074 vs_loss 0.810924 logits_loss 120.741586\n",
      "[16:100] F: 9609600 \t tot_loss 7.112967 rs_loss 0.185513 vs_loss 0.856273 logits_loss 121.423624\n",
      "[16:200] F: 9619200 \t tot_loss 7.107312 rs_loss 0.192939 vs_loss 0.838581 logits_loss 121.515851\n",
      "[16:300] F: 9628800 \t tot_loss 7.209905 rs_loss 0.217834 vs_loss 0.903394 logits_loss 121.773541\n",
      "[16:400] F: 9638400 \t tot_loss 7.216121 rs_loss 0.220675 vs_loss 0.881196 logits_loss 122.284983\n",
      "[16:500] F: 9648000 \t tot_loss 7.053856 rs_loss 0.177364 vs_loss 0.767534 logits_loss 122.179149\n",
      "[16:600] F: 9657600 \t tot_loss 6.995573 rs_loss 0.153592 vs_loss 0.726122 logits_loss 122.317163\n",
      "[16:700] F: 9667200 \t tot_loss 6.919310 rs_loss 0.127986 vs_loss 0.672602 logits_loss 122.374437\n",
      "[16:800] F: 9676800 \t tot_loss 7.010601 rs_loss 0.152800 vs_loss 0.746568 logits_loss 122.224673\n",
      "[16:900] F: 9686400 \t tot_loss 7.007233 rs_loss 0.148241 vs_loss 0.758803 logits_loss 122.003785\n",
      "[16:1000] F: 9696000 \t tot_loss 6.978661 rs_loss 0.141993 vs_loss 0.753869 logits_loss 121.655979\n",
      "[16:1100] F: 9705600 \t tot_loss 6.999621 rs_loss 0.161565 vs_loss 0.775192 logits_loss 121.257277\n",
      "[16:1200] F: 9715200 \t tot_loss 7.106451 rs_loss 0.184811 vs_loss 0.866759 logits_loss 121.097602\n",
      "[16:1300] F: 9724800 \t tot_loss 7.114603 rs_loss 0.201471 vs_loss 0.860496 logits_loss 121.052729\n",
      "[16:1400] F: 9734400 \t tot_loss 7.060166 rs_loss 0.179493 vs_loss 0.817927 logits_loss 121.254905\n",
      "[16:1500] F: 9744000 \t tot_loss 7.129734 rs_loss 0.204318 vs_loss 0.858150 logits_loss 121.345313\n",
      "[16:1600] F: 9753600 \t tot_loss 6.912920 rs_loss 0.144977 vs_loss 0.699838 logits_loss 121.362097\n",
      "[16:1700] F: 9763200 \t tot_loss 6.877666 rs_loss 0.124011 vs_loss 0.690660 logits_loss 121.259886\n",
      "[16:1800] F: 9772800 \t tot_loss 6.905968 rs_loss 0.134465 vs_loss 0.709734 logits_loss 121.235370\n",
      "[16:1900] F: 9782400 \t tot_loss 6.796740 rs_loss 0.086580 vs_loss 0.652011 logits_loss 121.162985\n",
      "[16:2000] F: 9792000 \t tot_loss 6.757425 rs_loss 0.074461 vs_loss 0.620085 logits_loss 121.257585\n",
      "[16:2100] F: 9801600 \t tot_loss 6.918594 rs_loss 0.117107 vs_loss 0.732439 logits_loss 121.380958\n",
      "[16:2200] F: 9811200 \t tot_loss 6.864318 rs_loss 0.100697 vs_loss 0.706621 logits_loss 121.139991\n",
      "[16:2300] F: 9820800 \t tot_loss 6.870550 rs_loss 0.102472 vs_loss 0.706147 logits_loss 121.238631\n",
      "[16:2400] F: 9830400 \t tot_loss 6.936940 rs_loss 0.122715 vs_loss 0.772371 logits_loss 120.837070\n",
      "[16:2500] F: 9840000 \t tot_loss 6.810573 rs_loss 0.098569 vs_loss 0.664097 logits_loss 120.958122\n",
      "[16:2600] F: 9849600 \t tot_loss 7.016142 rs_loss 0.149899 vs_loss 0.814056 logits_loss 121.043731\n",
      "[16:2700] F: 9859200 \t tot_loss 7.200748 rs_loss 0.201398 vs_loss 0.943279 logits_loss 121.121409\n",
      "[16:2800] F: 9868800 \t tot_loss 7.294110 rs_loss 0.221544 vs_loss 1.003013 logits_loss 121.391076\n",
      "[16:2900] F: 9878400 \t tot_loss 7.234452 rs_loss 0.201471 vs_loss 0.983105 logits_loss 120.997515\n",
      "[16:3000] F: 9888000 \t tot_loss 7.118739 rs_loss 0.178216 vs_loss 0.896983 logits_loss 120.870794\n",
      "[16:3100] F: 9897600 \t tot_loss 7.173741 rs_loss 0.197122 vs_loss 0.938286 logits_loss 120.766646\n",
      "[16:3200] F: 9907200 \t tot_loss 6.991676 rs_loss 0.151631 vs_loss 0.795241 logits_loss 120.896083\n",
      "[16:3300] F: 9916800 \t tot_loss 7.204467 rs_loss 0.220750 vs_loss 0.926996 logits_loss 121.134423\n",
      "[16:3400] F: 9926400 \t tot_loss 7.138650 rs_loss 0.208274 vs_loss 0.867877 logits_loss 121.249984\n",
      "[16:3500] F: 9936000 \t tot_loss 7.047525 rs_loss 0.190661 vs_loss 0.795545 logits_loss 121.226393\n",
      "[16:3600] F: 9945600 \t tot_loss 7.242437 rs_loss 0.259608 vs_loss 0.928273 logits_loss 121.091124\n",
      "[16:3700] F: 9955200 \t tot_loss 7.042649 rs_loss 0.191627 vs_loss 0.792982 logits_loss 121.160792\n",
      "[16:3800] F: 9964800 \t tot_loss 7.016606 rs_loss 0.179510 vs_loss 0.792940 logits_loss 120.883114\n",
      "[16:3900] F: 9974400 \t tot_loss 7.069558 rs_loss 0.194816 vs_loss 0.830067 logits_loss 120.893489\n",
      "[16:4000] F: 9984000 \t tot_loss 6.930072 rs_loss 0.148114 vs_loss 0.741851 logits_loss 120.802140\n",
      "[16:4100] F: 9993600 \t tot_loss 7.023074 rs_loss 0.198650 vs_loss 0.804509 logits_loss 120.398293\n",
      "[16:4200] F: 10003200 \t tot_loss 7.189157 rs_loss 0.251989 vs_loss 0.904030 logits_loss 120.662770\n",
      "[16:4300] F: 10012800 \t tot_loss 6.990873 rs_loss 0.186382 vs_loss 0.765912 logits_loss 120.771578\n",
      "[16:4400] F: 10022400 \t tot_loss 6.920818 rs_loss 0.163082 vs_loss 0.717583 logits_loss 120.803042\n",
      "[16:4500] F: 10032000 \t tot_loss 6.811492 rs_loss 0.109224 vs_loss 0.647505 logits_loss 121.095277\n",
      "[16:4600] F: 10041600 \t tot_loss 6.841939 rs_loss 0.105711 vs_loss 0.681719 logits_loss 121.090196\n",
      "[16:4700] F: 10051200 \t tot_loss 6.947310 rs_loss 0.155056 vs_loss 0.746985 logits_loss 120.905362\n",
      "[16:4800] F: 10060800 \t tot_loss 6.940546 rs_loss 0.156201 vs_loss 0.748427 logits_loss 120.718356\n",
      "[16:4900] F: 10070400 \t tot_loss 6.956892 rs_loss 0.163014 vs_loss 0.757530 logits_loss 120.726964\n",
      "[16:5000] F: 10080000 \t tot_loss 6.946903 rs_loss 0.163617 vs_loss 0.749527 logits_loss 120.675188\n",
      "[16:5100] F: 10089600 \t tot_loss 6.808387 rs_loss 0.104955 vs_loss 0.662207 logits_loss 120.824497\n",
      "[16:5200] F: 10099200 \t tot_loss 6.814910 rs_loss 0.107577 vs_loss 0.665479 logits_loss 120.837065\n",
      "[16:5300] F: 10108800 \t tot_loss 6.981063 rs_loss 0.153944 vs_loss 0.788129 logits_loss 120.779791\n",
      "[16:5400] F: 10118400 \t tot_loss 6.879679 rs_loss 0.126464 vs_loss 0.712950 logits_loss 120.805315\n",
      "[16:5500] F: 10128000 \t tot_loss 7.007101 rs_loss 0.183631 vs_loss 0.793089 logits_loss 120.607623\n",
      "[16:5600] F: 10137600 \t tot_loss 7.132098 rs_loss 0.236746 vs_loss 0.870943 logits_loss 120.488172\n",
      "[16:5700] F: 10147200 \t tot_loss 7.049269 rs_loss 0.211383 vs_loss 0.809954 logits_loss 120.558655\n",
      "[16:5800] F: 10156800 \t tot_loss 7.118899 rs_loss 0.235090 vs_loss 0.856477 logits_loss 120.546645\n",
      "[16:5900] F: 10166400 \t tot_loss 6.990649 rs_loss 0.188109 vs_loss 0.775890 logits_loss 120.532987\n",
      "[16:6000] F: 10176000 \t tot_loss 6.878320 rs_loss 0.147923 vs_loss 0.696882 logits_loss 120.670306\n",
      "[16:6100] F: 10185600 \t tot_loss 6.804173 rs_loss 0.125163 vs_loss 0.655442 logits_loss 120.471369\n",
      "[16:6200] F: 10195200 \t tot_loss 6.653527 rs_loss 0.087681 vs_loss 0.548460 logits_loss 120.347731\n",
      "Batch [17] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.45 (+-0.28) \n",
      "[17:0] F: 10200000 \t tot_loss 6.672431 rs_loss 0.087966 vs_loss 0.567984 logits_loss 120.329596\n",
      "[17:100] F: 10209600 \t tot_loss 6.803411 rs_loss 0.111371 vs_loss 0.645372 logits_loss 120.933352\n",
      "[17:200] F: 10219200 \t tot_loss 6.842635 rs_loss 0.096813 vs_loss 0.676404 logits_loss 121.388348\n",
      "[17:300] F: 10228800 \t tot_loss 6.861899 rs_loss 0.094484 vs_loss 0.676518 logits_loss 121.817946\n",
      "[17:400] F: 10238400 \t tot_loss 6.896147 rs_loss 0.086055 vs_loss 0.699057 logits_loss 122.220704\n",
      "[17:500] F: 10248000 \t tot_loss 6.975878 rs_loss 0.117033 vs_loss 0.761520 logits_loss 121.946524\n",
      "[17:600] F: 10257600 \t tot_loss 6.956411 rs_loss 0.113872 vs_loss 0.741952 logits_loss 122.011725\n",
      "[17:700] F: 10267200 \t tot_loss 7.103092 rs_loss 0.158121 vs_loss 0.843623 logits_loss 122.026947\n",
      "[17:800] F: 10276800 \t tot_loss 7.174675 rs_loss 0.186819 vs_loss 0.887773 logits_loss 122.001654\n",
      "[17:900] F: 10286400 \t tot_loss 7.082825 rs_loss 0.153540 vs_loss 0.815478 logits_loss 122.276127\n",
      "[17:1000] F: 10296000 \t tot_loss 7.053743 rs_loss 0.153536 vs_loss 0.797873 logits_loss 122.046687\n",
      "[17:1100] F: 10305600 \t tot_loss 7.126608 rs_loss 0.175751 vs_loss 0.852845 logits_loss 121.960254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:1200] F: 10315200 \t tot_loss 7.117506 rs_loss 0.173409 vs_loss 0.852164 logits_loss 121.838676\n",
      "[17:1300] F: 10324800 \t tot_loss 7.162788 rs_loss 0.199441 vs_loss 0.894110 logits_loss 121.384737\n",
      "[17:1400] F: 10334400 \t tot_loss 7.173962 rs_loss 0.191615 vs_loss 0.899994 logits_loss 121.647041\n",
      "[17:1500] F: 10344000 \t tot_loss 6.982782 rs_loss 0.143684 vs_loss 0.766842 logits_loss 121.445121\n",
      "[17:1600] F: 10353600 \t tot_loss 7.078238 rs_loss 0.190111 vs_loss 0.820511 logits_loss 121.352314\n",
      "[17:1700] F: 10363200 \t tot_loss 6.920092 rs_loss 0.147108 vs_loss 0.709036 logits_loss 121.278952\n",
      "[17:1800] F: 10372800 \t tot_loss 6.996035 rs_loss 0.175221 vs_loss 0.762101 logits_loss 121.174257\n",
      "[17:1900] F: 10382400 \t tot_loss 6.974618 rs_loss 0.163028 vs_loss 0.745522 logits_loss 121.321365\n",
      "[17:2000] F: 10392000 \t tot_loss 7.028410 rs_loss 0.171389 vs_loss 0.784313 logits_loss 121.454171\n",
      "[17:2100] F: 10401600 \t tot_loss 7.111845 rs_loss 0.189171 vs_loss 0.832573 logits_loss 121.802006\n",
      "[17:2200] F: 10411200 \t tot_loss 7.044624 rs_loss 0.165585 vs_loss 0.796279 logits_loss 121.655189\n",
      "[17:2300] F: 10420800 \t tot_loss 7.234367 rs_loss 0.243563 vs_loss 0.915292 logits_loss 121.510241\n",
      "[17:2400] F: 10430400 \t tot_loss 7.109005 rs_loss 0.211664 vs_loss 0.830204 logits_loss 121.342728\n",
      "[17:2500] F: 10440000 \t tot_loss 7.271345 rs_loss 0.254807 vs_loss 0.959011 logits_loss 121.150553\n",
      "[17:2600] F: 10449600 \t tot_loss 7.405911 rs_loss 0.307653 vs_loss 1.048649 logits_loss 120.992175\n",
      "[17:2700] F: 10459200 \t tot_loss 7.277701 rs_loss 0.238619 vs_loss 0.994302 logits_loss 120.895582\n",
      "[17:2800] F: 10468800 \t tot_loss 7.170947 rs_loss 0.189034 vs_loss 0.935253 logits_loss 120.933204\n",
      "[17:2900] F: 10478400 \t tot_loss 7.201260 rs_loss 0.212991 vs_loss 0.945137 logits_loss 120.862633\n",
      "[17:3000] F: 10488000 \t tot_loss 7.005010 rs_loss 0.154553 vs_loss 0.807426 logits_loss 120.860628\n",
      "[17:3100] F: 10497600 \t tot_loss 7.061370 rs_loss 0.187821 vs_loss 0.816158 logits_loss 121.147835\n",
      "[17:3200] F: 10507200 \t tot_loss 7.134170 rs_loss 0.207656 vs_loss 0.856573 logits_loss 121.398805\n",
      "[17:3300] F: 10516800 \t tot_loss 6.866118 rs_loss 0.116815 vs_loss 0.665466 logits_loss 121.676725\n",
      "[17:3400] F: 10526400 \t tot_loss 6.986800 rs_loss 0.166200 vs_loss 0.738089 logits_loss 121.650214\n",
      "[17:3500] F: 10536000 \t tot_loss 7.097440 rs_loss 0.203376 vs_loss 0.817885 logits_loss 121.523576\n",
      "[17:3600] F: 10545600 \t tot_loss 7.206309 rs_loss 0.233482 vs_loss 0.902285 logits_loss 121.410827\n",
      "[17:3700] F: 10555200 \t tot_loss 7.399941 rs_loss 0.282323 vs_loss 1.054534 logits_loss 121.261674\n",
      "[17:3800] F: 10564800 \t tot_loss 7.374413 rs_loss 0.256971 vs_loss 1.043380 logits_loss 121.481232\n",
      "[17:3900] F: 10574400 \t tot_loss 7.136571 rs_loss 0.190104 vs_loss 0.878650 logits_loss 121.356339\n",
      "[17:4000] F: 10584000 \t tot_loss 7.147614 rs_loss 0.184570 vs_loss 0.897372 logits_loss 121.313442\n",
      "[17:4100] F: 10593600 \t tot_loss 6.962048 rs_loss 0.138523 vs_loss 0.765339 logits_loss 121.163721\n",
      "[17:4200] F: 10603200 \t tot_loss 6.986469 rs_loss 0.160539 vs_loss 0.781977 logits_loss 120.879062\n",
      "[17:4300] F: 10612800 \t tot_loss 7.019266 rs_loss 0.170390 vs_loss 0.805032 logits_loss 120.876880\n",
      "[17:4400] F: 10622400 \t tot_loss 7.014965 rs_loss 0.179021 vs_loss 0.789668 logits_loss 120.925525\n",
      "[17:4500] F: 10632000 \t tot_loss 7.065914 rs_loss 0.200129 vs_loss 0.824343 logits_loss 120.828842\n",
      "[17:4600] F: 10641600 \t tot_loss 7.002275 rs_loss 0.174267 vs_loss 0.782122 logits_loss 120.917707\n",
      "[17:4700] F: 10651200 \t tot_loss 7.048347 rs_loss 0.186087 vs_loss 0.807605 logits_loss 121.093104\n",
      "[17:4800] F: 10660800 \t tot_loss 6.886508 rs_loss 0.145380 vs_loss 0.693984 logits_loss 120.942875\n",
      "[17:4900] F: 10670400 \t tot_loss 6.814311 rs_loss 0.127089 vs_loss 0.632642 logits_loss 121.091601\n",
      "[17:5000] F: 10680000 \t tot_loss 7.014547 rs_loss 0.177428 vs_loss 0.779941 logits_loss 121.143561\n",
      "[17:5100] F: 10689600 \t tot_loss 6.977713 rs_loss 0.162313 vs_loss 0.761259 logits_loss 121.082831\n",
      "[17:5200] F: 10699200 \t tot_loss 6.980015 rs_loss 0.177112 vs_loss 0.746719 logits_loss 121.123675\n",
      "[17:5300] F: 10708800 \t tot_loss 7.118463 rs_loss 0.223611 vs_loss 0.831682 logits_loss 121.263398\n",
      "[17:5400] F: 10718400 \t tot_loss 6.964617 rs_loss 0.182542 vs_loss 0.708983 logits_loss 121.461833\n",
      "[17:5500] F: 10728000 \t tot_loss 6.990009 rs_loss 0.200907 vs_loss 0.722135 logits_loss 121.339350\n",
      "[17:5600] F: 10737600 \t tot_loss 7.085559 rs_loss 0.222503 vs_loss 0.799322 logits_loss 121.274678\n",
      "[17:5700] F: 10747200 \t tot_loss 7.109094 rs_loss 0.242059 vs_loss 0.814540 logits_loss 121.049901\n",
      "[17:5800] F: 10756800 \t tot_loss 7.099684 rs_loss 0.240030 vs_loss 0.805882 logits_loss 121.075428\n",
      "[17:5900] F: 10766400 \t tot_loss 6.973986 rs_loss 0.192808 vs_loss 0.729295 logits_loss 121.037640\n",
      "[17:6000] F: 10776000 \t tot_loss 6.925883 rs_loss 0.170064 vs_loss 0.698592 logits_loss 121.144541\n",
      "[17:6100] F: 10785600 \t tot_loss 6.869667 rs_loss 0.127683 vs_loss 0.690683 logits_loss 121.026014\n",
      "[17:6200] F: 10795200 \t tot_loss 6.843601 rs_loss 0.126436 vs_loss 0.677885 logits_loss 120.785596\n",
      "Batch [18] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.08 (+-0.09) \n",
      "[18:0] F: 10800000 \t tot_loss 6.850061 rs_loss 0.123763 vs_loss 0.684126 logits_loss 120.843449\n",
      "[18:100] F: 10809600 \t tot_loss 6.951743 rs_loss 0.147942 vs_loss 0.749758 logits_loss 121.080863\n",
      "[18:200] F: 10819200 \t tot_loss 7.014363 rs_loss 0.166574 vs_loss 0.783411 logits_loss 121.287544\n",
      "[18:300] F: 10828800 \t tot_loss 7.013709 rs_loss 0.154951 vs_loss 0.787046 logits_loss 121.434246\n",
      "[18:400] F: 10838400 \t tot_loss 7.067027 rs_loss 0.164499 vs_loss 0.812678 logits_loss 121.797008\n",
      "[18:500] F: 10848000 \t tot_loss 7.055552 rs_loss 0.170803 vs_loss 0.810187 logits_loss 121.491259\n",
      "[18:600] F: 10857600 \t tot_loss 6.909745 rs_loss 0.119628 vs_loss 0.718706 logits_loss 121.428227\n",
      "[18:700] F: 10867200 \t tot_loss 6.824113 rs_loss 0.096914 vs_loss 0.650949 logits_loss 121.524977\n",
      "[18:800] F: 10876800 \t tot_loss 6.995208 rs_loss 0.162519 vs_loss 0.769200 logits_loss 121.269764\n",
      "[18:900] F: 10886400 \t tot_loss 7.147474 rs_loss 0.205094 vs_loss 0.863893 logits_loss 121.569751\n",
      "[18:1000] F: 10896000 \t tot_loss 7.287817 rs_loss 0.251668 vs_loss 0.958585 logits_loss 121.551258\n",
      "[18:1100] F: 10905600 \t tot_loss 7.284848 rs_loss 0.249036 vs_loss 0.954422 logits_loss 121.627794\n",
      "[18:1200] F: 10915200 \t tot_loss 7.077104 rs_loss 0.171721 vs_loss 0.819687 logits_loss 121.713895\n",
      "[18:1300] F: 10924800 \t tot_loss 7.024376 rs_loss 0.147419 vs_loss 0.799890 logits_loss 121.541338\n",
      "[18:1400] F: 10934400 \t tot_loss 6.872643 rs_loss 0.104732 vs_loss 0.692259 logits_loss 121.513037\n",
      "[18:1500] F: 10944000 \t tot_loss 6.838043 rs_loss 0.105825 vs_loss 0.671223 logits_loss 121.219882\n",
      "[18:1600] F: 10953600 \t tot_loss 7.041502 rs_loss 0.183598 vs_loss 0.811757 logits_loss 120.922944\n",
      "[18:1700] F: 10963200 \t tot_loss 7.084371 rs_loss 0.213864 vs_loss 0.816960 logits_loss 121.070943\n",
      "[18:1800] F: 10972800 \t tot_loss 7.206663 rs_loss 0.242535 vs_loss 0.906578 logits_loss 121.150993\n",
      "[18:1900] F: 10982400 \t tot_loss 7.246960 rs_loss 0.260573 vs_loss 0.923771 logits_loss 121.252324\n",
      "[18:2000] F: 10992000 \t tot_loss 7.024433 rs_loss 0.190021 vs_loss 0.768770 logits_loss 121.312851\n",
      "[18:2100] F: 11001600 \t tot_loss 6.806484 rs_loss 0.110945 vs_loss 0.631775 logits_loss 121.275288\n",
      "[18:2200] F: 11011200 \t tot_loss 6.809526 rs_loss 0.120960 vs_loss 0.628992 logits_loss 121.191494\n",
      "[18:2300] F: 11020800 \t tot_loss 6.870950 rs_loss 0.120316 vs_loss 0.697009 logits_loss 121.072495\n",
      "[18:2400] F: 11030400 \t tot_loss 6.959840 rs_loss 0.140434 vs_loss 0.771041 logits_loss 120.967289\n",
      "[18:2500] F: 11040000 \t tot_loss 7.235967 rs_loss 0.238062 vs_loss 0.961492 logits_loss 120.728243\n",
      "[18:2600] F: 11049600 \t tot_loss 7.243710 rs_loss 0.242957 vs_loss 0.976802 logits_loss 120.479038\n",
      "[18:2700] F: 11059200 \t tot_loss 7.205870 rs_loss 0.240323 vs_loss 0.934116 logits_loss 120.628618\n",
      "[18:2800] F: 11068800 \t tot_loss 7.127007 rs_loss 0.218255 vs_loss 0.862920 logits_loss 120.916646\n",
      "[18:2900] F: 11078400 \t tot_loss 7.042859 rs_loss 0.166566 vs_loss 0.823260 logits_loss 121.060648\n",
      "[18:3000] F: 11088000 \t tot_loss 6.974598 rs_loss 0.142549 vs_loss 0.773586 logits_loss 121.169241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:3100] F: 11097600 \t tot_loss 7.070258 rs_loss 0.170005 vs_loss 0.843880 logits_loss 121.127459\n",
      "[18:3200] F: 11107200 \t tot_loss 7.142027 rs_loss 0.193071 vs_loss 0.900087 logits_loss 120.977376\n",
      "[18:3300] F: 11116800 \t tot_loss 6.941890 rs_loss 0.147750 vs_loss 0.748893 logits_loss 120.904936\n",
      "[18:3400] F: 11126400 \t tot_loss 6.880039 rs_loss 0.126002 vs_loss 0.705013 logits_loss 120.980482\n",
      "[18:3500] F: 11136000 \t tot_loss 6.882462 rs_loss 0.149976 vs_loss 0.689691 logits_loss 120.855895\n",
      "[18:3600] F: 11145600 \t tot_loss 6.975235 rs_loss 0.185038 vs_loss 0.752870 logits_loss 120.746538\n",
      "[18:3700] F: 11155200 \t tot_loss 6.973628 rs_loss 0.188324 vs_loss 0.751220 logits_loss 120.681659\n",
      "[18:3800] F: 11164800 \t tot_loss 7.048508 rs_loss 0.207583 vs_loss 0.802954 logits_loss 120.759403\n",
      "[18:3900] F: 11174400 \t tot_loss 7.029390 rs_loss 0.174811 vs_loss 0.815506 logits_loss 120.781441\n",
      "[18:4000] F: 11184000 \t tot_loss 6.875466 rs_loss 0.113678 vs_loss 0.710766 logits_loss 121.020445\n",
      "[18:4100] F: 11193600 \t tot_loss 7.189507 rs_loss 0.201781 vs_loss 0.931904 logits_loss 121.116431\n",
      "[18:4200] F: 11203200 \t tot_loss 7.093676 rs_loss 0.180792 vs_loss 0.865892 logits_loss 120.939826\n",
      "[18:4300] F: 11212800 \t tot_loss 6.917629 rs_loss 0.138572 vs_loss 0.738993 logits_loss 120.801282\n",
      "[18:4400] F: 11222400 \t tot_loss 6.950538 rs_loss 0.169842 vs_loss 0.760393 logits_loss 120.406061\n",
      "[18:4500] F: 11232000 \t tot_loss 6.650460 rs_loss 0.082941 vs_loss 0.554799 logits_loss 120.254403\n",
      "[18:4600] F: 11241600 \t tot_loss 6.671707 rs_loss 0.085042 vs_loss 0.560775 logits_loss 120.517788\n",
      "[18:4700] F: 11251200 \t tot_loss 6.752963 rs_loss 0.108775 vs_loss 0.601666 logits_loss 120.850441\n",
      "[18:4800] F: 11260800 \t tot_loss 6.699881 rs_loss 0.080156 vs_loss 0.578218 logits_loss 120.830142\n",
      "[18:4900] F: 11270400 \t tot_loss 6.668127 rs_loss 0.074094 vs_loss 0.557470 logits_loss 120.731268\n",
      "[18:5000] F: 11280000 \t tot_loss 6.695151 rs_loss 0.090705 vs_loss 0.583981 logits_loss 120.409300\n",
      "[18:5100] F: 11289600 \t tot_loss 6.616944 rs_loss 0.074048 vs_loss 0.539864 logits_loss 120.060647\n",
      "[18:5200] F: 11299200 \t tot_loss 6.774603 rs_loss 0.119915 vs_loss 0.652575 logits_loss 120.042255\n",
      "[18:5300] F: 11308800 \t tot_loss 6.875123 rs_loss 0.143985 vs_loss 0.714318 logits_loss 120.336405\n",
      "[18:5400] F: 11318400 \t tot_loss 6.808576 rs_loss 0.125923 vs_loss 0.657026 logits_loss 120.512543\n",
      "[18:5500] F: 11328000 \t tot_loss 7.080924 rs_loss 0.186139 vs_loss 0.868692 logits_loss 120.521855\n",
      "[18:5600] F: 11337600 \t tot_loss 6.989867 rs_loss 0.156885 vs_loss 0.800487 logits_loss 120.649891\n",
      "[18:5700] F: 11347200 \t tot_loss 6.988285 rs_loss 0.172088 vs_loss 0.776346 logits_loss 120.797018\n",
      "[18:5800] F: 11356800 \t tot_loss 6.982276 rs_loss 0.171850 vs_loss 0.771262 logits_loss 120.783255\n",
      "[18:5900] F: 11366400 \t tot_loss 6.798336 rs_loss 0.129484 vs_loss 0.631441 logits_loss 120.748223\n",
      "[18:6000] F: 11376000 \t tot_loss 6.831754 rs_loss 0.140648 vs_loss 0.659188 logits_loss 120.638373\n",
      "[18:6100] F: 11385600 \t tot_loss 6.837319 rs_loss 0.127052 vs_loss 0.687819 logits_loss 120.448960\n",
      "[18:6200] F: 11395200 \t tot_loss 6.837423 rs_loss 0.125228 vs_loss 0.692826 logits_loss 120.387382\n",
      "Batch [19] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.27 (+-0.20) \n",
      "[19:0] F: 11400000 \t tot_loss 6.832818 rs_loss 0.121030 vs_loss 0.691679 logits_loss 120.402180\n",
      "[19:100] F: 11409600 \t tot_loss 6.872964 rs_loss 0.129409 vs_loss 0.706920 logits_loss 120.732713\n",
      "[19:200] F: 11419200 \t tot_loss 6.896516 rs_loss 0.126903 vs_loss 0.716660 logits_loss 121.059055\n",
      "[19:300] F: 11428800 \t tot_loss 6.892722 rs_loss 0.113632 vs_loss 0.705116 logits_loss 121.479465\n",
      "[19:400] F: 11438400 \t tot_loss 7.063386 rs_loss 0.172555 vs_loss 0.797729 logits_loss 121.862048\n",
      "[19:500] F: 11448000 \t tot_loss 7.057881 rs_loss 0.164407 vs_loss 0.794029 logits_loss 121.988910\n",
      "[19:600] F: 11457600 \t tot_loss 7.085428 rs_loss 0.186632 vs_loss 0.805661 logits_loss 121.862696\n",
      "[19:700] F: 11467200 \t tot_loss 7.078514 rs_loss 0.197298 vs_loss 0.819498 logits_loss 121.234360\n",
      "[19:800] F: 11476800 \t tot_loss 6.850747 rs_loss 0.119330 vs_loss 0.677398 logits_loss 121.080396\n",
      "[19:900] F: 11486400 \t tot_loss 6.902545 rs_loss 0.149697 vs_loss 0.705099 logits_loss 120.954967\n",
      "[19:1000] F: 11496000 \t tot_loss 6.806233 rs_loss 0.111900 vs_loss 0.644232 logits_loss 121.002031\n",
      "[19:1100] F: 11505600 \t tot_loss 6.862572 rs_loss 0.121559 vs_loss 0.662496 logits_loss 121.570349\n",
      "[19:1200] F: 11515200 \t tot_loss 6.997532 rs_loss 0.163612 vs_loss 0.756364 logits_loss 121.551120\n",
      "[19:1300] F: 11524800 \t tot_loss 6.932397 rs_loss 0.131087 vs_loss 0.725514 logits_loss 121.515899\n",
      "[19:1400] F: 11534400 \t tot_loss 6.911408 rs_loss 0.124434 vs_loss 0.715411 logits_loss 121.431259\n",
      "[19:1500] F: 11544000 \t tot_loss 6.842750 rs_loss 0.096159 vs_loss 0.682785 logits_loss 121.276117\n",
      "[19:1600] F: 11553600 \t tot_loss 6.716839 rs_loss 0.048135 vs_loss 0.600361 logits_loss 121.366849\n",
      "[19:1700] F: 11563200 \t tot_loss 6.859477 rs_loss 0.093575 vs_loss 0.681282 logits_loss 121.692389\n",
      "[19:1800] F: 11572800 \t tot_loss 6.855849 rs_loss 0.095824 vs_loss 0.680254 logits_loss 121.595415\n",
      "[19:1900] F: 11582400 \t tot_loss 6.846538 rs_loss 0.097369 vs_loss 0.667734 logits_loss 121.628694\n",
      "[19:2000] F: 11592000 \t tot_loss 6.864187 rs_loss 0.106235 vs_loss 0.681524 logits_loss 121.528568\n",
      "[19:2100] F: 11601600 \t tot_loss 6.715877 rs_loss 0.070188 vs_loss 0.589424 logits_loss 121.125290\n",
      "[19:2200] F: 11611200 \t tot_loss 6.763475 rs_loss 0.092148 vs_loss 0.601621 logits_loss 121.394117\n",
      "[19:2300] F: 11620800 \t tot_loss 6.776992 rs_loss 0.102222 vs_loss 0.615320 logits_loss 121.188996\n",
      "[19:2400] F: 11630400 \t tot_loss 6.758094 rs_loss 0.103095 vs_loss 0.600230 logits_loss 121.095387\n",
      "[19:2500] F: 11640000 \t tot_loss 6.800622 rs_loss 0.116095 vs_loss 0.637922 logits_loss 120.932112\n",
      "[19:2600] F: 11649600 \t tot_loss 6.707506 rs_loss 0.091612 vs_loss 0.588965 logits_loss 120.538572\n",
      "[19:2700] F: 11659200 \t tot_loss 6.807055 rs_loss 0.125739 vs_loss 0.653810 logits_loss 120.550113\n",
      "[19:2800] F: 11668800 \t tot_loss 7.044161 rs_loss 0.196178 vs_loss 0.817665 logits_loss 120.606354\n",
      "[19:2900] F: 11678400 \t tot_loss 7.027252 rs_loss 0.194707 vs_loss 0.793558 logits_loss 120.779728\n",
      "[19:3000] F: 11688000 \t tot_loss 7.172512 rs_loss 0.218732 vs_loss 0.910871 logits_loss 120.858193\n",
      "[19:3100] F: 11697600 \t tot_loss 7.180694 rs_loss 0.215913 vs_loss 0.921906 logits_loss 120.857498\n",
      "[19:3200] F: 11707200 \t tot_loss 7.024227 rs_loss 0.167165 vs_loss 0.814527 logits_loss 120.850717\n",
      "[19:3300] F: 11716800 \t tot_loss 7.004064 rs_loss 0.156125 vs_loss 0.799074 logits_loss 120.977288\n",
      "[19:3400] F: 11726400 \t tot_loss 6.867978 rs_loss 0.129668 vs_loss 0.685223 logits_loss 121.061723\n",
      "[19:3500] F: 11736000 \t tot_loss 6.725734 rs_loss 0.086799 vs_loss 0.589136 logits_loss 120.995978\n",
      "[19:3600] F: 11745600 \t tot_loss 6.691070 rs_loss 0.081506 vs_loss 0.559570 logits_loss 120.999864\n",
      "[19:3700] F: 11755200 \t tot_loss 6.882819 rs_loss 0.134194 vs_loss 0.688105 logits_loss 121.210394\n",
      "[19:3800] F: 11764800 \t tot_loss 6.986860 rs_loss 0.160678 vs_loss 0.757130 logits_loss 121.381045\n",
      "[19:3900] F: 11774400 \t tot_loss 6.977992 rs_loss 0.166082 vs_loss 0.743267 logits_loss 121.372857\n",
      "[19:4000] F: 11784000 \t tot_loss 6.947824 rs_loss 0.153371 vs_loss 0.720058 logits_loss 121.487893\n",
      "[19:4100] F: 11793600 \t tot_loss 6.759968 rs_loss 0.092364 vs_loss 0.603717 logits_loss 121.277728\n",
      "[19:4200] F: 11803200 \t tot_loss 6.697416 rs_loss 0.070687 vs_loss 0.570574 logits_loss 121.123099\n",
      "[19:4300] F: 11812800 \t tot_loss 6.836738 rs_loss 0.112173 vs_loss 0.674401 logits_loss 121.003294\n",
      "[19:4400] F: 11822400 \t tot_loss 6.803436 rs_loss 0.099721 vs_loss 0.672225 logits_loss 120.629796\n",
      "[19:4500] F: 11832000 \t tot_loss 6.864090 rs_loss 0.123989 vs_loss 0.709722 logits_loss 120.607569\n",
      "[19:4600] F: 11841600 \t tot_loss 6.928717 rs_loss 0.145854 vs_loss 0.754665 logits_loss 120.563952\n",
      "[19:4700] F: 11851200 \t tot_loss 6.859359 rs_loss 0.116950 vs_loss 0.699623 logits_loss 120.855727\n",
      "[19:4800] F: 11860800 \t tot_loss 7.003921 rs_loss 0.169114 vs_loss 0.781467 logits_loss 121.066786\n",
      "[19:4900] F: 11870400 \t tot_loss 7.128889 rs_loss 0.191964 vs_loss 0.888734 logits_loss 120.963819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:5000] F: 11880000 \t tot_loss 7.132267 rs_loss 0.187609 vs_loss 0.895913 logits_loss 120.974905\n",
      "[19:5100] F: 11889600 \t tot_loss 7.071883 rs_loss 0.167348 vs_loss 0.860019 logits_loss 120.890300\n",
      "[19:5200] F: 11899200 \t tot_loss 6.943163 rs_loss 0.115940 vs_loss 0.780008 logits_loss 120.944306\n",
      "[19:5300] F: 11908800 \t tot_loss 6.834004 rs_loss 0.087039 vs_loss 0.692697 logits_loss 121.085348\n",
      "[19:5400] F: 11918400 \t tot_loss 6.731850 rs_loss 0.070480 vs_loss 0.615729 logits_loss 120.912832\n",
      "[19:5500] F: 11928000 \t tot_loss 6.762130 rs_loss 0.088421 vs_loss 0.634369 logits_loss 120.786798\n",
      "[19:5600] F: 11937600 \t tot_loss 6.752680 rs_loss 0.088889 vs_loss 0.632218 logits_loss 120.631448\n",
      "[19:5700] F: 11947200 \t tot_loss 6.643270 rs_loss 0.069743 vs_loss 0.558217 logits_loss 120.306189\n",
      "[19:5800] F: 11956800 \t tot_loss 6.629026 rs_loss 0.063420 vs_loss 0.544403 logits_loss 120.424049\n",
      "[19:5900] F: 11966400 \t tot_loss 6.593845 rs_loss 0.063251 vs_loss 0.502792 logits_loss 120.556053\n",
      "[19:6000] F: 11976000 \t tot_loss 6.612207 rs_loss 0.072997 vs_loss 0.509986 logits_loss 120.584481\n",
      "[19:6100] F: 11985600 \t tot_loss 6.615336 rs_loss 0.079855 vs_loss 0.496849 logits_loss 120.772645\n",
      "[19:6200] F: 11995200 \t tot_loss 6.623048 rs_loss 0.082479 vs_loss 0.506898 logits_loss 120.673415\n",
      "Batch [20] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.03 (+-0.15) \n",
      "[20:0] F: 12000000 \t tot_loss 6.617344 rs_loss 0.080268 vs_loss 0.504892 logits_loss 120.643669\n",
      "[20:100] F: 12009600 \t tot_loss 6.691224 rs_loss 0.077561 vs_loss 0.567176 logits_loss 120.929742\n",
      "[20:200] F: 12019200 \t tot_loss 6.729931 rs_loss 0.073207 vs_loss 0.602364 logits_loss 121.087185\n",
      "[20:300] F: 12028800 \t tot_loss 6.788932 rs_loss 0.075069 vs_loss 0.636324 logits_loss 121.550784\n",
      "[20:400] F: 12038400 \t tot_loss 6.928112 rs_loss 0.118665 vs_loss 0.720209 logits_loss 121.784764\n",
      "[20:500] F: 12048000 \t tot_loss 6.878251 rs_loss 0.115548 vs_loss 0.687669 logits_loss 121.500669\n",
      "[20:600] F: 12057600 \t tot_loss 6.948493 rs_loss 0.142223 vs_loss 0.732046 logits_loss 121.484481\n",
      "[20:700] F: 12067200 \t tot_loss 7.102772 rs_loss 0.192343 vs_loss 0.850069 logits_loss 121.207197\n",
      "[20:800] F: 12076800 \t tot_loss 6.972595 rs_loss 0.139212 vs_loss 0.783675 logits_loss 120.994160\n",
      "[20:900] F: 12086400 \t tot_loss 6.959260 rs_loss 0.126186 vs_loss 0.771422 logits_loss 121.233037\n",
      "[20:1000] F: 12096000 \t tot_loss 6.880007 rs_loss 0.101431 vs_loss 0.712731 logits_loss 121.316889\n",
      "[20:1100] F: 12105600 \t tot_loss 6.759162 rs_loss 0.071585 vs_loss 0.622007 logits_loss 121.311400\n",
      "[20:1200] F: 12115200 \t tot_loss 6.840352 rs_loss 0.097551 vs_loss 0.671777 logits_loss 121.420467\n",
      "[20:1300] F: 12124800 \t tot_loss 6.848948 rs_loss 0.104391 vs_loss 0.677989 logits_loss 121.331380\n",
      "[20:1400] F: 12134400 \t tot_loss 6.868124 rs_loss 0.121354 vs_loss 0.700754 logits_loss 120.920313\n",
      "[20:1500] F: 12144000 \t tot_loss 6.949150 rs_loss 0.139760 vs_loss 0.778017 logits_loss 120.627471\n",
      "[20:1600] F: 12153600 \t tot_loss 6.877956 rs_loss 0.111923 vs_loss 0.730872 logits_loss 120.703218\n",
      "[20:1700] F: 12163200 \t tot_loss 6.891906 rs_loss 0.133451 vs_loss 0.726643 logits_loss 120.636223\n",
      "[20:1800] F: 12172800 \t tot_loss 6.935327 rs_loss 0.136430 vs_loss 0.755863 logits_loss 120.860673\n",
      "[20:1900] F: 12182400 \t tot_loss 6.862390 rs_loss 0.115198 vs_loss 0.690350 logits_loss 121.136837\n",
      "[20:2000] F: 12192000 \t tot_loss 7.032004 rs_loss 0.170071 vs_loss 0.821326 logits_loss 120.812143\n",
      "[20:2100] F: 12201600 \t tot_loss 7.063958 rs_loss 0.166047 vs_loss 0.865543 logits_loss 120.647363\n",
      "[20:2200] F: 12211200 \t tot_loss 7.006998 rs_loss 0.163941 vs_loss 0.818495 logits_loss 120.491239\n",
      "[20:2300] F: 12220800 \t tot_loss 6.896520 rs_loss 0.139882 vs_loss 0.749005 logits_loss 120.152637\n",
      "[20:2400] F: 12230400 \t tot_loss 6.775207 rs_loss 0.118310 vs_loss 0.646622 logits_loss 120.205490\n",
      "[20:2500] F: 12240000 \t tot_loss 6.755246 rs_loss 0.119964 vs_loss 0.609664 logits_loss 120.512355\n",
      "[20:2600] F: 12249600 \t tot_loss 6.708721 rs_loss 0.098507 vs_loss 0.591527 logits_loss 120.373740\n",
      "[20:2700] F: 12259200 \t tot_loss 6.822505 rs_loss 0.119969 vs_loss 0.666900 logits_loss 120.712713\n",
      "[20:2800] F: 12268800 \t tot_loss 6.817497 rs_loss 0.115794 vs_loss 0.662436 logits_loss 120.785352\n",
      "[20:2900] F: 12278400 \t tot_loss 6.814842 rs_loss 0.115722 vs_loss 0.668875 logits_loss 120.604903\n",
      "[20:3000] F: 12288000 \t tot_loss 6.945047 rs_loss 0.162701 vs_loss 0.739858 logits_loss 120.849771\n",
      "[20:3100] F: 12297600 \t tot_loss 6.904523 rs_loss 0.164348 vs_loss 0.706815 logits_loss 120.667203\n",
      "[20:3200] F: 12307200 \t tot_loss 7.085130 rs_loss 0.224632 vs_loss 0.822915 logits_loss 120.751651\n",
      "[20:3300] F: 12316800 \t tot_loss 7.212080 rs_loss 0.249838 vs_loss 0.917445 logits_loss 120.895945\n",
      "[20:3400] F: 12326400 \t tot_loss 7.272701 rs_loss 0.251689 vs_loss 0.977661 logits_loss 120.867009\n",
      "[20:3500] F: 12336000 \t tot_loss 7.321343 rs_loss 0.256700 vs_loss 1.021296 logits_loss 120.866943\n",
      "[20:3600] F: 12345600 \t tot_loss 7.124260 rs_loss 0.195104 vs_loss 0.888060 logits_loss 120.821897\n",
      "[20:3700] F: 12355200 \t tot_loss 7.057374 rs_loss 0.181274 vs_loss 0.836811 logits_loss 120.785791\n",
      "[20:3800] F: 12364800 \t tot_loss 6.921187 rs_loss 0.157412 vs_loss 0.723789 logits_loss 120.799717\n",
      "[20:3900] F: 12374400 \t tot_loss 7.023998 rs_loss 0.185638 vs_loss 0.791057 logits_loss 120.946042\n",
      "[20:4000] F: 12384000 \t tot_loss 7.083454 rs_loss 0.186801 vs_loss 0.843573 logits_loss 121.061593\n",
      "[20:4100] F: 12393600 \t tot_loss 6.989249 rs_loss 0.161153 vs_loss 0.775634 logits_loss 121.049242\n",
      "[20:4200] F: 12403200 \t tot_loss 6.934852 rs_loss 0.135640 vs_loss 0.742781 logits_loss 121.128613\n",
      "[20:4300] F: 12412800 \t tot_loss 6.828372 rs_loss 0.102087 vs_loss 0.676115 logits_loss 121.003412\n",
      "[20:4400] F: 12422400 \t tot_loss 6.816417 rs_loss 0.097382 vs_loss 0.664233 logits_loss 121.096041\n",
      "[20:4500] F: 12432000 \t tot_loss 6.896492 rs_loss 0.127781 vs_loss 0.718470 logits_loss 121.004810\n",
      "[20:4600] F: 12441600 \t tot_loss 6.948840 rs_loss 0.150984 vs_loss 0.755528 logits_loss 120.846551\n",
      "[20:4700] F: 12451200 \t tot_loss 6.851060 rs_loss 0.126942 vs_loss 0.682433 logits_loss 120.833706\n",
      "[20:4800] F: 12460800 \t tot_loss 6.742196 rs_loss 0.108402 vs_loss 0.618951 logits_loss 120.296856\n",
      "[20:4900] F: 12470400 \t tot_loss 6.649813 rs_loss 0.072524 vs_loss 0.558602 logits_loss 120.373726\n",
      "[20:5000] F: 12480000 \t tot_loss 6.683556 rs_loss 0.095382 vs_loss 0.573161 logits_loss 120.300262\n",
      "[20:5100] F: 12489600 \t tot_loss 6.765709 rs_loss 0.116099 vs_loss 0.625773 logits_loss 120.476732\n",
      "[20:5200] F: 12499200 \t tot_loss 6.774740 rs_loss 0.112801 vs_loss 0.621092 logits_loss 120.816945\n",
      "[20:5300] F: 12508800 \t tot_loss 6.961221 rs_loss 0.163747 vs_loss 0.762683 logits_loss 120.695818\n",
      "[20:5400] F: 12518400 \t tot_loss 6.988086 rs_loss 0.163368 vs_loss 0.792071 logits_loss 120.652942\n",
      "[20:5500] F: 12528000 \t tot_loss 7.079840 rs_loss 0.202047 vs_loss 0.860823 logits_loss 120.339394\n",
      "[20:5600] F: 12537600 \t tot_loss 7.093356 rs_loss 0.208000 vs_loss 0.868558 logits_loss 120.335935\n",
      "[20:5700] F: 12547200 \t tot_loss 6.987548 rs_loss 0.187948 vs_loss 0.788136 logits_loss 120.229277\n",
      "[20:5800] F: 12556800 \t tot_loss 6.928484 rs_loss 0.167150 vs_loss 0.742381 logits_loss 120.379056\n",
      "[20:5900] F: 12566400 \t tot_loss 6.812277 rs_loss 0.132260 vs_loss 0.643849 logits_loss 120.723373\n",
      "[20:6000] F: 12576000 \t tot_loss 6.776666 rs_loss 0.127113 vs_loss 0.630300 logits_loss 120.385072\n",
      "[20:6100] F: 12585600 \t tot_loss 6.829733 rs_loss 0.139923 vs_loss 0.667031 logits_loss 120.455573\n",
      "[20:6200] F: 12595200 \t tot_loss 7.002215 rs_loss 0.183027 vs_loss 0.804607 logits_loss 120.291619\n",
      "Batch [21] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.06 (+-0.20) \n",
      "[21:0] F: 12600000 \t tot_loss 6.947095 rs_loss 0.161494 vs_loss 0.773514 logits_loss 120.241742\n",
      "[21:100] F: 12609600 \t tot_loss 7.191204 rs_loss 0.211895 vs_loss 0.962928 logits_loss 120.327609\n",
      "[21:200] F: 12619200 \t tot_loss 7.321169 rs_loss 0.231917 vs_loss 1.058568 logits_loss 120.613660\n",
      "[21:300] F: 12628800 \t tot_loss 7.081959 rs_loss 0.145542 vs_loss 0.888016 logits_loss 120.968027\n",
      "[21:400] F: 12638400 \t tot_loss 7.098476 rs_loss 0.171677 vs_loss 0.873769 logits_loss 121.060616\n",
      "[21:500] F: 12648000 \t tot_loss 7.086168 rs_loss 0.193582 vs_loss 0.851470 logits_loss 120.822320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:600] F: 12657600 \t tot_loss 6.985091 rs_loss 0.174295 vs_loss 0.774792 logits_loss 120.720077\n",
      "[21:700] F: 12667200 \t tot_loss 7.054930 rs_loss 0.204720 vs_loss 0.822708 logits_loss 120.550048\n",
      "[21:800] F: 12676800 \t tot_loss 6.966763 rs_loss 0.164299 vs_loss 0.773181 logits_loss 120.585668\n",
      "[21:900] F: 12686400 \t tot_loss 6.831087 rs_loss 0.117138 vs_loss 0.676802 logits_loss 120.742936\n",
      "[21:1000] F: 12696000 \t tot_loss 6.894420 rs_loss 0.146625 vs_loss 0.716526 logits_loss 120.625382\n",
      "[21:1100] F: 12705600 \t tot_loss 6.805073 rs_loss 0.116167 vs_loss 0.674206 logits_loss 120.293999\n",
      "[21:1200] F: 12715200 \t tot_loss 6.878306 rs_loss 0.129413 vs_loss 0.729334 logits_loss 120.391186\n",
      "[21:1300] F: 12724800 \t tot_loss 6.895354 rs_loss 0.152229 vs_loss 0.730531 logits_loss 120.251875\n",
      "[21:1400] F: 12734400 \t tot_loss 6.844446 rs_loss 0.129001 vs_loss 0.703859 logits_loss 120.231723\n",
      "[21:1500] F: 12744000 \t tot_loss 6.926592 rs_loss 0.151061 vs_loss 0.749860 logits_loss 120.513412\n",
      "[21:1600] F: 12753600 \t tot_loss 7.007550 rs_loss 0.176599 vs_loss 0.814249 logits_loss 120.334026\n",
      "[21:1700] F: 12763200 \t tot_loss 6.908859 rs_loss 0.132889 vs_loss 0.748781 logits_loss 120.543777\n",
      "[21:1800] F: 12772800 \t tot_loss 6.868847 rs_loss 0.122116 vs_loss 0.716655 logits_loss 120.601539\n",
      "[21:1900] F: 12782400 \t tot_loss 6.780580 rs_loss 0.095153 vs_loss 0.665674 logits_loss 120.395058\n",
      "[21:2000] F: 12792000 \t tot_loss 6.723506 rs_loss 0.070647 vs_loss 0.626496 logits_loss 120.527247\n",
      "[21:2100] F: 12801600 \t tot_loss 6.819404 rs_loss 0.099804 vs_loss 0.696834 logits_loss 120.455319\n",
      "[21:2200] F: 12811200 \t tot_loss 7.131966 rs_loss 0.189406 vs_loss 0.918984 logits_loss 120.471538\n",
      "[21:2300] F: 12820800 \t tot_loss 7.272428 rs_loss 0.241310 vs_loss 1.005050 logits_loss 120.521363\n",
      "[21:2400] F: 12830400 \t tot_loss 7.246394 rs_loss 0.266211 vs_loss 0.955900 logits_loss 120.485656\n",
      "[21:2500] F: 12840000 \t tot_loss 7.272839 rs_loss 0.289303 vs_loss 0.970329 logits_loss 120.264132\n",
      "[21:2600] F: 12849600 \t tot_loss 7.005713 rs_loss 0.204829 vs_loss 0.800836 logits_loss 120.000967\n",
      "[21:2700] F: 12859200 \t tot_loss 7.099252 rs_loss 0.233479 vs_loss 0.862633 logits_loss 120.062798\n",
      "[21:2800] F: 12868800 \t tot_loss 7.188751 rs_loss 0.228838 vs_loss 0.969039 logits_loss 119.817473\n",
      "[21:2900] F: 12878400 \t tot_loss 7.112537 rs_loss 0.201783 vs_loss 0.931831 logits_loss 119.578457\n",
      "[21:3000] F: 12888000 \t tot_loss 7.199617 rs_loss 0.256246 vs_loss 0.962685 logits_loss 119.613709\n",
      "[21:3100] F: 12897600 \t tot_loss 6.954477 rs_loss 0.174209 vs_loss 0.798977 logits_loss 119.625813\n",
      "[21:3200] F: 12907200 \t tot_loss 6.879598 rs_loss 0.175190 vs_loss 0.720573 logits_loss 119.676702\n",
      "[21:3300] F: 12916800 \t tot_loss 6.926737 rs_loss 0.177596 vs_loss 0.763642 logits_loss 119.709983\n",
      "[21:3400] F: 12926400 \t tot_loss 6.886174 rs_loss 0.147565 vs_loss 0.744519 logits_loss 119.881813\n",
      "[21:3500] F: 12936000 \t tot_loss 6.925348 rs_loss 0.165940 vs_loss 0.773788 logits_loss 119.712394\n",
      "[21:3600] F: 12945600 \t tot_loss 6.965408 rs_loss 0.160227 vs_loss 0.811868 logits_loss 119.866271\n",
      "[21:3700] F: 12955200 \t tot_loss 6.959634 rs_loss 0.154558 vs_loss 0.814464 logits_loss 119.812238\n",
      "[21:3800] F: 12964800 \t tot_loss 6.883553 rs_loss 0.135016 vs_loss 0.761859 logits_loss 119.733559\n",
      "[21:3900] F: 12974400 \t tot_loss 6.887611 rs_loss 0.141620 vs_loss 0.776859 logits_loss 119.382638\n",
      "[21:4000] F: 12984000 \t tot_loss 6.797865 rs_loss 0.116357 vs_loss 0.714984 logits_loss 119.330475\n",
      "[21:4100] F: 12993600 \t tot_loss 6.904454 rs_loss 0.142381 vs_loss 0.790766 logits_loss 119.426137\n",
      "[21:4200] F: 13003200 \t tot_loss 7.082514 rs_loss 0.180352 vs_loss 0.926148 logits_loss 119.520284\n",
      "[21:4300] F: 13012800 \t tot_loss 7.055565 rs_loss 0.162624 vs_loss 0.891971 logits_loss 120.019414\n",
      "[21:4400] F: 13022400 \t tot_loss 7.056218 rs_loss 0.170409 vs_loss 0.881860 logits_loss 120.078995\n",
      "[21:4500] F: 13032000 \t tot_loss 7.043669 rs_loss 0.169398 vs_loss 0.862230 logits_loss 120.240823\n",
      "[21:4600] F: 13041600 \t tot_loss 7.096299 rs_loss 0.206122 vs_loss 0.899863 logits_loss 119.806290\n",
      "[21:4700] F: 13051200 \t tot_loss 7.156837 rs_loss 0.224075 vs_loss 0.947746 logits_loss 119.700308\n",
      "[21:4800] F: 13060800 \t tot_loss 7.156309 rs_loss 0.221542 vs_loss 0.957514 logits_loss 119.545060\n",
      "[21:4900] F: 13070400 \t tot_loss 7.079301 rs_loss 0.221832 vs_loss 0.881378 logits_loss 119.521822\n",
      "[21:5000] F: 13080000 \t tot_loss 6.895621 rs_loss 0.168583 vs_loss 0.737238 logits_loss 119.796008\n",
      "[21:5100] F: 13089600 \t tot_loss 6.872477 rs_loss 0.172477 vs_loss 0.710746 logits_loss 119.785083\n",
      "[21:5200] F: 13099200 \t tot_loss 6.939935 rs_loss 0.203623 vs_loss 0.739326 logits_loss 119.939702\n",
      "[21:5300] F: 13108800 \t tot_loss 6.907077 rs_loss 0.178736 vs_loss 0.736072 logits_loss 119.845384\n",
      "[21:5400] F: 13118400 \t tot_loss 6.980814 rs_loss 0.191551 vs_loss 0.792547 logits_loss 119.934324\n",
      "[21:5500] F: 13128000 \t tot_loss 7.186369 rs_loss 0.238377 vs_loss 0.947345 logits_loss 120.012922\n",
      "[21:5600] F: 13137600 \t tot_loss 7.138257 rs_loss 0.217412 vs_loss 0.936483 logits_loss 119.687237\n",
      "[21:5700] F: 13147200 \t tot_loss 7.166643 rs_loss 0.237288 vs_loss 0.951572 logits_loss 119.555662\n",
      "[21:5800] F: 13156800 \t tot_loss 7.035780 rs_loss 0.195920 vs_loss 0.872115 logits_loss 119.354903\n",
      "[21:5900] F: 13166400 \t tot_loss 6.938079 rs_loss 0.161320 vs_loss 0.805043 logits_loss 119.434326\n",
      "[21:6000] F: 13176000 \t tot_loss 7.096621 rs_loss 0.194474 vs_loss 0.919138 logits_loss 119.660182\n",
      "[21:6100] F: 13185600 \t tot_loss 7.017220 rs_loss 0.161027 vs_loss 0.861119 logits_loss 119.901487\n",
      "[21:6200] F: 13195200 \t tot_loss 7.146257 rs_loss 0.215394 vs_loss 0.923701 logits_loss 120.143249\n",
      "Batch [22] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.04 (+-0.21) \n",
      "[22:0] F: 13200000 \t tot_loss 7.089597 rs_loss 0.201166 vs_loss 0.884256 logits_loss 120.083498\n",
      "[22:100] F: 13209600 \t tot_loss 6.990635 rs_loss 0.181483 vs_loss 0.798997 logits_loss 120.203100\n",
      "[22:200] F: 13219200 \t tot_loss 7.086464 rs_loss 0.199375 vs_loss 0.868233 logits_loss 120.377115\n",
      "[22:300] F: 13228800 \t tot_loss 7.379381 rs_loss 0.254558 vs_loss 1.089792 logits_loss 120.700618\n",
      "[22:400] F: 13238400 \t tot_loss 7.441133 rs_loss 0.254789 vs_loss 1.132066 logits_loss 121.085543\n",
      "[22:500] F: 13248000 \t tot_loss 7.373964 rs_loss 0.235963 vs_loss 1.085271 logits_loss 121.054608\n",
      "[22:600] F: 13257600 \t tot_loss 7.478700 rs_loss 0.252975 vs_loss 1.169691 logits_loss 121.120695\n",
      "[22:700] F: 13267200 \t tot_loss 7.217743 rs_loss 0.202142 vs_loss 0.965883 logits_loss 120.994348\n",
      "[22:800] F: 13276800 \t tot_loss 7.053046 rs_loss 0.147430 vs_loss 0.848694 logits_loss 121.138430\n",
      "[22:900] F: 13286400 \t tot_loss 7.062912 rs_loss 0.164263 vs_loss 0.855284 logits_loss 120.867295\n",
      "[22:1000] F: 13296000 \t tot_loss 6.790539 rs_loss 0.112641 vs_loss 0.630692 logits_loss 120.944101\n",
      "[22:1100] F: 13305600 \t tot_loss 6.833552 rs_loss 0.124464 vs_loss 0.671125 logits_loss 120.759253\n",
      "[22:1200] F: 13315200 \t tot_loss 6.816683 rs_loss 0.125792 vs_loss 0.658061 logits_loss 120.656606\n",
      "[22:1300] F: 13324800 \t tot_loss 6.775275 rs_loss 0.107562 vs_loss 0.636055 logits_loss 120.633154\n",
      "[22:1400] F: 13334400 \t tot_loss 6.705405 rs_loss 0.074673 vs_loss 0.597816 logits_loss 120.658328\n",
      "[22:1500] F: 13344000 \t tot_loss 6.735409 rs_loss 0.078406 vs_loss 0.624385 logits_loss 120.652350\n",
      "[22:1600] F: 13353600 \t tot_loss 6.757690 rs_loss 0.096388 vs_loss 0.635243 logits_loss 120.521180\n",
      "[22:1700] F: 13363200 \t tot_loss 6.881947 rs_loss 0.137330 vs_loss 0.717090 logits_loss 120.550527\n",
      "[22:1800] F: 13372800 \t tot_loss 7.079901 rs_loss 0.208190 vs_loss 0.854352 logits_loss 120.347198\n",
      "[22:1900] F: 13382400 \t tot_loss 7.134030 rs_loss 0.228824 vs_loss 0.884400 logits_loss 120.416114\n",
      "[22:2000] F: 13392000 \t tot_loss 7.180038 rs_loss 0.246540 vs_loss 0.916778 logits_loss 120.334383\n",
      "[22:2100] F: 13401600 \t tot_loss 7.095257 rs_loss 0.221039 vs_loss 0.848976 logits_loss 120.504844\n",
      "[22:2200] F: 13411200 \t tot_loss 7.083382 rs_loss 0.197248 vs_loss 0.861566 logits_loss 120.491350\n",
      "[22:2300] F: 13420800 \t tot_loss 6.977128 rs_loss 0.174158 vs_loss 0.782428 logits_loss 120.410850\n",
      "[22:2400] F: 13430400 \t tot_loss 6.927064 rs_loss 0.161686 vs_loss 0.748752 logits_loss 120.332527\n",
      "[22:2500] F: 13440000 \t tot_loss 6.888699 rs_loss 0.140710 vs_loss 0.734195 logits_loss 120.275874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:2600] F: 13449600 \t tot_loss 6.690941 rs_loss 0.096615 vs_loss 0.576680 logits_loss 120.352931\n",
      "[22:2700] F: 13459200 \t tot_loss 6.900821 rs_loss 0.164098 vs_loss 0.709878 logits_loss 120.536882\n",
      "[22:2800] F: 13468800 \t tot_loss 6.964291 rs_loss 0.167952 vs_loss 0.776025 logits_loss 120.406270\n",
      "[22:2900] F: 13478400 \t tot_loss 7.056652 rs_loss 0.194242 vs_loss 0.834954 logits_loss 120.549101\n",
      "[22:3000] F: 13488000 \t tot_loss 7.045446 rs_loss 0.189227 vs_loss 0.838295 logits_loss 120.358476\n",
      "[22:3100] F: 13497600 \t tot_loss 6.774065 rs_loss 0.099250 vs_loss 0.665080 logits_loss 120.194712\n",
      "[22:3200] F: 13507200 \t tot_loss 6.740965 rs_loss 0.094680 vs_loss 0.623516 logits_loss 120.455362\n",
      "[22:3300] F: 13516800 \t tot_loss 6.666972 rs_loss 0.070665 vs_loss 0.570696 logits_loss 120.512233\n",
      "[22:3400] F: 13526400 \t tot_loss 6.788754 rs_loss 0.113961 vs_loss 0.646315 logits_loss 120.569559\n",
      "[22:3500] F: 13536000 \t tot_loss 6.789531 rs_loss 0.107761 vs_loss 0.657432 logits_loss 120.486772\n",
      "[22:3600] F: 13545600 \t tot_loss 6.870232 rs_loss 0.123904 vs_loss 0.712008 logits_loss 120.686388\n",
      "[22:3700] F: 13555200 \t tot_loss 6.867573 rs_loss 0.118688 vs_loss 0.717111 logits_loss 120.635492\n",
      "[22:3800] F: 13564800 \t tot_loss 6.776658 rs_loss 0.087315 vs_loss 0.660901 logits_loss 120.568844\n",
      "[22:3900] F: 13574400 \t tot_loss 6.865063 rs_loss 0.116525 vs_loss 0.716974 logits_loss 120.631265\n",
      "[22:4000] F: 13584000 \t tot_loss 6.713790 rs_loss 0.078914 vs_loss 0.612082 logits_loss 120.455885\n",
      "[22:4100] F: 13593600 \t tot_loss 6.764629 rs_loss 0.104841 vs_loss 0.646453 logits_loss 120.266706\n",
      "[22:4200] F: 13603200 \t tot_loss 6.840751 rs_loss 0.139970 vs_loss 0.685976 logits_loss 120.296100\n",
      "[22:4300] F: 13612800 \t tot_loss 6.891224 rs_loss 0.162551 vs_loss 0.719839 logits_loss 120.176665\n",
      "[22:4400] F: 13622400 \t tot_loss 7.080000 rs_loss 0.230814 vs_loss 0.848428 logits_loss 120.015172\n",
      "[22:4500] F: 13632000 \t tot_loss 7.065953 rs_loss 0.231115 vs_loss 0.837036 logits_loss 119.956049\n",
      "[22:4600] F: 13641600 \t tot_loss 7.046987 rs_loss 0.229016 vs_loss 0.817113 logits_loss 120.017156\n",
      "[22:4700] F: 13651200 \t tot_loss 7.004170 rs_loss 0.208860 vs_loss 0.782215 logits_loss 120.261922\n",
      "[22:4800] F: 13660800 \t tot_loss 6.886809 rs_loss 0.168153 vs_loss 0.704939 logits_loss 120.274337\n",
      "[22:4900] F: 13670400 \t tot_loss 7.018102 rs_loss 0.193914 vs_loss 0.812769 logits_loss 120.228382\n",
      "[22:5000] F: 13680000 \t tot_loss 6.907994 rs_loss 0.147912 vs_loss 0.756232 logits_loss 120.076995\n",
      "[22:5100] F: 13689600 \t tot_loss 7.037002 rs_loss 0.215134 vs_loss 0.833871 logits_loss 119.759929\n",
      "[22:5200] F: 13699200 \t tot_loss 7.052193 rs_loss 0.205073 vs_loss 0.861852 logits_loss 119.705365\n",
      "[22:5300] F: 13708800 \t tot_loss 6.900597 rs_loss 0.174244 vs_loss 0.740690 logits_loss 119.713261\n",
      "[22:5400] F: 13718400 \t tot_loss 6.891767 rs_loss 0.171517 vs_loss 0.728164 logits_loss 119.841710\n",
      "[22:5500] F: 13728000 \t tot_loss 6.648000 rs_loss 0.070009 vs_loss 0.574698 logits_loss 120.065854\n",
      "[22:5600] F: 13737600 \t tot_loss 6.700692 rs_loss 0.094429 vs_loss 0.599487 logits_loss 120.135507\n",
      "[22:5700] F: 13747200 \t tot_loss 6.820856 rs_loss 0.133474 vs_loss 0.688569 logits_loss 119.976253\n",
      "[22:5800] F: 13756800 \t tot_loss 6.848900 rs_loss 0.138805 vs_loss 0.720491 logits_loss 119.792075\n",
      "[22:5900] F: 13766400 \t tot_loss 6.955122 rs_loss 0.180419 vs_loss 0.787549 logits_loss 119.743057\n",
      "[22:6000] F: 13776000 \t tot_loss 6.985628 rs_loss 0.192553 vs_loss 0.803697 logits_loss 119.787536\n",
      "[22:6100] F: 13785600 \t tot_loss 6.826685 rs_loss 0.149061 vs_loss 0.687925 logits_loss 119.793981\n",
      "[22:6200] F: 13795200 \t tot_loss 6.824593 rs_loss 0.145245 vs_loss 0.678465 logits_loss 120.017660\n",
      "Batch [23] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.29 (+-0.21) \n",
      "[23:0] F: 13800000 \t tot_loss 6.762644 rs_loss 0.127729 vs_loss 0.634722 logits_loss 120.003871\n",
      "[23:100] F: 13809600 \t tot_loss 6.647872 rs_loss 0.072494 vs_loss 0.556774 logits_loss 120.372071\n",
      "[23:200] F: 13819200 \t tot_loss 6.802606 rs_loss 0.117241 vs_loss 0.635154 logits_loss 121.004215\n",
      "[23:300] F: 13828800 \t tot_loss 6.813769 rs_loss 0.117421 vs_loss 0.628686 logits_loss 121.353228\n",
      "[23:400] F: 13838400 \t tot_loss 6.903777 rs_loss 0.129692 vs_loss 0.690378 logits_loss 121.674141\n",
      "[23:500] F: 13848000 \t tot_loss 6.972443 rs_loss 0.148583 vs_loss 0.742060 logits_loss 121.635990\n",
      "[23:600] F: 13857600 \t tot_loss 6.836936 rs_loss 0.099206 vs_loss 0.669816 logits_loss 121.358282\n",
      "[23:700] F: 13867200 \t tot_loss 7.043945 rs_loss 0.168621 vs_loss 0.820027 logits_loss 121.105929\n",
      "[23:800] F: 13876800 \t tot_loss 6.966953 rs_loss 0.155863 vs_loss 0.766576 logits_loss 120.890268\n",
      "[23:900] F: 13886400 \t tot_loss 7.018574 rs_loss 0.177260 vs_loss 0.800143 logits_loss 120.823432\n",
      "[23:1000] F: 13896000 \t tot_loss 7.114951 rs_loss 0.224380 vs_loss 0.847043 logits_loss 120.870557\n",
      "[23:1100] F: 13905600 \t tot_loss 6.862285 rs_loss 0.137169 vs_loss 0.677051 logits_loss 120.961283\n",
      "[23:1200] F: 13915200 \t tot_loss 6.865085 rs_loss 0.129601 vs_loss 0.690739 logits_loss 120.894895\n",
      "[23:1300] F: 13924800 \t tot_loss 6.836036 rs_loss 0.126669 vs_loss 0.660253 logits_loss 120.982285\n",
      "[23:1400] F: 13934400 \t tot_loss 6.714911 rs_loss 0.072502 vs_loss 0.599399 logits_loss 120.860201\n",
      "[23:1500] F: 13944000 \t tot_loss 6.775380 rs_loss 0.086424 vs_loss 0.640895 logits_loss 120.961224\n",
      "[23:1600] F: 13953600 \t tot_loss 6.795937 rs_loss 0.098084 vs_loss 0.627928 logits_loss 121.398507\n",
      "[23:1700] F: 13963200 \t tot_loss 6.789136 rs_loss 0.095084 vs_loss 0.629605 logits_loss 121.288933\n",
      "[23:1800] F: 13972800 \t tot_loss 6.974468 rs_loss 0.150352 vs_loss 0.761593 logits_loss 121.250438\n",
      "[23:1900] F: 13982400 \t tot_loss 6.930728 rs_loss 0.146772 vs_loss 0.733602 logits_loss 121.007079\n",
      "[23:2000] F: 13992000 \t tot_loss 7.014185 rs_loss 0.167125 vs_loss 0.820290 logits_loss 120.535392\n",
      "[23:2100] F: 14001600 \t tot_loss 6.932410 rs_loss 0.135659 vs_loss 0.772160 logits_loss 120.491832\n",
      "[23:2200] F: 14011200 \t tot_loss 6.846944 rs_loss 0.114388 vs_loss 0.704304 logits_loss 120.565029\n",
      "[23:2300] F: 14020800 \t tot_loss 6.873962 rs_loss 0.127444 vs_loss 0.716493 logits_loss 120.600500\n",
      "[23:2400] F: 14030400 \t tot_loss 6.767621 rs_loss 0.110305 vs_loss 0.620674 logits_loss 120.732827\n",
      "[23:2500] F: 14040000 \t tot_loss 6.789256 rs_loss 0.123697 vs_loss 0.637852 logits_loss 120.554120\n",
      "[23:2600] F: 14049600 \t tot_loss 6.707486 rs_loss 0.095231 vs_loss 0.599258 logits_loss 120.259938\n",
      "[23:2700] F: 14059200 \t tot_loss 6.721002 rs_loss 0.091083 vs_loss 0.616464 logits_loss 120.269098\n",
      "[23:2800] F: 14068800 \t tot_loss 6.879203 rs_loss 0.127909 vs_loss 0.743292 logits_loss 120.160049\n",
      "[23:2900] F: 14078400 \t tot_loss 6.811516 rs_loss 0.104464 vs_loss 0.686956 logits_loss 120.401922\n",
      "[23:3000] F: 14088000 \t tot_loss 6.972520 rs_loss 0.147803 vs_loss 0.793188 logits_loss 120.630575\n",
      "[23:3100] F: 14097600 \t tot_loss 6.958282 rs_loss 0.145728 vs_loss 0.782563 logits_loss 120.599828\n",
      "[23:3200] F: 14107200 \t tot_loss 6.805476 rs_loss 0.100065 vs_loss 0.667244 logits_loss 120.763352\n",
      "[23:3300] F: 14116800 \t tot_loss 6.812292 rs_loss 0.107762 vs_loss 0.676561 logits_loss 120.559389\n",
      "[23:3400] F: 14126400 \t tot_loss 6.788009 rs_loss 0.115754 vs_loss 0.648910 logits_loss 120.466893\n",
      "[23:3500] F: 14136000 \t tot_loss 6.747561 rs_loss 0.103492 vs_loss 0.615213 logits_loss 120.577094\n",
      "[23:3600] F: 14145600 \t tot_loss 6.807655 rs_loss 0.133436 vs_loss 0.646004 logits_loss 120.564289\n",
      "[23:3700] F: 14155200 \t tot_loss 6.828594 rs_loss 0.131494 vs_loss 0.668166 logits_loss 120.578677\n",
      "[23:3800] F: 14164800 \t tot_loss 6.669357 rs_loss 0.081650 vs_loss 0.562866 logits_loss 120.496807\n",
      "[23:3900] F: 14174400 \t tot_loss 6.674805 rs_loss 0.087961 vs_loss 0.576511 logits_loss 120.206664\n",
      "[23:4000] F: 14184000 \t tot_loss 6.701830 rs_loss 0.077809 vs_loss 0.612455 logits_loss 120.231319\n",
      "[23:4100] F: 14193600 \t tot_loss 6.696905 rs_loss 0.079424 vs_loss 0.600198 logits_loss 120.345654\n",
      "[23:4200] F: 14203200 \t tot_loss 6.764120 rs_loss 0.101973 vs_loss 0.634117 logits_loss 120.560592\n",
      "[23:4300] F: 14212800 \t tot_loss 6.754586 rs_loss 0.092263 vs_loss 0.624974 logits_loss 120.746984\n",
      "[23:4400] F: 14222400 \t tot_loss 6.718137 rs_loss 0.102185 vs_loss 0.597189 logits_loss 120.375263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:4500] F: 14232000 \t tot_loss 6.877145 rs_loss 0.143903 vs_loss 0.722044 logits_loss 120.223959\n",
      "[23:4600] F: 14241600 \t tot_loss 6.931762 rs_loss 0.141216 vs_loss 0.783228 logits_loss 120.146367\n",
      "[23:4700] F: 14251200 \t tot_loss 6.962382 rs_loss 0.168377 vs_loss 0.789362 logits_loss 120.092854\n",
      "[23:4800] F: 14260800 \t tot_loss 6.977921 rs_loss 0.174894 vs_loss 0.788874 logits_loss 120.283052\n",
      "[23:4900] F: 14270400 \t tot_loss 6.805198 rs_loss 0.135587 vs_loss 0.645259 logits_loss 120.487032\n",
      "[23:5000] F: 14280000 \t tot_loss 6.705251 rs_loss 0.115640 vs_loss 0.558407 logits_loss 120.624074\n",
      "[23:5100] F: 14289600 \t tot_loss 6.694809 rs_loss 0.095522 vs_loss 0.560122 logits_loss 120.783313\n",
      "[23:5200] F: 14299200 \t tot_loss 6.617577 rs_loss 0.061529 vs_loss 0.507499 logits_loss 120.970975\n",
      "[23:5300] F: 14308800 \t tot_loss 6.621678 rs_loss 0.063606 vs_loss 0.527588 logits_loss 120.609679\n",
      "[23:5400] F: 14318400 \t tot_loss 6.609453 rs_loss 0.059859 vs_loss 0.525332 logits_loss 120.485232\n",
      "[23:5500] F: 14328000 \t tot_loss 6.653436 rs_loss 0.080471 vs_loss 0.563526 logits_loss 120.188786\n",
      "[23:5600] F: 14337600 \t tot_loss 6.708070 rs_loss 0.107156 vs_loss 0.586085 logits_loss 120.296573\n",
      "[23:5700] F: 14347200 \t tot_loss 6.767034 rs_loss 0.121432 vs_loss 0.622385 logits_loss 120.464342\n",
      "[23:5800] F: 14356800 \t tot_loss 6.836735 rs_loss 0.153442 vs_loss 0.659915 logits_loss 120.467547\n",
      "[23:5900] F: 14366400 \t tot_loss 6.813201 rs_loss 0.147663 vs_loss 0.636005 logits_loss 120.590656\n",
      "[23:6000] F: 14376000 \t tot_loss 6.754757 rs_loss 0.120901 vs_loss 0.611221 logits_loss 120.452709\n",
      "[23:6100] F: 14385600 \t tot_loss 6.768798 rs_loss 0.121449 vs_loss 0.614247 logits_loss 120.662034\n",
      "[23:6200] F: 14395200 \t tot_loss 6.690551 rs_loss 0.087241 vs_loss 0.565154 logits_loss 120.763128\n",
      "Batch [24] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.28 (+-0.21) \n",
      "[24:0] F: 14400000 \t tot_loss 6.681420 rs_loss 0.085182 vs_loss 0.554575 logits_loss 120.833258\n",
      "[24:100] F: 14409600 \t tot_loss 6.736685 rs_loss 0.084746 vs_loss 0.604963 logits_loss 120.939530\n",
      "[24:200] F: 14419200 \t tot_loss 6.696842 rs_loss 0.061888 vs_loss 0.574542 logits_loss 121.208224\n",
      "[24:300] F: 14428800 \t tot_loss 6.752136 rs_loss 0.065179 vs_loss 0.635513 logits_loss 121.028892\n",
      "[24:400] F: 14438400 \t tot_loss 6.968632 rs_loss 0.125624 vs_loss 0.791381 logits_loss 121.032537\n",
      "[24:500] F: 14448000 \t tot_loss 6.912705 rs_loss 0.106248 vs_loss 0.761690 logits_loss 120.895319\n",
      "[24:600] F: 14457600 \t tot_loss 6.912686 rs_loss 0.110534 vs_loss 0.764405 logits_loss 120.754947\n",
      "[24:700] F: 14467200 \t tot_loss 6.882922 rs_loss 0.109107 vs_loss 0.732853 logits_loss 120.819250\n",
      "[24:800] F: 14476800 \t tot_loss 6.956327 rs_loss 0.132596 vs_loss 0.771333 logits_loss 121.047950\n",
      "[24:900] F: 14486400 \t tot_loss 6.918514 rs_loss 0.130010 vs_loss 0.748130 logits_loss 120.807490\n",
      "[24:1000] F: 14496000 \t tot_loss 7.057598 rs_loss 0.176695 vs_loss 0.855380 logits_loss 120.510459\n",
      "[24:1100] F: 14505600 \t tot_loss 7.062631 rs_loss 0.174856 vs_loss 0.860659 logits_loss 120.542312\n",
      "[24:1200] F: 14515200 \t tot_loss 6.919717 rs_loss 0.126435 vs_loss 0.779026 logits_loss 120.285127\n",
      "[24:1300] F: 14524800 \t tot_loss 7.106582 rs_loss 0.170084 vs_loss 0.915585 logits_loss 120.418258\n",
      "[24:1400] F: 14534400 \t tot_loss 6.950213 rs_loss 0.123333 vs_loss 0.801193 logits_loss 120.513725\n",
      "[24:1500] F: 14544000 \t tot_loss 7.057742 rs_loss 0.150870 vs_loss 0.879228 logits_loss 120.552876\n",
      "[24:1600] F: 14553600 \t tot_loss 7.028637 rs_loss 0.150229 vs_loss 0.844287 logits_loss 120.682422\n",
      "[24:1700] F: 14563200 \t tot_loss 6.848362 rs_loss 0.104175 vs_loss 0.709464 logits_loss 120.694452\n",
      "[24:1800] F: 14572800 \t tot_loss 6.980726 rs_loss 0.145187 vs_loss 0.792444 logits_loss 120.861885\n",
      "[24:1900] F: 14582400 \t tot_loss 7.080897 rs_loss 0.187716 vs_loss 0.866882 logits_loss 120.525987\n",
      "[24:2000] F: 14592000 \t tot_loss 6.982729 rs_loss 0.159640 vs_loss 0.800882 logits_loss 120.444124\n",
      "[24:2100] F: 14601600 \t tot_loss 7.162988 rs_loss 0.214568 vs_loss 0.937241 logits_loss 120.223588\n",
      "[24:2200] F: 14611200 \t tot_loss 7.152324 rs_loss 0.213730 vs_loss 0.916505 logits_loss 120.441786\n",
      "[24:2300] F: 14620800 \t tot_loss 6.960352 rs_loss 0.155822 vs_loss 0.763923 logits_loss 120.812118\n",
      "[24:2400] F: 14630400 \t tot_loss 7.162695 rs_loss 0.220797 vs_loss 0.896445 logits_loss 120.909072\n",
      "[24:2500] F: 14640000 \t tot_loss 7.202160 rs_loss 0.232759 vs_loss 0.902212 logits_loss 121.343774\n",
      "[24:2600] F: 14649600 \t tot_loss 7.208437 rs_loss 0.221369 vs_loss 0.936902 logits_loss 121.003314\n",
      "[24:2700] F: 14659200 \t tot_loss 7.323922 rs_loss 0.258181 vs_loss 1.014043 logits_loss 121.033964\n",
      "[24:2800] F: 14668800 \t tot_loss 7.309981 rs_loss 0.241064 vs_loss 1.013553 logits_loss 121.107276\n",
      "[24:2900] F: 14678400 \t tot_loss 7.181824 rs_loss 0.196744 vs_loss 0.945899 logits_loss 120.783622\n",
      "[24:3000] F: 14688000 \t tot_loss 7.237882 rs_loss 0.238910 vs_loss 0.961403 logits_loss 120.751373\n",
      "[24:3100] F: 14697600 \t tot_loss 7.210654 rs_loss 0.249228 vs_loss 0.931643 logits_loss 120.595662\n",
      "[24:3200] F: 14707200 \t tot_loss 7.126768 rs_loss 0.228082 vs_loss 0.887325 logits_loss 120.227206\n",
      "[24:3300] F: 14716800 \t tot_loss 7.010278 rs_loss 0.201239 vs_loss 0.798689 logits_loss 120.206997\n",
      "[24:3400] F: 14726400 \t tot_loss 6.784758 rs_loss 0.122234 vs_loss 0.654452 logits_loss 120.161433\n",
      "[24:3500] F: 14736000 \t tot_loss 6.773361 rs_loss 0.089198 vs_loss 0.666036 logits_loss 120.362517\n",
      "[24:3600] F: 14745600 \t tot_loss 6.674110 rs_loss 0.068825 vs_loss 0.580237 logits_loss 120.500951\n",
      "[24:3700] F: 14755200 \t tot_loss 6.718741 rs_loss 0.082394 vs_loss 0.609663 logits_loss 120.533659\n",
      "[24:3800] F: 14764800 \t tot_loss 6.830430 rs_loss 0.108410 vs_loss 0.678135 logits_loss 120.877696\n",
      "[24:3900] F: 14774400 \t tot_loss 6.801266 rs_loss 0.101756 vs_loss 0.673831 logits_loss 120.513572\n",
      "[24:4000] F: 14784000 \t tot_loss 6.871884 rs_loss 0.125848 vs_loss 0.714591 logits_loss 120.628893\n",
      "[24:4100] F: 14793600 \t tot_loss 6.837492 rs_loss 0.114341 vs_loss 0.692189 logits_loss 120.619240\n",
      "[24:4200] F: 14803200 \t tot_loss 6.738712 rs_loss 0.096654 vs_loss 0.631611 logits_loss 120.208931\n",
      "[24:4300] F: 14812800 \t tot_loss 6.645045 rs_loss 0.080005 vs_loss 0.549616 logits_loss 120.308474\n",
      "[24:4400] F: 14822400 \t tot_loss 6.572750 rs_loss 0.053617 vs_loss 0.513629 logits_loss 120.110072\n",
      "[24:4500] F: 14832000 \t tot_loss 6.633013 rs_loss 0.088600 vs_loss 0.542149 logits_loss 120.045277\n",
      "[24:4600] F: 14841600 \t tot_loss 6.626294 rs_loss 0.080007 vs_loss 0.546191 logits_loss 120.001923\n",
      "[24:4700] F: 14851200 \t tot_loss 6.757105 rs_loss 0.135714 vs_loss 0.621933 logits_loss 119.989142\n",
      "[24:4800] F: 14860800 \t tot_loss 6.933351 rs_loss 0.181570 vs_loss 0.747642 logits_loss 120.082777\n",
      "[24:4900] F: 14870400 \t tot_loss 6.931905 rs_loss 0.173351 vs_loss 0.748275 logits_loss 120.205591\n",
      "[24:5000] F: 14880000 \t tot_loss 7.063961 rs_loss 0.209199 vs_loss 0.840856 logits_loss 120.278131\n",
      "[24:5100] F: 14889600 \t tot_loss 7.017982 rs_loss 0.176761 vs_loss 0.828177 logits_loss 120.260862\n",
      "[24:5200] F: 14899200 \t tot_loss 6.822895 rs_loss 0.126270 vs_loss 0.683255 logits_loss 120.267398\n",
      "[24:5300] F: 14908800 \t tot_loss 6.809268 rs_loss 0.124547 vs_loss 0.666316 logits_loss 120.368102\n",
      "[24:5400] F: 14918400 \t tot_loss 6.781088 rs_loss 0.117410 vs_loss 0.645614 logits_loss 120.361271\n",
      "[24:5500] F: 14928000 \t tot_loss 6.681859 rs_loss 0.088049 vs_loss 0.587187 logits_loss 120.132475\n",
      "[24:5600] F: 14937600 \t tot_loss 6.705376 rs_loss 0.087895 vs_loss 0.605063 logits_loss 120.248355\n",
      "[24:5700] F: 14947200 \t tot_loss 6.811510 rs_loss 0.115843 vs_loss 0.686312 logits_loss 120.187102\n",
      "[24:5800] F: 14956800 \t tot_loss 6.811293 rs_loss 0.115965 vs_loss 0.690083 logits_loss 120.104902\n",
      "[24:5900] F: 14966400 \t tot_loss 6.942196 rs_loss 0.170504 vs_loss 0.752654 logits_loss 120.380774\n",
      "[24:6000] F: 14976000 \t tot_loss 7.174242 rs_loss 0.255750 vs_loss 0.907830 logits_loss 120.213228\n",
      "[24:6100] F: 14985600 \t tot_loss 7.097988 rs_loss 0.230558 vs_loss 0.848932 logits_loss 120.369941\n",
      "[24:6200] F: 14995200 \t tot_loss 7.020071 rs_loss 0.215198 vs_loss 0.774934 logits_loss 120.598796\n",
      "Batch [25] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.28 (+-0.21) \n",
      "[25:0] F: 15000000 \t tot_loss 7.045280 rs_loss 0.222848 vs_loss 0.786070 logits_loss 120.727245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25:100] F: 15009600 \t tot_loss 6.842690 rs_loss 0.132424 vs_loss 0.662392 logits_loss 120.957481\n",
      "[25:200] F: 15019200 \t tot_loss 7.145094 rs_loss 0.199356 vs_loss 0.891294 logits_loss 121.088895\n",
      "[25:300] F: 15028800 \t tot_loss 7.218431 rs_loss 0.216789 vs_loss 0.963555 logits_loss 120.761741\n",
      "[25:400] F: 15038400 \t tot_loss 7.224210 rs_loss 0.203712 vs_loss 0.986539 logits_loss 120.679181\n",
      "[25:500] F: 15048000 \t tot_loss 7.153061 rs_loss 0.190507 vs_loss 0.939647 logits_loss 120.458144\n",
      "[25:600] F: 15057600 \t tot_loss 6.831779 rs_loss 0.102709 vs_loss 0.705357 logits_loss 120.474265\n",
      "[25:700] F: 15067200 \t tot_loss 6.761224 rs_loss 0.082201 vs_loss 0.652954 logits_loss 120.521388\n",
      "[25:800] F: 15076800 \t tot_loss 6.830109 rs_loss 0.114945 vs_loss 0.683338 logits_loss 120.636526\n",
      "[25:900] F: 15086400 \t tot_loss 6.885533 rs_loss 0.132896 vs_loss 0.718579 logits_loss 120.681153\n",
      "[25:1000] F: 15096000 \t tot_loss 6.899087 rs_loss 0.135937 vs_loss 0.743457 logits_loss 120.393846\n",
      "[25:1100] F: 15105600 \t tot_loss 6.976973 rs_loss 0.160213 vs_loss 0.795786 logits_loss 120.419472\n",
      "[25:1200] F: 15115200 \t tot_loss 6.972590 rs_loss 0.154896 vs_loss 0.804642 logits_loss 120.261034\n",
      "[25:1300] F: 15124800 \t tot_loss 7.156342 rs_loss 0.213915 vs_loss 0.931635 logits_loss 120.215841\n",
      "[25:1400] F: 15134400 \t tot_loss 7.017109 rs_loss 0.173509 vs_loss 0.819460 logits_loss 120.482794\n",
      "[25:1500] F: 15144000 \t tot_loss 6.940446 rs_loss 0.151603 vs_loss 0.761757 logits_loss 120.541722\n",
      "[25:1600] F: 15153600 \t tot_loss 6.971737 rs_loss 0.178289 vs_loss 0.762056 logits_loss 120.627830\n",
      "[25:1700] F: 15163200 \t tot_loss 6.935953 rs_loss 0.155124 vs_loss 0.750426 logits_loss 120.608050\n",
      "[25:1800] F: 15172800 \t tot_loss 7.001552 rs_loss 0.193602 vs_loss 0.791641 logits_loss 120.326179\n",
      "[25:1900] F: 15182400 \t tot_loss 7.116413 rs_loss 0.238243 vs_loss 0.873627 logits_loss 120.090865\n",
      "[25:2000] F: 15192000 \t tot_loss 7.085021 rs_loss 0.208343 vs_loss 0.882129 logits_loss 119.890965\n",
      "[25:2100] F: 15201600 \t tot_loss 6.919870 rs_loss 0.166641 vs_loss 0.758457 logits_loss 119.895434\n",
      "[25:2200] F: 15211200 \t tot_loss 7.042038 rs_loss 0.176156 vs_loss 0.858731 logits_loss 120.143033\n",
      "[25:2300] F: 15220800 \t tot_loss 7.026410 rs_loss 0.179216 vs_loss 0.835181 logits_loss 120.240264\n",
      "[25:2400] F: 15230400 \t tot_loss 7.109307 rs_loss 0.209805 vs_loss 0.877551 logits_loss 120.439028\n",
      "[25:2500] F: 15240000 \t tot_loss 7.263122 rs_loss 0.251170 vs_loss 0.981984 logits_loss 120.599343\n",
      "[25:2600] F: 15249600 \t tot_loss 7.166147 rs_loss 0.239181 vs_loss 0.904668 logits_loss 120.445965\n",
      "[25:2700] F: 15259200 \t tot_loss 7.100953 rs_loss 0.214736 vs_loss 0.862241 logits_loss 120.479522\n",
      "[25:2800] F: 15268800 \t tot_loss 7.059462 rs_loss 0.206914 vs_loss 0.821873 logits_loss 120.613508\n",
      "[25:2900] F: 15278400 \t tot_loss 6.857740 rs_loss 0.161499 vs_loss 0.678811 logits_loss 120.348606\n",
      "[25:3000] F: 15288000 \t tot_loss 6.851651 rs_loss 0.168691 vs_loss 0.668314 logits_loss 120.292930\n",
      "[25:3100] F: 15297600 \t tot_loss 6.897596 rs_loss 0.167442 vs_loss 0.734712 logits_loss 119.908819\n",
      "[25:3200] F: 15307200 \t tot_loss 6.779004 rs_loss 0.124150 vs_loss 0.678114 logits_loss 119.534788\n",
      "[25:3300] F: 15316800 \t tot_loss 6.846961 rs_loss 0.138460 vs_loss 0.722959 logits_loss 119.710843\n",
      "[25:3400] F: 15326400 \t tot_loss 6.861845 rs_loss 0.131894 vs_loss 0.755338 logits_loss 119.492273\n",
      "[25:3500] F: 15336000 \t tot_loss 6.901070 rs_loss 0.138400 vs_loss 0.768634 logits_loss 119.880722\n",
      "[25:3600] F: 15345600 \t tot_loss 7.251002 rs_loss 0.233619 vs_loss 1.026363 logits_loss 119.820404\n",
      "[25:3700] F: 15355200 \t tot_loss 7.374953 rs_loss 0.266642 vs_loss 1.122013 logits_loss 119.725960\n",
      "[25:3800] F: 15364800 \t tot_loss 7.397432 rs_loss 0.268847 vs_loss 1.121491 logits_loss 120.141853\n",
      "[25:3900] F: 15374400 \t tot_loss 7.252034 rs_loss 0.238914 vs_loss 1.023669 logits_loss 119.789025\n",
      "[25:4000] F: 15384000 \t tot_loss 7.114099 rs_loss 0.214261 vs_loss 0.899009 logits_loss 120.016587\n",
      "[25:4100] F: 15393600 \t tot_loss 6.943864 rs_loss 0.164460 vs_loss 0.784156 logits_loss 119.904961\n",
      "[25:4200] F: 15403200 \t tot_loss 6.848200 rs_loss 0.143977 vs_loss 0.710403 logits_loss 119.876382\n",
      "[25:4300] F: 15412800 \t tot_loss 6.895652 rs_loss 0.165086 vs_loss 0.731249 logits_loss 119.986347\n",
      "[25:4400] F: 15422400 \t tot_loss 6.999123 rs_loss 0.186184 vs_loss 0.810095 logits_loss 120.056885\n",
      "[25:4500] F: 15432000 \t tot_loss 7.072786 rs_loss 0.217157 vs_loss 0.865768 logits_loss 119.797208\n",
      "[25:4600] F: 15441600 \t tot_loss 7.180315 rs_loss 0.272444 vs_loss 0.926551 logits_loss 119.626415\n",
      "[25:4700] F: 15451200 \t tot_loss 7.193373 rs_loss 0.270061 vs_loss 0.932411 logits_loss 119.818022\n",
      "[25:4800] F: 15460800 \t tot_loss 6.899012 rs_loss 0.184187 vs_loss 0.728196 logits_loss 119.732586\n",
      "[25:4900] F: 15470400 \t tot_loss 6.859501 rs_loss 0.161925 vs_loss 0.694011 logits_loss 120.071303\n",
      "[25:5000] F: 15480000 \t tot_loss 6.759516 rs_loss 0.108407 vs_loss 0.641983 logits_loss 120.182516\n",
      "[25:5100] F: 15489600 \t tot_loss 6.779616 rs_loss 0.104891 vs_loss 0.676306 logits_loss 119.968363\n",
      "[25:5200] F: 15499200 \t tot_loss 7.014299 rs_loss 0.167251 vs_loss 0.865669 logits_loss 119.627580\n",
      "[25:5300] F: 15508800 \t tot_loss 6.969647 rs_loss 0.160983 vs_loss 0.837424 logits_loss 119.424780\n",
      "[25:5400] F: 15518400 \t tot_loss 6.979707 rs_loss 0.158365 vs_loss 0.861099 logits_loss 119.204841\n",
      "[25:5500] F: 15528000 \t tot_loss 7.049404 rs_loss 0.187755 vs_loss 0.889034 logits_loss 119.452307\n",
      "[25:5600] F: 15537600 \t tot_loss 6.934446 rs_loss 0.169390 vs_loss 0.784984 logits_loss 119.601434\n",
      "[25:5700] F: 15547200 \t tot_loss 7.060153 rs_loss 0.198950 vs_loss 0.870047 logits_loss 119.823118\n",
      "[25:5800] F: 15556800 \t tot_loss 7.064377 rs_loss 0.199180 vs_loss 0.875967 logits_loss 119.784600\n",
      "[25:5900] F: 15566400 \t tot_loss 7.378208 rs_loss 0.274945 vs_loss 1.118075 logits_loss 119.703760\n",
      "[25:6000] F: 15576000 \t tot_loss 7.372805 rs_loss 0.271555 vs_loss 1.095211 logits_loss 120.120789\n",
      "[25:6100] F: 15585600 \t tot_loss 7.347593 rs_loss 0.271051 vs_loss 1.080746 logits_loss 119.915900\n",
      "[25:6200] F: 15595200 \t tot_loss 7.246846 rs_loss 0.242160 vs_loss 0.999041 logits_loss 120.112881\n",
      "Batch [26] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.10 (+-0.08) \n",
      "[26:0] F: 15600000 \t tot_loss 7.150373 rs_loss 0.214703 vs_loss 0.922913 logits_loss 120.255148\n",
      "[26:100] F: 15609600 \t tot_loss 7.046042 rs_loss 0.192403 vs_loss 0.842025 logits_loss 120.232283\n",
      "[26:200] F: 15619200 \t tot_loss 7.400077 rs_loss 0.301406 vs_loss 1.073174 logits_loss 120.509948\n",
      "[26:300] F: 15628800 \t tot_loss 7.330759 rs_loss 0.275044 vs_loss 1.008420 logits_loss 120.945900\n",
      "[26:400] F: 15638400 \t tot_loss 7.356307 rs_loss 0.283296 vs_loss 1.026601 logits_loss 120.928197\n",
      "[26:500] F: 15648000 \t tot_loss 7.432709 rs_loss 0.311205 vs_loss 1.063283 logits_loss 121.164431\n",
      "[26:600] F: 15657600 \t tot_loss 7.046882 rs_loss 0.177231 vs_loss 0.809422 logits_loss 121.204570\n",
      "[26:700] F: 15667200 \t tot_loss 7.350609 rs_loss 0.259860 vs_loss 1.028177 logits_loss 121.251425\n",
      "[26:800] F: 15676800 \t tot_loss 7.513134 rs_loss 0.277734 vs_loss 1.155107 logits_loss 121.605864\n",
      "[26:900] F: 15686400 \t tot_loss 7.291318 rs_loss 0.210015 vs_loss 1.007290 logits_loss 121.480257\n",
      "[26:1000] F: 15696000 \t tot_loss 7.334173 rs_loss 0.236863 vs_loss 1.029517 logits_loss 121.355878\n",
      "[26:1100] F: 15705600 \t tot_loss 7.256326 rs_loss 0.221785 vs_loss 0.980053 logits_loss 121.089768\n",
      "[26:1200] F: 15715200 \t tot_loss 7.107147 rs_loss 0.203847 vs_loss 0.860566 logits_loss 120.854692\n",
      "[26:1300] F: 15724800 \t tot_loss 7.090319 rs_loss 0.204370 vs_loss 0.850511 logits_loss 120.708750\n",
      "[26:1400] F: 15734400 \t tot_loss 7.115448 rs_loss 0.228597 vs_loss 0.843561 logits_loss 120.865798\n",
      "[26:1500] F: 15744000 \t tot_loss 6.932535 rs_loss 0.186977 vs_loss 0.694678 logits_loss 121.017600\n",
      "[26:1600] F: 15753600 \t tot_loss 6.908689 rs_loss 0.181559 vs_loss 0.680616 logits_loss 120.930275\n",
      "[26:1700] F: 15763200 \t tot_loss 6.786554 rs_loss 0.127957 vs_loss 0.610129 logits_loss 120.969352\n",
      "[26:1800] F: 15772800 \t tot_loss 6.820178 rs_loss 0.112441 vs_loss 0.675191 logits_loss 120.650912\n",
      "[26:1900] F: 15782400 \t tot_loss 6.792743 rs_loss 0.098808 vs_loss 0.672199 logits_loss 120.434730\n",
      "[26:2000] F: 15792000 \t tot_loss 6.793219 rs_loss 0.097636 vs_loss 0.673893 logits_loss 120.433816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26:2100] F: 15801600 \t tot_loss 6.933920 rs_loss 0.142190 vs_loss 0.765549 logits_loss 120.523604\n",
      "[26:2200] F: 15811200 \t tot_loss 6.854238 rs_loss 0.129146 vs_loss 0.686518 logits_loss 120.771462\n",
      "[26:2300] F: 15820800 \t tot_loss 6.916958 rs_loss 0.137962 vs_loss 0.735691 logits_loss 120.866113\n",
      "[26:2400] F: 15830400 \t tot_loss 7.118940 rs_loss 0.198253 vs_loss 0.886517 logits_loss 120.683396\n",
      "[26:2500] F: 15840000 \t tot_loss 7.051610 rs_loss 0.185776 vs_loss 0.842997 logits_loss 120.456748\n",
      "[26:2600] F: 15849600 \t tot_loss 7.082066 rs_loss 0.205279 vs_loss 0.860130 logits_loss 120.333143\n",
      "[26:2700] F: 15859200 \t tot_loss 7.082705 rs_loss 0.219331 vs_loss 0.852997 logits_loss 120.207541\n",
      "[26:2800] F: 15868800 \t tot_loss 6.985431 rs_loss 0.190556 vs_loss 0.765135 logits_loss 120.594790\n",
      "[26:2900] F: 15878400 \t tot_loss 7.019314 rs_loss 0.186465 vs_loss 0.801109 logits_loss 120.634816\n",
      "[26:3000] F: 15888000 \t tot_loss 7.006009 rs_loss 0.163242 vs_loss 0.812306 logits_loss 120.609224\n",
      "[26:3100] F: 15897600 \t tot_loss 6.898258 rs_loss 0.125234 vs_loss 0.748234 logits_loss 120.495796\n",
      "[26:3200] F: 15907200 \t tot_loss 6.787622 rs_loss 0.094618 vs_loss 0.680787 logits_loss 120.244323\n",
      "[26:3300] F: 15916800 \t tot_loss 6.809753 rs_loss 0.098684 vs_loss 0.704562 logits_loss 120.130128\n",
      "[26:3400] F: 15926400 \t tot_loss 6.719748 rs_loss 0.075695 vs_loss 0.633625 logits_loss 120.208558\n",
      "[26:3500] F: 15936000 \t tot_loss 6.825257 rs_loss 0.104571 vs_loss 0.708054 logits_loss 120.252628\n",
      "[26:3600] F: 15945600 \t tot_loss 6.922150 rs_loss 0.134460 vs_loss 0.777280 logits_loss 120.208219\n",
      "[26:3700] F: 15955200 \t tot_loss 6.958849 rs_loss 0.150644 vs_loss 0.800808 logits_loss 120.147941\n",
      "[26:3800] F: 15964800 \t tot_loss 6.952401 rs_loss 0.149096 vs_loss 0.803586 logits_loss 119.994383\n",
      "[26:3900] F: 15974400 \t tot_loss 6.979480 rs_loss 0.161988 vs_loss 0.806230 logits_loss 120.225235\n",
      "[26:4000] F: 15984000 \t tot_loss 6.999653 rs_loss 0.167759 vs_loss 0.822609 logits_loss 120.185697\n",
      "[26:4100] F: 15993600 \t tot_loss 6.803007 rs_loss 0.113760 vs_loss 0.683151 logits_loss 120.121925\n",
      "[26:4200] F: 16003200 \t tot_loss 6.908344 rs_loss 0.164982 vs_loss 0.751107 logits_loss 119.845108\n",
      "[26:4300] F: 16012800 \t tot_loss 6.908663 rs_loss 0.170222 vs_loss 0.768932 logits_loss 119.390186\n",
      "[26:4400] F: 16022400 \t tot_loss 6.791737 rs_loss 0.138676 vs_loss 0.677574 logits_loss 119.509739\n",
      "[26:4500] F: 16032000 \t tot_loss 6.856046 rs_loss 0.168085 vs_loss 0.706077 logits_loss 119.637662\n",
      "[26:4600] F: 16041600 \t tot_loss 6.793275 rs_loss 0.133411 vs_loss 0.668234 logits_loss 119.832596\n",
      "[26:4700] F: 16051200 \t tot_loss 6.653887 rs_loss 0.085500 vs_loss 0.560389 logits_loss 120.159974\n",
      "[26:4800] F: 16060800 \t tot_loss 6.721178 rs_loss 0.096406 vs_loss 0.627044 logits_loss 119.954546\n",
      "[26:4900] F: 16070400 \t tot_loss 6.845824 rs_loss 0.104956 vs_loss 0.729082 logits_loss 120.235722\n",
      "[26:5000] F: 16080000 \t tot_loss 6.978781 rs_loss 0.145008 vs_loss 0.828930 logits_loss 120.096846\n",
      "[26:5100] F: 16089600 \t tot_loss 7.039265 rs_loss 0.168183 vs_loss 0.873469 logits_loss 119.952254\n",
      "[26:5200] F: 16099200 \t tot_loss 7.064746 rs_loss 0.182208 vs_loss 0.868527 logits_loss 120.280204\n",
      "[26:5300] F: 16108800 \t tot_loss 7.112428 rs_loss 0.193764 vs_loss 0.911314 logits_loss 120.146999\n",
      "[26:5400] F: 16118400 \t tot_loss 7.043857 rs_loss 0.195701 vs_loss 0.828164 logits_loss 120.399839\n",
      "[26:5500] F: 16128000 \t tot_loss 7.019729 rs_loss 0.195636 vs_loss 0.802309 logits_loss 120.435687\n",
      "[26:5600] F: 16137600 \t tot_loss 6.983349 rs_loss 0.195556 vs_loss 0.775438 logits_loss 120.247090\n",
      "[26:5700] F: 16147200 \t tot_loss 6.780268 rs_loss 0.157517 vs_loss 0.610535 logits_loss 120.244310\n",
      "[26:5800] F: 16156800 \t tot_loss 6.740219 rs_loss 0.126822 vs_loss 0.614179 logits_loss 119.984360\n",
      "[26:5900] F: 16166400 \t tot_loss 6.776933 rs_loss 0.106668 vs_loss 0.668471 logits_loss 120.035903\n",
      "[26:6000] F: 16176000 \t tot_loss 6.811626 rs_loss 0.096675 vs_loss 0.702433 logits_loss 120.250342\n",
      "[26:6100] F: 16185600 \t tot_loss 6.880869 rs_loss 0.105824 vs_loss 0.766308 logits_loss 120.174730\n",
      "[26:6200] F: 16195200 \t tot_loss 6.928603 rs_loss 0.120133 vs_loss 0.779688 logits_loss 120.575646\n",
      "Batch [27] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.24 (+-0.21) \n",
      "[27:0] F: 16200000 \t tot_loss 6.917158 rs_loss 0.115224 vs_loss 0.773472 logits_loss 120.569221\n",
      "[27:100] F: 16209600 \t tot_loss 6.877545 rs_loss 0.114190 vs_loss 0.724107 logits_loss 120.784957\n",
      "[27:200] F: 16219200 \t tot_loss 6.876489 rs_loss 0.127275 vs_loss 0.703708 logits_loss 120.910114\n",
      "[27:300] F: 16228800 \t tot_loss 6.877107 rs_loss 0.124380 vs_loss 0.703828 logits_loss 120.977971\n",
      "[27:400] F: 16238400 \t tot_loss 6.994763 rs_loss 0.147448 vs_loss 0.791065 logits_loss 121.124997\n",
      "[27:500] F: 16248000 \t tot_loss 7.411442 rs_loss 0.278335 vs_loss 1.073512 logits_loss 121.191906\n",
      "[27:600] F: 16257600 \t tot_loss 7.442460 rs_loss 0.273341 vs_loss 1.109486 logits_loss 121.192663\n",
      "[27:700] F: 16267200 \t tot_loss 7.472490 rs_loss 0.271186 vs_loss 1.130797 logits_loss 121.410132\n",
      "[27:800] F: 16276800 \t tot_loss 7.381233 rs_loss 0.252966 vs_loss 1.051441 logits_loss 121.536530\n",
      "[27:900] F: 16286400 \t tot_loss 6.972638 rs_loss 0.129330 vs_loss 0.776936 logits_loss 121.327422\n",
      "[27:1000] F: 16296000 \t tot_loss 6.880696 rs_loss 0.107284 vs_loss 0.711359 logits_loss 121.241065\n",
      "[27:1100] F: 16305600 \t tot_loss 6.793961 rs_loss 0.102249 vs_loss 0.647584 logits_loss 120.882574\n",
      "[27:1200] F: 16315200 \t tot_loss 6.777837 rs_loss 0.095764 vs_loss 0.654612 logits_loss 120.549224\n",
      "[27:1300] F: 16324800 \t tot_loss 6.839984 rs_loss 0.110527 vs_loss 0.710608 logits_loss 120.376988\n",
      "[27:1400] F: 16334400 \t tot_loss 6.953779 rs_loss 0.154987 vs_loss 0.779338 logits_loss 120.389083\n",
      "[27:1500] F: 16344000 \t tot_loss 6.915417 rs_loss 0.135784 vs_loss 0.748092 logits_loss 120.630817\n",
      "[27:1600] F: 16353600 \t tot_loss 7.027711 rs_loss 0.159646 vs_loss 0.827906 logits_loss 120.803184\n",
      "[27:1700] F: 16363200 \t tot_loss 7.093717 rs_loss 0.194560 vs_loss 0.852785 logits_loss 120.927424\n",
      "[27:1800] F: 16372800 \t tot_loss 7.025257 rs_loss 0.173086 vs_loss 0.807935 logits_loss 120.884729\n",
      "[27:1900] F: 16382400 \t tot_loss 7.142356 rs_loss 0.215700 vs_loss 0.888975 logits_loss 120.753604\n",
      "[27:2000] F: 16392000 \t tot_loss 6.991501 rs_loss 0.190305 vs_loss 0.775882 logits_loss 120.506281\n",
      "[27:2100] F: 16401600 \t tot_loss 7.020288 rs_loss 0.179563 vs_loss 0.822283 logits_loss 120.368829\n",
      "[27:2200] F: 16411200 \t tot_loss 7.133408 rs_loss 0.208399 vs_loss 0.922943 logits_loss 120.041307\n",
      "[27:2300] F: 16420800 \t tot_loss 7.270504 rs_loss 0.239555 vs_loss 1.021165 logits_loss 120.195681\n",
      "[27:2400] F: 16430400 \t tot_loss 7.364890 rs_loss 0.260504 vs_loss 1.090775 logits_loss 120.272226\n",
      "[27:2500] F: 16440000 \t tot_loss 7.247791 rs_loss 0.236689 vs_loss 1.006304 logits_loss 120.095967\n",
      "[27:2600] F: 16449600 \t tot_loss 7.245272 rs_loss 0.236792 vs_loss 0.992103 logits_loss 120.327532\n",
      "[27:2700] F: 16459200 \t tot_loss 6.966455 rs_loss 0.156828 vs_loss 0.802437 logits_loss 120.143809\n",
      "[27:2800] F: 16468800 \t tot_loss 7.064912 rs_loss 0.180095 vs_loss 0.869498 logits_loss 120.306390\n",
      "[27:2900] F: 16478400 \t tot_loss 7.058979 rs_loss 0.178966 vs_loss 0.852287 logits_loss 120.554536\n",
      "[27:3000] F: 16488000 \t tot_loss 6.968065 rs_loss 0.171097 vs_loss 0.773657 logits_loss 120.466226\n",
      "[27:3100] F: 16497600 \t tot_loss 6.985489 rs_loss 0.175825 vs_loss 0.776404 logits_loss 120.665183\n",
      "[27:3200] F: 16507200 \t tot_loss 6.875743 rs_loss 0.155340 vs_loss 0.691215 logits_loss 120.583749\n",
      "[27:3300] F: 16516800 \t tot_loss 6.924575 rs_loss 0.165578 vs_loss 0.726244 logits_loss 120.655073\n",
      "[27:3400] F: 16526400 \t tot_loss 6.848704 rs_loss 0.127120 vs_loss 0.686347 logits_loss 120.704740\n",
      "[27:3500] F: 16536000 \t tot_loss 6.891613 rs_loss 0.147388 vs_loss 0.723986 logits_loss 120.404782\n",
      "[27:3600] F: 16545600 \t tot_loss 6.879735 rs_loss 0.149283 vs_loss 0.708790 logits_loss 120.433226\n",
      "[27:3700] F: 16555200 \t tot_loss 6.778048 rs_loss 0.122000 vs_loss 0.635136 logits_loss 120.418234\n",
      "[27:3800] F: 16564800 \t tot_loss 6.956782 rs_loss 0.172702 vs_loss 0.761788 logits_loss 120.445835\n",
      "[27:3900] F: 16574400 \t tot_loss 6.977847 rs_loss 0.166311 vs_loss 0.783691 logits_loss 120.556900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27:4000] F: 16584000 \t tot_loss 6.852184 rs_loss 0.118416 vs_loss 0.717897 logits_loss 120.317422\n",
      "[27:4100] F: 16593600 \t tot_loss 7.019690 rs_loss 0.165722 vs_loss 0.841626 logits_loss 120.246836\n",
      "[27:4200] F: 16603200 \t tot_loss 7.011614 rs_loss 0.180374 vs_loss 0.819845 logits_loss 120.227887\n",
      "[27:4300] F: 16612800 \t tot_loss 7.041519 rs_loss 0.190360 vs_loss 0.844196 logits_loss 120.139254\n",
      "[27:4400] F: 16622400 \t tot_loss 7.192905 rs_loss 0.230359 vs_loss 0.955554 logits_loss 120.139836\n",
      "[27:4500] F: 16632000 \t tot_loss 7.177079 rs_loss 0.224807 vs_loss 0.943272 logits_loss 120.179977\n",
      "[27:4600] F: 16641600 \t tot_loss 7.054483 rs_loss 0.169005 vs_loss 0.883212 logits_loss 120.045324\n",
      "[27:4700] F: 16651200 \t tot_loss 6.939252 rs_loss 0.144709 vs_loss 0.794012 logits_loss 120.010626\n",
      "[27:4800] F: 16660800 \t tot_loss 6.828548 rs_loss 0.132248 vs_loss 0.696472 logits_loss 119.996554\n",
      "[27:4900] F: 16670400 \t tot_loss 6.668841 rs_loss 0.087394 vs_loss 0.591632 logits_loss 119.796284\n",
      "[27:5000] F: 16680000 \t tot_loss 6.598588 rs_loss 0.072395 vs_loss 0.545970 logits_loss 119.604482\n",
      "[27:5100] F: 16689600 \t tot_loss 6.608488 rs_loss 0.072295 vs_loss 0.556334 logits_loss 119.597171\n",
      "[27:5200] F: 16699200 \t tot_loss 6.612219 rs_loss 0.068099 vs_loss 0.566864 logits_loss 119.545106\n",
      "[27:5300] F: 16708800 \t tot_loss 6.754673 rs_loss 0.129187 vs_loss 0.638265 logits_loss 119.744412\n",
      "[27:5400] F: 16718400 \t tot_loss 6.764929 rs_loss 0.124418 vs_loss 0.638438 logits_loss 120.041458\n",
      "[27:5500] F: 16728000 \t tot_loss 6.898331 rs_loss 0.165477 vs_loss 0.726514 logits_loss 120.126807\n",
      "[27:5600] F: 16737600 \t tot_loss 6.946198 rs_loss 0.160457 vs_loss 0.768718 logits_loss 120.340468\n",
      "[27:5700] F: 16747200 \t tot_loss 6.870988 rs_loss 0.121057 vs_loss 0.730570 logits_loss 120.387207\n",
      "[27:5800] F: 16756800 \t tot_loss 6.871142 rs_loss 0.128411 vs_loss 0.728542 logits_loss 120.283785\n",
      "[27:5900] F: 16766400 \t tot_loss 6.787627 rs_loss 0.096974 vs_loss 0.666657 logits_loss 120.479923\n",
      "[27:6000] F: 16776000 \t tot_loss 6.755854 rs_loss 0.103703 vs_loss 0.629463 logits_loss 120.453756\n",
      "[27:6100] F: 16785600 \t tot_loss 6.673084 rs_loss 0.079081 vs_loss 0.566243 logits_loss 120.555195\n",
      "[27:6200] F: 16795200 \t tot_loss 6.757113 rs_loss 0.122774 vs_loss 0.605123 logits_loss 120.584331\n",
      "Batch [28] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.19 (+-0.24) \n",
      "[28:0] F: 16800000 \t tot_loss 6.766059 rs_loss 0.132831 vs_loss 0.598241 logits_loss 120.699736\n",
      "[28:100] F: 16809600 \t tot_loss 6.806241 rs_loss 0.136545 vs_loss 0.625469 logits_loss 120.884530\n",
      "[28:200] F: 16819200 \t tot_loss 6.831963 rs_loss 0.120930 vs_loss 0.656719 logits_loss 121.086275\n",
      "[28:300] F: 16828800 \t tot_loss 6.872025 rs_loss 0.112745 vs_loss 0.689792 logits_loss 121.389768\n",
      "[28:400] F: 16838400 \t tot_loss 6.878934 rs_loss 0.097311 vs_loss 0.708004 logits_loss 121.472369\n",
      "[28:500] F: 16848000 \t tot_loss 6.829216 rs_loss 0.088421 vs_loss 0.682815 logits_loss 121.159595\n",
      "[28:600] F: 16857600 \t tot_loss 6.874042 rs_loss 0.106511 vs_loss 0.728870 logits_loss 120.773208\n",
      "[28:700] F: 16867200 \t tot_loss 6.829132 rs_loss 0.098460 vs_loss 0.711119 logits_loss 120.391056\n",
      "[28:800] F: 16876800 \t tot_loss 6.834487 rs_loss 0.093550 vs_loss 0.725177 logits_loss 120.315206\n",
      "[28:900] F: 16886400 \t tot_loss 6.832516 rs_loss 0.094443 vs_loss 0.732394 logits_loss 120.113593\n",
      "[28:1000] F: 16896000 \t tot_loss 6.765075 rs_loss 0.071990 vs_loss 0.658934 logits_loss 120.683013\n",
      "[28:1100] F: 16905600 \t tot_loss 6.917829 rs_loss 0.113165 vs_loss 0.754739 logits_loss 120.998493\n",
      "[28:1200] F: 16915200 \t tot_loss 6.920813 rs_loss 0.117349 vs_loss 0.765281 logits_loss 120.763656\n",
      "[28:1300] F: 16924800 \t tot_loss 6.910539 rs_loss 0.118143 vs_loss 0.742586 logits_loss 120.996190\n",
      "[28:1400] F: 16934400 \t tot_loss 6.897672 rs_loss 0.119295 vs_loss 0.748426 logits_loss 120.599027\n",
      "[28:1500] F: 16944000 \t tot_loss 6.816084 rs_loss 0.094920 vs_loss 0.690936 logits_loss 120.604554\n",
      "[28:1600] F: 16953600 \t tot_loss 6.736436 rs_loss 0.066127 vs_loss 0.641284 logits_loss 120.580489\n",
      "[28:1700] F: 16963200 \t tot_loss 6.919729 rs_loss 0.140777 vs_loss 0.759159 logits_loss 120.395854\n",
      "[28:1800] F: 16972800 \t tot_loss 7.068652 rs_loss 0.186066 vs_loss 0.859631 logits_loss 120.459122\n",
      "[28:1900] F: 16982400 \t tot_loss 7.047867 rs_loss 0.189079 vs_loss 0.849258 logits_loss 120.190596\n",
      "[28:2000] F: 16992000 \t tot_loss 7.054375 rs_loss 0.193061 vs_loss 0.841439 logits_loss 120.397483\n",
      "[28:2100] F: 17001600 \t tot_loss 6.858767 rs_loss 0.114130 vs_loss 0.721795 logits_loss 120.456843\n",
      "[28:2200] F: 17011200 \t tot_loss 6.688394 rs_loss 0.074034 vs_loss 0.605122 logits_loss 120.184765\n",
      "[28:2300] F: 17020800 \t tot_loss 6.603898 rs_loss 0.045077 vs_loss 0.540276 logits_loss 120.370914\n",
      "[28:2400] F: 17030400 \t tot_loss 6.593485 rs_loss 0.049640 vs_loss 0.510273 logits_loss 120.671461\n",
      "[28:2500] F: 17040000 \t tot_loss 6.588318 rs_loss 0.049917 vs_loss 0.500473 logits_loss 120.758557\n",
      "[28:2600] F: 17049600 \t tot_loss 6.638715 rs_loss 0.063532 vs_loss 0.525122 logits_loss 121.001211\n",
      "[28:2700] F: 17059200 \t tot_loss 6.699139 rs_loss 0.093129 vs_loss 0.568686 logits_loss 120.746460\n",
      "[28:2800] F: 17068800 \t tot_loss 6.706330 rs_loss 0.092410 vs_loss 0.593932 logits_loss 120.399760\n",
      "[28:2900] F: 17078400 \t tot_loss 6.725102 rs_loss 0.100946 vs_loss 0.611230 logits_loss 120.258512\n",
      "[28:3000] F: 17088000 \t tot_loss 6.712640 rs_loss 0.084653 vs_loss 0.623446 logits_loss 120.090813\n",
      "[28:3100] F: 17097600 \t tot_loss 6.660106 rs_loss 0.061798 vs_loss 0.591978 logits_loss 120.126602\n",
      "[28:3200] F: 17107200 \t tot_loss 6.638905 rs_loss 0.057138 vs_loss 0.567858 logits_loss 120.278174\n",
      "[28:3300] F: 17116800 \t tot_loss 6.715319 rs_loss 0.078314 vs_loss 0.623155 logits_loss 120.276987\n",
      "[28:3400] F: 17126400 \t tot_loss 6.717083 rs_loss 0.099926 vs_loss 0.600628 logits_loss 120.330565\n",
      "[28:3500] F: 17136000 \t tot_loss 6.779483 rs_loss 0.122275 vs_loss 0.631306 logits_loss 120.518027\n",
      "[28:3600] F: 17145600 \t tot_loss 6.820715 rs_loss 0.138447 vs_loss 0.665193 logits_loss 120.341505\n",
      "[28:3700] F: 17155200 \t tot_loss 6.726697 rs_loss 0.113498 vs_loss 0.595801 logits_loss 120.347965\n",
      "[28:3800] F: 17164800 \t tot_loss 6.688156 rs_loss 0.083471 vs_loss 0.580929 logits_loss 120.475115\n",
      "[28:3900] F: 17174400 \t tot_loss 6.665042 rs_loss 0.073855 vs_loss 0.566414 logits_loss 120.495467\n",
      "[28:4000] F: 17184000 \t tot_loss 6.757148 rs_loss 0.120056 vs_loss 0.610981 logits_loss 120.522232\n",
      "[28:4100] F: 17193600 \t tot_loss 6.840323 rs_loss 0.151363 vs_loss 0.665549 logits_loss 120.468210\n",
      "[28:4200] F: 17203200 \t tot_loss 7.011029 rs_loss 0.222966 vs_loss 0.767616 logits_loss 120.408923\n",
      "[28:4300] F: 17212800 \t tot_loss 6.972753 rs_loss 0.208369 vs_loss 0.749830 logits_loss 120.291064\n",
      "[28:4400] F: 17222400 \t tot_loss 6.844593 rs_loss 0.146862 vs_loss 0.682197 logits_loss 120.310669\n",
      "[28:4500] F: 17232000 \t tot_loss 6.782422 rs_loss 0.122537 vs_loss 0.642411 logits_loss 120.349472\n",
      "[28:4600] F: 17241600 \t tot_loss 6.578173 rs_loss 0.048717 vs_loss 0.514721 logits_loss 120.294697\n",
      "[28:4700] F: 17251200 \t tot_loss 6.636624 rs_loss 0.079491 vs_loss 0.550150 logits_loss 120.139663\n",
      "[28:4800] F: 17260800 \t tot_loss 6.691719 rs_loss 0.106878 vs_loss 0.574981 logits_loss 120.197204\n",
      "[28:4900] F: 17270400 \t tot_loss 6.802213 rs_loss 0.147606 vs_loss 0.647070 logits_loss 120.150728\n",
      "[28:5000] F: 17280000 \t tot_loss 6.968658 rs_loss 0.194651 vs_loss 0.765418 logits_loss 120.171762\n",
      "[28:5100] F: 17289600 \t tot_loss 6.920389 rs_loss 0.183934 vs_loss 0.725853 logits_loss 120.212026\n",
      "[28:5200] F: 17299200 \t tot_loss 6.841142 rs_loss 0.149380 vs_loss 0.684423 logits_loss 120.146780\n",
      "[28:5300] F: 17308800 \t tot_loss 6.700459 rs_loss 0.104693 vs_loss 0.589407 logits_loss 120.127163\n",
      "[28:5400] F: 17318400 \t tot_loss 6.599094 rs_loss 0.078164 vs_loss 0.520656 logits_loss 120.005472\n",
      "[28:5500] F: 17328000 \t tot_loss 6.588375 rs_loss 0.061078 vs_loss 0.524113 logits_loss 120.063677\n",
      "[28:5600] F: 17337600 \t tot_loss 6.582181 rs_loss 0.060677 vs_loss 0.526493 logits_loss 119.900196\n",
      "[28:5700] F: 17347200 \t tot_loss 6.616121 rs_loss 0.076259 vs_loss 0.554634 logits_loss 119.704562\n",
      "[28:5800] F: 17356800 \t tot_loss 6.566054 rs_loss 0.055455 vs_loss 0.520988 logits_loss 119.792214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28:5900] F: 17366400 \t tot_loss 6.656079 rs_loss 0.081143 vs_loss 0.590182 logits_loss 119.695072\n",
      "[28:6000] F: 17376000 \t tot_loss 6.674517 rs_loss 0.082121 vs_loss 0.594193 logits_loss 119.964047\n",
      "[28:6100] F: 17385600 \t tot_loss 6.792077 rs_loss 0.111399 vs_loss 0.680499 logits_loss 120.003564\n",
      "[28:6200] F: 17395200 \t tot_loss 6.802641 rs_loss 0.115442 vs_loss 0.675847 logits_loss 120.227029\n",
      "Batch [29] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.05 (+-0.16) \n",
      "[29:0] F: 17400000 \t tot_loss 6.885934 rs_loss 0.138269 vs_loss 0.743842 logits_loss 120.076461\n",
      "[29:100] F: 17409600 \t tot_loss 7.077788 rs_loss 0.206707 vs_loss 0.870722 logits_loss 120.007174\n",
      "[29:200] F: 17419200 \t tot_loss 7.014338 rs_loss 0.186951 vs_loss 0.832357 logits_loss 119.900608\n",
      "[29:300] F: 17428800 \t tot_loss 7.066391 rs_loss 0.196578 vs_loss 0.877769 logits_loss 119.840879\n",
      "[29:400] F: 17438400 \t tot_loss 6.996152 rs_loss 0.174700 vs_loss 0.825213 logits_loss 119.924772\n",
      "[29:500] F: 17448000 \t tot_loss 6.861029 rs_loss 0.130438 vs_loss 0.728482 logits_loss 120.042193\n",
      "[29:600] F: 17457600 \t tot_loss 6.926498 rs_loss 0.151897 vs_loss 0.777543 logits_loss 119.941141\n",
      "[29:700] F: 17467200 \t tot_loss 7.041571 rs_loss 0.186965 vs_loss 0.855516 logits_loss 119.981793\n",
      "[29:800] F: 17476800 \t tot_loss 7.221369 rs_loss 0.232297 vs_loss 0.987590 logits_loss 120.029628\n",
      "[29:900] F: 17486400 \t tot_loss 7.171396 rs_loss 0.206488 vs_loss 0.955793 logits_loss 120.182286\n",
      "[29:1000] F: 17496000 \t tot_loss 7.196129 rs_loss 0.207924 vs_loss 0.975046 logits_loss 120.263184\n",
      "[29:1100] F: 17505600 \t tot_loss 6.969022 rs_loss 0.139727 vs_loss 0.831355 logits_loss 119.958818\n",
      "[29:1200] F: 17515200 \t tot_loss 6.857373 rs_loss 0.133077 vs_loss 0.745985 logits_loss 119.566232\n",
      "[29:1300] F: 17524800 \t tot_loss 6.780072 rs_loss 0.115461 vs_loss 0.705812 logits_loss 119.175978\n",
      "[29:1400] F: 17534400 \t tot_loss 6.882629 rs_loss 0.150555 vs_loss 0.767767 logits_loss 119.286142\n",
      "[29:1500] F: 17544000 \t tot_loss 6.981445 rs_loss 0.181561 vs_loss 0.833812 logits_loss 119.321451\n",
      "[29:1600] F: 17553600 \t tot_loss 6.975551 rs_loss 0.164401 vs_loss 0.837844 logits_loss 119.466109\n",
      "[29:1700] F: 17563200 \t tot_loss 7.070266 rs_loss 0.186859 vs_loss 0.900199 logits_loss 119.664160\n",
      "[29:1800] F: 17572800 \t tot_loss 6.918327 rs_loss 0.147810 vs_loss 0.798215 logits_loss 119.446046\n",
      "[29:1900] F: 17582400 \t tot_loss 6.978571 rs_loss 0.166288 vs_loss 0.827211 logits_loss 119.701440\n",
      "[29:2000] F: 17592000 \t tot_loss 7.037010 rs_loss 0.188558 vs_loss 0.850663 logits_loss 119.955782\n",
      "[29:2100] F: 17601600 \t tot_loss 7.144411 rs_loss 0.240261 vs_loss 0.906861 logits_loss 119.945785\n",
      "[29:2200] F: 17611200 \t tot_loss 7.248257 rs_loss 0.259530 vs_loss 0.999054 logits_loss 119.793439\n",
      "[29:2300] F: 17620800 \t tot_loss 7.097422 rs_loss 0.215275 vs_loss 0.895704 logits_loss 119.728861\n",
      "[29:2400] F: 17630400 \t tot_loss 7.045360 rs_loss 0.190592 vs_loss 0.877587 logits_loss 119.543615\n",
      "[29:2500] F: 17640000 \t tot_loss 6.866854 rs_loss 0.135315 vs_loss 0.759594 logits_loss 119.438902\n",
      "[29:2600] F: 17649600 \t tot_loss 6.801835 rs_loss 0.112540 vs_loss 0.703304 logits_loss 119.719822\n",
      "[29:2700] F: 17659200 \t tot_loss 6.864151 rs_loss 0.132177 vs_loss 0.752270 logits_loss 119.594082\n",
      "[29:2800] F: 17668800 \t tot_loss 6.909881 rs_loss 0.155656 vs_loss 0.789093 logits_loss 119.302626\n",
      "[29:2900] F: 17678400 \t tot_loss 6.881799 rs_loss 0.142117 vs_loss 0.770495 logits_loss 119.383729\n",
      "[29:3000] F: 17688000 \t tot_loss 6.825128 rs_loss 0.148829 vs_loss 0.699297 logits_loss 119.540036\n",
      "[29:3100] F: 17697600 \t tot_loss 6.844296 rs_loss 0.150212 vs_loss 0.711598 logits_loss 119.649712\n",
      "[29:3200] F: 17707200 \t tot_loss 6.728245 rs_loss 0.124381 vs_loss 0.620886 logits_loss 119.659569\n",
      "[29:3300] F: 17716800 \t tot_loss 6.935789 rs_loss 0.177919 vs_loss 0.783201 logits_loss 119.493403\n",
      "[29:3400] F: 17726400 \t tot_loss 6.971129 rs_loss 0.177153 vs_loss 0.839720 logits_loss 119.085115\n",
      "[29:3500] F: 17736000 \t tot_loss 6.965513 rs_loss 0.181205 vs_loss 0.849973 logits_loss 118.686706\n",
      "[29:3600] F: 17745600 \t tot_loss 6.935414 rs_loss 0.156580 vs_loss 0.826516 logits_loss 119.046349\n",
      "[29:3700] F: 17755200 \t tot_loss 6.728859 rs_loss 0.101567 vs_loss 0.675836 logits_loss 119.029107\n",
      "[29:3800] F: 17764800 \t tot_loss 6.805871 rs_loss 0.130071 vs_loss 0.725045 logits_loss 119.015089\n",
      "[29:3900] F: 17774400 \t tot_loss 6.825035 rs_loss 0.126936 vs_loss 0.730992 logits_loss 119.342122\n",
      "[29:4000] F: 17784000 \t tot_loss 6.836499 rs_loss 0.132238 vs_loss 0.732637 logits_loss 119.432479\n",
      "[29:4100] F: 17793600 \t tot_loss 6.829141 rs_loss 0.127138 vs_loss 0.721059 logits_loss 119.618887\n",
      "[29:4200] F: 17803200 \t tot_loss 6.672335 rs_loss 0.076197 vs_loss 0.603228 logits_loss 119.858202\n",
      "[29:4300] F: 17812800 \t tot_loss 6.689997 rs_loss 0.101879 vs_loss 0.599027 logits_loss 119.781822\n",
      "[29:4400] F: 17822400 \t tot_loss 6.666292 rs_loss 0.094493 vs_loss 0.600207 logits_loss 119.431835\n",
      "[29:4500] F: 17832000 \t tot_loss 6.751606 rs_loss 0.132018 vs_loss 0.659207 logits_loss 119.207603\n",
      "[29:4600] F: 17841600 \t tot_loss 6.803814 rs_loss 0.150143 vs_loss 0.689758 logits_loss 119.278253\n",
      "[29:4700] F: 17851200 \t tot_loss 6.767146 rs_loss 0.123120 vs_loss 0.675323 logits_loss 119.374056\n",
      "[29:4800] F: 17860800 \t tot_loss 6.788068 rs_loss 0.132733 vs_loss 0.685564 logits_loss 119.395422\n",
      "[29:4900] F: 17870400 \t tot_loss 6.720666 rs_loss 0.115125 vs_loss 0.638212 logits_loss 119.346565\n",
      "[29:5000] F: 17880000 \t tot_loss 6.715231 rs_loss 0.118770 vs_loss 0.638074 logits_loss 119.167735\n",
      "[29:5100] F: 17889600 \t tot_loss 6.706747 rs_loss 0.122148 vs_loss 0.629696 logits_loss 119.098062\n",
      "[29:5200] F: 17899200 \t tot_loss 6.764948 rs_loss 0.122874 vs_loss 0.675795 logits_loss 119.325580\n",
      "[29:5300] F: 17908800 \t tot_loss 6.783132 rs_loss 0.131930 vs_loss 0.672650 logits_loss 119.571027\n",
      "[29:5400] F: 17918400 \t tot_loss 6.705677 rs_loss 0.101584 vs_loss 0.623415 logits_loss 119.613574\n",
      "[29:5500] F: 17928000 \t tot_loss 6.734312 rs_loss 0.115659 vs_loss 0.644000 logits_loss 119.493068\n",
      "[29:5600] F: 17937600 \t tot_loss 6.677634 rs_loss 0.111916 vs_loss 0.592338 logits_loss 119.467582\n",
      "[29:5700] F: 17947200 \t tot_loss 6.612607 rs_loss 0.079471 vs_loss 0.570539 logits_loss 119.251929\n",
      "[29:5800] F: 17956800 \t tot_loss 6.841880 rs_loss 0.145410 vs_loss 0.745958 logits_loss 119.010219\n",
      "[29:5900] F: 17966400 \t tot_loss 6.721605 rs_loss 0.105693 vs_loss 0.656904 logits_loss 119.180158\n",
      "[29:6000] F: 17976000 \t tot_loss 6.694506 rs_loss 0.102887 vs_loss 0.637321 logits_loss 119.085938\n",
      "[29:6100] F: 17985600 \t tot_loss 6.690470 rs_loss 0.101975 vs_loss 0.626166 logits_loss 119.246573\n",
      "[29:6200] F: 17995200 \t tot_loss 6.551631 rs_loss 0.066864 vs_loss 0.512427 logits_loss 119.446798\n",
      "Batch [30] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.13 (+-0.08) \n",
      "[30:0] F: 18000000 \t tot_loss 6.566496 rs_loss 0.070698 vs_loss 0.522681 logits_loss 119.462322\n",
      "[30:100] F: 18009600 \t tot_loss 6.928947 rs_loss 0.185828 vs_loss 0.744516 logits_loss 119.972057\n",
      "[30:200] F: 18019200 \t tot_loss 7.031650 rs_loss 0.213295 vs_loss 0.812677 logits_loss 120.113572\n",
      "[30:300] F: 18028800 \t tot_loss 7.164991 rs_loss 0.238660 vs_loss 0.911514 logits_loss 120.296336\n",
      "[30:400] F: 18038400 \t tot_loss 7.130851 rs_loss 0.211763 vs_loss 0.880215 logits_loss 120.777460\n",
      "[30:500] F: 18048000 \t tot_loss 6.845308 rs_loss 0.116483 vs_loss 0.706964 logits_loss 120.437225\n",
      "[30:600] F: 18057600 \t tot_loss 6.854110 rs_loss 0.114752 vs_loss 0.701849 logits_loss 120.750182\n",
      "[30:700] F: 18067200 \t tot_loss 6.725089 rs_loss 0.086546 vs_loss 0.620574 logits_loss 120.359377\n",
      "[30:800] F: 18076800 \t tot_loss 6.723270 rs_loss 0.086404 vs_loss 0.624212 logits_loss 120.253094\n",
      "[30:900] F: 18086400 \t tot_loss 6.718500 rs_loss 0.094584 vs_loss 0.613962 logits_loss 120.199082\n",
      "[30:1000] F: 18096000 \t tot_loss 6.657441 rs_loss 0.068743 vs_loss 0.580072 logits_loss 120.172516\n",
      "[30:1100] F: 18105600 \t tot_loss 6.731483 rs_loss 0.098266 vs_loss 0.600700 logits_loss 120.650333\n",
      "[30:1200] F: 18115200 \t tot_loss 6.695540 rs_loss 0.090809 vs_loss 0.587339 logits_loss 120.347835\n",
      "[30:1300] F: 18124800 \t tot_loss 6.683382 rs_loss 0.081374 vs_loss 0.578714 logits_loss 120.465880\n",
      "[30:1400] F: 18134400 \t tot_loss 6.755068 rs_loss 0.111762 vs_loss 0.634426 logits_loss 120.177595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30:1500] F: 18144000 \t tot_loss 6.695131 rs_loss 0.082733 vs_loss 0.607824 logits_loss 120.091470\n",
      "[30:1600] F: 18153600 \t tot_loss 6.707160 rs_loss 0.085165 vs_loss 0.598469 logits_loss 120.470518\n",
      "[30:1700] F: 18163200 \t tot_loss 6.653295 rs_loss 0.066973 vs_loss 0.569130 logits_loss 120.343838\n",
      "[30:1800] F: 18172800 \t tot_loss 6.620040 rs_loss 0.063034 vs_loss 0.552795 logits_loss 120.084210\n",
      "[30:1900] F: 18182400 \t tot_loss 6.630515 rs_loss 0.066100 vs_loss 0.563275 logits_loss 120.022806\n",
      "[30:2000] F: 18192000 \t tot_loss 6.662789 rs_loss 0.092550 vs_loss 0.575362 logits_loss 119.897538\n",
      "[30:2100] F: 18201600 \t tot_loss 6.719084 rs_loss 0.112314 vs_loss 0.605098 logits_loss 120.033431\n",
      "[30:2200] F: 18211200 \t tot_loss 6.763706 rs_loss 0.113026 vs_loss 0.644407 logits_loss 120.125459\n",
      "[30:2300] F: 18220800 \t tot_loss 6.835791 rs_loss 0.133822 vs_loss 0.704972 logits_loss 119.939914\n",
      "[30:2400] F: 18230400 \t tot_loss 6.859890 rs_loss 0.129115 vs_loss 0.731033 logits_loss 119.994857\n",
      "[30:2500] F: 18240000 \t tot_loss 6.843613 rs_loss 0.117788 vs_loss 0.727567 logits_loss 119.965152\n",
      "[30:2600] F: 18249600 \t tot_loss 6.737630 rs_loss 0.092860 vs_loss 0.636709 logits_loss 120.161194\n",
      "[30:2700] F: 18259200 \t tot_loss 6.746304 rs_loss 0.113665 vs_loss 0.622855 logits_loss 120.195676\n",
      "[30:2800] F: 18268800 \t tot_loss 6.706248 rs_loss 0.103616 vs_loss 0.601756 logits_loss 120.017534\n",
      "[30:2900] F: 18278400 \t tot_loss 6.871174 rs_loss 0.171419 vs_loss 0.706038 logits_loss 119.874344\n",
      "[30:3000] F: 18288000 \t tot_loss 6.883986 rs_loss 0.173116 vs_loss 0.719131 logits_loss 119.834778\n",
      "[30:3100] F: 18297600 \t tot_loss 6.764282 rs_loss 0.129352 vs_loss 0.647955 logits_loss 119.739502\n",
      "[30:3200] F: 18307200 \t tot_loss 6.886275 rs_loss 0.172054 vs_loss 0.732169 logits_loss 119.641023\n",
      "[30:3300] F: 18316800 \t tot_loss 6.752852 rs_loss 0.117983 vs_loss 0.647958 logits_loss 119.738240\n",
      "[30:3400] F: 18326400 \t tot_loss 6.806632 rs_loss 0.148470 vs_loss 0.666357 logits_loss 119.836087\n",
      "[30:3500] F: 18336000 \t tot_loss 6.894925 rs_loss 0.173311 vs_loss 0.722170 logits_loss 119.988870\n",
      "[30:3600] F: 18345600 \t tot_loss 6.837149 rs_loss 0.149156 vs_loss 0.677878 logits_loss 120.202283\n",
      "[30:3700] F: 18355200 \t tot_loss 6.753848 rs_loss 0.125370 vs_loss 0.618255 logits_loss 120.204468\n",
      "[30:3800] F: 18364800 \t tot_loss 6.786214 rs_loss 0.118212 vs_loss 0.665352 logits_loss 120.052986\n",
      "[30:3900] F: 18374400 \t tot_loss 6.790389 rs_loss 0.113555 vs_loss 0.668524 logits_loss 120.166208\n",
      "[30:4000] F: 18384000 \t tot_loss 6.682328 rs_loss 0.081892 vs_loss 0.598706 logits_loss 120.034602\n",
      "[30:4100] F: 18393600 \t tot_loss 6.874644 rs_loss 0.123768 vs_loss 0.750882 logits_loss 119.999894\n",
      "[30:4200] F: 18403200 \t tot_loss 6.760097 rs_loss 0.104754 vs_loss 0.673653 logits_loss 119.633804\n",
      "[30:4300] F: 18412800 \t tot_loss 6.673351 rs_loss 0.086995 vs_loss 0.615756 logits_loss 119.412007\n",
      "[30:4400] F: 18422400 \t tot_loss 6.685689 rs_loss 0.085104 vs_loss 0.628250 logits_loss 119.446703\n",
      "[30:4500] F: 18432000 \t tot_loss 6.628540 rs_loss 0.086067 vs_loss 0.568128 logits_loss 119.486907\n",
      "[30:4600] F: 18441600 \t tot_loss 6.780595 rs_loss 0.128276 vs_loss 0.671173 logits_loss 119.622920\n",
      "[30:4700] F: 18451200 \t tot_loss 7.094407 rs_loss 0.204410 vs_loss 0.906101 logits_loss 119.677911\n",
      "[30:4800] F: 18460800 \t tot_loss 7.199907 rs_loss 0.227448 vs_loss 0.981834 logits_loss 119.812508\n",
      "[30:4900] F: 18470400 \t tot_loss 7.167488 rs_loss 0.202235 vs_loss 0.975180 logits_loss 119.801461\n",
      "[30:5000] F: 18480000 \t tot_loss 7.039928 rs_loss 0.158114 vs_loss 0.876636 logits_loss 120.103577\n",
      "[30:5100] F: 18489600 \t tot_loss 6.794146 rs_loss 0.109670 vs_loss 0.680254 logits_loss 120.084434\n",
      "[30:5200] F: 18499200 \t tot_loss 6.886215 rs_loss 0.136138 vs_loss 0.751738 logits_loss 119.966781\n",
      "[30:5300] F: 18508800 \t tot_loss 6.782637 rs_loss 0.115350 vs_loss 0.679330 logits_loss 119.759133\n",
      "[30:5400] F: 18518400 \t tot_loss 6.817016 rs_loss 0.133148 vs_loss 0.694349 logits_loss 119.790377\n",
      "[30:5500] F: 18528000 \t tot_loss 6.757284 rs_loss 0.114954 vs_loss 0.660094 logits_loss 119.644711\n",
      "[30:5600] F: 18537600 \t tot_loss 6.540539 rs_loss 0.064062 vs_loss 0.500084 logits_loss 119.527855\n",
      "[30:5700] F: 18547200 \t tot_loss 6.614015 rs_loss 0.101199 vs_loss 0.537000 logits_loss 119.516319\n",
      "[30:5800] F: 18556800 \t tot_loss 6.570818 rs_loss 0.089666 vs_loss 0.515777 logits_loss 119.307499\n",
      "[30:5900] F: 18566400 \t tot_loss 6.705881 rs_loss 0.126766 vs_loss 0.621377 logits_loss 119.154761\n",
      "[30:6000] F: 18576000 \t tot_loss 6.828554 rs_loss 0.154088 vs_loss 0.712406 logits_loss 119.241194\n",
      "[30:6100] F: 18585600 \t tot_loss 6.755248 rs_loss 0.116130 vs_loss 0.669127 logits_loss 119.399818\n",
      "[30:6200] F: 18595200 \t tot_loss 6.835293 rs_loss 0.125578 vs_loss 0.738250 logits_loss 119.429312\n",
      "Batch [31] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.01 (+-0.08) \n",
      "[31:0] F: 18600000 \t tot_loss 6.683784 rs_loss 0.079310 vs_loss 0.622094 logits_loss 119.647606\n",
      "[31:100] F: 18609600 \t tot_loss 6.822710 rs_loss 0.102344 vs_loss 0.723449 logits_loss 119.938332\n",
      "[31:200] F: 18619200 \t tot_loss 6.860913 rs_loss 0.110413 vs_loss 0.720996 logits_loss 120.590079\n",
      "[31:300] F: 18628800 \t tot_loss 6.979649 rs_loss 0.136839 vs_loss 0.795664 logits_loss 120.942920\n",
      "[31:400] F: 18638400 \t tot_loss 7.072570 rs_loss 0.168119 vs_loss 0.849245 logits_loss 121.104119\n",
      "[31:500] F: 18648000 \t tot_loss 6.972210 rs_loss 0.140741 vs_loss 0.775781 logits_loss 121.113750\n",
      "[31:600] F: 18657600 \t tot_loss 6.995209 rs_loss 0.161690 vs_loss 0.802800 logits_loss 120.614385\n",
      "[31:700] F: 18667200 \t tot_loss 6.820812 rs_loss 0.115250 vs_loss 0.685542 logits_loss 120.400401\n",
      "[31:800] F: 18676800 \t tot_loss 6.798991 rs_loss 0.090556 vs_loss 0.678126 logits_loss 120.606161\n",
      "[31:900] F: 18686400 \t tot_loss 6.771395 rs_loss 0.091085 vs_loss 0.667942 logits_loss 120.247369\n",
      "[31:1000] F: 18696000 \t tot_loss 6.708308 rs_loss 0.068315 vs_loss 0.621810 logits_loss 120.363655\n",
      "[31:1100] F: 18705600 \t tot_loss 6.793743 rs_loss 0.093868 vs_loss 0.676866 logits_loss 120.460190\n",
      "[31:1200] F: 18715200 \t tot_loss 6.856318 rs_loss 0.118031 vs_loss 0.733172 logits_loss 120.102302\n",
      "[31:1300] F: 18724800 \t tot_loss 6.910785 rs_loss 0.129600 vs_loss 0.759875 logits_loss 120.426192\n",
      "[31:1400] F: 18734400 \t tot_loss 7.036267 rs_loss 0.175516 vs_loss 0.840416 logits_loss 120.406705\n",
      "[31:1500] F: 18744000 \t tot_loss 7.024195 rs_loss 0.173947 vs_loss 0.844584 logits_loss 120.113290\n",
      "[31:1600] F: 18753600 \t tot_loss 7.059875 rs_loss 0.192736 vs_loss 0.855432 logits_loss 120.234153\n",
      "[31:1700] F: 18763200 \t tot_loss 7.018404 rs_loss 0.183492 vs_loss 0.830015 logits_loss 120.097926\n",
      "[31:1800] F: 18772800 \t tot_loss 6.808395 rs_loss 0.122638 vs_loss 0.691392 logits_loss 119.887310\n",
      "[31:1900] F: 18782400 \t tot_loss 6.709941 rs_loss 0.100941 vs_loss 0.617353 logits_loss 119.832945\n",
      "[31:2000] F: 18792000 \t tot_loss 6.742644 rs_loss 0.107017 vs_loss 0.660891 logits_loss 119.494710\n",
      "[31:2100] F: 18801600 \t tot_loss 6.940304 rs_loss 0.169818 vs_loss 0.793574 logits_loss 119.538224\n",
      "[31:2200] F: 18811200 \t tot_loss 7.103823 rs_loss 0.221622 vs_loss 0.899049 logits_loss 119.663051\n",
      "[31:2300] F: 18820800 \t tot_loss 7.364065 rs_loss 0.294567 vs_loss 1.069252 logits_loss 120.004931\n",
      "[31:2400] F: 18830400 \t tot_loss 7.463380 rs_loss 0.317713 vs_loss 1.127467 logits_loss 120.363982\n",
      "[31:2500] F: 18840000 \t tot_loss 7.379302 rs_loss 0.286920 vs_loss 1.073381 logits_loss 120.380018\n",
      "[31:2600] F: 18849600 \t tot_loss 7.280772 rs_loss 0.258592 vs_loss 0.998479 logits_loss 120.474005\n",
      "[31:2700] F: 18859200 \t tot_loss 6.999499 rs_loss 0.176948 vs_loss 0.812587 logits_loss 120.199281\n",
      "[31:2800] F: 18868800 \t tot_loss 6.791537 rs_loss 0.136233 vs_loss 0.655233 logits_loss 120.001423\n",
      "[31:2900] F: 18878400 \t tot_loss 6.790402 rs_loss 0.134693 vs_loss 0.656880 logits_loss 119.976577\n",
      "[31:3000] F: 18888000 \t tot_loss 6.697225 rs_loss 0.098047 vs_loss 0.608002 logits_loss 119.823520\n",
      "[31:3100] F: 18897600 \t tot_loss 6.712510 rs_loss 0.101015 vs_loss 0.610258 logits_loss 120.024748\n",
      "[31:3200] F: 18907200 \t tot_loss 6.804595 rs_loss 0.111966 vs_loss 0.686062 logits_loss 120.131324\n",
      "[31:3300] F: 18916800 \t tot_loss 6.692207 rs_loss 0.081535 vs_loss 0.614781 logits_loss 119.917813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31:3400] F: 18926400 \t tot_loss 6.941389 rs_loss 0.153881 vs_loss 0.791671 logits_loss 119.916731\n",
      "[31:3500] F: 18936000 \t tot_loss 7.083854 rs_loss 0.202504 vs_loss 0.897700 logits_loss 119.672991\n",
      "[31:3600] F: 18945600 \t tot_loss 7.025988 rs_loss 0.185892 vs_loss 0.858664 logits_loss 119.628639\n",
      "[31:3700] F: 18955200 \t tot_loss 7.088788 rs_loss 0.204254 vs_loss 0.904751 logits_loss 119.595664\n",
      "[31:3800] F: 18964800 \t tot_loss 7.172885 rs_loss 0.226588 vs_loss 0.971025 logits_loss 119.505435\n",
      "[31:3900] F: 18974400 \t tot_loss 7.052104 rs_loss 0.182897 vs_loss 0.883105 logits_loss 119.722039\n",
      "[31:4000] F: 18984000 \t tot_loss 7.085272 rs_loss 0.201528 vs_loss 0.889463 logits_loss 119.885616\n",
      "[31:4100] F: 18993600 \t tot_loss 6.996233 rs_loss 0.178637 vs_loss 0.821859 logits_loss 119.914725\n",
      "[31:4200] F: 19003200 \t tot_loss 6.670618 rs_loss 0.089213 vs_loss 0.579704 logits_loss 120.034006\n",
      "[31:4300] F: 19012800 \t tot_loss 6.648876 rs_loss 0.090675 vs_loss 0.565712 logits_loss 119.849781\n",
      "[31:4400] F: 19022400 \t tot_loss 6.561795 rs_loss 0.066061 vs_loss 0.517670 logits_loss 119.561282\n",
      "[31:4500] F: 19032000 \t tot_loss 6.660407 rs_loss 0.096442 vs_loss 0.583349 logits_loss 119.612326\n",
      "[31:4600] F: 19041600 \t tot_loss 6.710133 rs_loss 0.112915 vs_loss 0.607542 logits_loss 119.793507\n",
      "[31:4700] F: 19051200 \t tot_loss 6.807301 rs_loss 0.137916 vs_loss 0.682824 logits_loss 119.731222\n",
      "[31:4800] F: 19060800 \t tot_loss 6.865017 rs_loss 0.165025 vs_loss 0.718782 logits_loss 119.624201\n",
      "[31:4900] F: 19070400 \t tot_loss 6.910302 rs_loss 0.179075 vs_loss 0.753009 logits_loss 119.564379\n",
      "[31:5000] F: 19080000 \t tot_loss 6.946171 rs_loss 0.182527 vs_loss 0.804666 logits_loss 119.179550\n",
      "[31:5100] F: 19089600 \t tot_loss 6.995218 rs_loss 0.204963 vs_loss 0.832970 logits_loss 119.145693\n",
      "[31:5200] F: 19099200 \t tot_loss 6.892887 rs_loss 0.160315 vs_loss 0.756358 logits_loss 119.524296\n",
      "[31:5300] F: 19108800 \t tot_loss 6.829517 rs_loss 0.142306 vs_loss 0.709323 logits_loss 119.557750\n",
      "[31:5400] F: 19118400 \t tot_loss 6.747990 rs_loss 0.127672 vs_loss 0.638774 logits_loss 119.630882\n",
      "[31:5500] F: 19128000 \t tot_loss 6.670614 rs_loss 0.099912 vs_loss 0.579427 logits_loss 119.825489\n",
      "[31:5600] F: 19137600 \t tot_loss 6.778993 rs_loss 0.147421 vs_loss 0.654885 logits_loss 119.533742\n",
      "[31:5700] F: 19147200 \t tot_loss 6.845617 rs_loss 0.174824 vs_loss 0.687694 logits_loss 119.661992\n",
      "[31:5800] F: 19156800 \t tot_loss 6.913127 rs_loss 0.187790 vs_loss 0.734003 logits_loss 119.826682\n",
      "[31:5900] F: 19166400 \t tot_loss 6.848435 rs_loss 0.163859 vs_loss 0.685143 logits_loss 119.988668\n",
      "[31:6000] F: 19176000 \t tot_loss 6.814527 rs_loss 0.139680 vs_loss 0.652099 logits_loss 120.454935\n",
      "[31:6100] F: 19185600 \t tot_loss 6.885000 rs_loss 0.144679 vs_loss 0.720709 logits_loss 120.392231\n",
      "[31:6200] F: 19195200 \t tot_loss 6.829184 rs_loss 0.129554 vs_loss 0.679219 logits_loss 120.408218\n",
      "Batch [32] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.17 (+-0.15) \n",
      "[32:0] F: 19200000 \t tot_loss 6.828479 rs_loss 0.129139 vs_loss 0.679124 logits_loss 120.404317\n",
      "[32:100] F: 19209600 \t tot_loss 6.891764 rs_loss 0.132753 vs_loss 0.735201 logits_loss 120.476189\n",
      "[32:200] F: 19219200 \t tot_loss 6.926721 rs_loss 0.131394 vs_loss 0.760446 logits_loss 120.697638\n",
      "[32:300] F: 19228800 \t tot_loss 6.999915 rs_loss 0.151554 vs_loss 0.812529 logits_loss 120.716616\n",
      "[32:400] F: 19238400 \t tot_loss 7.145408 rs_loss 0.215472 vs_loss 0.890421 logits_loss 120.790287\n",
      "[32:500] F: 19248000 \t tot_loss 7.133738 rs_loss 0.218385 vs_loss 0.863200 logits_loss 121.043052\n",
      "[32:600] F: 19257600 \t tot_loss 7.134457 rs_loss 0.228911 vs_loss 0.867506 logits_loss 120.760788\n",
      "[32:700] F: 19267200 \t tot_loss 7.048718 rs_loss 0.198370 vs_loss 0.800721 logits_loss 120.992526\n",
      "[32:800] F: 19276800 \t tot_loss 6.983304 rs_loss 0.158608 vs_loss 0.778500 logits_loss 120.923931\n",
      "[32:900] F: 19286400 \t tot_loss 7.210026 rs_loss 0.232931 vs_loss 0.949388 logits_loss 120.554141\n",
      "[32:1000] F: 19296000 \t tot_loss 7.212069 rs_loss 0.225609 vs_loss 0.945471 logits_loss 120.819787\n",
      "[32:1100] F: 19305600 \t tot_loss 7.337233 rs_loss 0.261123 vs_loss 1.042693 logits_loss 120.668343\n",
      "[32:1200] F: 19315200 \t tot_loss 7.595878 rs_loss 0.327794 vs_loss 1.230086 logits_loss 120.759947\n",
      "[32:1300] F: 19324800 \t tot_loss 7.613066 rs_loss 0.323645 vs_loss 1.250917 logits_loss 120.770074\n",
      "[32:1400] F: 19334400 \t tot_loss 7.548024 rs_loss 0.299666 vs_loss 1.217328 logits_loss 120.620600\n",
      "[32:1500] F: 19344000 \t tot_loss 7.537847 rs_loss 0.328106 vs_loss 1.165149 logits_loss 120.891852\n",
      "[32:1600] F: 19353600 \t tot_loss 7.309540 rs_loss 0.273238 vs_loss 1.005810 logits_loss 120.609841\n",
      "[32:1700] F: 19363200 \t tot_loss 6.997426 rs_loss 0.174913 vs_loss 0.779738 logits_loss 120.855499\n",
      "[32:1800] F: 19372800 \t tot_loss 7.084950 rs_loss 0.219635 vs_loss 0.825903 logits_loss 120.788235\n",
      "[32:1900] F: 19382400 \t tot_loss 6.899213 rs_loss 0.141268 vs_loss 0.730944 logits_loss 120.540020\n",
      "[32:2000] F: 19392000 \t tot_loss 6.872715 rs_loss 0.137080 vs_loss 0.708479 logits_loss 120.543123\n",
      "[32:2100] F: 19401600 \t tot_loss 7.098537 rs_loss 0.220426 vs_loss 0.862609 logits_loss 120.310027\n",
      "[32:2200] F: 19411200 \t tot_loss 7.095451 rs_loss 0.213745 vs_loss 0.863619 logits_loss 120.361727\n",
      "[32:2300] F: 19420800 \t tot_loss 7.393861 rs_loss 0.306964 vs_loss 1.072047 logits_loss 120.297013\n",
      "[32:2400] F: 19430400 \t tot_loss 7.400305 rs_loss 0.295618 vs_loss 1.095993 logits_loss 120.173867\n",
      "[32:2500] F: 19440000 \t tot_loss 7.237584 rs_loss 0.249373 vs_loss 0.978157 logits_loss 120.201086\n",
      "[32:2600] F: 19449600 \t tot_loss 7.389876 rs_loss 0.302883 vs_loss 1.072742 logits_loss 120.284996\n",
      "[32:2700] F: 19459200 \t tot_loss 7.292174 rs_loss 0.295040 vs_loss 0.982487 logits_loss 120.292942\n",
      "[32:2800] F: 19468800 \t tot_loss 7.319721 rs_loss 0.314012 vs_loss 0.981699 logits_loss 120.480215\n",
      "[32:2900] F: 19478400 \t tot_loss 7.487852 rs_loss 0.379948 vs_loss 1.088077 logits_loss 120.396538\n",
      "[32:3000] F: 19488000 \t tot_loss 7.290287 rs_loss 0.306941 vs_loss 0.972871 logits_loss 120.209507\n",
      "[32:3100] F: 19497600 \t tot_loss 7.052811 rs_loss 0.214614 vs_loss 0.831899 logits_loss 120.125962\n",
      "[32:3200] F: 19507200 \t tot_loss 7.022863 rs_loss 0.192482 vs_loss 0.831413 logits_loss 119.979361\n",
      "[32:3300] F: 19516800 \t tot_loss 6.745192 rs_loss 0.091034 vs_loss 0.649706 logits_loss 120.089028\n",
      "[32:3400] F: 19526400 \t tot_loss 6.870374 rs_loss 0.130043 vs_loss 0.735434 logits_loss 120.097939\n",
      "[32:3500] F: 19536000 \t tot_loss 7.003954 rs_loss 0.165451 vs_loss 0.834814 logits_loss 120.073775\n",
      "[32:3600] F: 19545600 \t tot_loss 7.223237 rs_loss 0.219880 vs_loss 0.989255 logits_loss 120.282036\n",
      "[32:3700] F: 19555200 \t tot_loss 7.230912 rs_loss 0.224275 vs_loss 0.998907 logits_loss 120.154601\n",
      "[32:3800] F: 19564800 \t tot_loss 7.156251 rs_loss 0.217961 vs_loss 0.927852 logits_loss 120.208752\n",
      "[32:3900] F: 19574400 \t tot_loss 7.134274 rs_loss 0.215129 vs_loss 0.910395 logits_loss 120.175011\n",
      "[32:4000] F: 19584000 \t tot_loss 6.869216 rs_loss 0.164261 vs_loss 0.701569 logits_loss 120.067720\n",
      "[32:4100] F: 19593600 \t tot_loss 7.158127 rs_loss 0.230039 vs_loss 0.908813 logits_loss 120.385495\n",
      "[32:4200] F: 19603200 \t tot_loss 7.089178 rs_loss 0.205549 vs_loss 0.863383 logits_loss 120.404928\n",
      "[32:4300] F: 19612800 \t tot_loss 7.014547 rs_loss 0.188728 vs_loss 0.800018 logits_loss 120.516017\n",
      "[32:4400] F: 19622400 \t tot_loss 7.041227 rs_loss 0.183072 vs_loss 0.827592 logits_loss 120.611259\n",
      "[32:4500] F: 19632000 \t tot_loss 6.828073 rs_loss 0.159442 vs_loss 0.661601 logits_loss 120.140597\n",
      "[32:4600] F: 19641600 \t tot_loss 6.736581 rs_loss 0.127114 vs_loss 0.612655 logits_loss 119.936256\n",
      "[32:4700] F: 19651200 \t tot_loss 6.756530 rs_loss 0.135216 vs_loss 0.629513 logits_loss 119.836017\n",
      "[32:4800] F: 19660800 \t tot_loss 6.661676 rs_loss 0.109440 vs_loss 0.565048 logits_loss 119.743745\n",
      "[32:4900] F: 19670400 \t tot_loss 6.630245 rs_loss 0.081837 vs_loss 0.558250 logits_loss 119.803158\n",
      "[32:5000] F: 19680000 \t tot_loss 6.620861 rs_loss 0.079709 vs_loss 0.548877 logits_loss 119.845489\n",
      "[32:5100] F: 19689600 \t tot_loss 6.820159 rs_loss 0.144878 vs_loss 0.676936 logits_loss 119.966892\n",
      "[32:5200] F: 19699200 \t tot_loss 7.079410 rs_loss 0.225893 vs_loss 0.853959 logits_loss 119.991157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32:5300] F: 19708800 \t tot_loss 7.132540 rs_loss 0.239825 vs_loss 0.890080 logits_loss 120.052693\n",
      "[32:5400] F: 19718400 \t tot_loss 7.382199 rs_loss 0.312372 vs_loss 1.071503 logits_loss 119.966496\n",
      "[32:5500] F: 19728000 \t tot_loss 7.185721 rs_loss 0.248386 vs_loss 0.945487 logits_loss 119.836960\n",
      "[32:5600] F: 19737600 \t tot_loss 7.034572 rs_loss 0.192395 vs_loss 0.846375 logits_loss 119.916038\n",
      "[32:5700] F: 19747200 \t tot_loss 7.274063 rs_loss 0.276012 vs_loss 1.000159 logits_loss 119.957831\n",
      "[32:5800] F: 19756800 \t tot_loss 7.124346 rs_loss 0.227422 vs_loss 0.892933 logits_loss 120.079827\n",
      "[32:5900] F: 19766400 \t tot_loss 7.237884 rs_loss 0.254242 vs_loss 0.977816 logits_loss 120.116515\n",
      "[32:6000] F: 19776000 \t tot_loss 7.352891 rs_loss 0.307437 vs_loss 1.057357 logits_loss 119.761948\n",
      "[32:6100] F: 19785600 \t tot_loss 7.116199 rs_loss 0.221276 vs_loss 0.912407 logits_loss 119.650308\n",
      "[32:6200] F: 19795200 \t tot_loss 7.084123 rs_loss 0.228097 vs_loss 0.870024 logits_loss 119.720040\n",
      "Batch [33] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.14 (+-0.08) \n",
      "[33:0] F: 19800000 \t tot_loss 7.050292 rs_loss 0.210531 vs_loss 0.852345 logits_loss 119.748337\n",
      "[33:100] F: 19809600 \t tot_loss 6.925182 rs_loss 0.168900 vs_loss 0.769482 logits_loss 119.736004\n",
      "[33:200] F: 19819200 \t tot_loss 6.828244 rs_loss 0.145651 vs_loss 0.687910 logits_loss 119.893678\n",
      "[33:300] F: 19828800 \t tot_loss 6.884952 rs_loss 0.146449 vs_loss 0.746644 logits_loss 119.837187\n",
      "[33:400] F: 19838400 \t tot_loss 7.005597 rs_loss 0.191208 vs_loss 0.816786 logits_loss 119.952059\n",
      "[33:500] F: 19848000 \t tot_loss 6.919950 rs_loss 0.175652 vs_loss 0.743976 logits_loss 120.006458\n",
      "[33:600] F: 19857600 \t tot_loss 6.919593 rs_loss 0.168387 vs_loss 0.745505 logits_loss 120.114021\n",
      "[33:700] F: 19867200 \t tot_loss 6.820063 rs_loss 0.137089 vs_loss 0.668306 logits_loss 120.293356\n",
      "[33:800] F: 19876800 \t tot_loss 6.802304 rs_loss 0.108928 vs_loss 0.684669 logits_loss 120.174142\n",
      "[33:900] F: 19886400 \t tot_loss 6.759402 rs_loss 0.086687 vs_loss 0.662412 logits_loss 120.206054\n",
      "[33:1000] F: 19896000 \t tot_loss 6.696593 rs_loss 0.071795 vs_loss 0.624886 logits_loss 119.998254\n",
      "[33:1100] F: 19905600 \t tot_loss 6.738127 rs_loss 0.069774 vs_loss 0.674313 logits_loss 119.880810\n",
      "[33:1200] F: 19915200 \t tot_loss 6.732604 rs_loss 0.096412 vs_loss 0.646208 logits_loss 119.799700\n",
      "[33:1300] F: 19924800 \t tot_loss 6.818869 rs_loss 0.120240 vs_loss 0.703529 logits_loss 119.901994\n",
      "[33:1400] F: 19934400 \t tot_loss 6.829401 rs_loss 0.122416 vs_loss 0.710570 logits_loss 119.928312\n",
      "[33:1500] F: 19944000 \t tot_loss 6.839339 rs_loss 0.147212 vs_loss 0.688576 logits_loss 120.071018\n",
      "[33:1600] F: 19953600 \t tot_loss 6.809125 rs_loss 0.120481 vs_loss 0.679963 logits_loss 120.173611\n",
      "[33:1700] F: 19963200 \t tot_loss 6.712690 rs_loss 0.089624 vs_loss 0.617392 logits_loss 120.113481\n",
      "[33:1800] F: 19972800 \t tot_loss 6.676415 rs_loss 0.081765 vs_loss 0.588635 logits_loss 120.120310\n",
      "[33:1900] F: 19982400 \t tot_loss 6.610792 rs_loss 0.056955 vs_loss 0.553275 logits_loss 120.011239\n",
      "[33:2000] F: 19992000 \t tot_loss 6.683526 rs_loss 0.069128 vs_loss 0.619423 logits_loss 119.899500\n",
      "[33:2100] F: 20001600 \t tot_loss 6.706009 rs_loss 0.088171 vs_loss 0.616995 logits_loss 120.016838\n",
      "[33:2200] F: 20011200 \t tot_loss 6.707256 rs_loss 0.094068 vs_loss 0.606438 logits_loss 120.135019\n",
      "[33:2300] F: 20020800 \t tot_loss 6.771681 rs_loss 0.116143 vs_loss 0.651096 logits_loss 120.088853\n",
      "[33:2400] F: 20030400 \t tot_loss 6.710752 rs_loss 0.107711 vs_loss 0.609413 logits_loss 119.872547\n",
      "[33:2500] F: 20040000 \t tot_loss 6.691426 rs_loss 0.096012 vs_loss 0.606083 logits_loss 119.786614\n",
      "[33:2600] F: 20049600 \t tot_loss 6.773712 rs_loss 0.119284 vs_loss 0.668928 logits_loss 119.710006\n",
      "[33:2700] F: 20059200 \t tot_loss 6.783458 rs_loss 0.120675 vs_loss 0.687190 logits_loss 119.511844\n",
      "[33:2800] F: 20068800 \t tot_loss 6.771793 rs_loss 0.113559 vs_loss 0.686595 logits_loss 119.432790\n",
      "[33:2900] F: 20078400 \t tot_loss 6.851862 rs_loss 0.163664 vs_loss 0.715594 logits_loss 119.452086\n",
      "[33:3000] F: 20088000 \t tot_loss 6.881492 rs_loss 0.162565 vs_loss 0.748864 logits_loss 119.401261\n",
      "[33:3100] F: 20097600 \t tot_loss 6.808644 rs_loss 0.143079 vs_loss 0.687877 logits_loss 119.553748\n",
      "[33:3200] F: 20107200 \t tot_loss 6.712201 rs_loss 0.122629 vs_loss 0.595984 logits_loss 119.871763\n",
      "[33:3300] F: 20116800 \t tot_loss 6.619632 rs_loss 0.073932 vs_loss 0.565827 logits_loss 119.597457\n",
      "[33:3400] F: 20126400 \t tot_loss 6.576909 rs_loss 0.063202 vs_loss 0.525141 logits_loss 119.771301\n",
      "[33:3500] F: 20136000 \t tot_loss 6.620026 rs_loss 0.072820 vs_loss 0.552374 logits_loss 119.896643\n",
      "[33:3600] F: 20145600 \t tot_loss 6.644762 rs_loss 0.072338 vs_loss 0.581987 logits_loss 119.808741\n",
      "[33:3700] F: 20155200 \t tot_loss 6.762319 rs_loss 0.105717 vs_loss 0.651382 logits_loss 120.104375\n",
      "[33:3800] F: 20164800 \t tot_loss 6.775458 rs_loss 0.119334 vs_loss 0.650594 logits_loss 120.110593\n",
      "[33:3900] F: 20174400 \t tot_loss 6.758845 rs_loss 0.126437 vs_loss 0.642713 logits_loss 119.793901\n",
      "[33:4000] F: 20184000 \t tot_loss 6.764801 rs_loss 0.136663 vs_loss 0.642961 logits_loss 119.703538\n",
      "[33:4100] F: 20193600 \t tot_loss 6.712752 rs_loss 0.125770 vs_loss 0.605228 logits_loss 119.635093\n",
      "[33:4200] F: 20203200 \t tot_loss 6.638819 rs_loss 0.093053 vs_loss 0.569888 logits_loss 119.517560\n",
      "[33:4300] F: 20212800 \t tot_loss 6.636557 rs_loss 0.094034 vs_loss 0.553259 logits_loss 119.785289\n",
      "[33:4400] F: 20222400 \t tot_loss 6.669236 rs_loss 0.107821 vs_loss 0.562742 logits_loss 119.973457\n",
      "[33:4500] F: 20232000 \t tot_loss 6.625425 rs_loss 0.084735 vs_loss 0.545252 logits_loss 119.908751\n",
      "[33:4600] F: 20241600 \t tot_loss 6.671017 rs_loss 0.116569 vs_loss 0.558144 logits_loss 119.926070\n",
      "[33:4700] F: 20251200 \t tot_loss 6.737073 rs_loss 0.121209 vs_loss 0.627955 logits_loss 119.758184\n",
      "[33:4800] F: 20260800 \t tot_loss 6.664658 rs_loss 0.098357 vs_loss 0.595249 logits_loss 119.421027\n",
      "[33:4900] F: 20270400 \t tot_loss 6.674221 rs_loss 0.097693 vs_loss 0.604389 logits_loss 119.442771\n",
      "[33:5000] F: 20280000 \t tot_loss 6.690347 rs_loss 0.085987 vs_loss 0.637943 logits_loss 119.328319\n",
      "[33:5100] F: 20289600 \t tot_loss 6.654010 rs_loss 0.085032 vs_loss 0.597960 logits_loss 119.420367\n",
      "[33:5200] F: 20299200 \t tot_loss 6.690157 rs_loss 0.090757 vs_loss 0.611895 logits_loss 119.750093\n",
      "[33:5300] F: 20308800 \t tot_loss 6.852042 rs_loss 0.153869 vs_loss 0.715679 logits_loss 119.649875\n",
      "[33:5400] F: 20318400 \t tot_loss 6.888967 rs_loss 0.163788 vs_loss 0.734459 logits_loss 119.814389\n",
      "[33:5500] F: 20328000 \t tot_loss 6.822238 rs_loss 0.145369 vs_loss 0.689821 logits_loss 119.740972\n",
      "[33:5600] F: 20337600 \t tot_loss 6.815784 rs_loss 0.138567 vs_loss 0.701723 logits_loss 119.509878\n",
      "[33:5700] F: 20347200 \t tot_loss 6.650668 rs_loss 0.093003 vs_loss 0.594804 logits_loss 119.257226\n",
      "[33:5800] F: 20356800 \t tot_loss 6.550653 rs_loss 0.072000 vs_loss 0.524724 logits_loss 119.078595\n",
      "[33:5900] F: 20366400 \t tot_loss 6.659228 rs_loss 0.099373 vs_loss 0.612404 logits_loss 118.949038\n",
      "[33:6000] F: 20376000 \t tot_loss 6.686293 rs_loss 0.117566 vs_loss 0.607728 logits_loss 119.219966\n",
      "[33:6100] F: 20385600 \t tot_loss 6.798655 rs_loss 0.142031 vs_loss 0.688638 logits_loss 119.359728\n",
      "[33:6200] F: 20395200 \t tot_loss 6.869488 rs_loss 0.153197 vs_loss 0.750788 logits_loss 119.310066\n",
      "Batch [34] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.07 (+-0.08) \n",
      "[34:0] F: 20400000 \t tot_loss 6.977576 rs_loss 0.202105 vs_loss 0.796290 logits_loss 119.583613\n",
      "[34:100] F: 20409600 \t tot_loss 7.007308 rs_loss 0.200107 vs_loss 0.812802 logits_loss 119.887966\n",
      "[34:200] F: 20419200 \t tot_loss 6.990257 rs_loss 0.172108 vs_loss 0.814487 logits_loss 120.073234\n",
      "[34:300] F: 20428800 \t tot_loss 6.969710 rs_loss 0.174524 vs_loss 0.775459 logits_loss 120.394543\n",
      "[34:400] F: 20438400 \t tot_loss 6.879529 rs_loss 0.128637 vs_loss 0.717843 logits_loss 120.660985\n",
      "[34:500] F: 20448000 \t tot_loss 6.819227 rs_loss 0.127936 vs_loss 0.669706 logits_loss 120.431723\n",
      "[34:600] F: 20457600 \t tot_loss 6.724229 rs_loss 0.117442 vs_loss 0.596298 logits_loss 120.209771\n",
      "[34:700] F: 20467200 \t tot_loss 6.728825 rs_loss 0.118098 vs_loss 0.599176 logits_loss 120.231023\n",
      "[34:800] F: 20476800 \t tot_loss 6.737346 rs_loss 0.112828 vs_loss 0.617914 logits_loss 120.132074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34:900] F: 20486400 \t tot_loss 6.715610 rs_loss 0.096858 vs_loss 0.607761 logits_loss 120.219801\n",
      "[34:1000] F: 20496000 \t tot_loss 6.815017 rs_loss 0.116228 vs_loss 0.672018 logits_loss 120.535408\n",
      "[34:1100] F: 20505600 \t tot_loss 6.919836 rs_loss 0.136490 vs_loss 0.744844 logits_loss 120.770032\n",
      "[34:1200] F: 20515200 \t tot_loss 7.014719 rs_loss 0.160038 vs_loss 0.810250 logits_loss 120.888620\n",
      "[34:1300] F: 20524800 \t tot_loss 7.062971 rs_loss 0.173791 vs_loss 0.852256 logits_loss 120.738485\n",
      "[34:1400] F: 20534400 \t tot_loss 7.037565 rs_loss 0.179999 vs_loss 0.825683 logits_loss 120.637670\n",
      "[34:1500] F: 20544000 \t tot_loss 6.893728 rs_loss 0.132361 vs_loss 0.733841 logits_loss 120.550528\n",
      "[34:1600] F: 20553600 \t tot_loss 6.958698 rs_loss 0.147037 vs_loss 0.773041 logits_loss 120.772401\n",
      "[34:1700] F: 20563200 \t tot_loss 6.977550 rs_loss 0.156037 vs_loss 0.792036 logits_loss 120.589532\n",
      "[34:1800] F: 20572800 \t tot_loss 6.928793 rs_loss 0.129765 vs_loss 0.768020 logits_loss 120.620153\n",
      "[34:1900] F: 20582400 \t tot_loss 7.024300 rs_loss 0.170449 vs_loss 0.833531 logits_loss 120.406383\n",
      "[34:2000] F: 20592000 \t tot_loss 6.802698 rs_loss 0.114318 vs_loss 0.683871 logits_loss 120.090184\n",
      "[34:2100] F: 20601600 \t tot_loss 6.977216 rs_loss 0.187311 vs_loss 0.786765 logits_loss 120.062820\n",
      "[34:2200] F: 20611200 \t tot_loss 7.074042 rs_loss 0.227422 vs_loss 0.850999 logits_loss 119.912410\n",
      "[34:2300] F: 20620800 \t tot_loss 7.098458 rs_loss 0.235965 vs_loss 0.861569 logits_loss 120.018482\n",
      "[34:2400] F: 20630400 \t tot_loss 7.195886 rs_loss 0.254172 vs_loss 0.939187 logits_loss 120.050545\n",
      "[34:2500] F: 20640000 \t tot_loss 6.988384 rs_loss 0.173558 vs_loss 0.805093 logits_loss 120.194635\n",
      "[34:2600] F: 20649600 \t tot_loss 6.839030 rs_loss 0.123093 vs_loss 0.717929 logits_loss 119.960165\n",
      "[34:2700] F: 20659200 \t tot_loss 6.799879 rs_loss 0.101006 vs_loss 0.709689 logits_loss 119.783679\n",
      "[34:2800] F: 20668800 \t tot_loss 6.733721 rs_loss 0.107984 vs_loss 0.646572 logits_loss 119.583305\n",
      "[34:2900] F: 20678400 \t tot_loss 6.745274 rs_loss 0.102589 vs_loss 0.657464 logits_loss 119.704408\n",
      "[34:3000] F: 20688000 \t tot_loss 6.864071 rs_loss 0.128344 vs_loss 0.735154 logits_loss 120.011453\n",
      "[34:3100] F: 20697600 \t tot_loss 7.061474 rs_loss 0.203755 vs_loss 0.853031 logits_loss 120.093771\n",
      "[34:3200] F: 20707200 \t tot_loss 7.260084 rs_loss 0.268913 vs_loss 0.966791 logits_loss 120.487583\n",
      "[34:3300] F: 20716800 \t tot_loss 7.193053 rs_loss 0.262266 vs_loss 0.902801 logits_loss 120.559711\n",
      "[34:3400] F: 20726400 \t tot_loss 7.175643 rs_loss 0.266812 vs_loss 0.879708 logits_loss 120.582469\n",
      "[34:3500] F: 20736000 \t tot_loss 7.017383 rs_loss 0.209641 vs_loss 0.789083 logits_loss 120.373181\n",
      "[34:3600] F: 20745600 \t tot_loss 6.892469 rs_loss 0.163982 vs_loss 0.724006 logits_loss 120.089632\n",
      "[34:3700] F: 20755200 \t tot_loss 6.880248 rs_loss 0.158095 vs_loss 0.720234 logits_loss 120.038360\n",
      "[34:3800] F: 20764800 \t tot_loss 6.991433 rs_loss 0.194762 vs_loss 0.803981 logits_loss 119.853800\n",
      "[34:3900] F: 20774400 \t tot_loss 6.837892 rs_loss 0.161459 vs_loss 0.672446 logits_loss 120.079734\n",
      "[34:4000] F: 20784000 \t tot_loss 6.774992 rs_loss 0.141245 vs_loss 0.624167 logits_loss 120.191619\n",
      "[34:4100] F: 20793600 \t tot_loss 6.826299 rs_loss 0.161382 vs_loss 0.666376 logits_loss 119.970826\n",
      "[34:4200] F: 20803200 \t tot_loss 6.759583 rs_loss 0.141844 vs_loss 0.607917 logits_loss 120.196447\n",
      "[34:4300] F: 20812800 \t tot_loss 6.873025 rs_loss 0.171528 vs_loss 0.692361 logits_loss 120.182706\n",
      "[34:4400] F: 20822400 \t tot_loss 6.884049 rs_loss 0.172210 vs_loss 0.713745 logits_loss 119.961862\n",
      "[34:4500] F: 20832000 \t tot_loss 6.859036 rs_loss 0.163393 vs_loss 0.692797 logits_loss 120.056920\n",
      "[34:4600] F: 20841600 \t tot_loss 6.756362 rs_loss 0.130494 vs_loss 0.610673 logits_loss 120.303895\n",
      "[34:4700] F: 20851200 \t tot_loss 6.682623 rs_loss 0.100466 vs_loss 0.565890 logits_loss 120.325336\n",
      "[34:4800] F: 20860800 \t tot_loss 6.612105 rs_loss 0.076736 vs_loss 0.509778 logits_loss 120.511817\n",
      "[34:4900] F: 20870400 \t tot_loss 6.694699 rs_loss 0.088301 vs_loss 0.584930 logits_loss 120.429381\n",
      "[34:5000] F: 20880000 \t tot_loss 6.672334 rs_loss 0.076212 vs_loss 0.593436 logits_loss 120.053715\n",
      "[34:5100] F: 20889600 \t tot_loss 6.641773 rs_loss 0.068203 vs_loss 0.576007 logits_loss 119.951258\n",
      "[34:5200] F: 20899200 \t tot_loss 6.727034 rs_loss 0.082695 vs_loss 0.657141 logits_loss 119.743955\n",
      "[34:5300] F: 20908800 \t tot_loss 6.741323 rs_loss 0.092112 vs_loss 0.657268 logits_loss 119.838850\n",
      "[34:5400] F: 20918400 \t tot_loss 6.733315 rs_loss 0.094248 vs_loss 0.649055 logits_loss 119.800241\n",
      "[34:5500] F: 20928000 \t tot_loss 6.828867 rs_loss 0.114676 vs_loss 0.716940 logits_loss 119.945026\n",
      "[34:5600] F: 20937600 \t tot_loss 6.821657 rs_loss 0.121872 vs_loss 0.681030 logits_loss 120.375088\n",
      "[34:5700] F: 20947200 \t tot_loss 6.783698 rs_loss 0.104034 vs_loss 0.663493 logits_loss 120.323415\n",
      "[34:5800] F: 20956800 \t tot_loss 7.027737 rs_loss 0.170363 vs_loss 0.839830 logits_loss 120.350886\n",
      "[34:5900] F: 20966400 \t tot_loss 6.926875 rs_loss 0.145859 vs_loss 0.776527 logits_loss 120.089775\n",
      "[34:6000] F: 20976000 \t tot_loss 6.916899 rs_loss 0.146433 vs_loss 0.776435 logits_loss 119.880617\n",
      "[34:6100] F: 20985600 \t tot_loss 6.958830 rs_loss 0.176872 vs_loss 0.791958 logits_loss 119.799988\n",
      "[34:6200] F: 20995200 \t tot_loss 6.860933 rs_loss 0.159040 vs_loss 0.705501 logits_loss 119.927833\n",
      "Batch [35] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.03 (+-0.08) \n",
      "[35:0] F: 21000000 \t tot_loss 7.000445 rs_loss 0.191648 vs_loss 0.799729 logits_loss 120.181377\n",
      "[35:100] F: 21009600 \t tot_loss 7.000286 rs_loss 0.174769 vs_loss 0.803144 logits_loss 120.447449\n",
      "[35:200] F: 21019200 \t tot_loss 6.997241 rs_loss 0.160540 vs_loss 0.823573 logits_loss 120.262552\n",
      "[35:300] F: 21028800 \t tot_loss 7.043653 rs_loss 0.173737 vs_loss 0.866980 logits_loss 120.058728\n",
      "[35:400] F: 21038400 \t tot_loss 6.821084 rs_loss 0.102973 vs_loss 0.712858 logits_loss 120.105062\n",
      "[35:500] F: 21048000 \t tot_loss 6.934952 rs_loss 0.142903 vs_loss 0.790961 logits_loss 120.021760\n",
      "[35:600] F: 21057600 \t tot_loss 6.922353 rs_loss 0.144105 vs_loss 0.768544 logits_loss 120.194082\n",
      "[35:700] F: 21067200 \t tot_loss 6.825166 rs_loss 0.109559 vs_loss 0.700517 logits_loss 120.301794\n",
      "[35:800] F: 21076800 \t tot_loss 6.780366 rs_loss 0.098707 vs_loss 0.670861 logits_loss 120.215964\n",
      "[35:900] F: 21086400 \t tot_loss 6.657499 rs_loss 0.056614 vs_loss 0.594702 logits_loss 120.123639\n",
      "[35:1000] F: 21096000 \t tot_loss 6.643881 rs_loss 0.051368 vs_loss 0.599676 logits_loss 119.856738\n",
      "[35:1100] F: 21105600 \t tot_loss 6.757833 rs_loss 0.079964 vs_loss 0.692622 logits_loss 119.704938\n",
      "[35:1200] F: 21115200 \t tot_loss 6.790710 rs_loss 0.086483 vs_loss 0.719446 logits_loss 119.695620\n",
      "[35:1300] F: 21124800 \t tot_loss 6.847658 rs_loss 0.108488 vs_loss 0.773186 logits_loss 119.319695\n",
      "[35:1400] F: 21134400 \t tot_loss 6.847722 rs_loss 0.115685 vs_loss 0.768666 logits_loss 119.267428\n",
      "[35:1500] F: 21144000 \t tot_loss 6.922771 rs_loss 0.141016 vs_loss 0.809097 logits_loss 119.453153\n",
      "[35:1600] F: 21153600 \t tot_loss 6.986351 rs_loss 0.162998 vs_loss 0.855966 logits_loss 119.347729\n",
      "[35:1700] F: 21163200 \t tot_loss 7.016705 rs_loss 0.156978 vs_loss 0.881236 logits_loss 119.569816\n",
      "[35:1800] F: 21172800 \t tot_loss 6.939644 rs_loss 0.136221 vs_loss 0.830795 logits_loss 119.452537\n",
      "[35:1900] F: 21182400 \t tot_loss 6.794986 rs_loss 0.107243 vs_loss 0.713314 logits_loss 119.488585\n",
      "[35:2000] F: 21192000 \t tot_loss 6.744725 rs_loss 0.101747 vs_loss 0.671502 logits_loss 119.429527\n",
      "[35:2100] F: 21201600 \t tot_loss 6.743034 rs_loss 0.118303 vs_loss 0.644282 logits_loss 119.608971\n",
      "[35:2200] F: 21211200 \t tot_loss 6.870476 rs_loss 0.165330 vs_loss 0.712925 logits_loss 119.844423\n",
      "[35:2300] F: 21220800 \t tot_loss 6.984302 rs_loss 0.172924 vs_loss 0.823728 logits_loss 119.753003\n",
      "[35:2400] F: 21230400 \t tot_loss 7.225632 rs_loss 0.224457 vs_loss 1.000199 logits_loss 120.019521\n",
      "[35:2500] F: 21240000 \t tot_loss 7.342108 rs_loss 0.243000 vs_loss 1.085670 logits_loss 120.268761\n",
      "[35:2600] F: 21249600 \t tot_loss 7.285401 rs_loss 0.217368 vs_loss 1.063742 logits_loss 120.085815\n",
      "[35:2700] F: 21259200 \t tot_loss 7.247209 rs_loss 0.227795 vs_loss 1.012198 logits_loss 120.144326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35:2800] F: 21268800 \t tot_loss 7.077378 rs_loss 0.184946 vs_loss 0.889251 logits_loss 120.063621\n",
      "[35:2900] F: 21278400 \t tot_loss 7.121170 rs_loss 0.210416 vs_loss 0.927738 logits_loss 119.660305\n",
      "[35:3000] F: 21288000 \t tot_loss 7.237468 rs_loss 0.266769 vs_loss 0.982758 logits_loss 119.758815\n",
      "[35:3100] F: 21297600 \t tot_loss 7.164137 rs_loss 0.247340 vs_loss 0.927062 logits_loss 119.794702\n",
      "[35:3200] F: 21307200 \t tot_loss 7.057311 rs_loss 0.223759 vs_loss 0.848734 logits_loss 119.696362\n",
      "[35:3300] F: 21316800 \t tot_loss 6.770520 rs_loss 0.145759 vs_loss 0.650152 logits_loss 119.492170\n",
      "[35:3400] F: 21326400 \t tot_loss 6.639330 rs_loss 0.080583 vs_loss 0.596467 logits_loss 119.245597\n",
      "[35:3500] F: 21336000 \t tot_loss 6.607474 rs_loss 0.062621 vs_loss 0.592858 logits_loss 119.039878\n",
      "[35:3600] F: 21345600 \t tot_loss 6.651305 rs_loss 0.077908 vs_loss 0.631918 logits_loss 118.829587\n",
      "[35:3700] F: 21355200 \t tot_loss 6.836484 rs_loss 0.128303 vs_loss 0.763409 logits_loss 118.895428\n",
      "[35:3800] F: 21364800 \t tot_loss 6.773313 rs_loss 0.107857 vs_loss 0.715502 logits_loss 118.999076\n",
      "[35:3900] F: 21374400 \t tot_loss 7.142811 rs_loss 0.204973 vs_loss 0.971518 logits_loss 119.326407\n",
      "[35:4000] F: 21384000 \t tot_loss 7.253921 rs_loss 0.237828 vs_loss 1.028227 logits_loss 119.757320\n",
      "[35:4100] F: 21393600 \t tot_loss 7.233324 rs_loss 0.233868 vs_loss 1.010361 logits_loss 119.781918\n",
      "[35:4200] F: 21403200 \t tot_loss 7.252469 rs_loss 0.239938 vs_loss 1.017776 logits_loss 119.895117\n",
      "[35:4300] F: 21412800 \t tot_loss 6.937409 rs_loss 0.162679 vs_loss 0.786678 logits_loss 119.761047\n",
      "[35:4400] F: 21422400 \t tot_loss 6.818180 rs_loss 0.135675 vs_loss 0.713459 logits_loss 119.380914\n",
      "[35:4500] F: 21432000 \t tot_loss 6.655310 rs_loss 0.091186 vs_loss 0.589086 logits_loss 119.500767\n",
      "[35:4600] F: 21441600 \t tot_loss 6.698763 rs_loss 0.112173 vs_loss 0.615822 logits_loss 119.415362\n",
      "[35:4700] F: 21451200 \t tot_loss 6.817405 rs_loss 0.136100 vs_loss 0.711416 logits_loss 119.397757\n",
      "[35:4800] F: 21460800 \t tot_loss 6.781732 rs_loss 0.116818 vs_loss 0.697530 logits_loss 119.347676\n",
      "[35:4900] F: 21470400 \t tot_loss 6.917839 rs_loss 0.148524 vs_loss 0.799579 logits_loss 119.394701\n",
      "[35:5000] F: 21480000 \t tot_loss 6.927770 rs_loss 0.167404 vs_loss 0.780059 logits_loss 119.606154\n",
      "[35:5100] F: 21489600 \t tot_loss 6.952369 rs_loss 0.183938 vs_loss 0.788981 logits_loss 119.588993\n",
      "[35:5200] F: 21499200 \t tot_loss 7.130758 rs_loss 0.230509 vs_loss 0.925792 logits_loss 119.489127\n",
      "[35:5300] F: 21508800 \t tot_loss 7.083559 rs_loss 0.217638 vs_loss 0.889893 logits_loss 119.520537\n",
      "[35:5400] F: 21518400 \t tot_loss 6.984193 rs_loss 0.174521 vs_loss 0.854701 logits_loss 119.099408\n",
      "[35:5500] F: 21528000 \t tot_loss 6.732454 rs_loss 0.104671 vs_loss 0.685470 logits_loss 118.846251\n",
      "[35:5600] F: 21537600 \t tot_loss 6.585786 rs_loss 0.070976 vs_loss 0.563749 logits_loss 119.021232\n",
      "[35:5700] F: 21547200 \t tot_loss 6.622275 rs_loss 0.091563 vs_loss 0.581805 logits_loss 118.978131\n",
      "[35:5800] F: 21556800 \t tot_loss 6.717634 rs_loss 0.113655 vs_loss 0.643784 logits_loss 119.203898\n",
      "[35:5900] F: 21566400 \t tot_loss 6.864379 rs_loss 0.145241 vs_loss 0.748371 logits_loss 119.415329\n",
      "[35:6000] F: 21576000 \t tot_loss 6.929271 rs_loss 0.177050 vs_loss 0.771226 logits_loss 119.619907\n",
      "[35:6100] F: 21585600 \t tot_loss 6.997692 rs_loss 0.210241 vs_loss 0.812195 logits_loss 119.505117\n",
      "[35:6200] F: 21595200 \t tot_loss 6.993329 rs_loss 0.211409 vs_loss 0.790521 logits_loss 119.827980\n",
      "Batch [36] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.10 (+-0.16) \n",
      "[36:0] F: 21600000 \t tot_loss 6.926998 rs_loss 0.190224 vs_loss 0.747934 logits_loss 119.776801\n",
      "[36:100] F: 21609600 \t tot_loss 7.302306 rs_loss 0.293743 vs_loss 1.015918 logits_loss 119.852875\n",
      "[36:200] F: 21619200 \t tot_loss 7.437533 rs_loss 0.309584 vs_loss 1.119456 logits_loss 120.169858\n",
      "[36:300] F: 21628800 \t tot_loss 7.402835 rs_loss 0.286162 vs_loss 1.102572 logits_loss 120.282026\n",
      "[36:400] F: 21638400 \t tot_loss 7.586930 rs_loss 0.332040 vs_loss 1.239795 logits_loss 120.301888\n",
      "[36:500] F: 21648000 \t tot_loss 7.229688 rs_loss 0.225629 vs_loss 0.994565 logits_loss 120.189867\n",
      "[36:600] F: 21657600 \t tot_loss 7.109327 rs_loss 0.188005 vs_loss 0.916493 logits_loss 120.096576\n",
      "[36:700] F: 21667200 \t tot_loss 7.166304 rs_loss 0.206441 vs_loss 0.966843 logits_loss 119.860404\n",
      "[36:800] F: 21676800 \t tot_loss 7.298606 rs_loss 0.229871 vs_loss 1.090063 logits_loss 119.573442\n",
      "[36:900] F: 21686400 \t tot_loss 7.253126 rs_loss 0.211038 vs_loss 1.069511 logits_loss 119.451533\n",
      "[36:1000] F: 21696000 \t tot_loss 7.407532 rs_loss 0.253933 vs_loss 1.174521 logits_loss 119.581548\n",
      "[36:1100] F: 21705600 \t tot_loss 7.225483 rs_loss 0.211022 vs_loss 1.031748 logits_loss 119.654253\n",
      "[36:1200] F: 21715200 \t tot_loss 6.907997 rs_loss 0.135276 vs_loss 0.785151 logits_loss 119.751392\n",
      "[36:1300] F: 21724800 \t tot_loss 6.841123 rs_loss 0.120102 vs_loss 0.721825 logits_loss 119.983925\n",
      "[36:1400] F: 21734400 \t tot_loss 6.852152 rs_loss 0.111763 vs_loss 0.758525 logits_loss 119.637272\n",
      "[36:1500] F: 21744000 \t tot_loss 6.961855 rs_loss 0.152133 vs_loss 0.825483 logits_loss 119.684800\n",
      "[36:1600] F: 21753600 \t tot_loss 6.966978 rs_loss 0.168863 vs_loss 0.814085 logits_loss 119.680591\n",
      "[36:1700] F: 21763200 \t tot_loss 7.105638 rs_loss 0.224648 vs_loss 0.894939 logits_loss 119.721032\n",
      "[36:1800] F: 21772800 \t tot_loss 6.924571 rs_loss 0.191212 vs_loss 0.734853 logits_loss 119.970128\n",
      "[36:1900] F: 21782400 \t tot_loss 6.987170 rs_loss 0.185519 vs_loss 0.804835 logits_loss 119.936327\n",
      "[36:2000] F: 21792000 \t tot_loss 7.114575 rs_loss 0.205414 vs_loss 0.899413 logits_loss 120.194971\n",
      "[36:2100] F: 21801600 \t tot_loss 6.964562 rs_loss 0.154745 vs_loss 0.805300 logits_loss 120.090346\n",
      "[36:2200] F: 21811200 \t tot_loss 7.198890 rs_loss 0.224584 vs_loss 0.971027 logits_loss 120.065579\n",
      "[36:2300] F: 21820800 \t tot_loss 7.039280 rs_loss 0.188451 vs_loss 0.851135 logits_loss 119.993863\n",
      "[36:2400] F: 21830400 \t tot_loss 6.999819 rs_loss 0.170105 vs_loss 0.848536 logits_loss 119.623559\n",
      "[36:2500] F: 21840000 \t tot_loss 7.131992 rs_loss 0.175953 vs_loss 0.974841 logits_loss 119.623973\n",
      "[36:2600] F: 21849600 \t tot_loss 6.948501 rs_loss 0.114979 vs_loss 0.853803 logits_loss 119.594356\n",
      "[36:2700] F: 21859200 \t tot_loss 7.113778 rs_loss 0.162054 vs_loss 0.974775 logits_loss 119.538995\n",
      "[36:2800] F: 21868800 \t tot_loss 6.970733 rs_loss 0.134996 vs_loss 0.844584 logits_loss 119.823057\n",
      "[36:2900] F: 21878400 \t tot_loss 6.786573 rs_loss 0.114398 vs_loss 0.684245 logits_loss 119.758608\n",
      "[36:3000] F: 21888000 \t tot_loss 6.914887 rs_loss 0.156970 vs_loss 0.764413 logits_loss 119.870073\n",
      "[36:3100] F: 21897600 \t tot_loss 6.765496 rs_loss 0.114746 vs_loss 0.659308 logits_loss 119.828841\n",
      "[36:3200] F: 21907200 \t tot_loss 6.774299 rs_loss 0.115134 vs_loss 0.677438 logits_loss 119.634546\n",
      "[36:3300] F: 21916800 \t tot_loss 6.767281 rs_loss 0.106130 vs_loss 0.686673 logits_loss 119.489545\n",
      "[36:3400] F: 21926400 \t tot_loss 6.598444 rs_loss 0.057451 vs_loss 0.574419 logits_loss 119.331469\n",
      "[36:3500] F: 21936000 \t tot_loss 6.561700 rs_loss 0.048221 vs_loss 0.533586 logits_loss 119.597840\n",
      "[36:3600] F: 21945600 \t tot_loss 6.595088 rs_loss 0.044555 vs_loss 0.572079 logits_loss 119.569087\n",
      "[36:3700] F: 21955200 \t tot_loss 6.708635 rs_loss 0.074454 vs_loss 0.643226 logits_loss 119.819112\n",
      "[36:3800] F: 21964800 \t tot_loss 6.790896 rs_loss 0.094088 vs_loss 0.705486 logits_loss 119.826459\n",
      "[36:3900] F: 21974400 \t tot_loss 6.875898 rs_loss 0.108770 vs_loss 0.778773 logits_loss 119.767103\n",
      "[36:4000] F: 21984000 \t tot_loss 6.924420 rs_loss 0.155679 vs_loss 0.784542 logits_loss 119.683980\n",
      "[36:4100] F: 21993600 \t tot_loss 6.864191 rs_loss 0.155557 vs_loss 0.741486 logits_loss 119.342954\n",
      "[36:4200] F: 22003200 \t tot_loss 6.812907 rs_loss 0.150006 vs_loss 0.695574 logits_loss 119.346522\n",
      "[36:4300] F: 22012800 \t tot_loss 6.731448 rs_loss 0.135741 vs_loss 0.635960 logits_loss 119.194929\n",
      "[36:4400] F: 22022400 \t tot_loss 6.772562 rs_loss 0.107713 vs_loss 0.696719 logits_loss 119.362578\n",
      "[36:4500] F: 22032000 \t tot_loss 6.947522 rs_loss 0.136220 vs_loss 0.825673 logits_loss 119.712559\n",
      "[36:4600] F: 22041600 \t tot_loss 6.988445 rs_loss 0.127557 vs_loss 0.872561 logits_loss 119.766520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36:4700] F: 22051200 \t tot_loss 7.222667 rs_loss 0.182355 vs_loss 1.040436 logits_loss 119.997528\n",
      "[36:4800] F: 22060800 \t tot_loss 7.192422 rs_loss 0.179373 vs_loss 1.015813 logits_loss 119.944738\n",
      "[36:4900] F: 22070400 \t tot_loss 7.173453 rs_loss 0.150608 vs_loss 1.038505 logits_loss 119.686783\n",
      "[36:5000] F: 22080000 \t tot_loss 7.081257 rs_loss 0.148004 vs_loss 0.954500 logits_loss 119.575061\n",
      "[36:5100] F: 22089600 \t tot_loss 6.928887 rs_loss 0.127320 vs_loss 0.835415 logits_loss 119.323022\n",
      "[36:5200] F: 22099200 \t tot_loss 6.953858 rs_loss 0.148078 vs_loss 0.834644 logits_loss 119.422708\n",
      "[36:5300] F: 22108800 \t tot_loss 6.872314 rs_loss 0.150667 vs_loss 0.738026 logits_loss 119.672411\n",
      "[36:5400] F: 22118400 \t tot_loss 7.079874 rs_loss 0.192572 vs_loss 0.908405 logits_loss 119.577930\n",
      "[36:5500] F: 22128000 \t tot_loss 7.122005 rs_loss 0.171790 vs_loss 0.976975 logits_loss 119.464781\n",
      "[36:5600] F: 22137600 \t tot_loss 7.170418 rs_loss 0.171414 vs_loss 1.023979 logits_loss 119.500503\n",
      "[36:5700] F: 22147200 \t tot_loss 7.189362 rs_loss 0.183004 vs_loss 1.034133 logits_loss 119.444501\n",
      "[36:5800] F: 22156800 \t tot_loss 7.037205 rs_loss 0.160364 vs_loss 0.879356 logits_loss 119.949705\n",
      "[36:5900] F: 22166400 \t tot_loss 6.995514 rs_loss 0.155733 vs_loss 0.823319 logits_loss 120.329260\n",
      "[36:6000] F: 22176000 \t tot_loss 6.830744 rs_loss 0.117816 vs_loss 0.691328 logits_loss 120.432004\n",
      "[36:6100] F: 22185600 \t tot_loss 6.698423 rs_loss 0.079500 vs_loss 0.595492 logits_loss 120.468610\n",
      "[36:6200] F: 22195200 \t tot_loss 6.723026 rs_loss 0.071065 vs_loss 0.640545 logits_loss 120.228332\n",
      "Batch [37] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.04 (+-0.16) \n",
      "[37:0] F: 22200000 \t tot_loss 6.723796 rs_loss 0.069065 vs_loss 0.641768 logits_loss 120.259250\n",
      "[37:100] F: 22209600 \t tot_loss 6.662876 rs_loss 0.066797 vs_loss 0.583890 logits_loss 120.243772\n",
      "[37:200] F: 22219200 \t tot_loss 6.802800 rs_loss 0.098267 vs_loss 0.690473 logits_loss 120.281195\n",
      "[37:300] F: 22228800 \t tot_loss 6.823851 rs_loss 0.120306 vs_loss 0.684239 logits_loss 120.386123\n",
      "[37:400] F: 22238400 \t tot_loss 6.841162 rs_loss 0.118127 vs_loss 0.710333 logits_loss 120.254060\n",
      "[37:500] F: 22248000 \t tot_loss 6.882835 rs_loss 0.138262 vs_loss 0.723829 logits_loss 120.414895\n",
      "[37:600] F: 22257600 \t tot_loss 6.759831 rs_loss 0.107525 vs_loss 0.634107 logits_loss 120.363965\n",
      "[37:700] F: 22267200 \t tot_loss 6.901424 rs_loss 0.158724 vs_loss 0.737913 logits_loss 120.095753\n",
      "[37:800] F: 22276800 \t tot_loss 6.913881 rs_loss 0.169035 vs_loss 0.734835 logits_loss 120.200207\n",
      "[37:900] F: 22286400 \t tot_loss 6.861644 rs_loss 0.150816 vs_loss 0.714492 logits_loss 119.926710\n",
      "[37:1000] F: 22296000 \t tot_loss 6.983343 rs_loss 0.199370 vs_loss 0.796791 logits_loss 119.743630\n",
      "[37:1100] F: 22305600 \t tot_loss 6.882649 rs_loss 0.130678 vs_loss 0.753953 logits_loss 119.960341\n",
      "[37:1200] F: 22315200 \t tot_loss 6.943399 rs_loss 0.143561 vs_loss 0.802566 logits_loss 119.945458\n",
      "[37:1300] F: 22324800 \t tot_loss 7.052357 rs_loss 0.166726 vs_loss 0.885141 logits_loss 120.009780\n",
      "[37:1400] F: 22334400 \t tot_loss 7.211520 rs_loss 0.206786 vs_loss 1.006052 logits_loss 119.973631\n",
      "[37:1500] F: 22344000 \t tot_loss 7.229870 rs_loss 0.225393 vs_loss 1.004123 logits_loss 120.007088\n",
      "[37:1600] F: 22353600 \t tot_loss 7.191693 rs_loss 0.212765 vs_loss 0.966710 logits_loss 120.244348\n",
      "[37:1700] F: 22363200 \t tot_loss 7.154598 rs_loss 0.231418 vs_loss 0.909262 logits_loss 120.278346\n",
      "[37:1800] F: 22372800 \t tot_loss 7.021727 rs_loss 0.203865 vs_loss 0.805947 logits_loss 120.238282\n",
      "[37:1900] F: 22382400 \t tot_loss 6.972482 rs_loss 0.202812 vs_loss 0.776961 logits_loss 119.854186\n",
      "[37:2000] F: 22392000 \t tot_loss 7.052718 rs_loss 0.239145 vs_loss 0.831734 logits_loss 119.636766\n",
      "[37:2100] F: 22401600 \t tot_loss 7.023282 rs_loss 0.221494 vs_loss 0.831119 logits_loss 119.413381\n",
      "[37:2200] F: 22411200 \t tot_loss 6.840128 rs_loss 0.161721 vs_loss 0.702545 logits_loss 119.517235\n",
      "[37:2300] F: 22420800 \t tot_loss 6.803024 rs_loss 0.144610 vs_loss 0.681229 logits_loss 119.543696\n",
      "[37:2400] F: 22430400 \t tot_loss 6.797251 rs_loss 0.122366 vs_loss 0.695009 logits_loss 119.597510\n",
      "[37:2500] F: 22440000 \t tot_loss 6.789770 rs_loss 0.117433 vs_loss 0.692372 logits_loss 119.599306\n",
      "[37:2600] F: 22449600 \t tot_loss 6.828451 rs_loss 0.135950 vs_loss 0.719224 logits_loss 119.465549\n",
      "[37:2700] F: 22459200 \t tot_loss 6.815019 rs_loss 0.130525 vs_loss 0.707316 logits_loss 119.543559\n",
      "[37:2800] F: 22468800 \t tot_loss 6.681943 rs_loss 0.111051 vs_loss 0.607093 logits_loss 119.275980\n",
      "[37:2900] F: 22478400 \t tot_loss 6.799034 rs_loss 0.137067 vs_loss 0.700041 logits_loss 119.238514\n",
      "[37:3000] F: 22488000 \t tot_loss 6.849352 rs_loss 0.143764 vs_loss 0.740405 logits_loss 119.303658\n",
      "[37:3100] F: 22497600 \t tot_loss 6.941304 rs_loss 0.175776 vs_loss 0.804034 logits_loss 119.229871\n",
      "[37:3200] F: 22507200 \t tot_loss 7.089169 rs_loss 0.204780 vs_loss 0.917641 logits_loss 119.334945\n",
      "[37:3300] F: 22516800 \t tot_loss 7.106691 rs_loss 0.202924 vs_loss 0.921177 logits_loss 119.651786\n",
      "[37:3400] F: 22526400 \t tot_loss 7.197185 rs_loss 0.225762 vs_loss 0.990423 logits_loss 119.619991\n",
      "[37:3500] F: 22536000 \t tot_loss 7.138602 rs_loss 0.208587 vs_loss 0.949725 logits_loss 119.605817\n",
      "[37:3600] F: 22545600 \t tot_loss 7.027589 rs_loss 0.183888 vs_loss 0.877289 logits_loss 119.328248\n",
      "[37:3700] F: 22555200 \t tot_loss 6.928240 rs_loss 0.153277 vs_loss 0.818571 logits_loss 119.127834\n",
      "[37:3800] F: 22564800 \t tot_loss 6.743799 rs_loss 0.100046 vs_loss 0.682881 logits_loss 119.217438\n",
      "[37:3900] F: 22574400 \t tot_loss 6.726634 rs_loss 0.092589 vs_loss 0.669907 logits_loss 119.282764\n",
      "[37:4000] F: 22584000 \t tot_loss 6.774128 rs_loss 0.107102 vs_loss 0.700061 logits_loss 119.339303\n",
      "[37:4100] F: 22593600 \t tot_loss 6.769507 rs_loss 0.115636 vs_loss 0.688453 logits_loss 119.308351\n",
      "[37:4200] F: 22603200 \t tot_loss 6.845348 rs_loss 0.147147 vs_loss 0.736606 logits_loss 119.231910\n",
      "[37:4300] F: 22612800 \t tot_loss 6.819076 rs_loss 0.155190 vs_loss 0.705034 logits_loss 119.177039\n",
      "[37:4400] F: 22622400 \t tot_loss 6.760739 rs_loss 0.136991 vs_loss 0.650980 logits_loss 119.455348\n",
      "[37:4500] F: 22632000 \t tot_loss 6.741459 rs_loss 0.128757 vs_loss 0.642894 logits_loss 119.396174\n",
      "[37:4600] F: 22641600 \t tot_loss 6.737397 rs_loss 0.124291 vs_loss 0.656737 logits_loss 119.127379\n",
      "[37:4700] F: 22651200 \t tot_loss 6.783819 rs_loss 0.110849 vs_loss 0.718361 logits_loss 119.092184\n",
      "[37:4800] F: 22660800 \t tot_loss 6.858774 rs_loss 0.121362 vs_loss 0.794250 logits_loss 118.863227\n",
      "[37:4900] F: 22670400 \t tot_loss 6.964760 rs_loss 0.152467 vs_loss 0.859804 logits_loss 119.049793\n",
      "[37:5000] F: 22680000 \t tot_loss 6.936651 rs_loss 0.154863 vs_loss 0.805116 logits_loss 119.533451\n",
      "[37:5100] F: 22689600 \t tot_loss 6.941660 rs_loss 0.155864 vs_loss 0.798378 logits_loss 119.748365\n",
      "[37:5200] F: 22699200 \t tot_loss 6.920235 rs_loss 0.164094 vs_loss 0.770059 logits_loss 119.721658\n",
      "[37:5300] F: 22708800 \t tot_loss 6.821301 rs_loss 0.137061 vs_loss 0.697152 logits_loss 119.741744\n",
      "[37:5400] F: 22718400 \t tot_loss 6.955307 rs_loss 0.165204 vs_loss 0.805999 logits_loss 119.682063\n",
      "[37:5500] F: 22728000 \t tot_loss 6.957043 rs_loss 0.190550 vs_loss 0.787974 logits_loss 119.570392\n",
      "[37:5600] F: 22737600 \t tot_loss 6.983272 rs_loss 0.189523 vs_loss 0.813149 logits_loss 119.612006\n",
      "[37:5700] F: 22747200 \t tot_loss 6.902620 rs_loss 0.169830 vs_loss 0.766515 logits_loss 119.325510\n",
      "[37:5800] F: 22756800 \t tot_loss 6.730270 rs_loss 0.116611 vs_loss 0.656154 logits_loss 119.150107\n",
      "[37:5900] F: 22766400 \t tot_loss 6.834651 rs_loss 0.126767 vs_loss 0.750995 logits_loss 119.137783\n",
      "[37:6000] F: 22776000 \t tot_loss 6.934521 rs_loss 0.161887 vs_loss 0.814223 logits_loss 119.168211\n",
      "[37:6100] F: 22785600 \t tot_loss 7.019149 rs_loss 0.172723 vs_loss 0.873765 logits_loss 119.453204\n",
      "[37:6200] F: 22795200 \t tot_loss 7.242012 rs_loss 0.239152 vs_loss 1.025710 logits_loss 119.542993\n",
      "Batch [38] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.14 (+-0.09) \n",
      "[38:0] F: 22800000 \t tot_loss 7.187868 rs_loss 0.251412 vs_loss 0.957795 logits_loss 119.573226\n",
      "[38:100] F: 22809600 \t tot_loss 7.107907 rs_loss 0.221206 vs_loss 0.901504 logits_loss 119.703927\n",
      "[38:200] F: 22819200 \t tot_loss 7.057912 rs_loss 0.192401 vs_loss 0.870052 logits_loss 119.909190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38:300] F: 22828800 \t tot_loss 7.029376 rs_loss 0.177557 vs_loss 0.837894 logits_loss 120.278486\n",
      "[38:400] F: 22838400 \t tot_loss 6.940324 rs_loss 0.126510 vs_loss 0.778196 logits_loss 120.712356\n",
      "[38:500] F: 22848000 \t tot_loss 6.875522 rs_loss 0.110033 vs_loss 0.720738 logits_loss 120.895004\n",
      "[38:600] F: 22857600 \t tot_loss 7.055283 rs_loss 0.163329 vs_loss 0.845901 logits_loss 120.921054\n",
      "[38:700] F: 22867200 \t tot_loss 7.131040 rs_loss 0.207897 vs_loss 0.884090 logits_loss 120.781075\n",
      "[38:800] F: 22876800 \t tot_loss 7.107753 rs_loss 0.241449 vs_loss 0.851239 logits_loss 120.301299\n",
      "[38:900] F: 22886400 \t tot_loss 7.240695 rs_loss 0.273804 vs_loss 0.965870 logits_loss 120.020440\n",
      "[38:1000] F: 22896000 \t tot_loss 7.121022 rs_loss 0.243919 vs_loss 0.891118 logits_loss 119.719717\n",
      "[38:1100] F: 22905600 \t tot_loss 7.081133 rs_loss 0.207176 vs_loss 0.900404 logits_loss 119.471046\n",
      "[38:1200] F: 22915200 \t tot_loss 7.139311 rs_loss 0.202520 vs_loss 0.960134 logits_loss 119.533145\n",
      "[38:1300] F: 22924800 \t tot_loss 7.037312 rs_loss 0.182739 vs_loss 0.871564 logits_loss 119.660178\n",
      "[38:1400] F: 22934400 \t tot_loss 6.966653 rs_loss 0.156232 vs_loss 0.822730 logits_loss 119.753823\n",
      "[38:1500] F: 22944000 \t tot_loss 6.893430 rs_loss 0.144392 vs_loss 0.766485 logits_loss 119.651058\n",
      "[38:1600] F: 22953600 \t tot_loss 6.703718 rs_loss 0.074437 vs_loss 0.649212 logits_loss 119.601393\n",
      "[38:1700] F: 22963200 \t tot_loss 6.744277 rs_loss 0.085563 vs_loss 0.684406 logits_loss 119.486154\n",
      "[38:1800] F: 22972800 \t tot_loss 6.967657 rs_loss 0.159992 vs_loss 0.824160 logits_loss 119.670087\n",
      "[38:1900] F: 22982400 \t tot_loss 6.959292 rs_loss 0.160408 vs_loss 0.801061 logits_loss 119.956457\n",
      "[38:2000] F: 22992000 \t tot_loss 6.981618 rs_loss 0.174916 vs_loss 0.804186 logits_loss 120.050332\n",
      "[38:2100] F: 23001600 \t tot_loss 6.832743 rs_loss 0.137314 vs_loss 0.712369 logits_loss 119.661202\n",
      "[38:2200] F: 23011200 \t tot_loss 6.541413 rs_loss 0.066023 vs_loss 0.513916 logits_loss 119.229487\n",
      "[38:2300] F: 23020800 \t tot_loss 6.631906 rs_loss 0.094443 vs_loss 0.582568 logits_loss 119.097892\n",
      "[38:2400] F: 23030400 \t tot_loss 6.634393 rs_loss 0.099848 vs_loss 0.577576 logits_loss 119.139381\n",
      "[38:2500] F: 23040000 \t tot_loss 6.935746 rs_loss 0.166483 vs_loss 0.800844 logits_loss 119.368385\n",
      "[38:2600] F: 23049600 \t tot_loss 6.995493 rs_loss 0.166994 vs_loss 0.837958 logits_loss 119.810811\n",
      "[38:2700] F: 23059200 \t tot_loss 6.918834 rs_loss 0.145056 vs_loss 0.791553 logits_loss 119.644491\n",
      "[38:2800] F: 23068800 \t tot_loss 6.861443 rs_loss 0.126082 vs_loss 0.758370 logits_loss 119.539819\n",
      "[38:2900] F: 23078400 \t tot_loss 6.571655 rs_loss 0.058518 vs_loss 0.533323 logits_loss 119.596291\n",
      "[38:3000] F: 23088000 \t tot_loss 6.545501 rs_loss 0.053743 vs_loss 0.525512 logits_loss 119.324913\n",
      "[38:3100] F: 23097600 \t tot_loss 6.851853 rs_loss 0.108147 vs_loss 0.761973 logits_loss 119.634662\n",
      "[38:3200] F: 23107200 \t tot_loss 7.058887 rs_loss 0.184758 vs_loss 0.886988 logits_loss 119.742812\n",
      "[38:3300] F: 23116800 \t tot_loss 7.073031 rs_loss 0.179024 vs_loss 0.905561 logits_loss 119.768905\n",
      "[38:3400] F: 23126400 \t tot_loss 7.287788 rs_loss 0.225417 vs_loss 1.067232 logits_loss 119.902778\n",
      "[38:3500] F: 23136000 \t tot_loss 7.012776 rs_loss 0.188398 vs_loss 0.842935 logits_loss 119.628877\n",
      "[38:3600] F: 23145600 \t tot_loss 6.895091 rs_loss 0.135783 vs_loss 0.786570 logits_loss 119.454763\n",
      "[38:3700] F: 23155200 \t tot_loss 6.974937 rs_loss 0.155448 vs_loss 0.845319 logits_loss 119.483403\n",
      "[38:3800] F: 23164800 \t tot_loss 6.842371 rs_loss 0.131675 vs_loss 0.732217 logits_loss 119.569578\n",
      "[38:3900] F: 23174400 \t tot_loss 6.821645 rs_loss 0.126882 vs_loss 0.705517 logits_loss 119.784934\n",
      "[38:4000] F: 23184000 \t tot_loss 6.737818 rs_loss 0.104950 vs_loss 0.635045 logits_loss 119.956467\n",
      "[38:4100] F: 23193600 \t tot_loss 6.734000 rs_loss 0.111499 vs_loss 0.631795 logits_loss 119.814131\n",
      "[38:4200] F: 23203200 \t tot_loss 6.659950 rs_loss 0.093853 vs_loss 0.593436 logits_loss 119.453240\n",
      "[38:4300] F: 23212800 \t tot_loss 6.746277 rs_loss 0.120016 vs_loss 0.664795 logits_loss 119.229311\n",
      "[38:4400] F: 23222400 \t tot_loss 6.877702 rs_loss 0.167318 vs_loss 0.748451 logits_loss 119.238675\n",
      "[38:4500] F: 23232000 \t tot_loss 6.827094 rs_loss 0.164920 vs_loss 0.709461 logits_loss 119.054256\n",
      "[38:4600] F: 23241600 \t tot_loss 6.856855 rs_loss 0.187565 vs_loss 0.713733 logits_loss 119.111138\n",
      "[38:4700] F: 23251200 \t tot_loss 6.734884 rs_loss 0.141138 vs_loss 0.631667 logits_loss 119.241585\n",
      "[38:4800] F: 23260800 \t tot_loss 6.611894 rs_loss 0.097486 vs_loss 0.562711 logits_loss 119.033942\n",
      "[38:4900] F: 23270400 \t tot_loss 6.580910 rs_loss 0.079634 vs_loss 0.530171 logits_loss 119.422097\n",
      "[38:5000] F: 23280000 \t tot_loss 6.549367 rs_loss 0.058915 vs_loss 0.518467 logits_loss 119.439692\n",
      "[38:5100] F: 23289600 \t tot_loss 6.571104 rs_loss 0.059886 vs_loss 0.544424 logits_loss 119.335877\n",
      "[38:5200] F: 23299200 \t tot_loss 6.546752 rs_loss 0.050170 vs_loss 0.525957 logits_loss 119.412501\n",
      "[38:5300] F: 23308800 \t tot_loss 6.562182 rs_loss 0.068092 vs_loss 0.537248 logits_loss 119.136824\n",
      "[38:5400] F: 23318400 \t tot_loss 6.552015 rs_loss 0.063638 vs_loss 0.530311 logits_loss 119.161326\n",
      "[38:5500] F: 23328000 \t tot_loss 6.601868 rs_loss 0.079367 vs_loss 0.565655 logits_loss 119.136922\n",
      "[38:5600] F: 23337600 \t tot_loss 6.712620 rs_loss 0.102031 vs_loss 0.648140 logits_loss 119.248990\n",
      "[38:5700] F: 23347200 \t tot_loss 6.678364 rs_loss 0.084917 vs_loss 0.625675 logits_loss 119.355439\n",
      "[38:5800] F: 23356800 \t tot_loss 6.693331 rs_loss 0.083484 vs_loss 0.629938 logits_loss 119.598183\n",
      "[38:5900] F: 23366400 \t tot_loss 6.619671 rs_loss 0.069381 vs_loss 0.561053 logits_loss 119.784751\n",
      "[38:6000] F: 23376000 \t tot_loss 6.553655 rs_loss 0.073345 vs_loss 0.500976 logits_loss 119.586674\n",
      "[38:6100] F: 23385600 \t tot_loss 6.736852 rs_loss 0.133677 vs_loss 0.618242 logits_loss 119.698651\n",
      "[38:6200] F: 23395200 \t tot_loss 7.007246 rs_loss 0.209164 vs_loss 0.815260 logits_loss 119.656432\n",
      "Batch [39] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.06 (+-0.16) \n",
      "[39:0] F: 23400000 \t tot_loss 7.069001 rs_loss 0.234591 vs_loss 0.843739 logits_loss 119.813422\n",
      "[39:100] F: 23409600 \t tot_loss 7.164013 rs_loss 0.253475 vs_loss 0.911804 logits_loss 119.974676\n",
      "[39:200] F: 23419200 \t tot_loss 7.147135 rs_loss 0.242440 vs_loss 0.896164 logits_loss 120.170630\n",
      "[39:300] F: 23428800 \t tot_loss 7.065635 rs_loss 0.222474 vs_loss 0.847478 logits_loss 119.913649\n",
      "[39:400] F: 23438400 \t tot_loss 6.968600 rs_loss 0.172530 vs_loss 0.811970 logits_loss 119.681995\n",
      "[39:500] F: 23448000 \t tot_loss 6.903558 rs_loss 0.152397 vs_loss 0.771322 logits_loss 119.596760\n",
      "[39:600] F: 23457600 \t tot_loss 6.871963 rs_loss 0.145526 vs_loss 0.756825 logits_loss 119.392236\n",
      "[39:700] F: 23467200 \t tot_loss 6.809933 rs_loss 0.144265 vs_loss 0.679073 logits_loss 119.731891\n",
      "[39:800] F: 23476800 \t tot_loss 6.785538 rs_loss 0.143208 vs_loss 0.661078 logits_loss 119.625048\n",
      "[39:900] F: 23486400 \t tot_loss 6.973454 rs_loss 0.177342 vs_loss 0.809685 logits_loss 119.728534\n",
      "[39:1000] F: 23496000 \t tot_loss 6.989971 rs_loss 0.174062 vs_loss 0.844425 logits_loss 119.429683\n",
      "[39:1100] F: 23505600 \t tot_loss 7.015373 rs_loss 0.148700 vs_loss 0.896692 logits_loss 119.399610\n",
      "[39:1200] F: 23515200 \t tot_loss 7.092203 rs_loss 0.181383 vs_loss 0.921433 logits_loss 119.787733\n",
      "[39:1300] F: 23524800 \t tot_loss 6.976531 rs_loss 0.161857 vs_loss 0.828864 logits_loss 119.716197\n",
      "[39:1400] F: 23534400 \t tot_loss 6.930525 rs_loss 0.140154 vs_loss 0.779837 logits_loss 120.210678\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25704/819305019.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 tuple(np.average(l) for l in loss_stats)))\n\u001b[1;32m     46\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtot_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfree_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp = 5.\n",
    "\n",
    "# Load the preset policy\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_2.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"])   \n",
    "net.train(False)\n",
    "net.share_memory()\n",
    "\n",
    "# Get the actors to write on the buffer\n",
    "\n",
    "actor_processes = []\n",
    "free_queue = mp.SimpleQueue()\n",
    "loss_stats = [deque(maxlen=400) for _ in range(4)]\n",
    "\n",
    "net.train(False)\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)\n",
    "\n",
    "# Start training loop    \n",
    "\n",
    "model.train(True)\n",
    "for epoch in range(flags.tot_epoch):    \n",
    "    print(\"Batch [%d] starts\" % epoch)\n",
    "    while(not free_queue.empty()): time.sleep(1)\n",
    "    for step in range(tot_step):\n",
    "        if step == 0: \n",
    "            test_n_step_model(1, model, flags, eps_n=100, temp=temp)\n",
    "            model.train(True)\n",
    "        \n",
    "        batch = get_batch_m(flags, buffers)\n",
    "        rs_loss, vs_loss, logits_loss = compute_loss_m(model, batch)\n",
    "        tot_loss = rs_loss + vs_loss + 0.05 * logits_loss\n",
    "        for n, l in enumerate([tot_loss, rs_loss, vs_loss, logits_loss]):\n",
    "            loss_stats[n].append(l.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"[%d:%d] F: %d \\t tot_loss %f rs_loss %f vs_loss %f logits_loss %f\" % ((\n",
    "                epoch, step, (step + epoch * tot_step) * flags.bsz * flags.unroll_len,) +\n",
    "                tuple(np.average(l) for l in loss_stats)))\n",
    "        optimizer.zero_grad()\n",
    "        tot_loss.backward()\n",
    "        optimizer.step()    \n",
    "    for m in range(flags.seq_n): free_queue.put(m)\n",
    "        \n",
    "for _ in range(flags.num_actors): free_queue.put(None)        \n",
    "for actor in actor_processes: actor.join(timeout=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5319f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab596f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_queue.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ec2114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the threads\n",
    "\n",
    "for _ in range(flags.num_actors): free_queue.put(None)  \n",
    "for actor in actor_processes: actor.join(timeout=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc08045",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model testing / debug </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6e9773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict(),},\"../models/model_3.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d28e7cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())         \n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.discounting = 0.97\n",
    "flags.device = torch.device(\"cuda\")\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/model_3.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8834194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 step planning\n",
      "Finish 500 episode: avg. return: 0.38 (+-0.09) \n",
      "Time required for 1 step planning: 57.777408\n",
      "Testing 2 step planning\n",
      "Finish 500 episode: avg. return: 0.30 (+-0.08) \n",
      "Time required for 2 step planning: 124.808867\n",
      "Testing 3 step planning\n",
      "Finish 500 episode: avg. return: -0.23 (+-0.04) \n",
      "Time required for 3 step planning: 524.462036\n"
     ]
    }
   ],
   "source": [
    "all_returns = {}\n",
    "for n in range(1,4):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step_model(n, model, flags, eps_n=500, temp=20)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "773ea44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG:25704 pyplot:296 2022-11-05 08:45:13,666] Loaded backend module://matplotlib_inline.backend_inline version unknown.\n",
      "[DEBUG:25704 pyplot:296 2022-11-05 08:45:13,668] Loaded backend module://matplotlib_inline.backend_inline version unknown.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGgCAYAAADsNrNZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAooElEQVR4nO3df3RU9Z3/8VcCySQImciv/NgmQLfUoAhq0DCC3SppOay6smRdrXTFSmVlA+XH9lTDVm3ZltDt9wh2N2C1bKC1bLbsFtSywmJUBDaARLGwasTKkbSYoLvNTPiRH5LP9w/XsTO5wJ3JJJ874fk4Z85x7r3zvu8MM3l5cz/3c1OMMUYAAPSxVNsNAAAuTgQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMCKXgugqqoqjR49WhkZGSopKdH+/ft7a1cAgCSU0htzwf3rv/6r7r77bj3++OMqKSnR6tWrtWnTJjU0NGjkyJHnfW1XV5eOHz+uIUOGKCUlJdGtAQB6mTFGra2tys/PV2rqeY5zTC+47rrrTHl5efj52bNnTX5+vqmsrLzgaxsbG40kHjx48OCR5I/Gxsbz/r4fqATr6OhQfX29KioqwstSU1NVWlqqurq6btu3t7ervb09/Nz83wHZT66VBp2vuyundV92qDa+pqNrJapOImvRk7ta9OSuFj25q0VP7mpF1Tn9kfT1V6QhQ4act0zCA+jDDz/U2bNnlZOTE7E8JydHb731VrftKysr9d3vfrfb8kEDLxBA6Q4r4/1pomslqk4ia9GTu1r05K4WPbmrRU/uap2jzoVOo1gfBVdRUaFgMBh+NDY22m4JANAHEn4ENHz4cA0YMEDNzc0Ry5ubm5Wbm9tte5/PJ5/Pl+g2AAAel/AjoPT0dBUXF6u29tO/CXZ1dam2tlaBQCDRuwMAJKmEHwFJ0tKlSzVnzhxNmjRJ1113nVavXq1Tp07pa1/7Wm/sDgCQhHolgO644w598MEHevjhh9XU1KSrrrpK27Zt6zYwAQBw8eqVC1F7IhQKye/3a+NjKzQoM8N2OwCAGJ0+06a7Fi1TMBhUVlbWObezPgoOAHBxIoAAAFYQQAAAKwggAIAVvTIKLiFqlkV2N3F65PrXt8dXN7pOImvRk7ta9OSuFj25q0VP7mr1ZU8fuSvNERAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVzIYNAEgoZsMGAHgaAQQAsIIAAgBYQQABAKxIntmwo/WXWWPd1qInd7XoyV0tenJXi57c1Yquw2zYAAAvI4AAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWBFzAL388su69dZblZ+fr5SUFG3ZsiVivTFGDz/8sPLy8pSZmanS0lIdOXIkUf0CAPqJFGOMieUFzz33nPbs2aPi4mLNmjVLmzdv1syZM8Prf/CDH6iyslIbNmzQmDFj9NBDD+nQoUN64403lJGRccH6oVBIfr9fGx9boUGZF94eAOAtp8+06a5FyxQMBpWVlXXO7WK+JfeMGTM0Y8YMx3XGGK1evVrf/va3ddttt0mSfvrTnyonJ0dbtmzRnXfeGevuAAD9VELPAR09elRNTU0qLS0NL/P7/SopKVFdXZ3ja9rb2xUKhSIeAID+L6EB1NTUJEnKycmJWJ6TkxNeF62yslJ+vz/8KCgoSGRLAACPsj4KrqKiQsFgMPxobGy03RIAoA/EfA7ofHJzcyVJzc3NysvLCy9vbm7WVVdd5fgan88nn8/XfUXNssjuJk6PXP/69viajK6TyFr05K4WPbmrRU/uatGTu1p92dNH7kon9AhozJgxys3NVW1tbXhZKBTSvn37FAgEErkrAECSi/kI6OTJk3rnnXfCz48ePaqDBw9q6NChKiws1OLFi/W9731PY8eODQ/Dzs/PjxiqDQBAzAF04MAB3XjjjeHnS5culSTNmTNH69ev17e+9S2dOnVK8+bNU0tLi6ZOnapt27a5ugYIAHDxiDmAvvjFL+p8166mpKRo+fLlWr58eY8aAwD0b9ZHwQEALk4EEADACgIIAGAFAQQAsCLm2bB7G7NhA0ByczsbNkdAAAArCCAAgBUEEADACgIIAGBFQmfDTqjo2bCj9ZdZY93Woid3tejJXS16cleLntzViq5jYzZsAADcIoAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsSDHGGNtN/KFQKCS/36+Nj63QoMwM2+0AAGJ0+kyb7lq0TMFgUFlZWefcjiMgAIAVBBAAwAoCCABgBQEEALBioO0GzqlmWWR3E6dHrn99e3x1o+skshY9uatFT+5q0ZO7WvTkrlZf9vSRu9IcAQEArCCAAABWxBRAlZWVuvbaazVkyBCNHDlSM2fOVENDQ8Q2bW1tKi8v17BhwzR48GCVlZWpubk5oU0DAJJfTAG0c+dOlZeXa+/evdqxY4c6Ozv15S9/WadOnQpvs2TJEj377LPatGmTdu7cqePHj2vWrFkJbxwAkNxiGoSwbdu2iOfr16/XyJEjVV9fry984QsKBoNat26dNm7cqJtuukmSVF1drXHjxmnv3r2aPHly4joHACS1Hp0DCgaDkqShQ4dKkurr69XZ2anS0tLwNkVFRSosLFRdXZ1jjfb2doVCoYgHAKD/izuAurq6tHjxYk2ZMkXjx4+XJDU1NSk9PV3Z2dkR2+bk5KipqcmxTmVlpfx+f/hRUFAQb0sAgCQSdwCVl5fr8OHDqqmp6VEDFRUVCgaD4UdjY2OP6gEAkkNcs2EvWLBATz/9tF5++WWNGTMmvPyFF17QtGnT9Pvf/z7iKGjUqFFavHixlixZcsHazIYNAMmtV2bDNsZowYIF2rx5s1544YWI8JGk4uJipaWlqba2NrysoaFBx44dUyAQiPFHAAD0ZzGNgisvL9fGjRv19NNPa8iQIeHzOn6/X5mZmfL7/Zo7d66WLl2qoUOHKisrSwsXLlQgEGAEHAAgQkwBtHbtWknSF7/4xYjl1dXVuueeeyRJq1atUmpqqsrKytTe3q7p06drzZo1CWkWANB/xBRAbk4XZWRkqKqqSlVVVXE3BQDo/5JnNuxo/WXWWLe16MldLXpyV4ue3NWiJ3e1ouswGzYAwMsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACtSjJv7bPehUCgkv9+vjY+t0KDMDNvtAABidPpMm+5atEzBYFBZWVnn3I4jIACAFQQQAMAKAggAYAUBBACwYqDtBs6pZllkdxOnR65/fXt8daPrJLIWPbmrRU/uatGTu1r05K5WX/b0kbvSHAEBAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYwWzYAICEYjZsAICnEUAAACsIIACAFTEF0Nq1azVhwgRlZWUpKytLgUBAzz33XHh9W1ubysvLNWzYMA0ePFhlZWVqbm5OeNMAgOQX0yCEZ599VgMGDNDYsWNljNGGDRv0wx/+UK+99pquuOIKzZ8/X1u3btX69evl9/u1YMECpaamas+ePa4bCg9CCEiDzjdXd3+ZNdZtLXpyV4ue3NWiJ3e16Mldrag6pz+S7qrTBQchxHQ7hltvvTXi+fe//32tXbtWe/fu1Wc+8xmtW7dOGzdu1E033SRJqq6u1rhx47R3715Nnjw5ll0BAPq5uM8BnT17VjU1NTp16pQCgYDq6+vV2dmp0tLS8DZFRUUqLCxUXV3dOeu0t7crFApFPAAA/V/MAXTo0CENHjxYPp9P999/vzZv3qzLL79cTU1NSk9PV3Z2dsT2OTk5ampqOme9yspK+f3+8KOgoCDmHwIAkHxiDqDLLrtMBw8e1L59+zR//nzNmTNHb7zxRtwNVFRUKBgMhh+NjY1x1wIAJI+Yb8mdnp6uz33uc5Kk4uJivfLKK3rsscd0xx13qKOjQy0tLRFHQc3NzcrNzT1nPZ/PJ5/PF3vnAICk1uPrgLq6utTe3q7i4mKlpaWptrY2vK6hoUHHjh1TIBDo6W4AAP1MTEdAFRUVmjFjhgoLC9Xa2qqNGzfqpZde0vbt2+X3+zV37lwtXbpUQ4cOVVZWlhYuXKhAIMAIOABANzEF0IkTJ3T33Xfr/fffl9/v14QJE7R9+3Z96UtfkiStWrVKqampKisrU3t7u6ZPn641a9b0SuMAgOQWUwCtW7fuvOszMjJUVVWlqqqqHjUFJNLMXS422hXnBXm9WauPe5qant59WVpa5IJXdzu88pL4eoqqtbuz+/6/WdIRX20kBeaCAwBYQQABAKwggAAAVhBAAAArYr4QFUD/1G3AgaSVp0712f4fvMRpMAODEPozjoAAAFYQQAAAKwggAIAVMd0RtS+E74j62AoNysyw3Q76gZnzltpuISk4nYNxcw5oatRzp0tV493/5FV/H2c12HT6TJvuWrTsgndE5QgIAGAFAQQAsIIAAgBYQQABAKzw7oWoNcsiu5s4PXL963HOFBxdJ5G16MldLS/0hLhEDziQpK3Dh0c839Xa2m2bW9rb49vhzxwGkHjx80RPkT5yV5ojIACAFQQQAMAKAggAYIV3zwEB8Byni0yjz/msjPd8Dy46HAEBAKwggAAAVhBAAAArCCAAgBXMho1+j9mw3Yl3Nuze3D+zYScnZsMGAHgaAQQAsIIAAgBYQQABAKzw7kwI0bNhR+svs8a6rUVP7moxO3b/4jQbdjQvfp4u9p6YDRsA4GUEEADACgIIAGCFd88BAehTuzs7uy1zuji0L/c/uc/2Dhs4AgIAWEEAAQCsIIAAAFb0KIBWrlyplJQULV68OLysra1N5eXlGjZsmAYPHqyysjI1Nzf3tE8AQD8T9yCEV155RT/+8Y81YcKEiOVLlizR1q1btWnTJvn9fi1YsECzZs3Snj17etysTTN3udhoVwIvgkxULXrypDf/2mFhetTzCQ7bxL3DqOdt3TcZt6aj27LdHd2X9aUnL3dY2O19cvgMTIxzh29G1bqy+ybj1rio08ef8S03JG53fSmuI6CTJ09q9uzZevLJJ3XppZeGlweDQa1bt06PPvqobrrpJhUXF6u6ulr/9V//pb179yasaQBA8osrgMrLy3XzzTertLQ0Ynl9fb06OzsjlhcVFamwsFB1dXWOtdrb2xUKhSIeAID+L+Y/wdXU1OjVV1/VK6+80m1dU1OT0tPTlZ2dHbE8JydHTU1NjvUqKyv13e9+N9Y2AABJLqYjoMbGRi1atEg///nPlZGRmLuVVlRUKBgMhh+NjY0JqQsA8LaYjoDq6+t14sQJXXPNNeFlZ8+e1csvv6x/+qd/0vbt29XR0aGWlpaIo6Dm5mbl5uY61vT5fPL5fPF1DyQrp/9/i/6KDHXYJifO/UUPRB0QZ52+xvvUr8UUQNOmTdOhQ4ciln3ta19TUVGRHnjgARUUFCgtLU21tbUqKyuTJDU0NOjYsWMKBAKJ6xoAkPRiCqAhQ4Zo/PjxEcsuueQSDRs2LLx87ty5Wrp0qYYOHaqsrCwtXLhQgUBAkyczqxMA4FMJn4x01apVSk1NVVlZmdrb2zV9+nStWeNm4DwA4GKSYowxtpv4Q6FQSH6/XxsfW6FBmYkZ6JAIM+e5uDMj4NKbTzosHBH1/EWHbW6Mej7cYRuni6bHRT3/wGGT+xxeZxnvkztbnnjUdgsRTp9p012LlikYDCorK+uc2zEXHADACgIIAGAFAQQAsIIAAgBY4d1bctcsi+xu4vTI9a/HOdtsdJ2e1AISKfpE+Rcctqm9wGsk55mgo7dzOLmeNHifuvuZwyApm78zP3JXmiMgAIAVBBAAwAoCCABghXfPAQEXu3gn1LzY8D4lLY6AAABWEEAAACsIIACAFQQQAMAK7w5CuHOFdL7ZsCc4XBwVLze1djEbNhLoQ4dl0XfjfNthm2kuaruZHTpZ8D6581cuZsPuy9+ZZ9qkumUXLMMREADACgIIAGAFAQQAsIIAAgBY4d1BCNGzYUdL5KzWiZo1FnCryWHZ8ajnTifSnWZ1juY0O7TTCfdkwPvkjtNs2NH68ncms2EDALyMAAIAWEEAAQCs8O45IKA/a3NYFn3dtdPdOBN1h06n/XsR71O/xhEQAMAKAggAYAUBBACwggACAFjBIIQkNjU9vfuytLQ+2//uzs7uyzo6+mz/br351w4Lo9+6CYncYdRzhxPZ49YkcH/92Lgf2+6gOy9+7yTvfe/c4AgIAGAFAQQAsIIAAgBYQQABAKxgEEISczrxufLUqT7b/4OXXNJtmRcHIXS7cl6ScqOeD3XYJifO/UXfMnpAnHXgSV783jEIAQCAGBBAAAArYgqg73znO0pJSYl4FBUVhde3tbWpvLxcw4YN0+DBg1VWVqbm5ui/RwAAEMc5oCuuuELPP//8pwUGflpiyZIl2rp1qzZt2iS/368FCxZo1qxZ2rNnT+yd3blCynT64/15THC44188nOrscnHHwSQxNer5bitd9KHxDstGRD13uhPmjVHPne6yucvF/hI1MzOSWq9+7/7q0fhe11u/M8+0SXXLLviymANo4MCBys2NPoMrBYNBrVu3Ths3btRNN90kSaqurta4ceO0d+9eTZ48OdZdAQD6sZjPAR05ckT5+fn67Gc/q9mzZ+vYsWOSpPr6enV2dqq0tDS8bVFRkQoLC1VXV3fOeu3t7QqFQhEPAED/F1MAlZSUaP369dq2bZvWrl2ro0eP6oYbblBra6uampqUnp6u7OzsiNfk5OSoqanpnDUrKyvl9/vDj4KCgrh+EABAconpT3AzZswI//eECRNUUlKiUaNG6Re/+IUyMzPjaqCiokJLl356fiUUChFCAHAR6NGFqNnZ2fr85z+vd955R1/60pfU0dGhlpaWiKOg5uZmx3NGn/D5fPL5fN1X1CyL7G5i1Emu17fH13R0nZ7USgLRJz4laevwyLPpu1pbu21zS3t7L3XkEdEDCr7gsE3tBV4jSRNd1GYQwkWnz793P3MYJGXzd+ZH7kr36DqgkydP6je/+Y3y8vJUXFystLQ01dZ++q1taGjQsWPHFAgEerIbAEA/FNMR0De/+U3deuutGjVqlI4fP65HHnlEAwYM0Fe+8hX5/X7NnTtXS5cu1dChQ5WVlaWFCxcqEAgwAg4A0E1MAfTb3/5WX/nKV/Q///M/GjFihKZOnaq9e/dqxIiPL6pYtWqVUlNTVVZWpvb2dk2fPl1r1nDnLQBAdzEFUE1NzXnXZ2RkqKqqSlVVVT1qConjdLFb9N+eV/b38z1uxDvxKOCA7507zAUHALCCAAIAWEEAAQCsIIAAAFZ4946oF5oNO1GzuLqt1Y9mw+73F5lG+9BhWfRdQt522Gaai9puZtEG1MvfOzezYffl70yXs2FzBAQAsIIAAgBYQQABAKwggAAAVnh3EEL0bNjREjmrdaJmjYU3Od2O6njUc6cBB06zX0dzmkXbaWAC0JucZsOO1pe/M/tiNmwAAOJFAAEArCCAAABWePccEC5od2dnt2UPXnKJ1f17UpvDsuhrnJ3uWpqoO5k67R9Jy4vfu2S94xpHQAAAKwggAIAVBBAAwAoCCABgBYMQktg3SzoclkYt68WLzyY71Nm9K77Svem+6vRuy6ampfXZ/p0Hazj929m15QYXG/XxBeCXXe7wuuh/zgnx7d7Rm1HPHQaQjFvT/d9ud4fdf89vWt17/DgCAgBYQQABAKwggAAAVhBAAAArGISAfs9pwMHKU6f6bP9OV8nbPmmdNKJnrJCk3KjnQx22yYlzf9G3ah8QZx24whEQAMAKAggAYAUBBACwwrvngO5cIWU6/QH4PCY4XCQXD6c6u1zccbCv/dWj8b3uYnuf4jQ16vluK130IS9+nlIcPk8jop473YH2xqjnTne3dbpoenzU80TNiN7bvPZvd6ZNqlt2wZdxBAQAsIIAAgBYQQABAKwggAAAVnh3EELNssjuXMyc60oiZ/O17WcOJ2h5n+ISPeBAkrYOjzxzvau1tds2t7S391JHFnjx83S3w7LoAQVfcNim9gKvkaSJLmonyyAEr/3bfeSuNEdAAAArCCAAgBUxB9Dvfvc7ffWrX9WwYcOUmZmpK6+8UgcOHAivN8bo4YcfVl5enjIzM1VaWqojR44ktGkAQPKL6RzQ73//e02ZMkU33nijnnvuOY0YMUJHjhzRpZdeGt7mH/7hH/SjH/1IGzZs0JgxY/TQQw9p+vTpeuONN5SREeOFpUAfcbrINPqcz8r+dL6nP4l34lFYF1MA/eAHP1BBQYGqq6vDy8aMGRP+b2OMVq9erW9/+9u67bbbJEk//elPlZOToy1btujOO+9MUNsAgGQX05/gnnnmGU2aNEm33367Ro4cqauvvlpPPvlkeP3Ro0fV1NSk0tLS8DK/36+SkhLV1dU51mxvb1coFIp4AAD6v5gC6N1339XatWs1duxYbd++XfPnz9c3vvENbdiwQZLU1NQkScrJiTwmzsnJCa+LVllZKb/fH34UFBTE83MAAJJMTAHU1dWla665RitWrNDVV1+tefPm6b777tPjjz8edwMVFRUKBoPhR2NjY9y1AADJI6ZzQHl5ebr88ssjlo0bN07//u//LknKzf34VoXNzc3Ky8sLb9Pc3KyrrrrKsabP55PP5+u+4kKzYSdqFle3tbw4y7ObGXB5n+LWry4ydcOLn6cPHT5P0XctfdvhddNc7N/NLNrJwmv/dr0xG/aUKVPU0NAQseztt9/WqFGjJH08ICE3N1e1tZ9ehhwKhbRv3z4FAoFYdgUA6OdiOgJasmSJrr/+eq1YsUJ/+Zd/qf379+uJJ57QE088IUlKSUnR4sWL9b3vfU9jx44ND8POz8/XzJkze6N/AECSiimArr32Wm3evFkVFRVavny5xowZo9WrV2v27Nnhbb71rW/p1KlTmjdvnlpaWjR16lRt27aNa4AAABFinoz0lltu0S233HLO9SkpKVq+fLmWL1/eo8YAAP1b8syGHS2RszUnatbYvuY0A2403ie45cXP0zUOrzse9dxpwIHT7NfRnGbRdhqYkAy89m/HbNgAAC8jgAAAVhBAAAArvHsOCEiQ3Z2d3ZY9eMklVvcPl9oclkUPqHW6a2mi7mTqtH8kDEdAAAArCCAAgBUEEADACgIIAGAFgxBc2nKDi4246NOj71PHhZf1Yk+THers3tX9ZSPuGxLx/NSr3fs+XX/hGboHFXefXf6Sa9Ijnn/wZGu3bWY69NTNrgR+Dt3UctMTkhZHQAAAKwggAIAVBBAAwAoCCABgBYMQAI+IHhgwbn9+wmq/eV30FNKAfRwBAQCsIIAAAFYQQAAAK1KMMcZ2E38oFArJ7/dr42MrNCgzetpboH+YOa/7HSxHPR55G083F526FX1x6nv3f5iw2rBvyxOP2m4hwukzbbpr0TIFg0FlZWWdczuOgAAAVhBAAAArCCAAgBUEEADACu9eiFqzLLK7RM0O7cUZq+kpvjqJrNXHPUUPOJCkQVEzVkc/TySn/TMwIYn9rPugFquf8Y/cleYICABgBQEEALCCAAIAWOHdc0BAP9ab53eSYf+AxBEQAMASAggAYAUBBACwggACAFjBbNiABRVX/T/bLXTDXVOTF7NhAwAQAwIIAGBFTAE0evRopaSkdHuUl5dLktra2lReXq5hw4Zp8ODBKisrU3Nzc680DgBIbjEF0CuvvKL3338//NixY4ck6fbbb5ckLVmyRM8++6w2bdqknTt36vjx45o1a1biuwYAJL0eDUJYvHixfvWrX+nIkSMKhUIaMWKENm7cqL/4i7+QJL311lsaN26c6urqNHnyZFc1w4MQAtKg883T0I9mRnZVi57c1UqSnmbu6v6yEfcNiXgefRttyd1tut287oMnWy9YB8ljyw0uNurDz/jpj6S76tR7gxA6Ojr01FNP6d5771VKSorq6+vV2dmp0tLS8DZFRUUqLCxUXV3dOeu0t7crFApFPAAA/V/cAbRlyxa1tLTonnvukSQ1NTUpPT1d2dnZEdvl5OSoqanpnHUqKyvl9/vDj4KCgnhbAgAkkbgDaN26dZoxY4by8/N71EBFRYWCwWD40djY2KN6AIDkENds2O+9956ef/55/fKXvwwvy83NVUdHh1paWiKOgpqbm5Wbm3vOWj6fTz5f979ZAxeb6PMyI1xs4yTe1wF9La4joOrqao0cOVI333xzeFlxcbHS0tJUW1sbXtbQ0KBjx44pEAj0vFMAQL8S8xFQV1eXqqurNWfOHA0c+OnL/X6/5s6dq6VLl2ro0KHKysrSwoULFQgEXI+AAwBcPGIOoOeff17Hjh3Tvffe223dqlWrlJqaqrKyMrW3t2v69Olas2ZNQhoFAPQvMQfQl7/8ZZ3r0qGMjAxVVVWpqqqqx40BAPo3bskNWODqwkHf9d2X3eDiwsG3HAYcLEjOC3bjqpPIWv2pJw9iMlIAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArenRH1N4QviPqYys0KDPDdjsAgBidPtOmuxYt6707ogIA0BMEEADACgIIAGAFAQQAsMK7s2HXLIvsrj/PUktP8dVJZC16cleLntzVuth7+shdaY6AAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArGA2bABAQjEbNgDA0wggAIAVBBAAwAoCCABgRfLMhh2tv8wa67YWPbmrRU/uatGTu1r05K5WdB1mwwYAeBkBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFZ67DuiTuVFPX2gceYfDBi7Hnl+wVqLqJLIWPbmrRU/uatGTu1r05K5W1NNPfn9faK5rz82G/dvf/lYFBQW22wAA9FBjY6M+85nPnHO95wKoq6tLx48f15AhQ9Ta2qqCggI1Njaed0pvrwmFQvTdh+i77yVr7/TdN4wxam1tVX5+vlJTz32mx3N/gktNTQ0nZkpKiiQpKysrKd70aPTdt+i77yVr7/Td+/x+/wW3YRACAMAKAggAYIWnA8jn8+mRRx6Rz+ez3UpM6Ltv0XffS9be6dtbPDcIAQBwcfD0ERAAoP8igAAAVhBAAAArCCAAgBUEEADACs8GUFVVlUaPHq2MjAyVlJRo//79tlvq5uWXX9att96q/Px8paSkaMuWLRHrjTF6+OGHlZeXp8zMTJWWlurIkSN2mv0/lZWVuvbaazVkyBCNHDlSM2fOVENDQ8Q2bW1tKi8v17BhwzR48GCVlZWpubnZUsefWrt2rSZMmBC+GjwQCOi5554Lr/dq339o5cqVSklJ0eLFi8PLvNr3d77zHaWkpEQ8ioqKwuu92rck/e53v9NXv/pVDRs2TJmZmbryyit14MCB8HovfjdHjx7d7f1OSUlReXm5JG+/33EzHlRTU2PS09PNP//zP5v//u//Nvfdd5/Jzs42zc3NtluL8B//8R/m7/7u78wvf/lLI8ls3rw5Yv3KlSuN3+83W7ZsMa+//rr5sz/7MzNmzBhz5swZOw0bY6ZPn26qq6vN4cOHzcGDB82f/umfmsLCQnPy5MnwNvfff78pKCgwtbW15sCBA2by5Mnm+uuvt9bzJ5555hmzdetW8/bbb5uGhgazbNkyk5aWZg4fPmyM8W7fn9i/f78ZPXq0mTBhglm0aFF4uVf7fuSRR8wVV1xh3n///fDjgw8+CK/3at//+7//a0aNGmXuueces2/fPvPuu++a7du3m3feeSe8jRe/mydOnIh4r3fs2GEkmRdffNEY4933uyc8GUDXXXedKS8vDz8/e/asyc/PN5WVlRa7Or/oAOrq6jK5ubnmhz/8YXhZS0uL8fl85l/+5V8sdOjsxIkTRpLZuXOnMebjHtPS0symTZvC27z55ptGkqmrq7PV5jldeuml5ic/+Ynn+25tbTVjx441O3bsMH/yJ38SDiAv9/3II4+YiRMnOq7zct8PPPCAmTp16jnXJ8t3c9GiReaP//iPTVdXl6ff757w3J/gOjo6VF9fr9LS0vCy1NRUlZaWqq6uzmJnsTl69Kiampoifg6/36+SkhJP/RzBYFCSNHToUElSfX29Ojs7I/ouKipSYWGhp/o+e/asampqdOrUKQUCAc/3XV5erptvvjmiP8n77/eRI0eUn5+vz372s5o9e7aOHTsmydt9P/PMM5o0aZJuv/12jRw5UldffbWefPLJ8Ppk+G52dHToqaee0r333quUlBRPv9894bkA+vDDD3X27Fnl5ORELM/JyVFTU5OlrmL3Sa9e/jm6urq0ePFiTZkyRePHj5f0cd/p6enKzs6O2NYrfR86dEiDBw+Wz+fT/fffr82bN+vyyy/3dN81NTV69dVXVVlZ2W2dl/suKSnR+vXrtW3bNq1du1ZHjx7VDTfcoNbWVk/3/e6772rt2rUaO3astm/frvnz5+sb3/iGNmzYICk5vptbtmxRS0uL7rnnHkne/pz0hOdux4C+U15ersOHD2v37t22W3Htsssu08GDBxUMBvVv//ZvmjNnjnbu3Gm7rXNqbGzUokWLtGPHDmVkZNhuJyYzZswI//eECRNUUlKiUaNG6Re/+IUyMzMtdnZ+XV1dmjRpklasWCFJuvrqq3X48GE9/vjjmjNnjuXu3Fm3bp1mzJih/Px82630Ks8dAQ0fPlwDBgzoNrqjublZubm5lrqK3Se9evXnWLBggX71q1/pxRdfjLhjYW5urjo6OtTS0hKxvVf6Tk9P1+c+9zkVFxersrJSEydO1GOPPebZvuvr63XixAldc801GjhwoAYOHKidO3fqRz/6kQYOHKicnBxP9u0kOztbn//85/XOO+949v2WpLy8PF1++eURy8aNGxf+86HXv5vvvfeenn/+eX39618PL/Py+90Tngug9PR0FRcXq7a2Nrysq6tLtbW1CgQCFjuLzZgxY5Sbmxvxc4RCIe3bt8/qz2GM0YIFC7R582a98MILGjNmTMT64uJipaWlRfTd0NCgY8eOefL97+rqUnt7u2f7njZtmg4dOqSDBw+GH5MmTdLs2bPD/+3Fvp2cPHlSv/nNb5SXl+fZ91uSpkyZ0u3SgrffflujRo2S5N3v5ieqq6s1cuRI3XzzzeFlXn6/e8T2KAgnNTU1xufzmfXr15s33njDzJs3z2RnZ5umpibbrUVobW01r732mnnttdeMJPPoo4+a1157zbz33nvGmI+HemZnZ5unn37a/PrXvza33Xab9aGe8+fPN36/37z00ksRQz5Pnz4d3ub+++83hYWF5oUXXjAHDhwwgUDABAIBaz1/4sEHHzQ7d+40R48eNb/+9a/Ngw8+aFJSUsx//ud/GmO823e0PxwFZ4x3+/7bv/1b89JLL5mjR4+aPXv2mNLSUjN8+HBz4sQJY4x3+96/f78ZOHCg+f73v2+OHDlifv7zn5tBgwaZp556KryNF7+bxnw84rewsNA88MAD3dZ59f3uCU8GkDHG/OM//qMpLCw06enp5rrrrjN79+613VI3L774opHU7TFnzhxjzMfDPR966CGTk5NjfD6fmTZtmmloaLDas1O/kkx1dXV4mzNnzpi/+Zu/MZdeeqkZNGiQ+fM//3Pz/vvv22v6/9x7771m1KhRJj093YwYMcJMmzYtHD7GeLfvaNEB5NW+77jjDpOXl2fS09PNH/3RH5k77rgj4loar/ZtjDHPPvusGT9+vPH5fKaoqMg88cQTEeu9+N00xpjt27cbSY69ePn9jhf3AwIAWOG5c0AAgIsDAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBY8f8BNLhNLglyvJkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0008e-03],\n",
      "        [ 1.1497e-01],\n",
      "        [-1.9321e-02],\n",
      "        [ 1.2748e-01],\n",
      "        [ 3.2712e-01],\n",
      "        [ 6.4268e-02],\n",
      "        [ 1.4515e-01],\n",
      "        [ 2.6107e-01],\n",
      "        [-7.0306e-02],\n",
      "        [-1.4643e-01],\n",
      "        [-3.7516e-01],\n",
      "        [-1.4747e+00],\n",
      "        [-3.3316e+00],\n",
      "        [-6.4096e+00]], device='cuda:0', grad_fn=<CatBackward0>) tensor([[ 0.2557],\n",
      "        [ 0.4175],\n",
      "        [ 0.6345],\n",
      "        [ 1.5138],\n",
      "        [ 3.6506],\n",
      "        [ 4.5284],\n",
      "        [ 5.4905],\n",
      "        [ 6.8526],\n",
      "        [ 8.5195],\n",
      "        [10.2912],\n",
      "        [14.9444],\n",
      "        [26.6298],\n",
      "        [38.7544],\n",
      "        [42.7961],\n",
      "        [44.9071]], device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25704/138873245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mcur_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode_return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_exploration_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mnew_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;31m#plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/138873245.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, model, obs, add_exploration_noise)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 reward, value, policy_logits, hidden_state = model.forward_encoded(\n\u001b[0;32m---> 65\u001b[0;31m                     parent.hidden_state, torch.tensor([[action]]).to(parent.hidden_state.device), one_hot=False)\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/3153227459.py\u001b[0m in \u001b[0;36mforward_encoded\u001b[0;34m(self, encoded, actions, one_hot)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamicModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_rvpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mr_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/3153227459.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, actions)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mOutput_rvpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/3153227459.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MCTS testing\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"\n",
    "    Core Monte Carlo Tree Search algorithm.\n",
    "    To decide on an action, we run N simulations, always starting at the root of\n",
    "    the search tree and traversing the tree according to the UCB formula until we\n",
    "    reach a leaf node.\n",
    "    \"\"\"\n",
    "    def __init__(self, flags, num_actions):\n",
    "        self.flags = flags\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def run(self, model, obs, add_exploration_noise,):\n",
    "        \"\"\"\n",
    "        At the root of the search tree we use the representation function to obtain a\n",
    "        hidden state given the current observation.\n",
    "        We then run a Monte Carlo Tree Search using only action sequences and the model\n",
    "        learned by the network.\n",
    "        Only supports a batch size of 1.        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            root = Node(0)\n",
    "            _, root_predicted_value, policy_logits, hidden_state = model(\n",
    "                obs[\"frame\"][0], obs[\"last_action\"], one_hot=False)\n",
    "            reward = 0.\n",
    "            root_predicted_value = root_predicted_value[-1].item()\n",
    "            policy_logits = policy_logits[-1]\n",
    "            hidden_state = hidden_state[-1]\n",
    "\n",
    "            root.expand(num_actions, reward, policy_logits, hidden_state,)\n",
    "\n",
    "            if add_exploration_noise:\n",
    "                root.add_exploration_noise(\n",
    "                    dirichlet_alpha=self.flags.root_dirichlet_alpha,\n",
    "                    exploration_fraction=self.flags.root_exploration_fraction,\n",
    "                )\n",
    "\n",
    "            min_max_stats = MinMaxStats()\n",
    "\n",
    "            max_tree_depth = 0\n",
    "            \n",
    "            #print(\"p at root:\", torch.softmax(policy_logits, dim=-1))\n",
    "            for k in range(self.flags.num_simulations): \n",
    "                \n",
    "                #print(\"=======%d iteration======\"%k)\n",
    "                node = root\n",
    "                search_path = [node]\n",
    "                current_tree_depth = 0\n",
    "\n",
    "                while node.expanded():\n",
    "                    current_tree_depth += 1                    \n",
    "                    action, node = self.select_child(node, min_max_stats)                    \n",
    "                    search_path.append(node)\n",
    "                    #print(\"action sel: %d\" % action)\n",
    "                \n",
    "                #np.set_printoptions(precision=5)\n",
    "                #for x in [\"prior_score\", \"value_score\", \"pb_c\", \"prior\", \"visit_count\"]:                    \n",
    "                #    print(x, \"\\t\", np.array([getattr(search_path[0].children[n], x) for n in range(5)]))\n",
    "\n",
    "                # Inside the search tree we use the dynamics function to obtain the next hidden\n",
    "                # state given an action and the previous hidden state\n",
    "                parent = search_path[-2]     \n",
    "                reward, value, policy_logits, hidden_state = model.forward_encoded(\n",
    "                    parent.hidden_state, torch.tensor([[action]]).to(parent.hidden_state.device), one_hot=False)\n",
    "                reward = reward[-1].item()\n",
    "                value = value[-1].item()\n",
    "                #print(\"model final output: %4f\" % value)\n",
    "                policy_logits = policy_logits[-1]\n",
    "                hidden_state = hidden_state[-1]\n",
    "                node.expand(num_actions, reward, policy_logits, hidden_state)\n",
    "                self.backpropagate(search_path, value, min_max_stats)\n",
    "                max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
    "\n",
    "            extra_info = {\n",
    "                \"max_tree_depth\": max_tree_depth,\n",
    "                \"root_predicted_value\": root_predicted_value,\n",
    "            }\n",
    "        return root, extra_info\n",
    "\n",
    "    def select_child(self, node, min_max_stats):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        max_ucb = max(\n",
    "            self.ucb_score(node, child, min_max_stats)\n",
    "            for action, child in node.children.items()\n",
    "        )\n",
    "        action = np.random.choice(\n",
    "            [\n",
    "                action\n",
    "                for action, child in node.children.items()\n",
    "                if self.ucb_score(node, child, min_max_stats) == max_ucb\n",
    "            ]\n",
    "        )\n",
    "        return action, node.children[action]\n",
    "\n",
    "    def ucb_score(self, parent, child, min_max_stats):\n",
    "        \"\"\"\n",
    "        The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
    "        \"\"\"\n",
    "        pb_c = (\n",
    "            math.log(\n",
    "                (parent.visit_count + self.flags.pb_c_base + 1) / self.flags.pb_c_base\n",
    "            )\n",
    "            + self.flags.pb_c_init\n",
    "        )\n",
    "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior\n",
    "\n",
    "        if child.visit_count > 0:\n",
    "            # Mean value Q\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward + self.flags.discounting * child.value())\n",
    "        else:\n",
    "            value_score = 0\n",
    "            \n",
    "        child.pb_c = pb_c\n",
    "        child.prior_score = prior_score\n",
    "        child.value_score = value_score\n",
    "        \n",
    "        return prior_score + value_score\n",
    "\n",
    "    def backpropagate(self, search_path, value, min_max_stats):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        #print(\"bs value: %.4f\" % value)\n",
    "        for n, node in enumerate(reversed(search_path)):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            min_max_stats.update(node.reward + self.flags.discounting * node.value())\n",
    "            value = node.reward + self.flags.discounting * value\n",
    "            #print(\"%d - val: %.4f r: %.4f\" % (n, value, node.reward))\n",
    "            #print(\"node value_sum %.4f\" % node.value_sum)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior):\n",
    "        self.visit_count = 0\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def expand(self, num_actions, reward, policy_logits, hidden_state):\n",
    "        \"\"\"\n",
    "        We expand a node using the value, reward and policy prediction obtained from the\n",
    "        neural network.\n",
    "        \"\"\"\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        policy_values = torch.softmax(policy_logits[0], dim=0).tolist()\n",
    "        for a in range(num_actions):\n",
    "            self.children[a] = Node(policy_values[a])\n",
    "\n",
    "    def add_exploration_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        \"\"\"\n",
    "        At the start of each search, we add dirichlet noise to the prior of the root to\n",
    "        encourage the search to explore new actions.\n",
    "        \"\"\"\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "class MinMaxStats:\n",
    "    \"\"\"\n",
    "    A class that holds the min-max values of the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.maximum = -float(\"inf\")\n",
    "        self.minimum = float(\"inf\")\n",
    "\n",
    "    def update(self, value):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value):\n",
    "        if self.maximum > self.minimum:\n",
    "            # We normalize only when we have set the maximum and minimum values\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value            \n",
    "\n",
    "def select_action(node, temperature):\n",
    "    \"\"\"\n",
    "    Select action according to the visit count distribution and the temperature.\n",
    "    The temperature is changed dynamically with the visit_softmax_temperature function\n",
    "    in the config.\n",
    "    \"\"\"\n",
    "    visit_counts = np.array(\n",
    "        [child.visit_count for child in node.children.values()], dtype=\"int32\"\n",
    "    )\n",
    "    actions = [action for action in node.children.keys()]\n",
    "    if temperature == 0:\n",
    "        action = actions[np.argmax(visit_counts)]\n",
    "    elif temperature == float(\"inf\"):\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        # See paper appendix Data Generation\n",
    "        visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "        visit_count_distribution = visit_count_distribution / sum(\n",
    "            visit_count_distribution\n",
    "        )\n",
    "        action = np.random.choice(actions, p=visit_count_distribution)\n",
    "    #print(\"visit_counts\", visit_counts)\n",
    "    #print(\"visit_count_distribution\", visit_count_distribution)\n",
    "    return action\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()      \n",
    "flags = parser.parse_args([])   \n",
    "\n",
    "env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)\n",
    "env = Environment(env)\n",
    "env.initial()\n",
    "obs_shape, num_actions = env.gym_env.observation_space.shape, env.gym_env.action_space.n\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args([])   \n",
    "flags.discounting = 0.97\n",
    "flags.pb_c_init = 1.25\n",
    "flags.pb_c_base = 19652\n",
    "flags.root_dirichlet_alpha = 0.25\n",
    "flags.root_exploration_fraction = 0.\n",
    "flags.num_simulations = 3\n",
    "flags.temp = 0.5\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "eps_n = 10\n",
    "eps_n_cur = 0\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "obs = env.initial()\n",
    "returns = []\n",
    "mcts = MCTS(flags, num_actions)\n",
    "obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "root, extra_info = mcts.run(model, obs, add_exploration_noise=True)   \n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "actions = torch.tensor([0, 1, 3, 4, 4, 4, 2, 4, 2, 2, 2, 1, 1, 1, 1]).long().to(flags.device).reshape(-1, 1)\n",
    "reward, value, policy_logits, hidden_state  = model(obs[\"frame\"][0], actions, one_hot=False)\n",
    "print(reward, value)\n",
    "\n",
    "while(len(returns) <= eps_n):\n",
    "    cur_returns = obs['episode_return']    \n",
    "    obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "    root, extra_info = mcts.run(model, obs, add_exploration_noise=True)    \n",
    "    new_action = select_action(root, flags.temp)    \n",
    "    #plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    #flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "    #plt.show()\n",
    "    #print(\"action selected\", new_action)\n",
    "    #print(\"===========================================\")\n",
    "    obs = env.step(torch.tensor([new_action]))\n",
    "    if torch.any(obs['done']):\n",
    "        returns.extend(cur_returns[obs['done']].numpy())\n",
    "    if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "        eps_n_cur = len(returns) + 10\n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test planning algorithm\n",
    "bsz = 1\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "obs = env.initial()\n",
    "state = obs['frame'][0].to(flags.device).clone()\n",
    "action = torch.zeros(bsz).long().to(flags.device)\n",
    "encoded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e67032",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.Tensor([4]).long().to(flags.device)\n",
    "obs = env.step(action)\n",
    "state = obs['frame'][0].to(flags.device).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c551f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "device = flags.device\n",
    "\n",
    "for _ in range(1):\n",
    "    plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "    plt.show()\n",
    "    ret = np.zeros((5, 5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            for k in range(5):\n",
    "                test_action_seq = [i,j,k]\n",
    "                test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "                old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "                rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "                ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "    print(np.max(ret), (np.max(ret) == ret).nonzero())    \n",
    "    new_action = torch.Tensor((np.max(ret) == ret).nonzero()[0]).long().to(flags.device)\n",
    "    #obs = env.step(new_action)\n",
    "    #state = obs['frame'][0].to(flags.device).clone()\n",
    "    #action = new_action            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a9ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, prob, q_ret = n_step_greedy_model(state, action, model, 3, encoded=None, temp=10.)\n",
    "print(\"action: \", action)\n",
    "print(\"prob: \", prob)\n",
    "print(\"q_ret: \", q_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_action_seq = [2,3,1]\n",
    "test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "print(\"rs\", rs)\n",
    "print(\"vs\", vs)\n",
    "print(\"logits\", logits)\n",
    "print(\"ret\", ret[i,j,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 10.\n",
    "\n",
    "bsz = state.shape[0]\n",
    "device = state.device \n",
    "num_actions = model.num_actions    \n",
    "model.train(False)\n",
    "\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "\n",
    "for act in range(num_actions):        \n",
    "    new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "    old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "    rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "    ret = rs[0] + flags.discounting * vs[1]\n",
    "    rs_act[:, act] = rs[0]\n",
    "    vs_act[:, act] = vs[1]\n",
    "    q_ret[:, act] = ret\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = flags.device\n",
    "net_state = env.clone_state()\n",
    "\n",
    "bsz = 1\n",
    "temp = 10.\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)   \n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "for act in range(num_actions):\n",
    "    obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "    obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "    ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "    rs_act[:, act] = obs['reward']\n",
    "    vs_act[:, act] = net(obs)[0]['baseline']\n",
    "    q_ret[:, act] = ret\n",
    "    env.restore_state(net_state)\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)\n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch_m(flags, buffers)\n",
    "print(torch.max(batch[\"reward\"]), (torch.max(batch[\"reward\"]) == batch[\"reward\"]).nonzero())\n",
    "print(batch[\"done\"].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG LOSS\n",
    "\n",
    "#batch = get_batch_m(flags, buffers)\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "logits = logits[:-1]\n",
    "\n",
    "target_rewards = batch['reward'][1:]\n",
    "target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "target_vs = []\n",
    "target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0]    \n",
    "\n",
    "for t in range(vs.shape[0]-1, 0, -1):\n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))\n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "# if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "# rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "# vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "# logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "done_masks = []\n",
    "done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "for t in range(vs.shape[0]):\n",
    "    done = torch.logical_or(done, batch['done'][t])\n",
    "    done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "\n",
    "# compute final loss\n",
    "huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "rs_loss = torch.sum(huberloss(rs, target_rewards) * (~done_masks).float())\n",
    "#rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "vs_loss = torch.sum(huberloss(vs[:-1], target_vs) * (~done_masks).float())\n",
    "#vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "logits_loss = compute_cross_entropy_loss(logits, target_logits, done_masks)\n",
    "\n",
    "# debug\n",
    "ind = 21\n",
    "\n",
    "target_vs = []\n",
    "target_v = vs[-1]\n",
    "for t in range(vs.shape[0]-1, 0, -1):        \n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    print(t, \n",
    "          \"reward %2f\" % batch['reward'][t,ind].item(), \n",
    "          \"bootstrap %2f\" % (target_v * (~batch['done'][t]).float())[ind].item(), \n",
    "          \"truncated %2f\" % (vs[t-1] * (batch['truncated_done'][t]).float())[ind].item(),\n",
    "          \"vs[t-1] %2f\" % vs[t-1][ind].item(),\n",
    "          \"new_targ %2f\" % new_target_v[ind].item())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))    \n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)   \n",
    "print(\"done\", batch[\"done\"][:, ind])\n",
    "print(\"done_masks\", done_masks[:, ind])\n",
    "print(\"vs: \", vs[:, ind])\n",
    "print(\"target_vs: \", target_vs[:, ind])\n",
    "print(\"reward: \", rs[:, ind])\n",
    "print(\"target_reward: \", target_rewards[:, ind])\n",
    "print(\"logits: \", logits[:, ind])\n",
    "print(\"target_logits: \", target_logits[:, ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt. version of computing loss by treading terminal state as absorbing state (as in MuZero)\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_logits = batch['policy_logits'][1:].clone()\n",
    "    target_rewards = batch['reward'][1:].clone()\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "\n",
    "    c_logits = target_logits[0]\n",
    "    c_state = batch['frame'][0]\n",
    "    for t in range(vs.shape[0]-1):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        c_logits = torch.where(done.unsqueeze(-1), c_logits, target_logits[t])\n",
    "        target_logits[t] = c_logits\n",
    "        c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][t])  \n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "    done_masks = torch.concat(done_masks, dim=0)\n",
    "    done = torch.logical_or(done, batch['done'][-1])\n",
    "    c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][-1])\n",
    "    target_rewards = target_rewards * (~done_masks).float()\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(c_state, batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * target_v\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()))\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()))\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), None)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
