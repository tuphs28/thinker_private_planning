{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fc8a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core.environment import Environment, Vec_Environment\n",
    "from torchbeast.atari_wrappers import SokobanWrapper\n",
    "from torchbeast.base import BaseNet\n",
    "from torchbeast.train import create_env\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "def get_param(net, name=None):\n",
    "    keys = []\n",
    "    for (k, v) in actor_wrapper.model.named_parameters(): \n",
    "        if name is None:\n",
    "            print(k)\n",
    "        else:\n",
    "            if name == k: return v\n",
    "        keys.append(k)\n",
    "    return keys        \n",
    "\n",
    "def n_step_greedy(env, net, n, temp=10.):    \n",
    "    if isinstance(env, Vec_Environment):\n",
    "        num_actions = env.gym_env.action_space[0].n\n",
    "        bsz = len(env.gym_env.envs)\n",
    "    else:\n",
    "        num_actions = env.gym_env.action_space.n\n",
    "        bsz = 1\n",
    "\n",
    "    q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "    state = env.clone_state()\n",
    "\n",
    "    for act in range(num_actions):\n",
    "        obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "        \n",
    "        if n > 1:\n",
    "            action, prob, sub_q_ret = n_step_greedy(env, net, n-1)\n",
    "            ret = obs['reward'] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] * (~obs['done']).float()\n",
    "        else:\n",
    "            ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "\n",
    "        q_ret[:, act] = ret\n",
    "        env.restore_state(state)\n",
    "    \n",
    "    prob = F.softmax(temp*q_ret, dim=1)\n",
    "    action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893df32e",
   "metadata": {},
   "source": [
    "<font size=\"5\">Testing planning algo. for perfect model with bootstrapped values</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51bf8c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1095814\n",
      "Testing 2 step planning\n",
      "Finish 16 episode: avg. return: 0.36 (+-0.22) \n",
      "Finish 32 episode: avg. return: 0.46 (+-0.19) \n",
      "Finish 48 episode: avg. return: 0.59 (+-0.14) \n",
      "Finish 64 episode: avg. return: 0.61 (+-0.11) \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25704/3626975367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mall_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_n_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time required for %d step planning: %f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/3626975367.py\u001b[0m in \u001b[0;36mtest_n_step\u001b[0;34m(n, net, env, temp)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"action\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_step_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/706465574.py\u001b[0m in \u001b[0;36mn_step_greedy\u001b[0;34m(env, net, n, temp)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_q_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_step_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscounting\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_q_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/706465574.py\u001b[0m in \u001b[0;36mn_step_greedy\u001b[0;34m(env, net, n, temp)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscounting\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_q_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscounting\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'baseline'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mq_ret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RS/thinker/torchbeast/torchbeast/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, core_state, debug)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0menv_input_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_conv_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mcore_output_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_input_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore_output_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Synchronous version of testing \n",
    "\n",
    "def test_n_step(n, net, env, temp=10.):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    returns = []\n",
    "    obs = env.initial()\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}\n",
    "        net_out, core_state = net(obs)            \n",
    "        if n == 0:\n",
    "            action = net_out[\"action\"][0]\n",
    "        else:\n",
    "            action, _, _ = n_step_greedy(env, net, n, temp)\n",
    "        obs = env.step(action)\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns\n",
    "\n",
    "bsz = 16    \n",
    "eps_n = 500\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# create environments\n",
    "\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "num_actions = env.gym_env.action_space[0].n\n",
    "\n",
    "# import the net\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "flags.discounting = 0.97\n",
    "temp = 5\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=num_actions, flags=flags)  \n",
    "net = net.to(device)\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_2.tar\", map_location=\"cuda\")\n",
    "#checkpoint = torch.load(\"/home/schk/RS/thinker/logs/base/torchbeast-20221105-033530/model.tar\", map_location=\"cuda\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "# initialize net\n",
    "\n",
    "core_state = net.initial_state(batch_size=bsz)\n",
    "core_state = tuple(v.to(device) for v in core_state)\n",
    "net.train(False)\n",
    "\n",
    "all_returns = {}\n",
    "for n in range(2,3):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step(n, net, env, temp)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13ffb0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1095814\n",
      "Testing 0 step planning\n",
      "Finish 500 episode: avg. return: 0.27 (+-0.04)\n",
      "Time required for 0 step planning: 12.629324\n",
      "Testing 1 step planning\n",
      "Finish 502 episode: avg. return: 0.51 (+-0.04)\n",
      "Time required for 1 step planning: 74.194364\n",
      "Testing 2 step planning\n",
      "Finish 500 episode: avg. return: 0.74 (+-0.04)\n",
      "Time required for 2 step planning: 339.732901\n",
      "Testing 3 step planning\n",
      "Finish 500 episode: avg. return: 0.76 (+-0.04)\n",
      "Time required for 3 step planning: 1695.472523\n"
     ]
    }
   ],
   "source": [
    "# Asynchronous version of testing \n",
    "\n",
    "def act_m(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    returns: Manager().list,\n",
    "    eps_n: int,\n",
    "    n: int,\n",
    "    temp: float,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        net_out, unused_state = net(env_output, agent_state)      \n",
    "        while True:            \n",
    "            if len(returns) >= eps_n: break\n",
    "            with torch.no_grad():\n",
    "                net_out, agent_state = net(env_output, agent_state)                            \n",
    "            if n == 0:\n",
    "                action = net_out[\"action\"]\n",
    "            else:\n",
    "                action, _, _ = n_step_greedy(env, net, n, temp)            \n",
    "            env_output = env.step(action)           \n",
    "            if env_output['done']: returns.append(ret)\n",
    "            ret = env_output['episode_return'].item()        \n",
    "        #logging.info(\"Actor %i end\", actor_index)\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "def asy_test_n_step(n, net, flags, temp):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    mp.set_sharing_strategy('file_system')\n",
    "    net.share_memory()\n",
    "    ctx = mp.get_context()        \n",
    "    returns = Manager().list()\n",
    "\n",
    "    actor_processes = []\n",
    "    for i in range(flags.num_actors):\n",
    "        actor = ctx.Process(target=act_m, args=(flags, i, net, returns, eps_n, n, temp),)\n",
    "        actor.start()\n",
    "        actor_processes.append(actor)    \n",
    "\n",
    "    for actor in actor_processes:\n",
    "        actor.join()    \n",
    "\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f)\" % (len(returns),\n",
    "                    np.average(returns), np.std(returns) / np.sqrt(len(returns)),))        \n",
    "    return returns        \n",
    "        \n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.discounting = 0.97     \n",
    "flags.num_actors = 32\n",
    "bsz = 1\n",
    "eps_n = 500\n",
    "temp = 5\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "net = net.to(\"cpu\")\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_2.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "all_returns = {}\n",
    "for n in range(4):\n",
    "    t = time.time()\n",
    "    all_returns[n] = asy_test_n_step(n, net, flags, temp)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abac9a",
   "metadata": {},
   "source": [
    "Results (base_1.tar):\n",
    "    \n",
    "Testing 0 step planning <br>\n",
    "Finish 512 episode: avg. return: 0.12 (+-0.06) <br>\n",
    "Testing 1 step planning <br>\n",
    "Finish 502 episode: avg. return: 0.61 (+-0.04) <br>\n",
    "Testing 2 step planning <br>\n",
    "Finish 501 episode: avg. return: 0.92 (+-0.04) <br>\n",
    "Testing 3 step planning <br>\n",
    "Finish 501 episode: avg. return: 1.01 (+-0.04) <br>\n",
    "\n",
    "Results (base_2.tar):\n",
    "Testing 0 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.27 (+-0.04) <br>\n",
    "Time required for 0 step planning: 12.629324 <br>\n",
    "Testing 1 step planning <br>\n",
    "Finish 502 episode: avg. return: 0.51 (+-0.04) <br>\n",
    "Time required for 1 step planning: 74.194364 <br>\n",
    "Testing 2 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.74 (+-0.04) <br>\n",
    "Time required for 2 step planning: 339.732901 <br>\n",
    "Testing 3 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.76 (+-0.04) <br>\n",
    "Time required for 3 step planning: 1695.472523 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097bd18",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model Training Phase</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "968043ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data for learning model [RUN]\n",
    "\n",
    "Buffers = typing.Dict[str, typing.List[torch.Tensor]]\n",
    "\n",
    "def create_buffers_m(flags, obs_shape, num_actions) -> Buffers:\n",
    "    \n",
    "    seq_len = flags.seq_len\n",
    "    seq_n = flags.seq_n\n",
    "    specs = dict(\n",
    "        frame=dict(size=(seq_len + 1, *obs_shape), dtype=torch.uint8),\n",
    "        reward=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        truncated_done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        episode_return=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        episode_step=dict(size=(seq_len + 1,), dtype=torch.int32),\n",
    "        policy_logits=dict(size=(seq_len + 1, num_actions), dtype=torch.float32),\n",
    "        baseline=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        last_action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        reg_loss=dict(size=(seq_len + 1,), dtype=torch.float32)\n",
    "    )\n",
    "    buffers: Buffers = {key: [] for key in specs}\n",
    "    for _ in range(seq_n):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "            \n",
    "    return buffers\n",
    "\n",
    "def gen_data(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    buffers: Buffers,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = net(env_output, agent_state)     \n",
    "        \n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break         \n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:\n",
    "                buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                buffers[key][index][0, ...] = agent_output[key]\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.seq_len):\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = net(env_output, agent_state)\n",
    "                env_output = env.step(agent_output[\"action\"])\n",
    "                for key in env_output:\n",
    "                    buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "        \n",
    "\n",
    "# Models\n",
    "\n",
    "DOWNSCALE_C = 2\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation,\n",
    "        groups=groups, bias=False, dilation=dilation,)\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, inplanes, outplanes=None):\n",
    "        super().__init__()\n",
    "        if outplanes is None: outplanes = inplanes \n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        self.conv1 = conv3x3(inplanes, inplanes)\n",
    "        self.bn1 = norm_layer(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(inplanes, outplanes)\n",
    "        self.bn2 = norm_layer(outplanes)\n",
    "        self.skip_conv = (outplanes != inplanes)\n",
    "        if outplanes != inplanes:\n",
    "            self.conv3 = conv1x1(inplanes, outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.skip_conv:\n",
    "            out += self.conv3(identity)\n",
    "        else:\n",
    "            out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class FrameEncoder(nn.Module):    \n",
    "    def __init__(self, num_actions, frame_channels=3):\n",
    "        self.num_actions = num_actions\n",
    "        super(FrameEncoder, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=frame_channels+num_actions, out_channels=128//DOWNSCALE_C, kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=128//DOWNSCALE_C) for i in range(1)]) # Deep: 2 blocks here\n",
    "        self.res1 = torch.nn.Sequential(*res)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128//DOWNSCALE_C, out_channels=256//DOWNSCALE_C, \n",
    "                               kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(1)]) # Deep: 3 blocks here\n",
    "        self.res2 = torch.nn.Sequential(*res)\n",
    "        self.avg1 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(1)]) # Deep: 3 blocks here\n",
    "        self.res3 = torch.nn.Sequential(*res)\n",
    "        self.avg2 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x, actions):        \n",
    "        # input shape: B, C, H, W        \n",
    "        # action shape: B \n",
    "        \n",
    "        x = x.float() / 255.0    \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "        x = torch.concat([x, actions], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.res1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.res2(x)\n",
    "        x = self.avg1(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.avg2(x)\n",
    "        return x\n",
    "    \n",
    "class DynamicModel(nn.Module):\n",
    "    def __init__(self, num_actions, inplanes=256):        \n",
    "        super(DynamicModel, self).__init__()\n",
    "        res = nn.ModuleList([ResBlock(inplanes=inplanes) for i in range(15)] + \n",
    "                            [ResBlock(inplanes=inplanes, outplanes=inplanes*num_actions)]) # Deep: 15 blocks here\n",
    "        \n",
    "        self.res = torch.nn.Sequential(*res)\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def forward(self, x, actions):      \n",
    "        bsz, c, h, w = x.shape\n",
    "        res_out = self.res(x).view(bsz, self.num_actions, c, h, w)        \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        act_res_out = torch.sum(actions * res_out, dim=1)\n",
    "        return act_res_out\n",
    "    \n",
    "class DynamicModel_(nn.Module):\n",
    "    def __init__(self, num_actions, inplanes=256):        \n",
    "        super(DynamicModel, self).__init__()\n",
    "        res = nn.ModuleList([ResBlock(inplanes=inplanes+num_actions, outplanes=inplanes)] + [\n",
    "            ResBlock(inplanes=inplanes) for i in range(4)]) # Deep: 15 blocks here\n",
    "        self.res = torch.nn.Sequential(*res)\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def forward(self, x, actions):      \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "        x = torch.concat([x, actions], dim=1)\n",
    "        return self.res(x)\n",
    "    \n",
    "class Output_rvpi(nn.Module):   \n",
    "    def __init__(self, num_actions, input_shape):         \n",
    "        super(Output_rvpi, self).__init__()        \n",
    "        c, h, w = input_shape\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=c//2, kernel_size=3, padding='same') \n",
    "        self.conv2 = nn.Conv2d(in_channels=c//2, out_channels=c//4, kernel_size=3, padding='same') \n",
    "        fc_in = h * w * (c // 4)\n",
    "        self.fc_r = nn.Linear(fc_in, 1) \n",
    "        self.fc_v = nn.Linear(fc_in, 1) \n",
    "        self.fc_logits = nn.Linear(fc_in, num_actions)         \n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        r, v, logits = self.fc_r(x), self.fc_v(x), self.fc_logits(x)\n",
    "        return r, v, logits\n",
    "\n",
    "class Model(nn.Module):    \n",
    "    def __init__(self, flags, obs_shape, num_actions):\n",
    "        super(Model, self).__init__()      \n",
    "        self.flags = flags\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions          \n",
    "        self.frameEncoder = FrameEncoder(num_actions=num_actions, frame_channels=obs_shape[0])\n",
    "        self.dynamicModel = DynamicModel(num_actions=num_actions, inplanes=256//DOWNSCALE_C)\n",
    "        self.output_rvpi = Output_rvpi(num_actions=num_actions, input_shape=(256//DOWNSCALE_C, \n",
    "                      obs_shape[1]//16, obs_shape[1]//16))\n",
    "        \n",
    "    def forward(self, x, actions, one_hot=False):\n",
    "        # Input\n",
    "        # x: frames with shape (B, C, H, W), in the form of s_t\n",
    "        # actions: action (int64) with shape (k+1, B), in the form of a_{t-1}, a_{t}, a_{t+1}, .. a_{t+k-1}\n",
    "        # Output\n",
    "        # reward: predicted reward with shape (k, B), in the form of r_{t+1}, r_{t+2}, ..., r_{t+k}\n",
    "        # value: predicted value with shape (k+1, B), in the form of v_{t}, v_{t+1}, v_{t+2}, ..., v_{t+k}\n",
    "        # policy: predicted policy with shape (k+1, B), in the form of pi_{t}, pi_{t+1}, pi_{t+2}, ..., pi_{t+k}\n",
    "        # encoded: encoded states with shape (k+1, B), in the form of z_t, z_{t+1}, z_{t+2}, ..., z_{t+k}\n",
    "        # Recall the transition notation: s_t, a_t, r_{t+1}, s_{t+1}, ...\n",
    "        \n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        encoded = self.frameEncoder(x, actions[0])\n",
    "        return self.forward_encoded(encoded, actions[1:], one_hot=True)\n",
    "    \n",
    "    def forward_encoded(self, encoded, actions, one_hot=False):\n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        \n",
    "        r, v, logits = self.output_rvpi(encoded)\n",
    "        r_list, v_list, logits_list = [], [v.squeeze(-1).unsqueeze(0)], [logits.unsqueeze(0)]\n",
    "        encoded_list = [encoded.unsqueeze(0)]\n",
    "        \n",
    "        for k in range(actions.shape[0]):            \n",
    "            encoded = self.dynamicModel(encoded, actions[k])\n",
    "            r, v, logits = self.output_rvpi(encoded)\n",
    "            r_list.append(r.squeeze(-1).unsqueeze(0))\n",
    "            v_list.append(v.squeeze(-1).unsqueeze(0))\n",
    "            logits_list.append(logits.unsqueeze(0))\n",
    "            encoded_list.append(encoded.unsqueeze(0))        \n",
    "        \n",
    "        if len(r_list) > 0:\n",
    "            rs = torch.concat(r_list, dim=0)\n",
    "        else:\n",
    "            rs = None\n",
    "            \n",
    "        vs = torch.concat(v_list, dim=0)\n",
    "        logits = torch.concat(logits_list, dim=0)\n",
    "        encodeds = torch.concat(encoded_list, dim=0)        \n",
    "        \n",
    "        return rs, vs, logits, encodeds\n",
    "\n",
    "#model = Model(flags, (3, 80, 80), num_actions=5)\n",
    "#rs, vs, logits = model(torch.rand(16, 3, 80, 80), torch.ones(8, 16).long())\n",
    "\n",
    "# functions for training models\n",
    "\n",
    "def get_batch_m(flags, buffers: Buffers):\n",
    "    batch_indices = np.random.randint(flags.seq_n, size=flags.bsz)\n",
    "    time_indices = np.random.randint(flags.seq_len - flags.unroll_len, size=flags.bsz)\n",
    "    batch = {key: torch.stack([buffers[key][m][time_indices[n]:time_indices[n]+flags.unroll_len+1] \n",
    "                          for n, m in enumerate(batch_indices)], dim=1) for key in buffers}\n",
    "    batch = {k: t.to(device=flags.device, non_blocking=True) for k, t in batch.items()}\n",
    "    return batch\n",
    "\n",
    "def compute_cross_entropy_loss(logits, target_logits, mask):\n",
    "    target_policy = F.softmax(target_logits, dim=-1)\n",
    "    log_policy = F.log_softmax(logits, dim=-1)\n",
    "    return -torch.sum(target_policy * log_policy * (~mask).float().unsqueeze(-1))\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_rewards = batch['reward'][1:]\n",
    "    target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float())# +\n",
    "                           #vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "    # if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "    # rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "    # vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "    # logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "    for t in range(vs.shape[0]):\n",
    "        done = torch.logical_or(done, batch['done'][t])\n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "    done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()) * (~done_masks).float())\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()) * (~done_masks).float())\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), done_masks)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss\n",
    "\n",
    "# n_step_greedy for testing\n",
    "\n",
    "def n_step_greedy_model(state, action, model, n, encoded=None, temp=20.): \n",
    "    \n",
    "    # Either input state, action (S_t, A_{t-1}) or the encoded Z_t\n",
    "    # state / encoded in the shape of (B, C, H, W)\n",
    "    # action in the shape of (B)    \n",
    "    with torch.no_grad():    \n",
    "      bsz = state.shape[0] if encoded is None else encoded.shape[0]\n",
    "      device = state.device if encoded is None else encoded.device\n",
    "      num_actions = model.num_actions    \n",
    "\n",
    "      q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "\n",
    "      for act in range(num_actions):        \n",
    "          new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "          if encoded is None:            \n",
    "              old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "              rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "          else:\n",
    "              rs, vs, logits, encodeds = model.forward_encoded(encoded, new_action.unsqueeze(0))\n",
    "\n",
    "          if n > 1:\n",
    "              action, prob, sub_q_ret = n_step_greedy_model(state=None, action=None, \n",
    "                         model=model, n=n-1, encoded=encodeds[1])\n",
    "              ret = rs[0] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] \n",
    "          else:\n",
    "              ret = rs[0] + flags.discounting * vs[1]\n",
    "          q_ret[:, act] = ret\n",
    "\n",
    "      prob = F.softmax(temp*q_ret, dim=1)\n",
    "      action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret        \n",
    "   \n",
    "#n_step_greedy_model(batch['frame'][0], batch['action'][0], model, 4)  \n",
    "\n",
    "def test_n_step_model(n, model, flags, eps_n=100, temp=20.):    \n",
    "    \n",
    "    print(\"Testing %d step planning\" % n) \n",
    "    \n",
    "    bsz = 100\n",
    "    env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "    env = Vec_Environment(env, bsz)\n",
    "    num_actions = env.gym_env.action_space[0].n\n",
    "    \n",
    "    model.train(False)\n",
    "    returns = []\n",
    "    \n",
    "    obs = env.initial()\n",
    "    action = torch.zeros(bsz).long().to(flags.device)\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "        new_action, _, _ = n_step_greedy_model(obs['frame'][0], action, model, n, None, temp)        \n",
    "        obs = env.step(new_action)\n",
    "        action = new_action\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            #print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            #    np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    returns = returns[:eps_n]\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6fa3769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer created successfully.\n",
      "model size:  6240775\n",
      "frameEncoder.conv1.weight 4608\n",
      "frameEncoder.conv1.bias 64\n",
      "frameEncoder.res1.0.conv1.weight 36864\n",
      "frameEncoder.res1.0.bn1.weight 64\n",
      "frameEncoder.res1.0.bn1.bias 64\n",
      "frameEncoder.res1.0.conv2.weight 36864\n",
      "frameEncoder.res1.0.bn2.weight 64\n",
      "frameEncoder.res1.0.bn2.bias 64\n",
      "frameEncoder.conv2.weight 73728\n",
      "frameEncoder.conv2.bias 128\n",
      "frameEncoder.res2.0.conv1.weight 147456\n",
      "frameEncoder.res2.0.bn1.weight 128\n",
      "frameEncoder.res2.0.bn1.bias 128\n",
      "frameEncoder.res2.0.conv2.weight 147456\n",
      "frameEncoder.res2.0.bn2.weight 128\n",
      "frameEncoder.res2.0.bn2.bias 128\n",
      "frameEncoder.res3.0.conv1.weight 147456\n",
      "frameEncoder.res3.0.bn1.weight 128\n",
      "frameEncoder.res3.0.bn1.bias 128\n",
      "frameEncoder.res3.0.conv2.weight 147456\n",
      "frameEncoder.res3.0.bn2.weight 128\n",
      "frameEncoder.res3.0.bn2.bias 128\n",
      "dynamicModel.res.0.conv1.weight 147456\n",
      "dynamicModel.res.0.bn1.weight 128\n",
      "dynamicModel.res.0.bn1.bias 128\n",
      "dynamicModel.res.0.conv2.weight 147456\n",
      "dynamicModel.res.0.bn2.weight 128\n",
      "dynamicModel.res.0.bn2.bias 128\n",
      "dynamicModel.res.1.conv1.weight 147456\n",
      "dynamicModel.res.1.bn1.weight 128\n",
      "dynamicModel.res.1.bn1.bias 128\n",
      "dynamicModel.res.1.conv2.weight 147456\n",
      "dynamicModel.res.1.bn2.weight 128\n",
      "dynamicModel.res.1.bn2.bias 128\n",
      "dynamicModel.res.2.conv1.weight 147456\n",
      "dynamicModel.res.2.bn1.weight 128\n",
      "dynamicModel.res.2.bn1.bias 128\n",
      "dynamicModel.res.2.conv2.weight 147456\n",
      "dynamicModel.res.2.bn2.weight 128\n",
      "dynamicModel.res.2.bn2.bias 128\n",
      "dynamicModel.res.3.conv1.weight 147456\n",
      "dynamicModel.res.3.bn1.weight 128\n",
      "dynamicModel.res.3.bn1.bias 128\n",
      "dynamicModel.res.3.conv2.weight 147456\n",
      "dynamicModel.res.3.bn2.weight 128\n",
      "dynamicModel.res.3.bn2.bias 128\n",
      "dynamicModel.res.4.conv1.weight 147456\n",
      "dynamicModel.res.4.bn1.weight 128\n",
      "dynamicModel.res.4.bn1.bias 128\n",
      "dynamicModel.res.4.conv2.weight 147456\n",
      "dynamicModel.res.4.bn2.weight 128\n",
      "dynamicModel.res.4.bn2.bias 128\n",
      "dynamicModel.res.5.conv1.weight 147456\n",
      "dynamicModel.res.5.bn1.weight 128\n",
      "dynamicModel.res.5.bn1.bias 128\n",
      "dynamicModel.res.5.conv2.weight 147456\n",
      "dynamicModel.res.5.bn2.weight 128\n",
      "dynamicModel.res.5.bn2.bias 128\n",
      "dynamicModel.res.6.conv1.weight 147456\n",
      "dynamicModel.res.6.bn1.weight 128\n",
      "dynamicModel.res.6.bn1.bias 128\n",
      "dynamicModel.res.6.conv2.weight 147456\n",
      "dynamicModel.res.6.bn2.weight 128\n",
      "dynamicModel.res.6.bn2.bias 128\n",
      "dynamicModel.res.7.conv1.weight 147456\n",
      "dynamicModel.res.7.bn1.weight 128\n",
      "dynamicModel.res.7.bn1.bias 128\n",
      "dynamicModel.res.7.conv2.weight 147456\n",
      "dynamicModel.res.7.bn2.weight 128\n",
      "dynamicModel.res.7.bn2.bias 128\n",
      "dynamicModel.res.8.conv1.weight 147456\n",
      "dynamicModel.res.8.bn1.weight 128\n",
      "dynamicModel.res.8.bn1.bias 128\n",
      "dynamicModel.res.8.conv2.weight 147456\n",
      "dynamicModel.res.8.bn2.weight 128\n",
      "dynamicModel.res.8.bn2.bias 128\n",
      "dynamicModel.res.9.conv1.weight 147456\n",
      "dynamicModel.res.9.bn1.weight 128\n",
      "dynamicModel.res.9.bn1.bias 128\n",
      "dynamicModel.res.9.conv2.weight 147456\n",
      "dynamicModel.res.9.bn2.weight 128\n",
      "dynamicModel.res.9.bn2.bias 128\n",
      "dynamicModel.res.10.conv1.weight 147456\n",
      "dynamicModel.res.10.bn1.weight 128\n",
      "dynamicModel.res.10.bn1.bias 128\n",
      "dynamicModel.res.10.conv2.weight 147456\n",
      "dynamicModel.res.10.bn2.weight 128\n",
      "dynamicModel.res.10.bn2.bias 128\n",
      "dynamicModel.res.11.conv1.weight 147456\n",
      "dynamicModel.res.11.bn1.weight 128\n",
      "dynamicModel.res.11.bn1.bias 128\n",
      "dynamicModel.res.11.conv2.weight 147456\n",
      "dynamicModel.res.11.bn2.weight 128\n",
      "dynamicModel.res.11.bn2.bias 128\n",
      "dynamicModel.res.12.conv1.weight 147456\n",
      "dynamicModel.res.12.bn1.weight 128\n",
      "dynamicModel.res.12.bn1.bias 128\n",
      "dynamicModel.res.12.conv2.weight 147456\n",
      "dynamicModel.res.12.bn2.weight 128\n",
      "dynamicModel.res.12.bn2.bias 128\n",
      "dynamicModel.res.13.conv1.weight 147456\n",
      "dynamicModel.res.13.bn1.weight 128\n",
      "dynamicModel.res.13.bn1.bias 128\n",
      "dynamicModel.res.13.conv2.weight 147456\n",
      "dynamicModel.res.13.bn2.weight 128\n",
      "dynamicModel.res.13.bn2.bias 128\n",
      "dynamicModel.res.14.conv1.weight 147456\n",
      "dynamicModel.res.14.bn1.weight 128\n",
      "dynamicModel.res.14.bn1.bias 128\n",
      "dynamicModel.res.14.conv2.weight 147456\n",
      "dynamicModel.res.14.bn2.weight 128\n",
      "dynamicModel.res.14.bn2.bias 128\n",
      "dynamicModel.res.15.conv1.weight 147456\n",
      "dynamicModel.res.15.bn1.weight 128\n",
      "dynamicModel.res.15.bn1.bias 128\n",
      "dynamicModel.res.15.conv2.weight 737280\n",
      "dynamicModel.res.15.bn2.weight 640\n",
      "dynamicModel.res.15.bn2.bias 640\n",
      "dynamicModel.res.15.conv3.weight 81920\n",
      "output_rvpi.conv1.weight 73728\n",
      "output_rvpi.conv1.bias 64\n",
      "output_rvpi.conv2.weight 18432\n",
      "output_rvpi.conv2.bias 32\n",
      "output_rvpi.fc_r.weight 800\n",
      "output_rvpi.fc_r.bias 1\n",
      "output_rvpi.fc_v.weight 800\n",
      "output_rvpi.fc_v.bias 1\n",
      "output_rvpi.fc_logits.weight 4000\n",
      "output_rvpi.fc_logits.bias 5\n"
     ]
    }
   ],
   "source": [
    "# Start training models\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())       \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.bsz = 32\n",
    "flags.unroll_len = 5\n",
    "flags.num_actors = 32\n",
    "flags.seq_n = 1000\n",
    "flags.seq_len = 200\n",
    "flags.learning_rate = 0.0001\n",
    "flags.loop_batch_n = 3\n",
    "flags.discounting = 0.97\n",
    "flags.tot_epoch = 10000\n",
    "flags.grad_norm_clipping = 60\n",
    "\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "# Create buffer for actors to write\n",
    "\n",
    "mp.set_sharing_strategy('file_system')\n",
    "ctx = mp.get_context()        \n",
    "\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "buffers = create_buffers_m(flags, obs_shape, num_actions)\n",
    "print(\"Buffer created successfully.\")\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "\n",
    "env = create_env(flags)\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=flags.learning_rate)\n",
    "\n",
    "print(\"model size: \", sum(p.numel() for p in model.parameters()))\n",
    "for k, v in model.named_parameters(): print(k, v.numel())    \n",
    "    \n",
    "tot_step = int(flags.loop_batch_n * flags.seq_n * flags.seq_len / flags.bsz / flags.unroll_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301892ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1095814\n",
      "Batch [0] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.89 (+-0.05) \n",
      "[0:0] F: 0 \t tot_loss 14.640764 rs_loss 1.029019 vs_loss 1.043833 logits_loss 251.358246\n",
      "[0:100] F: 16000 \t tot_loss 16.105540 rs_loss 1.046412 vs_loss 2.643464 logits_loss 248.313289\n",
      "[0:200] F: 32000 \t tot_loss 16.298016 rs_loss 1.103860 vs_loss 2.820005 logits_loss 247.483026\n",
      "[0:300] F: 48000 \t tot_loss 16.296381 rs_loss 1.098234 vs_loss 2.851524 logits_loss 246.932449\n",
      "[0:400] F: 64000 \t tot_loss 16.202269 rs_loss 1.063980 vs_loss 2.811422 logits_loss 246.537344\n",
      "[0:500] F: 80000 \t tot_loss 16.160249 rs_loss 1.041836 vs_loss 2.832167 logits_loss 245.724923\n",
      "[0:600] F: 96000 \t tot_loss 16.122170 rs_loss 1.054497 vs_loss 2.809314 logits_loss 245.167159\n",
      "[0:700] F: 112000 \t tot_loss 15.965977 rs_loss 1.031396 vs_loss 2.703742 logits_loss 244.616772\n",
      "[0:800] F: 128000 \t tot_loss 15.895542 rs_loss 1.016114 vs_loss 2.666313 logits_loss 244.262317\n",
      "[0:900] F: 144000 \t tot_loss 15.672175 rs_loss 0.977136 vs_loss 2.490738 logits_loss 244.086018\n",
      "[0:1000] F: 160000 \t tot_loss 15.752443 rs_loss 0.955057 vs_loss 2.589410 logits_loss 244.159500\n",
      "[0:1100] F: 176000 \t tot_loss 15.567228 rs_loss 0.903416 vs_loss 2.450095 logits_loss 244.274341\n",
      "[0:1200] F: 192000 \t tot_loss 15.631229 rs_loss 0.938114 vs_loss 2.489914 logits_loss 244.064011\n",
      "[0:1300] F: 208000 \t tot_loss 15.631730 rs_loss 0.942257 vs_loss 2.500640 logits_loss 243.776654\n",
      "[0:1400] F: 224000 \t tot_loss 15.391040 rs_loss 0.900627 vs_loss 2.303482 logits_loss 243.738614\n",
      "[0:1500] F: 240000 \t tot_loss 15.526229 rs_loss 0.921754 vs_loss 2.418169 logits_loss 243.726120\n",
      "[0:1600] F: 256000 \t tot_loss 15.573233 rs_loss 0.936197 vs_loss 2.435978 logits_loss 244.021168\n",
      "[0:1700] F: 272000 \t tot_loss 15.659035 rs_loss 0.975950 vs_loss 2.471729 logits_loss 244.227110\n",
      "[0:1800] F: 288000 \t tot_loss 15.859923 rs_loss 1.028252 vs_loss 2.642756 logits_loss 243.778303\n",
      "[0:1900] F: 304000 \t tot_loss 15.889266 rs_loss 1.047839 vs_loss 2.645413 logits_loss 243.920282\n",
      "[0:2000] F: 320000 \t tot_loss 15.716094 rs_loss 0.992827 vs_loss 2.542092 logits_loss 243.623506\n",
      "[0:2100] F: 336000 \t tot_loss 15.708393 rs_loss 0.978269 vs_loss 2.561135 logits_loss 243.379766\n",
      "[0:2200] F: 352000 \t tot_loss 15.528434 rs_loss 0.920777 vs_loss 2.430191 logits_loss 243.549317\n",
      "[0:2300] F: 368000 \t tot_loss 15.630023 rs_loss 0.940443 vs_loss 2.531454 logits_loss 243.162528\n",
      "[0:2400] F: 384000 \t tot_loss 15.837516 rs_loss 0.984304 vs_loss 2.692085 logits_loss 243.222539\n",
      "[0:2500] F: 400000 \t tot_loss 16.062617 rs_loss 1.023405 vs_loss 2.859180 logits_loss 243.600633\n",
      "[0:2600] F: 416000 \t tot_loss 15.945117 rs_loss 0.990837 vs_loss 2.776964 logits_loss 243.546311\n",
      "[0:2700] F: 432000 \t tot_loss 15.995097 rs_loss 0.993488 vs_loss 2.805272 logits_loss 243.926731\n",
      "[0:2800] F: 448000 \t tot_loss 15.823906 rs_loss 0.955425 vs_loss 2.657321 logits_loss 244.223187\n",
      "[0:2900] F: 464000 \t tot_loss 16.145861 rs_loss 1.023182 vs_loss 2.928194 logits_loss 243.889694\n",
      "[0:3000] F: 480000 \t tot_loss 16.170348 rs_loss 1.064400 vs_loss 2.945470 logits_loss 243.209568\n",
      "[0:3100] F: 496000 \t tot_loss 16.234330 rs_loss 1.104229 vs_loss 3.037595 logits_loss 241.850119\n",
      "[0:3200] F: 512000 \t tot_loss 16.126508 rs_loss 1.105478 vs_loss 3.022331 logits_loss 239.973975\n",
      "[0:3300] F: 528000 \t tot_loss 15.404949 rs_loss 0.972899 vs_loss 2.521586 logits_loss 238.209270\n",
      "[0:3400] F: 544000 \t tot_loss 15.193083 rs_loss 0.925851 vs_loss 2.430977 logits_loss 236.725099\n",
      "[0:3500] F: 560000 \t tot_loss 15.047522 rs_loss 0.897400 vs_loss 2.371121 logits_loss 235.580016\n",
      "[0:3600] F: 576000 \t tot_loss 15.377650 rs_loss 1.008864 vs_loss 2.642921 logits_loss 234.517283\n",
      "[0:3700] F: 592000 \t tot_loss 15.465347 rs_loss 1.059157 vs_loss 2.720802 logits_loss 233.707762\n",
      "Batch [1] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.60 (+-0.16) \n",
      "[1:0] F: 600000 \t tot_loss 15.711426 rs_loss 1.125787 vs_loss 2.920862 logits_loss 233.295545\n",
      "[1:100] F: 616000 \t tot_loss 15.895787 rs_loss 1.187751 vs_loss 3.039667 logits_loss 233.367371\n",
      "[1:200] F: 632000 \t tot_loss 15.214531 rs_loss 1.054411 vs_loss 2.507681 logits_loss 233.048790\n",
      "[1:300] F: 648000 \t tot_loss 14.962223 rs_loss 0.961465 vs_loss 2.348199 logits_loss 233.051180\n",
      "[1:400] F: 664000 \t tot_loss 14.692072 rs_loss 0.935167 vs_loss 2.123322 logits_loss 232.671637\n",
      "[1:500] F: 680000 \t tot_loss 14.368283 rs_loss 0.864782 vs_loss 1.900243 logits_loss 232.065170\n",
      "[1:600] F: 696000 \t tot_loss 14.804382 rs_loss 0.965698 vs_loss 2.242383 logits_loss 231.926035\n",
      "[1:700] F: 712000 \t tot_loss 15.068455 rs_loss 1.009639 vs_loss 2.457418 logits_loss 232.027954\n",
      "[1:800] F: 728000 \t tot_loss 15.020738 rs_loss 0.966196 vs_loss 2.456807 logits_loss 231.954689\n",
      "[1:900] F: 744000 \t tot_loss 15.026126 rs_loss 0.971671 vs_loss 2.469317 logits_loss 231.702766\n",
      "[1:1000] F: 760000 \t tot_loss 14.779636 rs_loss 0.962547 vs_loss 2.269708 logits_loss 230.947606\n",
      "[1:1100] F: 776000 \t tot_loss 14.472647 rs_loss 0.909479 vs_loss 2.031208 logits_loss 230.639211\n",
      "[1:1200] F: 792000 \t tot_loss 14.496277 rs_loss 0.933499 vs_loss 2.040734 logits_loss 230.440891\n",
      "[1:1300] F: 808000 \t tot_loss 14.391839 rs_loss 0.905810 vs_loss 1.960601 logits_loss 230.508555\n",
      "[1:1400] F: 824000 \t tot_loss 14.110410 rs_loss 0.792211 vs_loss 1.788778 logits_loss 230.588424\n",
      "[1:1500] F: 840000 \t tot_loss 14.129924 rs_loss 0.791366 vs_loss 1.816150 logits_loss 230.448158\n",
      "[1:1600] F: 856000 \t tot_loss 14.444423 rs_loss 0.870788 vs_loss 2.063140 logits_loss 230.209907\n",
      "[1:1700] F: 872000 \t tot_loss 14.734193 rs_loss 0.932649 vs_loss 2.316038 logits_loss 229.710112\n",
      "[1:1800] F: 888000 \t tot_loss 14.948540 rs_loss 1.007117 vs_loss 2.466148 logits_loss 229.505487\n",
      "[1:1900] F: 904000 \t tot_loss 14.908573 rs_loss 1.002598 vs_loss 2.440540 logits_loss 229.308705\n",
      "[1:2000] F: 920000 \t tot_loss 14.605505 rs_loss 0.919453 vs_loss 2.215504 logits_loss 229.410949\n",
      "[1:2100] F: 936000 \t tot_loss 14.450522 rs_loss 0.915453 vs_loss 2.077323 logits_loss 229.154912\n",
      "[1:2200] F: 952000 \t tot_loss 14.308086 rs_loss 0.854993 vs_loss 1.983499 logits_loss 229.391866\n",
      "[1:2300] F: 968000 \t tot_loss 14.361250 rs_loss 0.863505 vs_loss 2.021418 logits_loss 229.526541\n",
      "[1:2400] F: 984000 \t tot_loss 14.274376 rs_loss 0.854510 vs_loss 1.968938 logits_loss 229.018560\n",
      "[1:2500] F: 1000000 \t tot_loss 14.243766 rs_loss 0.829204 vs_loss 1.968899 logits_loss 228.913254\n",
      "[1:2600] F: 1016000 \t tot_loss 14.393220 rs_loss 0.890882 vs_loss 2.061003 logits_loss 228.826713\n",
      "[1:2700] F: 1032000 \t tot_loss 14.411527 rs_loss 0.882739 vs_loss 2.092013 logits_loss 228.735499\n",
      "[1:2800] F: 1048000 \t tot_loss 14.633459 rs_loss 0.944971 vs_loss 2.244304 logits_loss 228.883670\n",
      "[1:2900] F: 1064000 \t tot_loss 14.896524 rs_loss 0.983138 vs_loss 2.441506 logits_loss 229.437590\n",
      "[1:3000] F: 1080000 \t tot_loss 14.825261 rs_loss 0.945553 vs_loss 2.434323 logits_loss 228.907678\n",
      "[1:3100] F: 1096000 \t tot_loss 14.720072 rs_loss 0.938059 vs_loss 2.345196 logits_loss 228.736331\n",
      "[1:3200] F: 1112000 \t tot_loss 14.802748 rs_loss 0.942873 vs_loss 2.452621 logits_loss 228.145085\n",
      "[1:3300] F: 1128000 \t tot_loss 14.510798 rs_loss 0.897393 vs_loss 2.229715 logits_loss 227.673798\n",
      "[1:3400] F: 1144000 \t tot_loss 14.550245 rs_loss 0.892457 vs_loss 2.253599 logits_loss 228.083783\n",
      "[1:3500] F: 1160000 \t tot_loss 14.715961 rs_loss 0.949372 vs_loss 2.393533 logits_loss 227.461143\n",
      "[1:3600] F: 1176000 \t tot_loss 14.408948 rs_loss 0.872002 vs_loss 2.138432 logits_loss 227.970274\n",
      "[1:3700] F: 1192000 \t tot_loss 14.597103 rs_loss 0.908378 vs_loss 2.296392 logits_loss 227.846652\n",
      "Batch [2] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.70 (+-0.06) \n",
      "[2:0] F: 1200000 \t tot_loss 14.452698 rs_loss 0.891582 vs_loss 2.186206 logits_loss 227.498198\n",
      "[2:100] F: 1216000 \t tot_loss 14.489875 rs_loss 0.894359 vs_loss 2.215017 logits_loss 227.609984\n",
      "[2:200] F: 1232000 \t tot_loss 14.792384 rs_loss 0.966506 vs_loss 2.432856 logits_loss 227.860415\n",
      "[2:300] F: 1248000 \t tot_loss 14.873270 rs_loss 0.994928 vs_loss 2.495167 logits_loss 227.663495\n",
      "[2:400] F: 1264000 \t tot_loss 15.032982 rs_loss 1.061780 vs_loss 2.577733 logits_loss 227.869375\n",
      "[2:500] F: 1280000 \t tot_loss 15.172925 rs_loss 1.093256 vs_loss 2.672355 logits_loss 228.146280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2:600] F: 1296000 \t tot_loss 14.845886 rs_loss 1.007950 vs_loss 2.436346 logits_loss 228.031804\n",
      "[2:700] F: 1312000 \t tot_loss 14.646563 rs_loss 0.958572 vs_loss 2.296913 logits_loss 227.821552\n",
      "[2:800] F: 1328000 \t tot_loss 14.556474 rs_loss 0.912315 vs_loss 2.244836 logits_loss 227.986440\n",
      "[2:900] F: 1344000 \t tot_loss 14.371935 rs_loss 0.879037 vs_loss 2.119026 logits_loss 227.477449\n",
      "[2:1000] F: 1360000 \t tot_loss 14.340837 rs_loss 0.875311 vs_loss 2.099806 logits_loss 227.314389\n",
      "[2:1100] F: 1376000 \t tot_loss 14.320544 rs_loss 0.875194 vs_loss 2.096319 logits_loss 226.980629\n",
      "[2:1200] F: 1392000 \t tot_loss 14.400485 rs_loss 0.915019 vs_loss 2.146988 logits_loss 226.769555\n",
      "[2:1300] F: 1408000 \t tot_loss 14.532853 rs_loss 0.935491 vs_loss 2.270988 logits_loss 226.527466\n",
      "[2:1400] F: 1424000 \t tot_loss 14.611504 rs_loss 0.930826 vs_loss 2.370307 logits_loss 226.207411\n",
      "[2:1500] F: 1440000 \t tot_loss 14.828284 rs_loss 1.017202 vs_loss 2.481347 logits_loss 226.594686\n",
      "[2:1600] F: 1456000 \t tot_loss 14.825798 rs_loss 0.986347 vs_loss 2.535916 logits_loss 226.070696\n",
      "[2:1700] F: 1472000 \t tot_loss 14.856249 rs_loss 0.979959 vs_loss 2.548111 logits_loss 226.563596\n",
      "[2:1800] F: 1488000 \t tot_loss 14.854625 rs_loss 0.986679 vs_loss 2.512681 logits_loss 227.105308\n",
      "[2:1900] F: 1504000 \t tot_loss 14.784861 rs_loss 0.942333 vs_loss 2.481893 logits_loss 227.212686\n",
      "[2:2000] F: 1520000 \t tot_loss 15.004634 rs_loss 0.971717 vs_loss 2.653481 logits_loss 227.588707\n",
      "[2:2100] F: 1536000 \t tot_loss 15.250793 rs_loss 1.049180 vs_loss 2.844286 logits_loss 227.146516\n",
      "[2:2200] F: 1552000 \t tot_loss 15.384011 rs_loss 1.073655 vs_loss 2.980448 logits_loss 226.598142\n",
      "[2:2300] F: 1568000 \t tot_loss 15.489930 rs_loss 1.058842 vs_loss 3.119928 logits_loss 226.223197\n",
      "[2:2400] F: 1584000 \t tot_loss 15.304480 rs_loss 1.060782 vs_loss 2.952277 logits_loss 225.828403\n",
      "[2:2500] F: 1600000 \t tot_loss 14.830771 rs_loss 0.952846 vs_loss 2.585328 logits_loss 225.851942\n",
      "[2:2600] F: 1616000 \t tot_loss 14.796377 rs_loss 0.969747 vs_loss 2.544989 logits_loss 225.632808\n",
      "[2:2700] F: 1632000 \t tot_loss 14.638480 rs_loss 0.939748 vs_loss 2.401068 logits_loss 225.953265\n",
      "[2:2800] F: 1648000 \t tot_loss 14.613545 rs_loss 0.931576 vs_loss 2.393192 logits_loss 225.775539\n",
      "[2:2900] F: 1664000 \t tot_loss 14.713401 rs_loss 0.951598 vs_loss 2.477460 logits_loss 225.686860\n",
      "[2:3000] F: 1680000 \t tot_loss 14.805549 rs_loss 0.989294 vs_loss 2.529467 logits_loss 225.735742\n",
      "[2:3100] F: 1696000 \t tot_loss 14.675534 rs_loss 0.994507 vs_loss 2.415589 logits_loss 225.308746\n",
      "[2:3200] F: 1712000 \t tot_loss 14.726165 rs_loss 0.998901 vs_loss 2.459786 logits_loss 225.349560\n",
      "[2:3300] F: 1728000 \t tot_loss 14.763838 rs_loss 0.997074 vs_loss 2.498872 logits_loss 225.357826\n",
      "[2:3400] F: 1744000 \t tot_loss 14.678340 rs_loss 0.944427 vs_loss 2.468730 logits_loss 225.303654\n",
      "[2:3500] F: 1760000 \t tot_loss 14.621588 rs_loss 0.895170 vs_loss 2.466466 logits_loss 225.199041\n",
      "[2:3600] F: 1776000 \t tot_loss 14.591638 rs_loss 0.890612 vs_loss 2.454900 logits_loss 224.922520\n",
      "[2:3700] F: 1792000 \t tot_loss 14.738026 rs_loss 0.937132 vs_loss 2.562402 logits_loss 224.769843\n",
      "Batch [3] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.56 (+-0.07) \n",
      "[3:0] F: 1800000 \t tot_loss 14.825986 rs_loss 0.965200 vs_loss 2.626602 logits_loss 224.683684\n",
      "[3:100] F: 1816000 \t tot_loss 14.794049 rs_loss 0.965035 vs_loss 2.586180 logits_loss 224.856668\n",
      "[3:200] F: 1832000 \t tot_loss 14.768193 rs_loss 0.935968 vs_loss 2.569015 logits_loss 225.264214\n",
      "[3:300] F: 1848000 \t tot_loss 14.677502 rs_loss 0.925791 vs_loss 2.462621 logits_loss 225.781805\n",
      "[3:400] F: 1864000 \t tot_loss 14.613349 rs_loss 0.899558 vs_loss 2.409566 logits_loss 226.084498\n",
      "[3:500] F: 1880000 \t tot_loss 14.389465 rs_loss 0.860429 vs_loss 2.231268 logits_loss 225.955364\n",
      "[3:600] F: 1896000 \t tot_loss 14.303586 rs_loss 0.855957 vs_loss 2.167840 logits_loss 225.595771\n",
      "[3:700] F: 1912000 \t tot_loss 14.414577 rs_loss 0.851614 vs_loss 2.298138 logits_loss 225.296490\n",
      "[3:800] F: 1928000 \t tot_loss 14.608483 rs_loss 0.885856 vs_loss 2.454084 logits_loss 225.370862\n",
      "[3:900] F: 1944000 \t tot_loss 14.729827 rs_loss 0.886080 vs_loss 2.563259 logits_loss 225.609739\n",
      "[3:1000] F: 1960000 \t tot_loss 14.714623 rs_loss 0.883686 vs_loss 2.530067 logits_loss 226.017402\n",
      "[3:1100] F: 1976000 \t tot_loss 14.811542 rs_loss 0.957991 vs_loss 2.555027 logits_loss 225.970473\n",
      "[3:1200] F: 1992000 \t tot_loss 14.674494 rs_loss 0.937262 vs_loss 2.460049 logits_loss 225.543641\n",
      "[3:1300] F: 2008000 \t tot_loss 14.555191 rs_loss 0.915595 vs_loss 2.374632 logits_loss 225.299283\n",
      "[3:1400] F: 2024000 \t tot_loss 14.616037 rs_loss 0.942932 vs_loss 2.401321 logits_loss 225.435670\n",
      "[3:1500] F: 2040000 \t tot_loss 14.725414 rs_loss 0.901834 vs_loss 2.543090 logits_loss 225.609802\n",
      "[3:1600] F: 2056000 \t tot_loss 14.698927 rs_loss 0.882096 vs_loss 2.516004 logits_loss 226.016543\n",
      "[3:1700] F: 2072000 \t tot_loss 14.918675 rs_loss 0.931366 vs_loss 2.693847 logits_loss 225.869217\n",
      "[3:1800] F: 2088000 \t tot_loss 14.906092 rs_loss 0.904237 vs_loss 2.731462 logits_loss 225.407845\n",
      "[3:1900] F: 2104000 \t tot_loss 14.760840 rs_loss 0.888968 vs_loss 2.600450 logits_loss 225.428431\n",
      "[3:2000] F: 2120000 \t tot_loss 14.685530 rs_loss 0.888418 vs_loss 2.559927 logits_loss 224.743693\n",
      "[3:2100] F: 2136000 \t tot_loss 14.663521 rs_loss 0.909829 vs_loss 2.507230 logits_loss 224.929248\n",
      "[3:2200] F: 2152000 \t tot_loss 14.853315 rs_loss 0.953317 vs_loss 2.657059 logits_loss 224.858767\n",
      "[3:2300] F: 2168000 \t tot_loss 14.619398 rs_loss 0.911627 vs_loss 2.477480 logits_loss 224.605818\n",
      "[3:2400] F: 2184000 \t tot_loss 14.682333 rs_loss 0.920080 vs_loss 2.532671 logits_loss 224.591636\n",
      "[3:2500] F: 2200000 \t tot_loss 14.675431 rs_loss 0.887355 vs_loss 2.564568 logits_loss 224.470152\n",
      "[3:2600] F: 2216000 \t tot_loss 14.377162 rs_loss 0.814243 vs_loss 2.344547 logits_loss 224.367431\n",
      "[3:2700] F: 2232000 \t tot_loss 14.384306 rs_loss 0.814169 vs_loss 2.355684 logits_loss 224.289058\n",
      "[3:2800] F: 2248000 \t tot_loss 14.495621 rs_loss 0.830076 vs_loss 2.426095 logits_loss 224.789002\n",
      "[3:2900] F: 2264000 \t tot_loss 14.311038 rs_loss 0.788069 vs_loss 2.270727 logits_loss 225.044820\n",
      "[3:3000] F: 2280000 \t tot_loss 14.424821 rs_loss 0.811670 vs_loss 2.342577 logits_loss 225.411467\n",
      "[3:3100] F: 2296000 \t tot_loss 14.618417 rs_loss 0.851881 vs_loss 2.510506 logits_loss 225.120591\n",
      "[3:3200] F: 2312000 \t tot_loss 14.750696 rs_loss 0.874660 vs_loss 2.620455 logits_loss 225.111611\n",
      "[3:3300] F: 2328000 \t tot_loss 14.825784 rs_loss 0.896819 vs_loss 2.701079 logits_loss 224.557709\n",
      "[3:3400] F: 2344000 \t tot_loss 14.717347 rs_loss 0.885566 vs_loss 2.619973 logits_loss 224.236150\n",
      "[3:3500] F: 2360000 \t tot_loss 14.640115 rs_loss 0.871088 vs_loss 2.535869 logits_loss 224.663162\n",
      "[3:3600] F: 2376000 \t tot_loss 14.368511 rs_loss 0.816804 vs_loss 2.333223 logits_loss 224.369681\n",
      "[3:3700] F: 2392000 \t tot_loss 14.593842 rs_loss 0.854828 vs_loss 2.501951 logits_loss 224.741257\n",
      "Batch [4] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.63 (+-0.06) \n",
      "[4:0] F: 2400000 \t tot_loss 14.685027 rs_loss 0.870182 vs_loss 2.591320 logits_loss 224.470496\n",
      "[4:100] F: 2416000 \t tot_loss 14.699803 rs_loss 0.853587 vs_loss 2.590164 logits_loss 225.121055\n",
      "[4:200] F: 2432000 \t tot_loss 14.648600 rs_loss 0.860209 vs_loss 2.512525 logits_loss 225.517323\n",
      "[4:300] F: 2448000 \t tot_loss 14.586978 rs_loss 0.854595 vs_loss 2.461413 logits_loss 225.419394\n",
      "[4:400] F: 2464000 \t tot_loss 14.144678 rs_loss 0.742286 vs_loss 2.096312 logits_loss 226.121596\n",
      "[4:500] F: 2480000 \t tot_loss 14.043756 rs_loss 0.717941 vs_loss 2.038680 logits_loss 225.742702\n",
      "[4:600] F: 2496000 \t tot_loss 14.137374 rs_loss 0.686764 vs_loss 2.145652 logits_loss 226.099143\n",
      "[4:700] F: 2512000 \t tot_loss 14.200560 rs_loss 0.688051 vs_loss 2.199643 logits_loss 226.257332\n",
      "[4:800] F: 2528000 \t tot_loss 14.491182 rs_loss 0.769737 vs_loss 2.424702 logits_loss 225.934858\n",
      "[4:900] F: 2544000 \t tot_loss 14.426785 rs_loss 0.767339 vs_loss 2.385167 logits_loss 225.485577\n",
      "[4:1000] F: 2560000 \t tot_loss 14.293411 rs_loss 0.745819 vs_loss 2.287642 logits_loss 225.198989\n",
      "[4:1100] F: 2576000 \t tot_loss 14.280566 rs_loss 0.738725 vs_loss 2.270439 logits_loss 225.428058\n",
      "[4:1200] F: 2592000 \t tot_loss 14.258825 rs_loss 0.743518 vs_loss 2.243062 logits_loss 225.444896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4:1300] F: 2608000 \t tot_loss 14.372716 rs_loss 0.761748 vs_loss 2.321020 logits_loss 225.798967\n",
      "[4:1400] F: 2624000 \t tot_loss 14.522732 rs_loss 0.783690 vs_loss 2.445882 logits_loss 225.863211\n",
      "[4:1500] F: 2640000 \t tot_loss 14.675504 rs_loss 0.831929 vs_loss 2.572531 logits_loss 225.420866\n",
      "[4:1600] F: 2656000 \t tot_loss 14.754399 rs_loss 0.823619 vs_loss 2.643042 logits_loss 225.754763\n",
      "[4:1700] F: 2672000 \t tot_loss 14.685498 rs_loss 0.801195 vs_loss 2.590714 logits_loss 225.871789\n",
      "[4:1800] F: 2688000 \t tot_loss 14.631081 rs_loss 0.797148 vs_loss 2.558476 logits_loss 225.509129\n",
      "[4:1900] F: 2704000 \t tot_loss 14.624679 rs_loss 0.780388 vs_loss 2.568900 logits_loss 225.507794\n",
      "[4:2000] F: 2720000 \t tot_loss 14.451653 rs_loss 0.785000 vs_loss 2.396265 logits_loss 225.407754\n",
      "[4:2100] F: 2736000 \t tot_loss 14.370537 rs_loss 0.772310 vs_loss 2.348072 logits_loss 225.003089\n",
      "[4:2200] F: 2752000 \t tot_loss 14.420027 rs_loss 0.801249 vs_loss 2.367655 logits_loss 225.022456\n",
      "[4:2300] F: 2768000 \t tot_loss 14.251664 rs_loss 0.767645 vs_loss 2.218589 logits_loss 225.308582\n",
      "[4:2400] F: 2784000 \t tot_loss 14.097018 rs_loss 0.699094 vs_loss 2.142531 logits_loss 225.107856\n",
      "[4:2500] F: 2800000 \t tot_loss 14.200960 rs_loss 0.735617 vs_loss 2.205707 logits_loss 225.192725\n",
      "[4:2600] F: 2816000 \t tot_loss 14.539454 rs_loss 0.808592 vs_loss 2.479788 logits_loss 225.021479\n",
      "[4:2700] F: 2832000 \t tot_loss 14.670716 rs_loss 0.831206 vs_loss 2.606982 logits_loss 224.650567\n",
      "[4:2800] F: 2848000 \t tot_loss 14.846585 rs_loss 0.876319 vs_loss 2.744751 logits_loss 224.510294\n",
      "[4:2900] F: 2864000 \t tot_loss 14.715524 rs_loss 0.823646 vs_loss 2.669226 logits_loss 224.453036\n",
      "[4:3000] F: 2880000 \t tot_loss 14.280217 rs_loss 0.722534 vs_loss 2.326014 logits_loss 224.633368\n",
      "[4:3100] F: 2896000 \t tot_loss 14.337296 rs_loss 0.739263 vs_loss 2.369228 logits_loss 224.576102\n",
      "[4:3200] F: 2912000 \t tot_loss 14.177159 rs_loss 0.697819 vs_loss 2.252780 logits_loss 224.531186\n",
      "[4:3300] F: 2928000 \t tot_loss 14.186528 rs_loss 0.714239 vs_loss 2.258650 logits_loss 224.272769\n",
      "[4:3400] F: 2944000 \t tot_loss 14.123714 rs_loss 0.701952 vs_loss 2.231082 logits_loss 223.813586\n",
      "[4:3500] F: 2960000 \t tot_loss 14.176183 rs_loss 0.713851 vs_loss 2.255080 logits_loss 224.145042\n",
      "[4:3600] F: 2976000 \t tot_loss 14.240084 rs_loss 0.722300 vs_loss 2.305913 logits_loss 224.237415\n",
      "[4:3700] F: 2992000 \t tot_loss 14.209558 rs_loss 0.715795 vs_loss 2.277382 logits_loss 224.327616\n",
      "Batch [5] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.59 (+-0.07) \n",
      "[5:0] F: 3000000 \t tot_loss 14.084498 rs_loss 0.666725 vs_loss 2.203490 logits_loss 224.285655\n",
      "[5:100] F: 3016000 \t tot_loss 14.006993 rs_loss 0.652696 vs_loss 2.126742 logits_loss 224.551097\n",
      "[5:200] F: 3032000 \t tot_loss 14.208851 rs_loss 0.714893 vs_loss 2.317617 logits_loss 223.526819\n",
      "[5:300] F: 3048000 \t tot_loss 14.587446 rs_loss 0.802570 vs_loss 2.607139 logits_loss 223.554715\n",
      "[5:400] F: 3064000 \t tot_loss 14.595673 rs_loss 0.805850 vs_loss 2.622822 logits_loss 223.340015\n",
      "[5:500] F: 3080000 \t tot_loss 14.509340 rs_loss 0.804427 vs_loss 2.576740 logits_loss 222.563448\n",
      "[5:600] F: 3096000 \t tot_loss 14.232574 rs_loss 0.720485 vs_loss 2.366020 logits_loss 222.921363\n",
      "[5:700] F: 3112000 \t tot_loss 13.836677 rs_loss 0.630698 vs_loss 2.088011 logits_loss 222.359334\n",
      "[5:800] F: 3128000 \t tot_loss 14.147311 rs_loss 0.715667 vs_loss 2.316521 logits_loss 222.302448\n",
      "[5:900] F: 3144000 \t tot_loss 14.307085 rs_loss 0.737512 vs_loss 2.404178 logits_loss 223.307899\n",
      "[5:1000] F: 3160000 \t tot_loss 14.526430 rs_loss 0.796509 vs_loss 2.566826 logits_loss 223.261902\n",
      "[5:1100] F: 3176000 \t tot_loss 14.580791 rs_loss 0.807234 vs_loss 2.637721 logits_loss 222.716712\n",
      "[5:1200] F: 3192000 \t tot_loss 14.661703 rs_loss 0.819867 vs_loss 2.698021 logits_loss 222.876314\n",
      "[5:1300] F: 3208000 \t tot_loss 14.394611 rs_loss 0.750706 vs_loss 2.532884 logits_loss 222.220404\n",
      "[5:1400] F: 3224000 \t tot_loss 14.262980 rs_loss 0.718760 vs_loss 2.442886 logits_loss 222.026680\n",
      "[5:1500] F: 3240000 \t tot_loss 14.162225 rs_loss 0.694553 vs_loss 2.335509 logits_loss 222.643257\n",
      "[5:1600] F: 3256000 \t tot_loss 13.992066 rs_loss 0.671321 vs_loss 2.203218 logits_loss 222.350537\n",
      "[5:1700] F: 3272000 \t tot_loss 13.976279 rs_loss 0.682729 vs_loss 2.175658 logits_loss 222.357838\n",
      "[5:1800] F: 3288000 \t tot_loss 13.905830 rs_loss 0.689897 vs_loss 2.092851 logits_loss 222.461639\n",
      "[5:1900] F: 3304000 \t tot_loss 14.238890 rs_loss 0.755626 vs_loss 2.354522 logits_loss 222.574829\n",
      "[5:2000] F: 3320000 \t tot_loss 14.161997 rs_loss 0.741633 vs_loss 2.305960 logits_loss 222.288059\n",
      "[5:2100] F: 3336000 \t tot_loss 14.432864 rs_loss 0.776422 vs_loss 2.540937 logits_loss 222.310091\n",
      "[5:2200] F: 3352000 \t tot_loss 14.479322 rs_loss 0.762014 vs_loss 2.607479 logits_loss 222.196569\n",
      "[5:2300] F: 3368000 \t tot_loss 14.323837 rs_loss 0.738304 vs_loss 2.480759 logits_loss 222.095475\n",
      "[5:2400] F: 3384000 \t tot_loss 14.505464 rs_loss 0.748060 vs_loss 2.647558 logits_loss 222.196922\n",
      "[5:2500] F: 3400000 \t tot_loss 14.318342 rs_loss 0.724298 vs_loss 2.491194 logits_loss 222.057001\n",
      "[5:2600] F: 3416000 \t tot_loss 14.378036 rs_loss 0.732839 vs_loss 2.533517 logits_loss 222.233607\n",
      "[5:2700] F: 3432000 \t tot_loss 14.220244 rs_loss 0.682627 vs_loss 2.436878 logits_loss 222.014787\n",
      "[5:2800] F: 3448000 \t tot_loss 14.218771 rs_loss 0.716301 vs_loss 2.381962 logits_loss 222.410156\n",
      "[5:2900] F: 3464000 \t tot_loss 14.169786 rs_loss 0.716905 vs_loss 2.334729 logits_loss 222.363028\n",
      "[5:3000] F: 3480000 \t tot_loss 14.006363 rs_loss 0.685619 vs_loss 2.213873 logits_loss 222.137417\n",
      "[5:3100] F: 3496000 \t tot_loss 13.969209 rs_loss 0.698321 vs_loss 2.176787 logits_loss 221.882024\n",
      "[5:3200] F: 3512000 \t tot_loss 13.585702 rs_loss 0.587127 vs_loss 1.938652 logits_loss 221.198453\n",
      "[5:3300] F: 3528000 \t tot_loss 13.652913 rs_loss 0.587287 vs_loss 2.003984 logits_loss 221.232841\n",
      "[5:3400] F: 3544000 \t tot_loss 13.669859 rs_loss 0.598982 vs_loss 2.014030 logits_loss 221.136935\n",
      "[5:3500] F: 3560000 \t tot_loss 13.845194 rs_loss 0.626177 vs_loss 2.130493 logits_loss 221.770494\n",
      "[5:3600] F: 3576000 \t tot_loss 14.004243 rs_loss 0.667516 vs_loss 2.238641 logits_loss 221.961720\n",
      "[5:3700] F: 3592000 \t tot_loss 13.964473 rs_loss 0.677998 vs_loss 2.199383 logits_loss 221.741841\n",
      "Batch [6] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.62 (+-0.07) \n",
      "[6:0] F: 3600000 \t tot_loss 14.048405 rs_loss 0.698510 vs_loss 2.268897 logits_loss 221.619948\n",
      "[6:100] F: 3616000 \t tot_loss 14.098110 rs_loss 0.693239 vs_loss 2.276597 logits_loss 222.565467\n",
      "[6:200] F: 3632000 \t tot_loss 14.248068 rs_loss 0.715793 vs_loss 2.381946 logits_loss 223.006574\n",
      "[6:300] F: 3648000 \t tot_loss 14.353512 rs_loss 0.726499 vs_loss 2.448811 logits_loss 223.564051\n",
      "[6:400] F: 3664000 \t tot_loss 14.192816 rs_loss 0.666990 vs_loss 2.310237 logits_loss 224.311792\n",
      "[6:500] F: 3680000 \t tot_loss 14.132222 rs_loss 0.660447 vs_loss 2.280620 logits_loss 223.823112\n",
      "[6:600] F: 3696000 \t tot_loss 13.982164 rs_loss 0.637119 vs_loss 2.148316 logits_loss 223.934577\n",
      "[6:700] F: 3712000 \t tot_loss 14.105418 rs_loss 0.675996 vs_loss 2.221764 logits_loss 224.153143\n",
      "[6:800] F: 3728000 \t tot_loss 14.017372 rs_loss 0.662838 vs_loss 2.151983 logits_loss 224.051005\n",
      "[6:900] F: 3744000 \t tot_loss 14.021799 rs_loss 0.652011 vs_loss 2.168608 logits_loss 224.023584\n",
      "[6:1000] F: 3760000 \t tot_loss 14.029329 rs_loss 0.645826 vs_loss 2.201311 logits_loss 223.643838\n",
      "[6:1100] F: 3776000 \t tot_loss 13.846966 rs_loss 0.594761 vs_loss 2.060239 logits_loss 223.839317\n",
      "[6:1200] F: 3792000 \t tot_loss 13.954367 rs_loss 0.607409 vs_loss 2.144899 logits_loss 224.041184\n",
      "[6:1300] F: 3808000 \t tot_loss 13.926870 rs_loss 0.606258 vs_loss 2.107933 logits_loss 224.253569\n",
      "[6:1400] F: 3824000 \t tot_loss 13.824254 rs_loss 0.569576 vs_loss 2.032956 logits_loss 224.434430\n",
      "[6:1500] F: 3840000 \t tot_loss 13.822587 rs_loss 0.580038 vs_loss 2.048555 logits_loss 223.879884\n",
      "[6:1600] F: 3856000 \t tot_loss 13.786774 rs_loss 0.585062 vs_loss 2.027772 logits_loss 223.478790\n",
      "[6:1700] F: 3872000 \t tot_loss 13.838943 rs_loss 0.623441 vs_loss 2.055465 logits_loss 223.200734\n",
      "[6:1800] F: 3888000 \t tot_loss 13.890005 rs_loss 0.643284 vs_loss 2.097913 logits_loss 222.976163\n",
      "[6:1900] F: 3904000 \t tot_loss 13.889056 rs_loss 0.644983 vs_loss 2.082447 logits_loss 223.232522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6:2000] F: 3920000 \t tot_loss 13.861703 rs_loss 0.635288 vs_loss 2.077131 logits_loss 222.985686\n",
      "[6:2100] F: 3936000 \t tot_loss 14.122073 rs_loss 0.662774 vs_loss 2.318453 logits_loss 222.816900\n",
      "[6:2200] F: 3952000 \t tot_loss 14.007699 rs_loss 0.617173 vs_loss 2.238451 logits_loss 223.041506\n",
      "[6:2300] F: 3968000 \t tot_loss 14.070953 rs_loss 0.625802 vs_loss 2.280424 logits_loss 223.294542\n",
      "[6:2400] F: 3984000 \t tot_loss 14.113893 rs_loss 0.631159 vs_loss 2.298908 logits_loss 223.676515\n",
      "[6:2500] F: 4000000 \t tot_loss 13.721252 rs_loss 0.547922 vs_loss 2.001617 logits_loss 223.434262\n",
      "[6:2600] F: 4016000 \t tot_loss 13.855898 rs_loss 0.596238 vs_loss 2.105573 logits_loss 223.081746\n",
      "[6:2700] F: 4032000 \t tot_loss 14.134079 rs_loss 0.676104 vs_loss 2.323981 logits_loss 222.679875\n",
      "[6:2800] F: 4048000 \t tot_loss 14.572633 rs_loss 0.763855 vs_loss 2.672934 logits_loss 222.716873\n",
      "[6:2900] F: 4064000 \t tot_loss 14.743299 rs_loss 0.800815 vs_loss 2.788286 logits_loss 223.083960\n",
      "[6:3000] F: 4080000 \t tot_loss 14.703937 rs_loss 0.776819 vs_loss 2.757482 logits_loss 223.392715\n",
      "[6:3100] F: 4096000 \t tot_loss 14.575080 rs_loss 0.754437 vs_loss 2.644107 logits_loss 223.530714\n",
      "[6:3200] F: 4112000 \t tot_loss 14.087440 rs_loss 0.631156 vs_loss 2.273171 logits_loss 223.662248\n",
      "[6:3300] F: 4128000 \t tot_loss 14.150357 rs_loss 0.644886 vs_loss 2.329240 logits_loss 223.524609\n",
      "[6:3400] F: 4144000 \t tot_loss 14.074598 rs_loss 0.643805 vs_loss 2.250536 logits_loss 223.605153\n",
      "[6:3500] F: 4160000 \t tot_loss 13.968075 rs_loss 0.614436 vs_loss 2.182893 logits_loss 223.414915\n",
      "[6:3600] F: 4176000 \t tot_loss 13.968791 rs_loss 0.628275 vs_loss 2.196510 logits_loss 222.880111\n",
      "[6:3700] F: 4192000 \t tot_loss 13.784882 rs_loss 0.605985 vs_loss 2.036387 logits_loss 222.850200\n",
      "Batch [7] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.58 (+-0.07) \n",
      "[7:0] F: 4200000 \t tot_loss 13.783148 rs_loss 0.602233 vs_loss 2.039858 logits_loss 222.821131\n",
      "[7:100] F: 4216000 \t tot_loss 14.131148 rs_loss 0.656110 vs_loss 2.350462 logits_loss 222.491494\n",
      "[7:200] F: 4232000 \t tot_loss 14.031762 rs_loss 0.615107 vs_loss 2.290727 logits_loss 222.518571\n",
      "[7:300] F: 4248000 \t tot_loss 13.924337 rs_loss 0.592173 vs_loss 2.209811 logits_loss 222.447074\n",
      "[7:400] F: 4264000 \t tot_loss 13.885159 rs_loss 0.579491 vs_loss 2.201058 logits_loss 222.092199\n",
      "[7:500] F: 4280000 \t tot_loss 13.804288 rs_loss 0.572901 vs_loss 2.135730 logits_loss 221.913136\n",
      "[7:600] F: 4296000 \t tot_loss 13.845461 rs_loss 0.590740 vs_loss 2.160912 logits_loss 221.876175\n",
      "[7:700] F: 4312000 \t tot_loss 14.107791 rs_loss 0.645263 vs_loss 2.376256 logits_loss 221.725442\n",
      "[7:800] F: 4328000 \t tot_loss 14.283580 rs_loss 0.677717 vs_loss 2.501872 logits_loss 222.079813\n",
      "[7:900] F: 4344000 \t tot_loss 14.242082 rs_loss 0.681716 vs_loss 2.456308 logits_loss 222.081150\n",
      "[7:1000] F: 4360000 \t tot_loss 14.108419 rs_loss 0.655283 vs_loss 2.367934 logits_loss 221.704033\n",
      "[7:1100] F: 4376000 \t tot_loss 13.894176 rs_loss 0.625436 vs_loss 2.167580 logits_loss 222.023188\n",
      "[7:1200] F: 4392000 \t tot_loss 13.728916 rs_loss 0.591455 vs_loss 2.038106 logits_loss 221.987099\n",
      "[7:1300] F: 4408000 \t tot_loss 13.522820 rs_loss 0.543815 vs_loss 1.876840 logits_loss 222.043317\n",
      "[7:1400] F: 4424000 \t tot_loss 13.566374 rs_loss 0.553839 vs_loss 1.896619 logits_loss 222.318313\n",
      "[7:1500] F: 4440000 \t tot_loss 13.562342 rs_loss 0.542613 vs_loss 1.923200 logits_loss 221.930570\n",
      "[7:1600] F: 4456000 \t tot_loss 13.602048 rs_loss 0.553863 vs_loss 1.944489 logits_loss 222.073925\n",
      "[7:1700] F: 4472000 \t tot_loss 13.589691 rs_loss 0.533054 vs_loss 1.945622 logits_loss 222.220290\n",
      "[7:1800] F: 4488000 \t tot_loss 13.533296 rs_loss 0.530438 vs_loss 1.908753 logits_loss 221.882092\n",
      "[7:1900] F: 4504000 \t tot_loss 13.653580 rs_loss 0.563177 vs_loss 1.987446 logits_loss 222.059134\n",
      "[7:2000] F: 4520000 \t tot_loss 13.680730 rs_loss 0.597210 vs_loss 2.010698 logits_loss 221.456423\n",
      "[7:2100] F: 4536000 \t tot_loss 13.887007 rs_loss 0.662366 vs_loss 2.150458 logits_loss 221.483663\n",
      "[7:2200] F: 4552000 \t tot_loss 13.965070 rs_loss 0.682166 vs_loss 2.196193 logits_loss 221.734220\n",
      "[7:2300] F: 4568000 \t tot_loss 13.742662 rs_loss 0.619419 vs_loss 2.046349 logits_loss 221.537867\n",
      "[7:2400] F: 4584000 \t tot_loss 13.836446 rs_loss 0.601473 vs_loss 2.134287 logits_loss 222.013702\n",
      "[7:2500] F: 4600000 \t tot_loss 13.643843 rs_loss 0.542910 vs_loss 1.989600 logits_loss 222.226660\n",
      "[7:2600] F: 4616000 \t tot_loss 13.518157 rs_loss 0.495592 vs_loss 1.930103 logits_loss 221.849229\n",
      "[7:2700] F: 4632000 \t tot_loss 13.953472 rs_loss 0.594497 vs_loss 2.266836 logits_loss 221.842782\n",
      "[7:2800] F: 4648000 \t tot_loss 13.784133 rs_loss 0.566504 vs_loss 2.132528 logits_loss 221.702021\n",
      "[7:2900] F: 4664000 \t tot_loss 13.676104 rs_loss 0.551673 vs_loss 2.051489 logits_loss 221.458843\n",
      "[7:3000] F: 4680000 \t tot_loss 13.717818 rs_loss 0.554274 vs_loss 2.083547 logits_loss 221.599941\n",
      "[7:3100] F: 4696000 \t tot_loss 13.475255 rs_loss 0.495062 vs_loss 1.889904 logits_loss 221.805768\n",
      "[7:3200] F: 4712000 \t tot_loss 13.573720 rs_loss 0.540239 vs_loss 1.936828 logits_loss 221.933057\n",
      "[7:3300] F: 4728000 \t tot_loss 13.670000 rs_loss 0.558010 vs_loss 2.034300 logits_loss 221.553795\n",
      "[7:3400] F: 4744000 \t tot_loss 13.819607 rs_loss 0.590229 vs_loss 2.141301 logits_loss 221.761551\n",
      "[7:3500] F: 4760000 \t tot_loss 13.924829 rs_loss 0.611394 vs_loss 2.219266 logits_loss 221.883382\n",
      "[7:3600] F: 4776000 \t tot_loss 13.816840 rs_loss 0.565737 vs_loss 2.172365 logits_loss 221.574752\n",
      "[7:3700] F: 4792000 \t tot_loss 13.707938 rs_loss 0.537544 vs_loss 2.067627 logits_loss 222.055334\n",
      "Batch [8] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.53 (+-0.06) \n",
      "[8:0] F: 4800000 \t tot_loss 13.687403 rs_loss 0.533481 vs_loss 2.052661 logits_loss 222.025213\n",
      "[8:100] F: 4816000 \t tot_loss 13.587676 rs_loss 0.516404 vs_loss 1.939467 logits_loss 222.636090\n",
      "[8:200] F: 4832000 \t tot_loss 13.744738 rs_loss 0.540386 vs_loss 2.069171 logits_loss 222.703619\n",
      "[8:300] F: 4848000 \t tot_loss 13.911308 rs_loss 0.583887 vs_loss 2.181757 logits_loss 222.913281\n",
      "[8:400] F: 4864000 \t tot_loss 13.948068 rs_loss 0.599461 vs_loss 2.184500 logits_loss 223.282148\n",
      "[8:500] F: 4880000 \t tot_loss 13.778327 rs_loss 0.574654 vs_loss 2.070538 logits_loss 222.662707\n",
      "[8:600] F: 4896000 \t tot_loss 13.750141 rs_loss 0.597337 vs_loss 2.024619 logits_loss 222.563711\n",
      "[8:700] F: 4912000 \t tot_loss 13.849167 rs_loss 0.606525 vs_loss 2.110953 logits_loss 222.633770\n",
      "[8:800] F: 4928000 \t tot_loss 14.015000 rs_loss 0.653994 vs_loss 2.237006 logits_loss 222.479988\n",
      "[8:900] F: 4944000 \t tot_loss 13.933032 rs_loss 0.612870 vs_loss 2.170367 logits_loss 222.995905\n",
      "[8:1000] F: 4960000 \t tot_loss 13.926996 rs_loss 0.580420 vs_loss 2.192991 logits_loss 223.071692\n",
      "[8:1100] F: 4976000 \t tot_loss 13.811631 rs_loss 0.542293 vs_loss 2.106357 logits_loss 223.259605\n",
      "[8:1200] F: 4992000 \t tot_loss 13.688164 rs_loss 0.502041 vs_loss 2.027560 logits_loss 223.171257\n",
      "[8:1300] F: 5008000 \t tot_loss 13.776184 rs_loss 0.524517 vs_loss 2.108807 logits_loss 222.857201\n",
      "[8:1400] F: 5024000 \t tot_loss 13.833808 rs_loss 0.539361 vs_loss 2.151003 logits_loss 222.868868\n",
      "[8:1500] F: 5040000 \t tot_loss 13.837591 rs_loss 0.560604 vs_loss 2.149996 logits_loss 222.539807\n",
      "[8:1600] F: 5056000 \t tot_loss 13.798664 rs_loss 0.561552 vs_loss 2.117977 logits_loss 222.382704\n",
      "[8:1700] F: 5072000 \t tot_loss 13.771637 rs_loss 0.553532 vs_loss 2.099016 logits_loss 222.381765\n",
      "[8:1800] F: 5088000 \t tot_loss 13.581406 rs_loss 0.502282 vs_loss 1.954711 logits_loss 222.488244\n",
      "[8:1900] F: 5104000 \t tot_loss 13.558966 rs_loss 0.472026 vs_loss 1.937587 logits_loss 222.987055\n",
      "[8:2000] F: 5120000 \t tot_loss 13.626720 rs_loss 0.477653 vs_loss 2.011778 logits_loss 222.745766\n",
      "[8:2100] F: 5136000 \t tot_loss 13.619995 rs_loss 0.478096 vs_loss 2.009770 logits_loss 222.642599\n",
      "[8:2200] F: 5152000 \t tot_loss 13.713329 rs_loss 0.510962 vs_loss 2.078879 logits_loss 222.469755\n",
      "[8:2300] F: 5168000 \t tot_loss 13.664043 rs_loss 0.508241 vs_loss 2.030711 logits_loss 222.501815\n",
      "[8:2400] F: 5184000 \t tot_loss 13.631524 rs_loss 0.511050 vs_loss 1.972435 logits_loss 222.960770\n",
      "[8:2500] F: 5200000 \t tot_loss 13.620865 rs_loss 0.514331 vs_loss 1.966728 logits_loss 222.796108\n",
      "[8:2600] F: 5216000 \t tot_loss 13.712519 rs_loss 0.552518 vs_loss 2.032838 logits_loss 222.543255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8:2700] F: 5232000 \t tot_loss 13.783597 rs_loss 0.564036 vs_loss 2.104679 logits_loss 222.297639\n",
      "[8:2800] F: 5248000 \t tot_loss 13.831311 rs_loss 0.557914 vs_loss 2.157359 logits_loss 222.320750\n",
      "[8:2900] F: 5264000 \t tot_loss 14.059356 rs_loss 0.622404 vs_loss 2.322425 logits_loss 222.290533\n",
      "[8:3000] F: 5280000 \t tot_loss 13.988823 rs_loss 0.584619 vs_loss 2.273310 logits_loss 222.617886\n",
      "[8:3100] F: 5296000 \t tot_loss 13.921036 rs_loss 0.581884 vs_loss 2.211744 logits_loss 222.548158\n",
      "[8:3200] F: 5312000 \t tot_loss 13.813422 rs_loss 0.545931 vs_loss 2.151036 logits_loss 222.329103\n",
      "[8:3300] F: 5328000 \t tot_loss 13.673392 rs_loss 0.487498 vs_loss 2.055003 logits_loss 222.617814\n",
      "[8:3400] F: 5344000 \t tot_loss 13.631098 rs_loss 0.507068 vs_loss 2.010950 logits_loss 222.261591\n",
      "[8:3500] F: 5360000 \t tot_loss 13.884881 rs_loss 0.576771 vs_loss 2.200959 logits_loss 222.143005\n",
      "[8:3600] F: 5376000 \t tot_loss 14.105633 rs_loss 0.648381 vs_loss 2.347147 logits_loss 222.202095\n",
      "[8:3700] F: 5392000 \t tot_loss 14.389127 rs_loss 0.728942 vs_loss 2.557412 logits_loss 222.055457\n",
      "Batch [9] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.65 (+-0.07) \n",
      "[9:0] F: 5400000 \t tot_loss 14.578728 rs_loss 0.752459 vs_loss 2.720241 logits_loss 222.120552\n",
      "[9:100] F: 5416000 \t tot_loss 14.549088 rs_loss 0.737511 vs_loss 2.706551 logits_loss 222.100522\n",
      "[9:200] F: 5432000 \t tot_loss 14.395703 rs_loss 0.679554 vs_loss 2.606350 logits_loss 222.195970\n",
      "[9:300] F: 5448000 \t tot_loss 14.105656 rs_loss 0.617733 vs_loss 2.377677 logits_loss 222.204916\n",
      "[9:400] F: 5464000 \t tot_loss 13.694061 rs_loss 0.522338 vs_loss 2.049478 logits_loss 222.444895\n",
      "[9:500] F: 5480000 \t tot_loss 13.553544 rs_loss 0.488141 vs_loss 1.929196 logits_loss 222.724117\n",
      "[9:600] F: 5496000 \t tot_loss 13.806586 rs_loss 0.521573 vs_loss 2.143986 logits_loss 222.820528\n",
      "[9:700] F: 5512000 \t tot_loss 13.947165 rs_loss 0.550035 vs_loss 2.260347 logits_loss 222.735636\n",
      "[9:800] F: 5528000 \t tot_loss 13.903558 rs_loss 0.537254 vs_loss 2.226262 logits_loss 222.800815\n",
      "[9:900] F: 5544000 \t tot_loss 14.432038 rs_loss 0.646614 vs_loss 2.662386 logits_loss 222.460773\n",
      "[9:1000] F: 5560000 \t tot_loss 14.179139 rs_loss 0.610507 vs_loss 2.449275 logits_loss 222.387143\n",
      "[9:1100] F: 5576000 \t tot_loss 14.151720 rs_loss 0.605303 vs_loss 2.425198 logits_loss 222.424369\n",
      "[9:1200] F: 5592000 \t tot_loss 14.173035 rs_loss 0.622590 vs_loss 2.437965 logits_loss 222.249585\n",
      "[9:1300] F: 5608000 \t tot_loss 13.607966 rs_loss 0.498624 vs_loss 1.995238 logits_loss 222.282081\n",
      "[9:1400] F: 5624000 \t tot_loss 13.477058 rs_loss 0.478262 vs_loss 1.899711 logits_loss 221.981696\n",
      "[9:1500] F: 5640000 \t tot_loss 13.338618 rs_loss 0.452825 vs_loss 1.786702 logits_loss 221.981822\n",
      "[9:1600] F: 5656000 \t tot_loss 13.475178 rs_loss 0.464628 vs_loss 1.897492 logits_loss 222.261157\n",
      "[9:1700] F: 5672000 \t tot_loss 13.571660 rs_loss 0.491627 vs_loss 1.959848 logits_loss 222.403712\n",
      "[9:1800] F: 5688000 \t tot_loss 13.644557 rs_loss 0.495983 vs_loss 2.010593 logits_loss 222.759598\n",
      "[9:1900] F: 5704000 \t tot_loss 13.929000 rs_loss 0.542798 vs_loss 2.251718 logits_loss 222.689669\n",
      "[9:2000] F: 5720000 \t tot_loss 13.741334 rs_loss 0.506092 vs_loss 2.121774 logits_loss 222.269351\n",
      "[9:2100] F: 5736000 \t tot_loss 13.589981 rs_loss 0.484793 vs_loss 2.006338 logits_loss 221.976990\n",
      "[9:2200] F: 5752000 \t tot_loss 13.508202 rs_loss 0.480758 vs_loss 1.941410 logits_loss 221.720674\n",
      "[9:2300] F: 5768000 \t tot_loss 13.205768 rs_loss 0.430866 vs_loss 1.700103 logits_loss 221.495969\n",
      "[9:2400] F: 5784000 \t tot_loss 13.277138 rs_loss 0.454563 vs_loss 1.743126 logits_loss 221.588964\n",
      "[9:2500] F: 5800000 \t tot_loss 13.403699 rs_loss 0.494797 vs_loss 1.828870 logits_loss 221.600649\n",
      "[9:2600] F: 5816000 \t tot_loss 13.416835 rs_loss 0.494534 vs_loss 1.832085 logits_loss 221.804321\n",
      "[9:2700] F: 5832000 \t tot_loss 13.732945 rs_loss 0.570660 vs_loss 2.054391 logits_loss 222.157877\n",
      "[9:2800] F: 5848000 \t tot_loss 13.814166 rs_loss 0.575832 vs_loss 2.139089 logits_loss 221.984888\n",
      "[9:2900] F: 5864000 \t tot_loss 13.961084 rs_loss 0.589774 vs_loss 2.249444 logits_loss 222.437321\n",
      "[9:3000] F: 5880000 \t tot_loss 13.976157 rs_loss 0.593162 vs_loss 2.273364 logits_loss 222.192612\n",
      "[9:3100] F: 5896000 \t tot_loss 13.691048 rs_loss 0.524780 vs_loss 2.065021 logits_loss 222.024951\n",
      "[9:3200] F: 5912000 \t tot_loss 13.570789 rs_loss 0.505237 vs_loss 1.971854 logits_loss 221.873971\n",
      "[9:3300] F: 5928000 \t tot_loss 13.587563 rs_loss 0.514206 vs_loss 2.005590 logits_loss 221.355337\n",
      "[9:3400] F: 5944000 \t tot_loss 13.819923 rs_loss 0.561579 vs_loss 2.194443 logits_loss 221.278020\n",
      "[9:3500] F: 5960000 \t tot_loss 13.748407 rs_loss 0.542329 vs_loss 2.139933 logits_loss 221.322905\n",
      "[9:3600] F: 5976000 \t tot_loss 13.700442 rs_loss 0.545994 vs_loss 2.070163 logits_loss 221.685683\n",
      "[9:3700] F: 5992000 \t tot_loss 13.389207 rs_loss 0.468146 vs_loss 1.830852 logits_loss 221.804171\n",
      "Batch [10] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.59 (+-0.07) \n",
      "[10:0] F: 6000000 \t tot_loss 13.287697 rs_loss 0.448347 vs_loss 1.752357 logits_loss 221.739850\n",
      "[10:100] F: 6016000 \t tot_loss 13.155286 rs_loss 0.445775 vs_loss 1.625779 logits_loss 221.674636\n",
      "[10:200] F: 6032000 \t tot_loss 13.394023 rs_loss 0.502753 vs_loss 1.815255 logits_loss 221.520301\n",
      "[10:300] F: 6048000 \t tot_loss 13.679613 rs_loss 0.573028 vs_loss 2.020759 logits_loss 221.716508\n",
      "[10:400] F: 6064000 \t tot_loss 13.813674 rs_loss 0.586423 vs_loss 2.154519 logits_loss 221.454654\n",
      "[10:500] F: 6080000 \t tot_loss 13.778925 rs_loss 0.571459 vs_loss 2.142275 logits_loss 221.303815\n",
      "[10:600] F: 6096000 \t tot_loss 13.661699 rs_loss 0.552624 vs_loss 2.053066 logits_loss 221.120164\n",
      "[10:700] F: 6112000 \t tot_loss 13.555859 rs_loss 0.512738 vs_loss 1.988420 logits_loss 221.094020\n",
      "[10:800] F: 6128000 \t tot_loss 13.506420 rs_loss 0.515196 vs_loss 1.935036 logits_loss 221.123777\n",
      "[10:900] F: 6144000 \t tot_loss 13.597162 rs_loss 0.541564 vs_loss 2.003923 logits_loss 221.033496\n",
      "[10:1000] F: 6160000 \t tot_loss 13.596682 rs_loss 0.514358 vs_loss 2.015155 logits_loss 221.343388\n",
      "[10:1100] F: 6176000 \t tot_loss 13.577155 rs_loss 0.502164 vs_loss 2.025406 logits_loss 220.991713\n",
      "[10:1200] F: 6192000 \t tot_loss 13.554709 rs_loss 0.482548 vs_loss 2.022640 logits_loss 220.990422\n",
      "[10:1300] F: 6208000 \t tot_loss 13.491809 rs_loss 0.460162 vs_loss 1.984905 logits_loss 220.934824\n",
      "[10:1400] F: 6224000 \t tot_loss 13.367126 rs_loss 0.455395 vs_loss 1.898658 logits_loss 220.261462\n",
      "[10:1500] F: 6240000 \t tot_loss 13.398391 rs_loss 0.470279 vs_loss 1.935113 logits_loss 219.859976\n",
      "[10:1600] F: 6256000 \t tot_loss 13.468556 rs_loss 0.496086 vs_loss 1.956250 logits_loss 220.324402\n",
      "[10:1700] F: 6272000 \t tot_loss 13.439032 rs_loss 0.497187 vs_loss 1.935173 logits_loss 220.133442\n",
      "[10:1800] F: 6288000 \t tot_loss 13.565735 rs_loss 0.529569 vs_loss 1.993220 logits_loss 220.858925\n",
      "[10:1900] F: 6304000 \t tot_loss 13.471539 rs_loss 0.518419 vs_loss 1.898635 logits_loss 221.089698\n",
      "[10:2000] F: 6320000 \t tot_loss 13.306618 rs_loss 0.478293 vs_loss 1.808452 logits_loss 220.397453\n",
      "[10:2100] F: 6336000 \t tot_loss 13.282762 rs_loss 0.466120 vs_loss 1.803770 logits_loss 220.257452\n",
      "[10:2200] F: 6352000 \t tot_loss 13.198149 rs_loss 0.433241 vs_loss 1.772916 logits_loss 219.839850\n",
      "[10:2300] F: 6368000 \t tot_loss 13.194015 rs_loss 0.417701 vs_loss 1.775981 logits_loss 220.006645\n",
      "[10:2400] F: 6384000 \t tot_loss 13.265904 rs_loss 0.429769 vs_loss 1.822548 logits_loss 220.271742\n",
      "[10:2500] F: 6400000 \t tot_loss 13.337623 rs_loss 0.455035 vs_loss 1.863204 logits_loss 220.387675\n",
      "[10:2600] F: 6416000 \t tot_loss 13.363765 rs_loss 0.463413 vs_loss 1.869094 logits_loss 220.625164\n",
      "[10:2700] F: 6432000 \t tot_loss 13.321408 rs_loss 0.457416 vs_loss 1.825496 logits_loss 220.769934\n",
      "[10:2800] F: 6448000 \t tot_loss 13.333868 rs_loss 0.456107 vs_loss 1.826873 logits_loss 221.017767\n",
      "[10:2900] F: 6464000 \t tot_loss 13.150600 rs_loss 0.401972 vs_loss 1.691588 logits_loss 221.140796\n",
      "[10:3000] F: 6480000 \t tot_loss 13.327536 rs_loss 0.426092 vs_loss 1.838166 logits_loss 221.265548\n",
      "[10:3100] F: 6496000 \t tot_loss 13.275297 rs_loss 0.419390 vs_loss 1.812785 logits_loss 220.862429\n",
      "[10:3200] F: 6512000 \t tot_loss 13.192509 rs_loss 0.406855 vs_loss 1.758508 logits_loss 220.542910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:3300] F: 6528000 \t tot_loss 13.433308 rs_loss 0.452504 vs_loss 1.948202 logits_loss 220.652034\n",
      "[10:3400] F: 6544000 \t tot_loss 13.489810 rs_loss 0.458980 vs_loss 2.006658 logits_loss 220.483433\n",
      "[10:3500] F: 6560000 \t tot_loss 13.662176 rs_loss 0.495583 vs_loss 2.160829 logits_loss 220.115261\n",
      "[10:3600] F: 6576000 \t tot_loss 13.776449 rs_loss 0.527173 vs_loss 2.235413 logits_loss 220.277246\n",
      "[10:3700] F: 6592000 \t tot_loss 13.592101 rs_loss 0.483752 vs_loss 2.109819 logits_loss 219.970609\n",
      "Batch [11] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.65 (+-0.07) \n",
      "[11:0] F: 6600000 \t tot_loss 13.506740 rs_loss 0.481747 vs_loss 2.032847 logits_loss 219.842906\n",
      "[11:100] F: 6616000 \t tot_loss 13.236214 rs_loss 0.417667 vs_loss 1.794712 logits_loss 220.476683\n",
      "[11:200] F: 6632000 \t tot_loss 13.315889 rs_loss 0.423988 vs_loss 1.826167 logits_loss 221.314675\n",
      "[11:300] F: 6648000 \t tot_loss 13.608262 rs_loss 0.496201 vs_loss 2.022318 logits_loss 221.794849\n",
      "[11:400] F: 6664000 \t tot_loss 13.910088 rs_loss 0.568876 vs_loss 2.248024 logits_loss 221.863750\n",
      "[11:500] F: 6680000 \t tot_loss 14.059133 rs_loss 0.606714 vs_loss 2.368998 logits_loss 221.668410\n",
      "[11:600] F: 6696000 \t tot_loss 13.961612 rs_loss 0.583826 vs_loss 2.321989 logits_loss 221.115931\n",
      "[11:700] F: 6712000 \t tot_loss 13.793318 rs_loss 0.537546 vs_loss 2.189386 logits_loss 221.327706\n",
      "[11:800] F: 6728000 \t tot_loss 13.679104 rs_loss 0.511859 vs_loss 2.080129 logits_loss 221.742318\n",
      "[11:900] F: 6744000 \t tot_loss 13.528372 rs_loss 0.488117 vs_loss 1.967493 logits_loss 221.455246\n",
      "[11:1000] F: 6760000 \t tot_loss 13.544626 rs_loss 0.497125 vs_loss 1.962003 logits_loss 221.709961\n",
      "[11:1100] F: 6776000 \t tot_loss 13.463277 rs_loss 0.477257 vs_loss 1.929585 logits_loss 221.128669\n",
      "[11:1200] F: 6792000 \t tot_loss 13.647960 rs_loss 0.517724 vs_loss 2.074898 logits_loss 221.106748\n",
      "[11:1300] F: 6808000 \t tot_loss 13.703713 rs_loss 0.538905 vs_loss 2.101907 logits_loss 221.258008\n",
      "[11:1400] F: 6824000 \t tot_loss 13.668326 rs_loss 0.535607 vs_loss 2.084067 logits_loss 220.973019\n",
      "[11:1500] F: 6840000 \t tot_loss 13.580990 rs_loss 0.516809 vs_loss 2.014744 logits_loss 220.988733\n",
      "[11:1600] F: 6856000 \t tot_loss 13.478006 rs_loss 0.469250 vs_loss 1.959081 logits_loss 220.993489\n",
      "[11:1700] F: 6872000 \t tot_loss 13.365070 rs_loss 0.432549 vs_loss 1.876653 logits_loss 221.117356\n",
      "[11:1800] F: 6888000 \t tot_loss 13.463865 rs_loss 0.435300 vs_loss 1.968504 logits_loss 221.201217\n",
      "[11:1900] F: 6904000 \t tot_loss 13.526154 rs_loss 0.457648 vs_loss 2.003062 logits_loss 221.308887\n",
      "[11:2000] F: 6920000 \t tot_loss 13.429662 rs_loss 0.459244 vs_loss 1.922818 logits_loss 220.951997\n",
      "[11:2100] F: 6936000 \t tot_loss 13.447231 rs_loss 0.466065 vs_loss 1.928120 logits_loss 221.060902\n",
      "[11:2200] F: 6952000 \t tot_loss 13.309535 rs_loss 0.437532 vs_loss 1.821799 logits_loss 221.004069\n",
      "[11:2300] F: 6968000 \t tot_loss 13.375335 rs_loss 0.458661 vs_loss 1.874124 logits_loss 220.850992\n",
      "[11:2400] F: 6984000 \t tot_loss 13.238658 rs_loss 0.437651 vs_loss 1.769416 logits_loss 220.631826\n",
      "[11:2500] F: 7000000 \t tot_loss 13.249326 rs_loss 0.434797 vs_loss 1.786332 logits_loss 220.563935\n",
      "[11:2600] F: 7016000 \t tot_loss 13.279444 rs_loss 0.460167 vs_loss 1.802604 logits_loss 220.333450\n",
      "[11:2700] F: 7032000 \t tot_loss 13.180575 rs_loss 0.427755 vs_loss 1.744856 logits_loss 220.159284\n",
      "[11:2800] F: 7048000 \t tot_loss 13.323811 rs_loss 0.454560 vs_loss 1.850831 logits_loss 220.368403\n",
      "[11:2900] F: 7064000 \t tot_loss 13.266046 rs_loss 0.445701 vs_loss 1.806168 logits_loss 220.283533\n",
      "[11:3000] F: 7080000 \t tot_loss 13.331651 rs_loss 0.469919 vs_loss 1.842749 logits_loss 220.379658\n",
      "[11:3100] F: 7096000 \t tot_loss 13.460454 rs_loss 0.504226 vs_loss 1.942181 logits_loss 220.280941\n",
      "[11:3200] F: 7112000 \t tot_loss 13.345875 rs_loss 0.473524 vs_loss 1.847794 logits_loss 220.491132\n",
      "[11:3300] F: 7128000 \t tot_loss 13.340728 rs_loss 0.474150 vs_loss 1.837359 logits_loss 220.584384\n",
      "[11:3400] F: 7144000 \t tot_loss 13.309602 rs_loss 0.439600 vs_loss 1.835379 logits_loss 220.692446\n",
      "[11:3500] F: 7160000 \t tot_loss 13.237922 rs_loss 0.407367 vs_loss 1.779806 logits_loss 221.014979\n",
      "[11:3600] F: 7176000 \t tot_loss 13.298886 rs_loss 0.416045 vs_loss 1.839471 logits_loss 220.867379\n",
      "[11:3700] F: 7192000 \t tot_loss 13.294945 rs_loss 0.395992 vs_loss 1.855627 logits_loss 220.866524\n",
      "Batch [12] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.49 (+-0.08) \n",
      "[12:0] F: 7200000 \t tot_loss 13.355136 rs_loss 0.414115 vs_loss 1.900586 logits_loss 220.808703\n",
      "[12:100] F: 7216000 \t tot_loss 13.373435 rs_loss 0.419975 vs_loss 1.892218 logits_loss 221.224830\n",
      "[12:200] F: 7232000 \t tot_loss 13.463227 rs_loss 0.482100 vs_loss 1.911122 logits_loss 221.400091\n",
      "[12:300] F: 7248000 \t tot_loss 13.814333 rs_loss 0.534072 vs_loss 2.187540 logits_loss 221.854424\n",
      "[12:400] F: 7264000 \t tot_loss 13.856763 rs_loss 0.556662 vs_loss 2.200105 logits_loss 221.999930\n",
      "[12:500] F: 7280000 \t tot_loss 13.909774 rs_loss 0.570658 vs_loss 2.249170 logits_loss 221.798913\n",
      "[12:600] F: 7296000 \t tot_loss 13.931349 rs_loss 0.543503 vs_loss 2.313596 logits_loss 221.484988\n",
      "[12:700] F: 7312000 \t tot_loss 13.743234 rs_loss 0.546443 vs_loss 2.143453 logits_loss 221.066759\n",
      "[12:800] F: 7328000 \t tot_loss 13.747060 rs_loss 0.543013 vs_loss 2.147339 logits_loss 221.134143\n",
      "[12:900] F: 7344000 \t tot_loss 13.657086 rs_loss 0.525490 vs_loss 2.083837 logits_loss 220.955179\n",
      "[12:1000] F: 7360000 \t tot_loss 13.509789 rs_loss 0.510365 vs_loss 1.947146 logits_loss 221.045546\n",
      "[12:1100] F: 7376000 \t tot_loss 13.377495 rs_loss 0.458624 vs_loss 1.849205 logits_loss 221.393324\n",
      "[12:1200] F: 7392000 \t tot_loss 13.237610 rs_loss 0.442505 vs_loss 1.736528 logits_loss 221.171517\n",
      "[12:1300] F: 7408000 \t tot_loss 13.330264 rs_loss 0.472967 vs_loss 1.802842 logits_loss 221.089107\n",
      "[12:1400] F: 7424000 \t tot_loss 13.281464 rs_loss 0.460561 vs_loss 1.770270 logits_loss 221.012658\n",
      "[12:1500] F: 7440000 \t tot_loss 13.271404 rs_loss 0.462395 vs_loss 1.778894 logits_loss 220.602282\n",
      "[12:1600] F: 7456000 \t tot_loss 13.379058 rs_loss 0.471099 vs_loss 1.890455 logits_loss 220.350073\n",
      "[12:1700] F: 7472000 \t tot_loss 13.290825 rs_loss 0.443819 vs_loss 1.817416 logits_loss 220.591792\n",
      "[12:1800] F: 7488000 \t tot_loss 13.281527 rs_loss 0.444387 vs_loss 1.816669 logits_loss 220.409413\n",
      "[12:1900] F: 7504000 \t tot_loss 13.406204 rs_loss 0.478117 vs_loss 1.895645 logits_loss 220.648849\n",
      "[12:2000] F: 7520000 \t tot_loss 13.334676 rs_loss 0.475916 vs_loss 1.825291 logits_loss 220.669362\n",
      "[12:2100] F: 7536000 \t tot_loss 13.332041 rs_loss 0.476086 vs_loss 1.836021 logits_loss 220.398692\n",
      "[12:2200] F: 7552000 \t tot_loss 13.384102 rs_loss 0.489945 vs_loss 1.875573 logits_loss 220.371684\n",
      "[12:2300] F: 7568000 \t tot_loss 13.417955 rs_loss 0.493297 vs_loss 1.917567 logits_loss 220.141835\n",
      "[12:2400] F: 7584000 \t tot_loss 13.353147 rs_loss 0.478668 vs_loss 1.846727 logits_loss 220.555018\n",
      "[12:2500] F: 7600000 \t tot_loss 13.669810 rs_loss 0.545539 vs_loss 2.093516 logits_loss 220.615104\n",
      "[12:2600] F: 7616000 \t tot_loss 13.763062 rs_loss 0.550728 vs_loss 2.168636 logits_loss 220.873958\n",
      "[12:2700] F: 7632000 \t tot_loss 13.866714 rs_loss 0.558802 vs_loss 2.261926 logits_loss 220.919725\n",
      "[12:2800] F: 7648000 \t tot_loss 13.869684 rs_loss 0.548111 vs_loss 2.283847 logits_loss 220.754500\n",
      "[12:2900] F: 7664000 \t tot_loss 13.760211 rs_loss 0.545005 vs_loss 2.170266 logits_loss 220.898813\n",
      "[12:3000] F: 7680000 \t tot_loss 13.834644 rs_loss 0.575176 vs_loss 2.215761 logits_loss 220.874132\n",
      "[12:3100] F: 7696000 \t tot_loss 13.717015 rs_loss 0.555162 vs_loss 2.122854 logits_loss 220.779957\n",
      "[12:3200] F: 7712000 \t tot_loss 13.822611 rs_loss 0.585156 vs_loss 2.207970 logits_loss 220.589702\n",
      "[12:3300] F: 7728000 \t tot_loss 13.890445 rs_loss 0.585895 vs_loss 2.277964 logits_loss 220.531707\n",
      "[12:3400] F: 7744000 \t tot_loss 13.714295 rs_loss 0.546664 vs_loss 2.157551 logits_loss 220.201608\n",
      "[12:3500] F: 7760000 \t tot_loss 13.770668 rs_loss 0.554105 vs_loss 2.181828 logits_loss 220.694701\n",
      "[12:3600] F: 7776000 \t tot_loss 13.760319 rs_loss 0.557352 vs_loss 2.192292 logits_loss 220.213492\n",
      "[12:3700] F: 7792000 \t tot_loss 13.630700 rs_loss 0.521275 vs_loss 2.097858 logits_loss 220.231334\n",
      "Batch [13] starts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.73 (+-0.06) \n",
      "[13:0] F: 7800000 \t tot_loss 13.733178 rs_loss 0.534518 vs_loss 2.186574 logits_loss 220.241710\n",
      "[13:100] F: 7816000 \t tot_loss 13.818169 rs_loss 0.575236 vs_loss 2.213371 logits_loss 220.591238\n",
      "[13:200] F: 7832000 \t tot_loss 13.837067 rs_loss 0.549078 vs_loss 2.237596 logits_loss 221.007860\n",
      "[13:300] F: 7848000 \t tot_loss 13.669624 rs_loss 0.505906 vs_loss 2.091334 logits_loss 221.447682\n",
      "[13:400] F: 7864000 \t tot_loss 13.545986 rs_loss 0.475636 vs_loss 1.987222 logits_loss 221.662561\n",
      "[13:500] F: 7880000 \t tot_loss 13.357102 rs_loss 0.426456 vs_loss 1.865593 logits_loss 221.301055\n",
      "[13:600] F: 7896000 \t tot_loss 13.390432 rs_loss 0.438653 vs_loss 1.893005 logits_loss 221.175470\n",
      "[13:700] F: 7912000 \t tot_loss 13.324909 rs_loss 0.433001 vs_loss 1.867699 logits_loss 220.484184\n",
      "[13:800] F: 7928000 \t tot_loss 13.286562 rs_loss 0.428249 vs_loss 1.835729 logits_loss 220.451686\n",
      "[13:900] F: 7944000 \t tot_loss 13.449719 rs_loss 0.445478 vs_loss 1.967667 logits_loss 220.731462\n",
      "[13:1000] F: 7960000 \t tot_loss 13.398795 rs_loss 0.446706 vs_loss 1.917436 logits_loss 220.693065\n",
      "[13:1100] F: 7976000 \t tot_loss 13.388794 rs_loss 0.440613 vs_loss 1.907933 logits_loss 220.804946\n",
      "[13:1200] F: 7992000 \t tot_loss 13.554888 rs_loss 0.488552 vs_loss 2.042378 logits_loss 220.479143\n",
      "[13:1300] F: 8008000 \t tot_loss 13.417295 rs_loss 0.480428 vs_loss 1.921789 logits_loss 220.301542\n",
      "[13:1400] F: 8024000 \t tot_loss 13.323709 rs_loss 0.452320 vs_loss 1.862307 logits_loss 220.181625\n",
      "[13:1500] F: 8040000 \t tot_loss 13.459078 rs_loss 0.468587 vs_loss 1.974072 logits_loss 220.328369\n",
      "[13:1600] F: 8056000 \t tot_loss 13.329134 rs_loss 0.440602 vs_loss 1.844873 logits_loss 220.873163\n",
      "[13:1700] F: 8072000 \t tot_loss 13.420571 rs_loss 0.441892 vs_loss 1.925277 logits_loss 221.068047\n",
      "[13:1800] F: 8088000 \t tot_loss 13.700303 rs_loss 0.497062 vs_loss 2.155293 logits_loss 220.958968\n",
      "[13:1900] F: 8104000 \t tot_loss 13.658265 rs_loss 0.516192 vs_loss 2.107321 logits_loss 220.695032\n",
      "[13:2000] F: 8120000 \t tot_loss 13.755232 rs_loss 0.529263 vs_loss 2.207506 logits_loss 220.369258\n",
      "[13:2100] F: 8136000 \t tot_loss 13.582430 rs_loss 0.498539 vs_loss 2.076840 logits_loss 220.141019\n",
      "[13:2200] F: 8152000 \t tot_loss 13.326029 rs_loss 0.450145 vs_loss 1.862945 logits_loss 220.258776\n",
      "[13:2300] F: 8168000 \t tot_loss 13.373077 rs_loss 0.442663 vs_loss 1.914708 logits_loss 220.314117\n",
      "[13:2400] F: 8184000 \t tot_loss 13.369222 rs_loss 0.434270 vs_loss 1.901134 logits_loss 220.676366\n",
      "[13:2500] F: 8200000 \t tot_loss 13.280939 rs_loss 0.404818 vs_loss 1.857515 logits_loss 220.372116\n",
      "[13:2600] F: 8216000 \t tot_loss 13.381033 rs_loss 0.426031 vs_loss 1.946409 logits_loss 220.171846\n",
      "[13:2700] F: 8232000 \t tot_loss 13.196412 rs_loss 0.373363 vs_loss 1.813106 logits_loss 220.198866\n",
      "[13:2800] F: 8248000 \t tot_loss 13.309446 rs_loss 0.403442 vs_loss 1.915742 logits_loss 219.805233\n",
      "[13:2900] F: 8264000 \t tot_loss 13.508735 rs_loss 0.462703 vs_loss 2.047752 logits_loss 219.965611\n",
      "[13:3000] F: 8280000 \t tot_loss 13.431295 rs_loss 0.449670 vs_loss 1.980148 logits_loss 220.029551\n",
      "[13:3100] F: 8296000 \t tot_loss 13.532806 rs_loss 0.476316 vs_loss 2.053571 logits_loss 220.058378\n",
      "[13:3200] F: 8312000 \t tot_loss 13.321303 rs_loss 0.433751 vs_loss 1.857019 logits_loss 220.610660\n",
      "[13:3300] F: 8328000 \t tot_loss 13.187185 rs_loss 0.404736 vs_loss 1.762473 logits_loss 220.399513\n",
      "[13:3400] F: 8344000 \t tot_loss 13.319137 rs_loss 0.437555 vs_loss 1.851661 logits_loss 220.598421\n",
      "[13:3500] F: 8360000 \t tot_loss 13.299619 rs_loss 0.437558 vs_loss 1.829340 logits_loss 220.654412\n",
      "[13:3600] F: 8376000 \t tot_loss 13.381430 rs_loss 0.455043 vs_loss 1.910116 logits_loss 220.325421\n",
      "[13:3700] F: 8392000 \t tot_loss 13.400529 rs_loss 0.452272 vs_loss 1.909640 logits_loss 220.772331\n",
      "Batch [14] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.65 (+-0.06) \n",
      "[14:0] F: 8400000 \t tot_loss 13.189044 rs_loss 0.411098 vs_loss 1.752780 logits_loss 220.503320\n",
      "[14:100] F: 8416000 \t tot_loss 13.441200 rs_loss 0.462955 vs_loss 1.937005 logits_loss 220.824806\n",
      "[14:200] F: 8432000 \t tot_loss 13.425023 rs_loss 0.476833 vs_loss 1.907038 logits_loss 220.823022\n",
      "[14:300] F: 8448000 \t tot_loss 13.573074 rs_loss 0.532505 vs_loss 1.995659 logits_loss 220.898189\n",
      "[14:400] F: 8464000 \t tot_loss 13.840476 rs_loss 0.594768 vs_loss 2.185682 logits_loss 221.200517\n",
      "[14:500] F: 8480000 \t tot_loss 13.703061 rs_loss 0.580322 vs_loss 2.070728 logits_loss 221.040222\n",
      "[14:600] F: 8496000 \t tot_loss 13.605711 rs_loss 0.539614 vs_loss 2.011242 logits_loss 221.097090\n",
      "[14:700] F: 8512000 \t tot_loss 13.457854 rs_loss 0.494256 vs_loss 1.916996 logits_loss 220.932028\n",
      "[14:800] F: 8528000 \t tot_loss 13.244440 rs_loss 0.449255 vs_loss 1.778288 logits_loss 220.337926\n",
      "[14:900] F: 8544000 \t tot_loss 13.391157 rs_loss 0.472764 vs_loss 1.913019 logits_loss 220.107469\n",
      "[14:1000] F: 8560000 \t tot_loss 13.359991 rs_loss 0.479807 vs_loss 1.898441 logits_loss 219.634857\n",
      "[14:1100] F: 8576000 \t tot_loss 13.334749 rs_loss 0.469342 vs_loss 1.903837 logits_loss 219.231407\n",
      "[14:1200] F: 8592000 \t tot_loss 13.489465 rs_loss 0.479648 vs_loss 2.041146 logits_loss 219.373414\n",
      "[14:1300] F: 8608000 \t tot_loss 13.438770 rs_loss 0.463433 vs_loss 1.986189 logits_loss 219.782941\n",
      "[14:1400] F: 8624000 \t tot_loss 13.786923 rs_loss 0.534865 vs_loss 2.232889 logits_loss 220.383360\n",
      "[14:1500] F: 8640000 \t tot_loss 13.769221 rs_loss 0.526815 vs_loss 2.197827 logits_loss 220.891563\n",
      "[14:1600] F: 8656000 \t tot_loss 13.622336 rs_loss 0.501238 vs_loss 2.069978 logits_loss 221.022371\n",
      "[14:1700] F: 8672000 \t tot_loss 13.453799 rs_loss 0.473185 vs_loss 1.937013 logits_loss 220.872023\n",
      "[14:1800] F: 8688000 \t tot_loss 13.272280 rs_loss 0.424440 vs_loss 1.819243 logits_loss 220.571936\n",
      "[14:1900] F: 8704000 \t tot_loss 13.354228 rs_loss 0.472161 vs_loss 1.877601 logits_loss 220.089314\n",
      "[14:2000] F: 8720000 \t tot_loss 13.311760 rs_loss 0.467110 vs_loss 1.858118 logits_loss 219.730645\n",
      "[14:2100] F: 8736000 \t tot_loss 13.285675 rs_loss 0.458698 vs_loss 1.849849 logits_loss 219.542575\n",
      "[14:2200] F: 8752000 \t tot_loss 13.240845 rs_loss 0.451361 vs_loss 1.812398 logits_loss 219.541722\n",
      "[14:2300] F: 8768000 \t tot_loss 13.226386 rs_loss 0.429384 vs_loss 1.809234 logits_loss 219.755361\n",
      "[14:2400] F: 8784000 \t tot_loss 13.250253 rs_loss 0.437358 vs_loss 1.802847 logits_loss 220.200949\n",
      "[14:2500] F: 8800000 \t tot_loss 13.277406 rs_loss 0.452747 vs_loss 1.812303 logits_loss 220.247110\n",
      "[14:2600] F: 8816000 \t tot_loss 13.105657 rs_loss 0.429774 vs_loss 1.674214 logits_loss 220.033374\n",
      "[14:2700] F: 8832000 \t tot_loss 13.056379 rs_loss 0.400340 vs_loss 1.660774 logits_loss 219.905297\n",
      "[14:2800] F: 8848000 \t tot_loss 13.232257 rs_loss 0.445106 vs_loss 1.806353 logits_loss 219.615956\n",
      "[14:2900] F: 8864000 \t tot_loss 13.129586 rs_loss 0.410833 vs_loss 1.726905 logits_loss 219.836959\n",
      "[14:3000] F: 8880000 \t tot_loss 13.197911 rs_loss 0.433680 vs_loss 1.768042 logits_loss 219.923795\n",
      "[14:3100] F: 8896000 \t tot_loss 13.158586 rs_loss 0.449435 vs_loss 1.711821 logits_loss 219.946599\n",
      "[14:3200] F: 8912000 \t tot_loss 12.907839 rs_loss 0.390326 vs_loss 1.526299 logits_loss 219.824273\n",
      "[14:3300] F: 8928000 \t tot_loss 13.056705 rs_loss 0.432501 vs_loss 1.642204 logits_loss 219.640005\n",
      "[14:3400] F: 8944000 \t tot_loss 13.149015 rs_loss 0.428608 vs_loss 1.743832 logits_loss 219.531493\n",
      "[14:3500] F: 8960000 \t tot_loss 13.205698 rs_loss 0.422518 vs_loss 1.812416 logits_loss 219.415270\n",
      "[14:3600] F: 8976000 \t tot_loss 13.302915 rs_loss 0.438792 vs_loss 1.891585 logits_loss 219.450779\n",
      "[14:3700] F: 8992000 \t tot_loss 13.130115 rs_loss 0.393038 vs_loss 1.790004 logits_loss 218.941454\n",
      "Batch [15] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.54 (+-0.08) \n",
      "[15:0] F: 9000000 \t tot_loss 13.184849 rs_loss 0.417825 vs_loss 1.830599 logits_loss 218.728489\n",
      "[15:100] F: 9016000 \t tot_loss 13.253119 rs_loss 0.450789 vs_loss 1.827004 logits_loss 219.506520\n",
      "[15:200] F: 9032000 \t tot_loss 13.407729 rs_loss 0.489645 vs_loss 1.921661 logits_loss 219.928467\n",
      "[15:300] F: 9048000 \t tot_loss 13.389827 rs_loss 0.485743 vs_loss 1.873704 logits_loss 220.607591\n",
      "[15:400] F: 9064000 \t tot_loss 13.599736 rs_loss 0.523063 vs_loss 2.005417 logits_loss 221.425125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:500] F: 9080000 \t tot_loss 13.742848 rs_loss 0.525559 vs_loss 2.162841 logits_loss 221.088953\n",
      "[15:600] F: 9096000 \t tot_loss 13.596171 rs_loss 0.487892 vs_loss 2.066227 logits_loss 220.841020\n",
      "[15:700] F: 9112000 \t tot_loss 13.642038 rs_loss 0.493207 vs_loss 2.110255 logits_loss 220.771537\n",
      "[15:800] F: 9128000 \t tot_loss 13.638466 rs_loss 0.481424 vs_loss 2.127822 logits_loss 220.584402\n",
      "[15:900] F: 9144000 \t tot_loss 13.443426 rs_loss 0.472347 vs_loss 1.921690 logits_loss 220.987790\n",
      "[15:1000] F: 9160000 \t tot_loss 13.554178 rs_loss 0.512234 vs_loss 2.008254 logits_loss 220.673817\n",
      "[15:1100] F: 9176000 \t tot_loss 13.701352 rs_loss 0.540793 vs_loss 2.124863 logits_loss 220.713919\n",
      "[15:1200] F: 9192000 \t tot_loss 13.657468 rs_loss 0.528618 vs_loss 2.090509 logits_loss 220.766806\n",
      "[15:1300] F: 9208000 \t tot_loss 13.627916 rs_loss 0.483214 vs_loss 2.097823 logits_loss 220.937580\n",
      "[15:1400] F: 9224000 \t tot_loss 13.558696 rs_loss 0.453876 vs_loss 2.028524 logits_loss 221.525912\n",
      "[15:1500] F: 9240000 \t tot_loss 13.430034 rs_loss 0.429032 vs_loss 1.920371 logits_loss 221.612622\n",
      "[15:1600] F: 9256000 \t tot_loss 13.316217 rs_loss 0.423727 vs_loss 1.806856 logits_loss 221.712681\n",
      "[15:1700] F: 9272000 \t tot_loss 13.302383 rs_loss 0.441619 vs_loss 1.799817 logits_loss 221.218921\n",
      "[15:1800] F: 9288000 \t tot_loss 13.205359 rs_loss 0.421659 vs_loss 1.734926 logits_loss 220.975500\n",
      "[15:1900] F: 9304000 \t tot_loss 13.093509 rs_loss 0.384978 vs_loss 1.669302 logits_loss 220.784572\n",
      "[15:2000] F: 9320000 \t tot_loss 13.079520 rs_loss 0.360068 vs_loss 1.682202 logits_loss 220.744991\n",
      "[15:2100] F: 9336000 \t tot_loss 13.102785 rs_loss 0.373043 vs_loss 1.689296 logits_loss 220.808919\n",
      "[15:2200] F: 9352000 \t tot_loss 13.184646 rs_loss 0.403292 vs_loss 1.750340 logits_loss 220.620286\n",
      "[15:2300] F: 9368000 \t tot_loss 13.420221 rs_loss 0.471127 vs_loss 1.919878 logits_loss 220.584320\n",
      "[15:2400] F: 9384000 \t tot_loss 13.605748 rs_loss 0.539531 vs_loss 2.046062 logits_loss 220.403101\n",
      "[15:2500] F: 9400000 \t tot_loss 13.667211 rs_loss 0.559556 vs_loss 2.080693 logits_loss 220.539227\n",
      "[15:2600] F: 9416000 \t tot_loss 13.580525 rs_loss 0.528519 vs_loss 2.009418 logits_loss 220.851741\n",
      "[15:2700] F: 9432000 \t tot_loss 13.455615 rs_loss 0.496061 vs_loss 1.908564 logits_loss 221.019793\n",
      "[15:2800] F: 9448000 \t tot_loss 13.279375 rs_loss 0.444776 vs_loss 1.789055 logits_loss 220.910890\n",
      "[15:2900] F: 9464000 \t tot_loss 13.251968 rs_loss 0.420777 vs_loss 1.797385 logits_loss 220.676111\n",
      "[15:3000] F: 9480000 \t tot_loss 13.335269 rs_loss 0.437977 vs_loss 1.895261 logits_loss 220.040616\n",
      "[15:3100] F: 9496000 \t tot_loss 13.327000 rs_loss 0.432106 vs_loss 1.913393 logits_loss 219.630032\n",
      "[15:3200] F: 9512000 \t tot_loss 13.385794 rs_loss 0.451762 vs_loss 1.945316 logits_loss 219.774312\n",
      "[15:3300] F: 9528000 \t tot_loss 13.339926 rs_loss 0.451331 vs_loss 1.913500 logits_loss 219.501909\n",
      "[15:3400] F: 9544000 \t tot_loss 13.304898 rs_loss 0.430850 vs_loss 1.879632 logits_loss 219.888336\n",
      "[15:3500] F: 9560000 \t tot_loss 13.348967 rs_loss 0.439988 vs_loss 1.895777 logits_loss 220.264034\n",
      "[15:3600] F: 9576000 \t tot_loss 13.286071 rs_loss 0.416605 vs_loss 1.855441 logits_loss 220.280487\n",
      "[15:3700] F: 9592000 \t tot_loss 13.258524 rs_loss 0.427581 vs_loss 1.795871 logits_loss 220.701437\n",
      "Batch [16] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.49 (+-0.07) \n",
      "[16:0] F: 9600000 \t tot_loss 13.311606 rs_loss 0.429878 vs_loss 1.847282 logits_loss 220.688913\n",
      "[16:100] F: 9616000 \t tot_loss 13.434598 rs_loss 0.477040 vs_loss 1.915297 logits_loss 220.845205\n",
      "[16:200] F: 9632000 \t tot_loss 13.421393 rs_loss 0.459230 vs_loss 1.922473 logits_loss 220.793778\n",
      "[16:300] F: 9648000 \t tot_loss 13.383075 rs_loss 0.446352 vs_loss 1.882787 logits_loss 221.078724\n",
      "[16:400] F: 9664000 \t tot_loss 13.473941 rs_loss 0.465863 vs_loss 1.947654 logits_loss 221.208468\n",
      "[16:500] F: 9680000 \t tot_loss 13.481566 rs_loss 0.453466 vs_loss 1.965041 logits_loss 221.261191\n",
      "[16:600] F: 9696000 \t tot_loss 13.439555 rs_loss 0.449517 vs_loss 1.926616 logits_loss 221.268437\n",
      "[16:700] F: 9712000 \t tot_loss 13.439293 rs_loss 0.455751 vs_loss 1.936389 logits_loss 220.943050\n",
      "[16:800] F: 9728000 \t tot_loss 13.592605 rs_loss 0.483970 vs_loss 2.067519 logits_loss 220.822327\n",
      "[16:900] F: 9744000 \t tot_loss 13.498215 rs_loss 0.488890 vs_loss 1.983383 logits_loss 220.518844\n",
      "[16:1000] F: 9760000 \t tot_loss 13.492163 rs_loss 0.499189 vs_loss 1.967523 logits_loss 220.509012\n",
      "[16:1100] F: 9776000 \t tot_loss 13.444990 rs_loss 0.477473 vs_loss 1.963928 logits_loss 220.071770\n",
      "[16:1200] F: 9792000 \t tot_loss 13.489148 rs_loss 0.484284 vs_loss 2.009275 logits_loss 219.911754\n",
      "[16:1300] F: 9808000 \t tot_loss 13.476975 rs_loss 0.456204 vs_loss 2.018257 logits_loss 220.050270\n",
      "[16:1400] F: 9824000 \t tot_loss 13.464701 rs_loss 0.452225 vs_loss 2.016445 logits_loss 219.920604\n",
      "[16:1500] F: 9840000 \t tot_loss 13.632910 rs_loss 0.490569 vs_loss 2.132696 logits_loss 220.192897\n",
      "[16:1600] F: 9856000 \t tot_loss 13.334196 rs_loss 0.444482 vs_loss 1.879308 logits_loss 220.208106\n",
      "[16:1700] F: 9872000 \t tot_loss 13.231962 rs_loss 0.428009 vs_loss 1.791780 logits_loss 220.243453\n",
      "[16:1800] F: 9888000 \t tot_loss 13.180056 rs_loss 0.408360 vs_loss 1.758287 logits_loss 220.268162\n",
      "[16:1900] F: 9904000 \t tot_loss 13.090337 rs_loss 0.407768 vs_loss 1.670790 logits_loss 220.235573\n",
      "[16:2000] F: 9920000 \t tot_loss 13.058151 rs_loss 0.386868 vs_loss 1.648016 logits_loss 220.465350\n",
      "[16:2100] F: 9936000 \t tot_loss 13.105050 rs_loss 0.378884 vs_loss 1.705810 logits_loss 220.407105\n",
      "[16:2200] F: 9952000 \t tot_loss 13.123865 rs_loss 0.374619 vs_loss 1.720007 logits_loss 220.584775\n",
      "[16:2300] F: 9968000 \t tot_loss 13.369618 rs_loss 0.391846 vs_loss 1.940548 logits_loss 220.744467\n",
      "[16:2400] F: 9984000 \t tot_loss 13.323765 rs_loss 0.392328 vs_loss 1.914627 logits_loss 220.336193\n",
      "[16:2500] F: 10000000 \t tot_loss 13.318605 rs_loss 0.399688 vs_loss 1.915177 logits_loss 220.074793\n",
      "[16:2600] F: 10016000 \t tot_loss 13.324757 rs_loss 0.413255 vs_loss 1.919863 logits_loss 219.832767\n",
      "[16:2700] F: 10032000 \t tot_loss 13.072720 rs_loss 0.362991 vs_loss 1.703074 logits_loss 220.133104\n",
      "[16:2800] F: 10048000 \t tot_loss 13.274908 rs_loss 0.421375 vs_loss 1.838814 logits_loss 220.294396\n",
      "[16:2900] F: 10064000 \t tot_loss 13.364657 rs_loss 0.448768 vs_loss 1.875832 logits_loss 220.801130\n",
      "[16:3000] F: 10080000 \t tot_loss 13.385606 rs_loss 0.472366 vs_loss 1.874720 logits_loss 220.770384\n",
      "[16:3100] F: 10096000 \t tot_loss 13.300324 rs_loss 0.467353 vs_loss 1.837490 logits_loss 219.909620\n",
      "[16:3200] F: 10112000 \t tot_loss 13.369369 rs_loss 0.457599 vs_loss 1.935233 logits_loss 219.530748\n",
      "[16:3300] F: 10128000 \t tot_loss 13.254005 rs_loss 0.431086 vs_loss 1.867090 logits_loss 219.116564\n",
      "[16:3400] F: 10144000 \t tot_loss 13.204982 rs_loss 0.393693 vs_loss 1.851098 logits_loss 219.203823\n",
      "[16:3500] F: 10160000 \t tot_loss 13.312223 rs_loss 0.434499 vs_loss 1.909493 logits_loss 219.364611\n",
      "[16:3600] F: 10176000 \t tot_loss 13.177220 rs_loss 0.421725 vs_loss 1.791867 logits_loss 219.272555\n",
      "[16:3700] F: 10192000 \t tot_loss 13.129984 rs_loss 0.416633 vs_loss 1.746725 logits_loss 219.332531\n",
      "Batch [17] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.66 (+-0.06) \n",
      "[17:0] F: 10200000 \t tot_loss 13.329267 rs_loss 0.507117 vs_loss 1.855465 logits_loss 219.333698\n",
      "[17:100] F: 10216000 \t tot_loss 13.316305 rs_loss 0.487481 vs_loss 1.860158 logits_loss 219.373318\n",
      "[17:200] F: 10232000 \t tot_loss 13.326340 rs_loss 0.476286 vs_loss 1.866222 logits_loss 219.676636\n",
      "[17:300] F: 10248000 \t tot_loss 13.341700 rs_loss 0.468123 vs_loss 1.884917 logits_loss 219.773189\n",
      "[17:400] F: 10264000 \t tot_loss 13.445360 rs_loss 0.458693 vs_loss 1.993048 logits_loss 219.872367\n",
      "[17:500] F: 10280000 \t tot_loss 13.540209 rs_loss 0.473163 vs_loss 2.077254 logits_loss 219.795832\n",
      "[17:600] F: 10296000 \t tot_loss 13.587983 rs_loss 0.498991 vs_loss 2.113260 logits_loss 219.514626\n",
      "[17:700] F: 10312000 \t tot_loss 13.773013 rs_loss 0.525879 vs_loss 2.260825 logits_loss 219.726197\n",
      "[17:800] F: 10328000 \t tot_loss 13.701260 rs_loss 0.503656 vs_loss 2.225667 logits_loss 219.438740\n",
      "[17:900] F: 10344000 \t tot_loss 13.825535 rs_loss 0.543174 vs_loss 2.310422 logits_loss 219.438772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:1000] F: 10360000 \t tot_loss 13.882486 rs_loss 0.562997 vs_loss 2.334656 logits_loss 219.696666\n",
      "[17:1100] F: 10376000 \t tot_loss 13.686943 rs_loss 0.529005 vs_loss 2.179214 logits_loss 219.574477\n",
      "[17:1200] F: 10392000 \t tot_loss 13.634929 rs_loss 0.502063 vs_loss 2.142629 logits_loss 219.804742\n",
      "[17:1300] F: 10408000 \t tot_loss 13.496040 rs_loss 0.475167 vs_loss 2.049255 logits_loss 219.432365\n",
      "[17:1400] F: 10424000 \t tot_loss 13.510760 rs_loss 0.464105 vs_loss 2.069181 logits_loss 219.549491\n",
      "[17:1500] F: 10440000 \t tot_loss 13.761804 rs_loss 0.524846 vs_loss 2.276558 logits_loss 219.207984\n",
      "[17:1600] F: 10456000 \t tot_loss 13.784885 rs_loss 0.547151 vs_loss 2.297153 logits_loss 218.811614\n",
      "[17:1700] F: 10472000 \t tot_loss 13.644850 rs_loss 0.516183 vs_loss 2.182456 logits_loss 218.924207\n",
      "[17:1800] F: 10488000 \t tot_loss 13.476964 rs_loss 0.467920 vs_loss 2.059597 logits_loss 218.988946\n",
      "[17:1900] F: 10504000 \t tot_loss 13.229572 rs_loss 0.421361 vs_loss 1.853770 logits_loss 219.088821\n",
      "[17:2000] F: 10520000 \t tot_loss 13.089249 rs_loss 0.376094 vs_loss 1.764862 logits_loss 218.965880\n",
      "[17:2100] F: 10536000 \t tot_loss 13.306655 rs_loss 0.414365 vs_loss 1.930760 logits_loss 219.230592\n",
      "[17:2200] F: 10552000 \t tot_loss 13.253933 rs_loss 0.435656 vs_loss 1.879216 logits_loss 218.781216\n",
      "[17:2300] F: 10568000 \t tot_loss 13.403019 rs_loss 0.477795 vs_loss 1.974699 logits_loss 219.010499\n",
      "[17:2400] F: 10584000 \t tot_loss 13.343401 rs_loss 0.480514 vs_loss 1.921585 logits_loss 218.826043\n",
      "[17:2500] F: 10600000 \t tot_loss 13.306544 rs_loss 0.487626 vs_loss 1.883255 logits_loss 218.713257\n",
      "[17:2600] F: 10616000 \t tot_loss 13.839391 rs_loss 0.566825 vs_loss 2.332681 logits_loss 218.797692\n",
      "[17:2700] F: 10632000 \t tot_loss 13.777002 rs_loss 0.547561 vs_loss 2.320064 logits_loss 218.187539\n",
      "[17:2800] F: 10648000 \t tot_loss 13.850056 rs_loss 0.571410 vs_loss 2.343446 logits_loss 218.703992\n",
      "[17:2900] F: 10664000 \t tot_loss 13.748301 rs_loss 0.565793 vs_loss 2.257703 logits_loss 218.496079\n",
      "[17:3000] F: 10680000 \t tot_loss 13.162923 rs_loss 0.459977 vs_loss 1.779916 logits_loss 218.460591\n",
      "[17:3100] F: 10696000 \t tot_loss 13.334331 rs_loss 0.509121 vs_loss 1.883014 logits_loss 218.843917\n",
      "[17:3200] F: 10712000 \t tot_loss 13.436007 rs_loss 0.531332 vs_loss 1.958556 logits_loss 218.922359\n",
      "[17:3300] F: 10728000 \t tot_loss 13.622568 rs_loss 0.573800 vs_loss 2.098139 logits_loss 219.012569\n",
      "[17:3400] F: 10744000 \t tot_loss 13.673297 rs_loss 0.575977 vs_loss 2.138996 logits_loss 219.166489\n",
      "[17:3500] F: 10760000 \t tot_loss 13.526393 rs_loss 0.532157 vs_loss 2.042901 logits_loss 219.026682\n",
      "[17:3600] F: 10776000 \t tot_loss 13.588058 rs_loss 0.551581 vs_loss 2.101688 logits_loss 218.695787\n",
      "[17:3700] F: 10792000 \t tot_loss 13.381574 rs_loss 0.499592 vs_loss 1.957550 logits_loss 218.488655\n",
      "Batch [18] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.58 (+-0.07) \n",
      "[18:0] F: 10800000 \t tot_loss 13.565301 rs_loss 0.527543 vs_loss 2.110057 logits_loss 218.554015\n",
      "[18:100] F: 10816000 \t tot_loss 13.416272 rs_loss 0.481552 vs_loss 1.991586 logits_loss 218.862667\n",
      "[18:200] F: 10832000 \t tot_loss 13.509686 rs_loss 0.497215 vs_loss 2.043608 logits_loss 219.377256\n",
      "[18:300] F: 10848000 \t tot_loss 13.460817 rs_loss 0.502664 vs_loss 1.987192 logits_loss 219.419221\n",
      "[18:400] F: 10864000 \t tot_loss 13.563598 rs_loss 0.518787 vs_loss 2.027251 logits_loss 220.351204\n",
      "[18:500] F: 10880000 \t tot_loss 13.536614 rs_loss 0.526121 vs_loss 1.982795 logits_loss 220.553954\n",
      "[18:600] F: 10896000 \t tot_loss 13.537310 rs_loss 0.515799 vs_loss 1.999042 logits_loss 220.449390\n",
      "[18:700] F: 10912000 \t tot_loss 13.560134 rs_loss 0.500917 vs_loss 2.021124 logits_loss 220.761855\n",
      "[18:800] F: 10928000 \t tot_loss 13.371996 rs_loss 0.440561 vs_loss 1.900039 logits_loss 220.627920\n",
      "[18:900] F: 10944000 \t tot_loss 13.408388 rs_loss 0.435636 vs_loss 1.964677 logits_loss 220.161497\n",
      "[18:1000] F: 10960000 \t tot_loss 13.501226 rs_loss 0.472990 vs_loss 2.029783 logits_loss 219.969070\n",
      "[18:1100] F: 10976000 \t tot_loss 13.417990 rs_loss 0.464992 vs_loss 1.963039 logits_loss 219.799174\n",
      "[18:1200] F: 10992000 \t tot_loss 13.330071 rs_loss 0.465060 vs_loss 1.897045 logits_loss 219.359306\n",
      "[18:1300] F: 11008000 \t tot_loss 13.332455 rs_loss 0.466501 vs_loss 1.895752 logits_loss 219.404025\n",
      "[18:1400] F: 11024000 \t tot_loss 13.408419 rs_loss 0.481972 vs_loss 1.956035 logits_loss 219.408234\n",
      "[18:1500] F: 11040000 \t tot_loss 13.372631 rs_loss 0.459558 vs_loss 1.936874 logits_loss 219.523986\n",
      "[18:1600] F: 11056000 \t tot_loss 13.315310 rs_loss 0.442381 vs_loss 1.893727 logits_loss 219.584028\n",
      "[18:1700] F: 11072000 \t tot_loss 13.226522 rs_loss 0.412752 vs_loss 1.833183 logits_loss 219.611745\n",
      "[18:1800] F: 11088000 \t tot_loss 12.881148 rs_loss 0.318836 vs_loss 1.572779 logits_loss 219.790659\n",
      "[18:1900] F: 11104000 \t tot_loss 12.957748 rs_loss 0.330257 vs_loss 1.652504 logits_loss 219.499739\n",
      "[18:2000] F: 11120000 \t tot_loss 13.141889 rs_loss 0.364494 vs_loss 1.795546 logits_loss 219.636993\n",
      "[18:2100] F: 11136000 \t tot_loss 13.164271 rs_loss 0.371937 vs_loss 1.807507 logits_loss 219.696548\n",
      "[18:2200] F: 11152000 \t tot_loss 13.319208 rs_loss 0.400332 vs_loss 1.938643 logits_loss 219.604647\n",
      "[18:2300] F: 11168000 \t tot_loss 13.327274 rs_loss 0.414211 vs_loss 1.926080 logits_loss 219.739646\n",
      "[18:2400] F: 11184000 \t tot_loss 13.058069 rs_loss 0.344672 vs_loss 1.737844 logits_loss 219.511042\n",
      "[18:2500] F: 11200000 \t tot_loss 13.349744 rs_loss 0.416210 vs_loss 1.957394 logits_loss 219.522796\n",
      "[18:2600] F: 11216000 \t tot_loss 13.339520 rs_loss 0.410981 vs_loss 1.939769 logits_loss 219.775391\n",
      "[18:2700] F: 11232000 \t tot_loss 13.795478 rs_loss 0.513450 vs_loss 2.288285 logits_loss 219.874846\n",
      "[18:2800] F: 11248000 \t tot_loss 13.951467 rs_loss 0.551330 vs_loss 2.403802 logits_loss 219.926696\n",
      "[18:2900] F: 11264000 \t tot_loss 13.845797 rs_loss 0.522721 vs_loss 2.339221 logits_loss 219.677107\n",
      "[18:3000] F: 11280000 \t tot_loss 13.716211 rs_loss 0.502982 vs_loss 2.249499 logits_loss 219.274594\n",
      "[18:3100] F: 11296000 \t tot_loss 13.433222 rs_loss 0.447309 vs_loss 2.027007 logits_loss 219.178105\n",
      "[18:3200] F: 11312000 \t tot_loss 13.356852 rs_loss 0.437726 vs_loss 1.951275 logits_loss 219.357019\n",
      "[18:3300] F: 11328000 \t tot_loss 13.103619 rs_loss 0.391641 vs_loss 1.749028 logits_loss 219.259001\n",
      "[18:3400] F: 11344000 \t tot_loss 13.122719 rs_loss 0.413268 vs_loss 1.745654 logits_loss 219.275947\n",
      "[18:3500] F: 11360000 \t tot_loss 12.958151 rs_loss 0.372234 vs_loss 1.620923 logits_loss 219.299881\n",
      "[18:3600] F: 11376000 \t tot_loss 12.901872 rs_loss 0.357618 vs_loss 1.583052 logits_loss 219.224045\n",
      "[18:3700] F: 11392000 \t tot_loss 13.043267 rs_loss 0.384876 vs_loss 1.690828 logits_loss 219.351262\n",
      "Batch [19] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.73 (+-0.06) \n",
      "[19:0] F: 11400000 \t tot_loss 13.257785 rs_loss 0.421157 vs_loss 1.866299 logits_loss 219.406582\n",
      "[19:100] F: 11416000 \t tot_loss 13.215869 rs_loss 0.398339 vs_loss 1.853462 logits_loss 219.281336\n",
      "[19:200] F: 11432000 \t tot_loss 13.349972 rs_loss 0.430225 vs_loss 1.966816 logits_loss 219.058618\n",
      "[19:300] F: 11448000 \t tot_loss 13.355984 rs_loss 0.436078 vs_loss 1.942946 logits_loss 219.539200\n",
      "[19:400] F: 11464000 \t tot_loss 13.271654 rs_loss 0.456135 vs_loss 1.845909 logits_loss 219.392209\n",
      "[19:500] F: 11480000 \t tot_loss 13.272230 rs_loss 0.458000 vs_loss 1.848358 logits_loss 219.317446\n",
      "[19:600] F: 11496000 \t tot_loss 13.174527 rs_loss 0.450180 vs_loss 1.773383 logits_loss 219.019263\n",
      "[19:700] F: 11512000 \t tot_loss 13.333981 rs_loss 0.488063 vs_loss 1.915572 logits_loss 218.606931\n",
      "[19:800] F: 11528000 \t tot_loss 13.409424 rs_loss 0.470901 vs_loss 2.019096 logits_loss 218.388545\n",
      "[19:900] F: 11544000 \t tot_loss 13.796917 rs_loss 0.573451 vs_loss 2.313884 logits_loss 218.191630\n",
      "[19:1000] F: 11560000 \t tot_loss 13.986334 rs_loss 0.592524 vs_loss 2.481574 logits_loss 218.244716\n",
      "[19:1100] F: 11576000 \t tot_loss 13.993202 rs_loss 0.604155 vs_loss 2.463886 logits_loss 218.503209\n",
      "[19:1200] F: 11592000 \t tot_loss 13.850131 rs_loss 0.567635 vs_loss 2.328219 logits_loss 219.085548\n",
      "[19:1300] F: 11608000 \t tot_loss 13.631340 rs_loss 0.499449 vs_loss 2.175045 logits_loss 219.136931\n",
      "[19:1400] F: 11624000 \t tot_loss 13.412664 rs_loss 0.463311 vs_loss 1.987388 logits_loss 219.239306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:1500] F: 11640000 \t tot_loss 13.270142 rs_loss 0.385329 vs_loss 1.905217 logits_loss 219.591917\n",
      "[19:1600] F: 11656000 \t tot_loss 13.366128 rs_loss 0.409769 vs_loss 2.003100 logits_loss 219.065188\n",
      "[19:1700] F: 11672000 \t tot_loss 13.194708 rs_loss 0.390058 vs_loss 1.861110 logits_loss 218.870803\n",
      "[19:1800] F: 11688000 \t tot_loss 13.290670 rs_loss 0.420889 vs_loss 1.925594 logits_loss 218.883720\n",
      "[19:1900] F: 11704000 \t tot_loss 13.219655 rs_loss 0.436437 vs_loss 1.861483 logits_loss 218.434707\n",
      "[19:2000] F: 11720000 \t tot_loss 13.097529 rs_loss 0.418077 vs_loss 1.760091 logits_loss 218.387211\n",
      "[19:2100] F: 11736000 \t tot_loss 13.106634 rs_loss 0.394130 vs_loss 1.765227 logits_loss 218.945538\n",
      "[19:2200] F: 11752000 \t tot_loss 13.123908 rs_loss 0.385531 vs_loss 1.815351 logits_loss 218.460514\n",
      "[19:2300] F: 11768000 \t tot_loss 13.094157 rs_loss 0.368636 vs_loss 1.800666 logits_loss 218.497102\n",
      "[19:2400] F: 11784000 \t tot_loss 13.139459 rs_loss 0.387826 vs_loss 1.835970 logits_loss 218.313249\n",
      "[19:2500] F: 11800000 \t tot_loss 13.149086 rs_loss 0.429216 vs_loss 1.818003 logits_loss 218.037352\n",
      "[19:2600] F: 11816000 \t tot_loss 12.963339 rs_loss 0.386068 vs_loss 1.661999 logits_loss 218.305424\n",
      "[19:2700] F: 11832000 \t tot_loss 13.196467 rs_loss 0.433696 vs_loss 1.850420 logits_loss 218.247020\n",
      "[19:2800] F: 11848000 \t tot_loss 13.108567 rs_loss 0.401411 vs_loss 1.784965 logits_loss 218.443810\n",
      "[19:2900] F: 11864000 \t tot_loss 13.413138 rs_loss 0.446610 vs_loss 2.035870 logits_loss 218.613168\n",
      "[19:3000] F: 11880000 \t tot_loss 13.873500 rs_loss 0.540954 vs_loss 2.389992 logits_loss 218.851071\n",
      "[19:3100] F: 11896000 \t tot_loss 13.839080 rs_loss 0.527683 vs_loss 2.396631 logits_loss 218.295327\n",
      "[19:3200] F: 11912000 \t tot_loss 14.062797 rs_loss 0.580369 vs_loss 2.560520 logits_loss 218.438163\n",
      "[19:3300] F: 11928000 \t tot_loss 13.948140 rs_loss 0.576620 vs_loss 2.462535 logits_loss 218.179709\n",
      "[19:3400] F: 11944000 \t tot_loss 13.874207 rs_loss 0.553281 vs_loss 2.398681 logits_loss 218.444901\n",
      "[19:3500] F: 11960000 \t tot_loss 13.936225 rs_loss 0.580353 vs_loss 2.411376 logits_loss 218.889927\n",
      "[19:3600] F: 11976000 \t tot_loss 13.803289 rs_loss 0.566293 vs_loss 2.300077 logits_loss 218.738385\n",
      "[19:3700] F: 11992000 \t tot_loss 13.713185 rs_loss 0.536260 vs_loss 2.246850 logits_loss 218.601505\n",
      "Batch [20] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.74 (+-0.06) \n",
      "[20:0] F: 12000000 \t tot_loss 13.625317 rs_loss 0.515652 vs_loss 2.175127 logits_loss 218.690758\n",
      "[20:100] F: 12016000 \t tot_loss 13.338344 rs_loss 0.441516 vs_loss 1.962468 logits_loss 218.687194\n",
      "[20:200] F: 12032000 \t tot_loss 13.340630 rs_loss 0.446511 vs_loss 1.953093 logits_loss 218.820534\n",
      "[20:300] F: 12048000 \t tot_loss 13.617651 rs_loss 0.497451 vs_loss 2.159363 logits_loss 219.216734\n",
      "[20:400] F: 12064000 \t tot_loss 13.459782 rs_loss 0.463729 vs_loss 2.036257 logits_loss 219.195915\n",
      "[20:500] F: 12080000 \t tot_loss 13.261197 rs_loss 0.430517 vs_loss 1.890222 logits_loss 218.809150\n",
      "[20:600] F: 12096000 \t tot_loss 13.188460 rs_loss 0.392730 vs_loss 1.856710 logits_loss 218.780400\n",
      "[20:700] F: 12112000 \t tot_loss 12.978850 rs_loss 0.342025 vs_loss 1.694938 logits_loss 218.837734\n",
      "[20:800] F: 12128000 \t tot_loss 13.097268 rs_loss 0.374287 vs_loss 1.766266 logits_loss 219.134309\n",
      "[20:900] F: 12144000 \t tot_loss 13.199892 rs_loss 0.404637 vs_loss 1.839307 logits_loss 219.118948\n",
      "[20:1000] F: 12160000 \t tot_loss 13.273482 rs_loss 0.446360 vs_loss 1.896831 logits_loss 218.605821\n",
      "[20:1100] F: 12176000 \t tot_loss 13.417217 rs_loss 0.490458 vs_loss 2.004400 logits_loss 218.447194\n",
      "[20:1200] F: 12192000 \t tot_loss 13.288643 rs_loss 0.445915 vs_loss 1.926656 logits_loss 218.321414\n",
      "[20:1300] F: 12208000 \t tot_loss 13.342251 rs_loss 0.456635 vs_loss 1.971683 logits_loss 218.278666\n",
      "[20:1400] F: 12224000 \t tot_loss 13.346468 rs_loss 0.424295 vs_loss 1.988554 logits_loss 218.672392\n",
      "[20:1500] F: 12240000 \t tot_loss 13.374216 rs_loss 0.415348 vs_loss 2.020434 logits_loss 218.768667\n",
      "[20:1600] F: 12256000 \t tot_loss 13.586534 rs_loss 0.464647 vs_loss 2.191490 logits_loss 218.607941\n",
      "[20:1700] F: 12272000 \t tot_loss 13.730406 rs_loss 0.489015 vs_loss 2.306558 logits_loss 218.696651\n",
      "[20:1800] F: 12288000 \t tot_loss 13.931234 rs_loss 0.534978 vs_loss 2.460262 logits_loss 218.719860\n",
      "[20:1900] F: 12304000 \t tot_loss 13.950022 rs_loss 0.549960 vs_loss 2.468031 logits_loss 218.640622\n",
      "[20:2000] F: 12320000 \t tot_loss 13.743319 rs_loss 0.515857 vs_loss 2.295698 logits_loss 218.635275\n",
      "[20:2100] F: 12336000 \t tot_loss 13.556321 rs_loss 0.495114 vs_loss 2.134211 logits_loss 218.539911\n",
      "[20:2200] F: 12352000 \t tot_loss 13.398442 rs_loss 0.453415 vs_loss 2.011582 logits_loss 218.668890\n",
      "[20:2300] F: 12368000 \t tot_loss 13.471067 rs_loss 0.451802 vs_loss 2.089539 logits_loss 218.594517\n",
      "[20:2400] F: 12384000 \t tot_loss 13.401430 rs_loss 0.433711 vs_loss 2.030814 logits_loss 218.738097\n",
      "[20:2500] F: 12400000 \t tot_loss 13.245699 rs_loss 0.384660 vs_loss 1.924607 logits_loss 218.728646\n",
      "[20:2600] F: 12416000 \t tot_loss 13.121735 rs_loss 0.366701 vs_loss 1.798955 logits_loss 219.121594\n",
      "[20:2700] F: 12432000 \t tot_loss 12.894055 rs_loss 0.334427 vs_loss 1.595942 logits_loss 219.273727\n",
      "[20:2800] F: 12448000 \t tot_loss 12.911199 rs_loss 0.338360 vs_loss 1.614915 logits_loss 219.158480\n",
      "[20:2900] F: 12464000 \t tot_loss 13.122964 rs_loss 0.389571 vs_loss 1.771252 logits_loss 219.242813\n",
      "[20:3000] F: 12480000 \t tot_loss 13.008713 rs_loss 0.368184 vs_loss 1.715892 logits_loss 218.492719\n",
      "[20:3100] F: 12496000 \t tot_loss 13.289114 rs_loss 0.426508 vs_loss 1.945170 logits_loss 218.348697\n",
      "[20:3200] F: 12512000 \t tot_loss 13.426845 rs_loss 0.446032 vs_loss 2.053644 logits_loss 218.543373\n",
      "[20:3300] F: 12528000 \t tot_loss 13.683508 rs_loss 0.515188 vs_loss 2.238463 logits_loss 218.597143\n",
      "[20:3400] F: 12544000 \t tot_loss 13.853780 rs_loss 0.536960 vs_loss 2.365654 logits_loss 219.023318\n",
      "[20:3500] F: 12560000 \t tot_loss 13.590549 rs_loss 0.466555 vs_loss 2.162090 logits_loss 219.238079\n",
      "[20:3600] F: 12576000 \t tot_loss 13.638182 rs_loss 0.472290 vs_loss 2.199208 logits_loss 219.333694\n",
      "[20:3700] F: 12592000 \t tot_loss 13.212287 rs_loss 0.370958 vs_loss 1.873299 logits_loss 219.360583\n",
      "Batch [21] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.58 (+-0.07) \n",
      "[21:0] F: 12600000 \t tot_loss 13.246047 rs_loss 0.378412 vs_loss 1.911166 logits_loss 219.129359\n",
      "[21:100] F: 12616000 \t tot_loss 13.301208 rs_loss 0.425773 vs_loss 1.915827 logits_loss 219.192144\n",
      "[21:200] F: 12632000 \t tot_loss 13.408927 rs_loss 0.483000 vs_loss 1.958222 logits_loss 219.354107\n",
      "[21:300] F: 12648000 \t tot_loss 13.348687 rs_loss 0.459936 vs_loss 1.929426 logits_loss 219.186491\n",
      "[21:400] F: 12664000 \t tot_loss 13.447975 rs_loss 0.488486 vs_loss 1.980946 logits_loss 219.570846\n",
      "[21:500] F: 12680000 \t tot_loss 13.297742 rs_loss 0.449725 vs_loss 1.891119 logits_loss 219.137958\n",
      "[21:600] F: 12696000 \t tot_loss 13.181203 rs_loss 0.406439 vs_loss 1.850091 logits_loss 218.493446\n",
      "[21:700] F: 12712000 \t tot_loss 13.398768 rs_loss 0.457100 vs_loss 2.025977 logits_loss 218.313800\n",
      "[21:800] F: 12728000 \t tot_loss 13.505038 rs_loss 0.491412 vs_loss 2.105495 logits_loss 218.162602\n",
      "[21:900] F: 12744000 \t tot_loss 13.648885 rs_loss 0.509197 vs_loss 2.225829 logits_loss 218.277185\n",
      "[21:1000] F: 12760000 \t tot_loss 13.567013 rs_loss 0.502738 vs_loss 2.124837 logits_loss 218.788758\n",
      "[21:1100] F: 12776000 \t tot_loss 13.282272 rs_loss 0.433931 vs_loss 1.887020 logits_loss 219.226417\n",
      "[21:1200] F: 12792000 \t tot_loss 13.324920 rs_loss 0.430585 vs_loss 1.950732 logits_loss 218.872071\n",
      "[21:1300] F: 12808000 \t tot_loss 13.227421 rs_loss 0.405013 vs_loss 1.860668 logits_loss 219.234791\n",
      "[21:1400] F: 12824000 \t tot_loss 13.339589 rs_loss 0.417290 vs_loss 1.989907 logits_loss 218.647843\n",
      "[21:1500] F: 12840000 \t tot_loss 13.637359 rs_loss 0.497209 vs_loss 2.233201 logits_loss 218.138979\n",
      "[21:1600] F: 12856000 \t tot_loss 13.388080 rs_loss 0.449116 vs_loss 2.011848 logits_loss 218.542312\n",
      "[21:1700] F: 12872000 \t tot_loss 13.451733 rs_loss 0.478701 vs_loss 2.073974 logits_loss 217.981159\n",
      "[21:1800] F: 12888000 \t tot_loss 13.589291 rs_loss 0.505412 vs_loss 2.166861 logits_loss 218.340357\n",
      "[21:1900] F: 12904000 \t tot_loss 13.307045 rs_loss 0.423781 vs_loss 1.951398 logits_loss 218.637306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:2000] F: 12920000 \t tot_loss 13.632988 rs_loss 0.473807 vs_loss 2.245379 logits_loss 218.276020\n",
      "[21:2100] F: 12936000 \t tot_loss 13.606561 rs_loss 0.459972 vs_loss 2.227962 logits_loss 218.372532\n",
      "[21:2200] F: 12952000 \t tot_loss 13.377857 rs_loss 0.433011 vs_loss 2.025837 logits_loss 218.380180\n",
      "[21:2300] F: 12968000 \t tot_loss 13.859024 rs_loss 0.547083 vs_loss 2.402549 logits_loss 218.187833\n",
      "[21:2400] F: 12984000 \t tot_loss 13.602588 rs_loss 0.488473 vs_loss 2.196277 logits_loss 218.356742\n",
      "[21:2500] F: 13000000 \t tot_loss 13.671454 rs_loss 0.510878 vs_loss 2.229201 logits_loss 218.627476\n",
      "[21:2600] F: 13016000 \t tot_loss 13.882447 rs_loss 0.530044 vs_loss 2.422348 logits_loss 218.601089\n",
      "[21:2700] F: 13032000 \t tot_loss 13.472482 rs_loss 0.453610 vs_loss 2.068227 logits_loss 219.012892\n",
      "[21:2800] F: 13048000 \t tot_loss 13.762667 rs_loss 0.520741 vs_loss 2.281275 logits_loss 219.213007\n",
      "[21:2900] F: 13064000 \t tot_loss 13.632032 rs_loss 0.482638 vs_loss 2.217886 logits_loss 218.630160\n",
      "[21:3000] F: 13080000 \t tot_loss 13.566626 rs_loss 0.477728 vs_loss 2.172404 logits_loss 218.329876\n",
      "[21:3100] F: 13096000 \t tot_loss 13.605258 rs_loss 0.477539 vs_loss 2.229839 logits_loss 217.957601\n",
      "[21:3200] F: 13112000 \t tot_loss 13.379983 rs_loss 0.444882 vs_loss 2.034856 logits_loss 218.004878\n",
      "[21:3300] F: 13128000 \t tot_loss 13.478034 rs_loss 0.466123 vs_loss 2.097250 logits_loss 218.293217\n",
      "[21:3400] F: 13144000 \t tot_loss 13.591345 rs_loss 0.512351 vs_loss 2.149226 logits_loss 218.595361\n",
      "[21:3500] F: 13160000 \t tot_loss 13.579432 rs_loss 0.521340 vs_loss 2.114828 logits_loss 218.865285\n",
      "[21:3600] F: 13176000 \t tot_loss 13.406389 rs_loss 0.481340 vs_loss 1.996650 logits_loss 218.567975\n",
      "[21:3700] F: 13192000 \t tot_loss 13.338246 rs_loss 0.479997 vs_loss 1.925451 logits_loss 218.655953\n",
      "Batch [22] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.67 (+-0.07) \n",
      "[22:0] F: 13200000 \t tot_loss 13.308268 rs_loss 0.463291 vs_loss 1.914120 logits_loss 218.617148\n",
      "[22:100] F: 13216000 \t tot_loss 13.044194 rs_loss 0.393842 vs_loss 1.731161 logits_loss 218.383803\n",
      "[22:200] F: 13232000 \t tot_loss 13.091530 rs_loss 0.416103 vs_loss 1.772236 logits_loss 218.063803\n",
      "[22:300] F: 13248000 \t tot_loss 13.308053 rs_loss 0.487613 vs_loss 1.901858 logits_loss 218.371638\n",
      "[22:400] F: 13264000 \t tot_loss 13.284547 rs_loss 0.466057 vs_loss 1.898687 logits_loss 218.396064\n",
      "[22:500] F: 13280000 \t tot_loss 13.371983 rs_loss 0.487752 vs_loss 1.976108 logits_loss 218.162455\n",
      "[22:600] F: 13296000 \t tot_loss 13.505171 rs_loss 0.521240 vs_loss 2.066454 logits_loss 218.349519\n",
      "[22:700] F: 13312000 \t tot_loss 13.258790 rs_loss 0.454504 vs_loss 1.886009 logits_loss 218.365526\n",
      "[22:800] F: 13328000 \t tot_loss 13.204409 rs_loss 0.435367 vs_loss 1.820780 logits_loss 218.965239\n",
      "[22:900] F: 13344000 \t tot_loss 13.178017 rs_loss 0.429264 vs_loss 1.795155 logits_loss 219.071956\n",
      "[22:1000] F: 13360000 \t tot_loss 13.130765 rs_loss 0.396819 vs_loss 1.775804 logits_loss 219.162826\n",
      "[22:1100] F: 13376000 \t tot_loss 13.374045 rs_loss 0.450517 vs_loss 1.966545 logits_loss 219.139657\n",
      "[22:1200] F: 13392000 \t tot_loss 13.512918 rs_loss 0.502651 vs_loss 2.079824 logits_loss 218.608860\n",
      "[22:1300] F: 13408000 \t tot_loss 13.786998 rs_loss 0.553514 vs_loss 2.307917 logits_loss 218.511339\n",
      "[22:1400] F: 13424000 \t tot_loss 13.692549 rs_loss 0.538444 vs_loss 2.237155 logits_loss 218.338993\n",
      "[22:1500] F: 13440000 \t tot_loss 13.520316 rs_loss 0.506251 vs_loss 2.102209 logits_loss 218.237120\n",
      "[22:1600] F: 13456000 \t tot_loss 13.535290 rs_loss 0.520880 vs_loss 2.095524 logits_loss 218.377705\n",
      "[22:1700] F: 13472000 \t tot_loss 13.265637 rs_loss 0.461150 vs_loss 1.884911 logits_loss 218.391516\n",
      "[22:1800] F: 13488000 \t tot_loss 13.249721 rs_loss 0.462189 vs_loss 1.848076 logits_loss 218.789104\n",
      "[22:1900] F: 13504000 \t tot_loss 13.232544 rs_loss 0.452330 vs_loss 1.835322 logits_loss 218.897822\n",
      "[22:2000] F: 13520000 \t tot_loss 13.024930 rs_loss 0.380687 vs_loss 1.708225 logits_loss 218.720366\n",
      "[22:2100] F: 13536000 \t tot_loss 13.201725 rs_loss 0.418276 vs_loss 1.849689 logits_loss 218.675190\n",
      "[22:2200] F: 13552000 \t tot_loss 13.464924 rs_loss 0.460034 vs_loss 2.070816 logits_loss 218.681471\n",
      "[22:2300] F: 13568000 \t tot_loss 13.669931 rs_loss 0.499418 vs_loss 2.251631 logits_loss 218.377637\n",
      "[22:2400] F: 13584000 \t tot_loss 13.653534 rs_loss 0.494161 vs_loss 2.231286 logits_loss 218.561744\n",
      "[22:2500] F: 13600000 \t tot_loss 13.405301 rs_loss 0.456586 vs_loss 2.021666 logits_loss 218.540974\n",
      "[22:2600] F: 13616000 \t tot_loss 13.102948 rs_loss 0.403808 vs_loss 1.788154 logits_loss 218.219713\n",
      "[22:2700] F: 13632000 \t tot_loss 12.940807 rs_loss 0.376297 vs_loss 1.644362 logits_loss 218.402968\n",
      "[22:2800] F: 13648000 \t tot_loss 12.917662 rs_loss 0.372527 vs_loss 1.641763 logits_loss 218.067437\n",
      "[22:2900] F: 13664000 \t tot_loss 12.827823 rs_loss 0.354039 vs_loss 1.594069 logits_loss 217.594303\n",
      "[22:3000] F: 13680000 \t tot_loss 12.815278 rs_loss 0.345976 vs_loss 1.579297 logits_loss 217.800083\n",
      "[22:3100] F: 13696000 \t tot_loss 12.811447 rs_loss 0.331699 vs_loss 1.593905 logits_loss 217.716863\n",
      "[22:3200] F: 13712000 \t tot_loss 12.824298 rs_loss 0.334884 vs_loss 1.602138 logits_loss 217.745503\n",
      "[22:3300] F: 13728000 \t tot_loss 12.999885 rs_loss 0.369976 vs_loss 1.713964 logits_loss 218.318887\n",
      "[22:3400] F: 13744000 \t tot_loss 13.370122 rs_loss 0.450757 vs_loss 2.014201 logits_loss 218.103278\n",
      "[22:3500] F: 13760000 \t tot_loss 13.533740 rs_loss 0.501786 vs_loss 2.124609 logits_loss 218.146894\n",
      "[22:3600] F: 13776000 \t tot_loss 13.678696 rs_loss 0.528920 vs_loss 2.223224 logits_loss 218.531041\n",
      "[22:3700] F: 13792000 \t tot_loss 13.922004 rs_loss 0.566361 vs_loss 2.452535 logits_loss 218.062134\n",
      "Batch [23] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.68 (+-0.06) \n",
      "[23:0] F: 13800000 \t tot_loss 13.686487 rs_loss 0.511528 vs_loss 2.292379 logits_loss 217.651600\n",
      "[23:100] F: 13816000 \t tot_loss 13.484905 rs_loss 0.458928 vs_loss 2.119129 logits_loss 218.136953\n",
      "[23:200] F: 13832000 \t tot_loss 13.442326 rs_loss 0.438988 vs_loss 2.103994 logits_loss 217.986877\n",
      "[23:300] F: 13848000 \t tot_loss 13.260771 rs_loss 0.390818 vs_loss 1.953487 logits_loss 218.329316\n",
      "[23:400] F: 13864000 \t tot_loss 13.274503 rs_loss 0.399864 vs_loss 1.921921 logits_loss 219.054339\n",
      "[23:500] F: 13880000 \t tot_loss 13.195068 rs_loss 0.387340 vs_loss 1.870769 logits_loss 218.739184\n",
      "[23:600] F: 13896000 \t tot_loss 13.244198 rs_loss 0.404020 vs_loss 1.890336 logits_loss 218.996853\n",
      "[23:700] F: 13912000 \t tot_loss 13.378170 rs_loss 0.441106 vs_loss 1.987367 logits_loss 218.993943\n",
      "[23:800] F: 13928000 \t tot_loss 13.161750 rs_loss 0.399230 vs_loss 1.819599 logits_loss 218.858424\n",
      "[23:900] F: 13944000 \t tot_loss 13.172050 rs_loss 0.402334 vs_loss 1.813133 logits_loss 219.131655\n",
      "[23:1000] F: 13960000 \t tot_loss 13.177407 rs_loss 0.388754 vs_loss 1.846143 logits_loss 218.850183\n",
      "[23:1100] F: 13976000 \t tot_loss 12.984523 rs_loss 0.337727 vs_loss 1.690322 logits_loss 219.129482\n",
      "[23:1200] F: 13992000 \t tot_loss 13.010649 rs_loss 0.326262 vs_loss 1.728137 logits_loss 219.124997\n",
      "[23:1300] F: 14008000 \t tot_loss 13.012997 rs_loss 0.333334 vs_loss 1.715800 logits_loss 219.277243\n",
      "[23:1400] F: 14024000 \t tot_loss 12.952659 rs_loss 0.327679 vs_loss 1.653969 logits_loss 219.420210\n",
      "[23:1500] F: 14040000 \t tot_loss 13.045076 rs_loss 0.346165 vs_loss 1.736574 logits_loss 219.246721\n",
      "[23:1600] F: 14056000 \t tot_loss 13.083935 rs_loss 0.362753 vs_loss 1.746269 logits_loss 219.498280\n",
      "[23:1700] F: 14072000 \t tot_loss 13.241061 rs_loss 0.389765 vs_loss 1.890986 logits_loss 219.206198\n",
      "[23:1800] F: 14088000 \t tot_loss 13.288231 rs_loss 0.396529 vs_loss 1.946571 logits_loss 218.902613\n",
      "[23:1900] F: 14104000 \t tot_loss 13.163060 rs_loss 0.370219 vs_loss 1.838960 logits_loss 219.077631\n",
      "[23:2000] F: 14120000 \t tot_loss 13.074756 rs_loss 0.353646 vs_loss 1.776073 logits_loss 218.900747\n",
      "[23:2100] F: 14136000 \t tot_loss 12.893414 rs_loss 0.316570 vs_loss 1.626702 logits_loss 219.002833\n",
      "[23:2200] F: 14152000 \t tot_loss 12.774686 rs_loss 0.297961 vs_loss 1.521915 logits_loss 219.096199\n",
      "[23:2300] F: 14168000 \t tot_loss 12.900948 rs_loss 0.316334 vs_loss 1.639053 logits_loss 218.911215\n",
      "[23:2400] F: 14184000 \t tot_loss 12.909065 rs_loss 0.319488 vs_loss 1.649453 logits_loss 218.802457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:2500] F: 14200000 \t tot_loss 12.871848 rs_loss 0.322210 vs_loss 1.633856 logits_loss 218.315627\n",
      "[23:2600] F: 14216000 \t tot_loss 12.982219 rs_loss 0.348740 vs_loss 1.712709 logits_loss 218.415394\n",
      "[23:2700] F: 14232000 \t tot_loss 12.867242 rs_loss 0.335788 vs_loss 1.615862 logits_loss 218.311830\n",
      "[23:2800] F: 14248000 \t tot_loss 12.831781 rs_loss 0.331565 vs_loss 1.583602 logits_loss 218.332287\n",
      "[23:2900] F: 14264000 \t tot_loss 12.795060 rs_loss 0.321752 vs_loss 1.556293 logits_loss 218.340280\n",
      "[23:3000] F: 14280000 \t tot_loss 12.748750 rs_loss 0.312102 vs_loss 1.531509 logits_loss 218.102755\n",
      "[23:3100] F: 14296000 \t tot_loss 12.707492 rs_loss 0.302601 vs_loss 1.489584 logits_loss 218.306146\n",
      "[23:3200] F: 14312000 \t tot_loss 12.712898 rs_loss 0.317270 vs_loss 1.482111 logits_loss 218.270339\n",
      "[23:3300] F: 14328000 \t tot_loss 12.736489 rs_loss 0.322160 vs_loss 1.487454 logits_loss 218.537498\n",
      "[23:3400] F: 14344000 \t tot_loss 12.734410 rs_loss 0.318978 vs_loss 1.479328 logits_loss 218.722070\n",
      "[23:3500] F: 14360000 \t tot_loss 12.767817 rs_loss 0.323740 vs_loss 1.523292 logits_loss 218.415724\n",
      "[23:3600] F: 14376000 \t tot_loss 12.878338 rs_loss 0.332596 vs_loss 1.600183 logits_loss 218.911168\n",
      "[23:3700] F: 14392000 \t tot_loss 12.893198 rs_loss 0.331604 vs_loss 1.618421 logits_loss 218.863453\n",
      "Batch [24] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.52 (+-0.08) \n",
      "[24:0] F: 14400000 \t tot_loss 12.895047 rs_loss 0.327873 vs_loss 1.607950 logits_loss 219.184471\n",
      "[24:100] F: 14416000 \t tot_loss 12.914147 rs_loss 0.350775 vs_loss 1.616391 logits_loss 218.939634\n",
      "[24:200] F: 14432000 \t tot_loss 12.978407 rs_loss 0.366819 vs_loss 1.687604 logits_loss 218.479668\n",
      "[24:300] F: 14448000 \t tot_loss 13.024298 rs_loss 0.401253 vs_loss 1.733479 logits_loss 217.791332\n",
      "[24:400] F: 14464000 \t tot_loss 12.952600 rs_loss 0.378994 vs_loss 1.696806 logits_loss 217.536021\n",
      "[24:500] F: 14480000 \t tot_loss 12.994639 rs_loss 0.383179 vs_loss 1.729689 logits_loss 217.635414\n",
      "[24:600] F: 14496000 \t tot_loss 12.963364 rs_loss 0.379084 vs_loss 1.700520 logits_loss 217.675199\n",
      "[24:700] F: 14512000 \t tot_loss 12.856758 rs_loss 0.339685 vs_loss 1.637489 logits_loss 217.591662\n",
      "[24:800] F: 14528000 \t tot_loss 12.903989 rs_loss 0.353376 vs_loss 1.673158 logits_loss 217.549097\n",
      "[24:900] F: 14544000 \t tot_loss 13.043883 rs_loss 0.372294 vs_loss 1.788780 logits_loss 217.656169\n",
      "[24:1000] F: 14560000 \t tot_loss 13.020702 rs_loss 0.370235 vs_loss 1.776588 logits_loss 217.477568\n",
      "[24:1100] F: 14576000 \t tot_loss 13.191456 rs_loss 0.400617 vs_loss 1.897753 logits_loss 217.861697\n",
      "[24:1200] F: 14592000 \t tot_loss 13.358826 rs_loss 0.441175 vs_loss 2.012676 logits_loss 218.099501\n",
      "[24:1300] F: 14608000 \t tot_loss 13.071169 rs_loss 0.394828 vs_loss 1.785085 logits_loss 217.825130\n",
      "[24:1400] F: 14624000 \t tot_loss 13.023578 rs_loss 0.370932 vs_loss 1.763385 logits_loss 217.785224\n",
      "[24:1500] F: 14640000 \t tot_loss 12.813694 rs_loss 0.339965 vs_loss 1.593763 logits_loss 217.599333\n",
      "[24:1600] F: 14656000 \t tot_loss 12.667858 rs_loss 0.305811 vs_loss 1.494602 logits_loss 217.348909\n",
      "[24:1700] F: 14672000 \t tot_loss 12.759715 rs_loss 0.312573 vs_loss 1.561603 logits_loss 217.710777\n",
      "[24:1800] F: 14688000 \t tot_loss 12.814639 rs_loss 0.338792 vs_loss 1.586762 logits_loss 217.781704\n",
      "[24:1900] F: 14704000 \t tot_loss 12.976433 rs_loss 0.363778 vs_loss 1.734812 logits_loss 217.556872\n",
      "[24:2000] F: 14720000 \t tot_loss 12.955833 rs_loss 0.361907 vs_loss 1.720482 logits_loss 217.468869\n",
      "[24:2100] F: 14736000 \t tot_loss 13.063029 rs_loss 0.407539 vs_loss 1.788088 logits_loss 217.348049\n",
      "[24:2200] F: 14752000 \t tot_loss 13.168169 rs_loss 0.424718 vs_loss 1.869994 logits_loss 217.469146\n",
      "[24:2300] F: 14768000 \t tot_loss 13.066008 rs_loss 0.410824 vs_loss 1.761107 logits_loss 217.881550\n",
      "[24:2400] F: 14784000 \t tot_loss 13.173039 rs_loss 0.413501 vs_loss 1.870271 logits_loss 217.785339\n",
      "[24:2500] F: 14800000 \t tot_loss 13.132201 rs_loss 0.402570 vs_loss 1.853179 logits_loss 217.529034\n",
      "[24:2600] F: 14816000 \t tot_loss 12.965596 rs_loss 0.369578 vs_loss 1.736564 logits_loss 217.189085\n",
      "[24:2700] F: 14832000 \t tot_loss 13.024366 rs_loss 0.387143 vs_loss 1.798030 logits_loss 216.783875\n",
      "[24:2800] F: 14848000 \t tot_loss 12.971634 rs_loss 0.400825 vs_loss 1.734825 logits_loss 216.719673\n",
      "[24:2900] F: 14864000 \t tot_loss 12.852112 rs_loss 0.361818 vs_loss 1.634973 logits_loss 217.106412\n",
      "[24:3000] F: 14880000 \t tot_loss 12.904569 rs_loss 0.375421 vs_loss 1.677339 logits_loss 217.036166\n",
      "[24:3100] F: 14896000 \t tot_loss 12.948368 rs_loss 0.385947 vs_loss 1.705394 logits_loss 217.140534\n",
      "[24:3200] F: 14912000 \t tot_loss 13.040573 rs_loss 0.404553 vs_loss 1.777740 logits_loss 217.165601\n",
      "[24:3300] F: 14928000 \t tot_loss 13.063876 rs_loss 0.405499 vs_loss 1.820953 logits_loss 216.748484\n",
      "[24:3400] F: 14944000 \t tot_loss 13.234822 rs_loss 0.440169 vs_loss 1.945629 logits_loss 216.980465\n",
      "[24:3500] F: 14960000 \t tot_loss 13.188292 rs_loss 0.429623 vs_loss 1.909492 logits_loss 216.983541\n",
      "[24:3600] F: 14976000 \t tot_loss 13.034041 rs_loss 0.382459 vs_loss 1.794603 logits_loss 217.139571\n",
      "[24:3700] F: 14992000 \t tot_loss 13.084806 rs_loss 0.402351 vs_loss 1.830613 logits_loss 217.036827\n",
      "Batch [25] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.59 (+-0.07) \n",
      "[25:0] F: 15000000 \t tot_loss 12.963345 rs_loss 0.370811 vs_loss 1.743178 logits_loss 216.987123\n",
      "[25:100] F: 15016000 \t tot_loss 13.051866 rs_loss 0.373056 vs_loss 1.799741 logits_loss 217.581378\n",
      "[25:200] F: 15032000 \t tot_loss 13.022643 rs_loss 0.342037 vs_loss 1.772740 logits_loss 218.157332\n",
      "[25:300] F: 15048000 \t tot_loss 13.268795 rs_loss 0.370707 vs_loss 1.974002 logits_loss 218.481738\n",
      "[25:400] F: 15064000 \t tot_loss 13.697698 rs_loss 0.434716 vs_loss 2.302425 logits_loss 219.211145\n",
      "[25:500] F: 15080000 \t tot_loss 13.531874 rs_loss 0.421524 vs_loss 2.148770 logits_loss 219.231589\n",
      "[25:600] F: 15096000 \t tot_loss 13.436365 rs_loss 0.412582 vs_loss 2.062056 logits_loss 219.234528\n",
      "[25:700] F: 15112000 \t tot_loss 13.223390 rs_loss 0.374950 vs_loss 1.889378 logits_loss 219.181245\n",
      "[25:800] F: 15128000 \t tot_loss 13.210383 rs_loss 0.375648 vs_loss 1.878324 logits_loss 219.128233\n",
      "[25:900] F: 15144000 \t tot_loss 13.109870 rs_loss 0.335999 vs_loss 1.827208 logits_loss 218.933252\n",
      "[25:1000] F: 15160000 \t tot_loss 13.202939 rs_loss 0.342032 vs_loss 1.905585 logits_loss 219.106433\n",
      "[25:1100] F: 15176000 \t tot_loss 13.293652 rs_loss 0.380274 vs_loss 1.949497 logits_loss 219.277610\n",
      "[25:1200] F: 15192000 \t tot_loss 12.949966 rs_loss 0.322136 vs_loss 1.659193 logits_loss 219.372734\n",
      "[25:1300] F: 15208000 \t tot_loss 13.229843 rs_loss 0.388186 vs_loss 1.872723 logits_loss 219.378662\n",
      "[25:1400] F: 15224000 \t tot_loss 13.276177 rs_loss 0.415753 vs_loss 1.892394 logits_loss 219.360579\n",
      "[25:1500] F: 15240000 \t tot_loss 13.182946 rs_loss 0.401938 vs_loss 1.822128 logits_loss 219.177582\n",
      "[25:1600] F: 15256000 \t tot_loss 13.228426 rs_loss 0.415039 vs_loss 1.850311 logits_loss 219.261509\n",
      "[25:1700] F: 15272000 \t tot_loss 13.001793 rs_loss 0.374914 vs_loss 1.665738 logits_loss 219.222815\n",
      "[25:1800] F: 15288000 \t tot_loss 12.985449 rs_loss 0.369280 vs_loss 1.662396 logits_loss 219.075461\n",
      "[25:1900] F: 15304000 \t tot_loss 13.105765 rs_loss 0.370810 vs_loss 1.775105 logits_loss 219.196995\n",
      "[25:2000] F: 15320000 \t tot_loss 13.104913 rs_loss 0.375391 vs_loss 1.798973 logits_loss 218.610987\n",
      "[25:2100] F: 15336000 \t tot_loss 13.208729 rs_loss 0.393233 vs_loss 1.865568 logits_loss 218.998532\n",
      "[25:2200] F: 15352000 \t tot_loss 13.326175 rs_loss 0.411836 vs_loss 1.977704 logits_loss 218.732704\n",
      "[25:2300] F: 15368000 \t tot_loss 13.186762 rs_loss 0.391686 vs_loss 1.862473 logits_loss 218.652048\n",
      "[25:2400] F: 15384000 \t tot_loss 13.179941 rs_loss 0.370329 vs_loss 1.859462 logits_loss 219.002988\n",
      "[25:2500] F: 15400000 \t tot_loss 13.120100 rs_loss 0.353104 vs_loss 1.821252 logits_loss 218.914877\n",
      "[25:2600] F: 15416000 \t tot_loss 12.859016 rs_loss 0.295546 vs_loss 1.629135 logits_loss 218.686689\n",
      "[25:2700] F: 15432000 \t tot_loss 13.286247 rs_loss 0.365980 vs_loss 2.006502 logits_loss 218.275286\n",
      "[25:2800] F: 15448000 \t tot_loss 13.296603 rs_loss 0.357293 vs_loss 2.018200 logits_loss 218.422195\n",
      "[25:2900] F: 15464000 \t tot_loss 13.212217 rs_loss 0.348302 vs_loss 1.970104 logits_loss 217.876213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25:3000] F: 15480000 \t tot_loss 13.270954 rs_loss 0.362549 vs_loss 1.999327 logits_loss 218.181552\n",
      "[25:3100] F: 15496000 \t tot_loss 12.834583 rs_loss 0.290902 vs_loss 1.609047 logits_loss 218.692653\n",
      "[25:3200] F: 15512000 \t tot_loss 12.774665 rs_loss 0.303506 vs_loss 1.560982 logits_loss 218.203541\n",
      "[25:3300] F: 15528000 \t tot_loss 12.912363 rs_loss 0.324159 vs_loss 1.673499 logits_loss 218.294103\n",
      "[25:3400] F: 15544000 \t tot_loss 12.878757 rs_loss 0.321591 vs_loss 1.634023 logits_loss 218.462841\n",
      "[25:3500] F: 15560000 \t tot_loss 12.937125 rs_loss 0.350553 vs_loss 1.671287 logits_loss 218.305703\n",
      "[25:3600] F: 15576000 \t tot_loss 12.868367 rs_loss 0.323625 vs_loss 1.617917 logits_loss 218.536521\n",
      "[25:3700] F: 15592000 \t tot_loss 12.746339 rs_loss 0.305327 vs_loss 1.524967 logits_loss 218.320888\n",
      "Batch [26] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.63 (+-0.07) \n",
      "[26:0] F: 15600000 \t tot_loss 12.761292 rs_loss 0.310230 vs_loss 1.545249 logits_loss 218.116247\n",
      "[26:100] F: 15616000 \t tot_loss 13.080085 rs_loss 0.357881 vs_loss 1.781091 logits_loss 218.822248\n",
      "[26:200] F: 15632000 \t tot_loss 13.212816 rs_loss 0.387813 vs_loss 1.885819 logits_loss 218.783677\n",
      "[26:300] F: 15648000 \t tot_loss 13.353957 rs_loss 0.418037 vs_loss 1.991813 logits_loss 218.882138\n",
      "[26:400] F: 15664000 \t tot_loss 13.500134 rs_loss 0.451110 vs_loss 2.078511 logits_loss 219.410243\n",
      "[26:500] F: 15680000 \t tot_loss 13.478661 rs_loss 0.462465 vs_loss 2.064797 logits_loss 219.027982\n",
      "[26:600] F: 15696000 \t tot_loss 13.309820 rs_loss 0.414096 vs_loss 1.956300 logits_loss 218.788471\n",
      "[26:700] F: 15712000 \t tot_loss 13.297243 rs_loss 0.426909 vs_loss 1.938457 logits_loss 218.637551\n",
      "[26:800] F: 15728000 \t tot_loss 13.162950 rs_loss 0.391696 vs_loss 1.854150 logits_loss 218.342074\n",
      "[26:900] F: 15744000 \t tot_loss 13.200578 rs_loss 0.409907 vs_loss 1.877224 logits_loss 218.268942\n",
      "[26:1000] F: 15760000 \t tot_loss 13.263084 rs_loss 0.436751 vs_loss 1.908733 logits_loss 218.352008\n",
      "[26:1100] F: 15776000 \t tot_loss 13.155427 rs_loss 0.390599 vs_loss 1.847362 logits_loss 218.349307\n",
      "[26:1200] F: 15792000 \t tot_loss 13.203664 rs_loss 0.401372 vs_loss 1.886157 logits_loss 218.322712\n",
      "[26:1300] F: 15808000 \t tot_loss 12.916090 rs_loss 0.325659 vs_loss 1.683179 logits_loss 218.145026\n",
      "[26:1400] F: 15824000 \t tot_loss 13.030638 rs_loss 0.369463 vs_loss 1.773523 logits_loss 217.753043\n",
      "[26:1500] F: 15840000 \t tot_loss 12.984542 rs_loss 0.353844 vs_loss 1.736213 logits_loss 217.889701\n",
      "[26:1600] F: 15856000 \t tot_loss 12.993975 rs_loss 0.351329 vs_loss 1.740991 logits_loss 218.033116\n",
      "[26:1700] F: 15872000 \t tot_loss 13.049144 rs_loss 0.365379 vs_loss 1.783758 logits_loss 218.000125\n",
      "[26:1800] F: 15888000 \t tot_loss 13.036853 rs_loss 0.336379 vs_loss 1.790809 logits_loss 218.193282\n",
      "[26:1900] F: 15904000 \t tot_loss 13.154026 rs_loss 0.364803 vs_loss 1.881969 logits_loss 218.145066\n",
      "[26:2000] F: 15920000 \t tot_loss 13.169001 rs_loss 0.370215 vs_loss 1.897235 logits_loss 218.031024\n",
      "[26:2100] F: 15936000 \t tot_loss 13.113049 rs_loss 0.351867 vs_loss 1.872834 logits_loss 217.766960\n",
      "[26:2200] F: 15952000 \t tot_loss 13.218499 rs_loss 0.381600 vs_loss 1.944677 logits_loss 217.844438\n",
      "[26:2300] F: 15968000 \t tot_loss 13.258408 rs_loss 0.407358 vs_loss 1.946860 logits_loss 218.083787\n",
      "[26:2400] F: 15984000 \t tot_loss 13.233189 rs_loss 0.402080 vs_loss 1.934888 logits_loss 217.924433\n",
      "[26:2500] F: 16000000 \t tot_loss 13.225222 rs_loss 0.404781 vs_loss 1.917818 logits_loss 218.052459\n",
      "[26:2600] F: 16016000 \t tot_loss 12.929054 rs_loss 0.329972 vs_loss 1.700132 logits_loss 217.979020\n",
      "[26:2700] F: 16032000 \t tot_loss 13.373390 rs_loss 0.417095 vs_loss 2.074559 logits_loss 217.634704\n",
      "[26:2800] F: 16048000 \t tot_loss 13.450363 rs_loss 0.438610 vs_loss 2.116037 logits_loss 217.914315\n",
      "[26:2900] F: 16064000 \t tot_loss 13.490044 rs_loss 0.441898 vs_loss 2.135053 logits_loss 218.261854\n",
      "[26:3000] F: 16080000 \t tot_loss 13.689808 rs_loss 0.497766 vs_loss 2.284725 logits_loss 218.146323\n",
      "[26:3100] F: 16096000 \t tot_loss 13.113397 rs_loss 0.378677 vs_loss 1.825176 logits_loss 218.190880\n",
      "[26:3200] F: 16112000 \t tot_loss 13.268498 rs_loss 0.405850 vs_loss 1.960910 logits_loss 218.034759\n",
      "[26:3300] F: 16128000 \t tot_loss 13.308704 rs_loss 0.417628 vs_loss 2.000570 logits_loss 217.810105\n",
      "[26:3400] F: 16144000 \t tot_loss 13.353745 rs_loss 0.416382 vs_loss 2.017859 logits_loss 218.390084\n",
      "[26:3500] F: 16160000 \t tot_loss 13.480049 rs_loss 0.465387 vs_loss 2.090150 logits_loss 218.490235\n",
      "[26:3600] F: 16176000 \t tot_loss 13.216300 rs_loss 0.419519 vs_loss 1.888123 logits_loss 218.173151\n",
      "[26:3700] F: 16192000 \t tot_loss 13.141624 rs_loss 0.402857 vs_loss 1.808595 logits_loss 218.603413\n",
      "Batch [27] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.54 (+-0.08) \n",
      "[27:0] F: 16200000 \t tot_loss 13.074332 rs_loss 0.386165 vs_loss 1.770461 logits_loss 218.354114\n",
      "[27:100] F: 16216000 \t tot_loss 12.948289 rs_loss 0.352276 vs_loss 1.657400 logits_loss 218.772254\n",
      "[27:200] F: 16232000 \t tot_loss 13.001958 rs_loss 0.367406 vs_loss 1.688711 logits_loss 218.916801\n",
      "[27:300] F: 16248000 \t tot_loss 13.104095 rs_loss 0.392188 vs_loss 1.755700 logits_loss 219.124131\n",
      "[27:400] F: 16264000 \t tot_loss 13.284983 rs_loss 0.431207 vs_loss 1.902183 logits_loss 219.031861\n",
      "[27:500] F: 16280000 \t tot_loss 13.191228 rs_loss 0.421679 vs_loss 1.837847 logits_loss 218.634031\n",
      "[27:600] F: 16296000 \t tot_loss 13.062372 rs_loss 0.380768 vs_loss 1.758259 logits_loss 218.466875\n",
      "[27:700] F: 16312000 \t tot_loss 13.108693 rs_loss 0.388921 vs_loss 1.805012 logits_loss 218.295199\n",
      "[27:800] F: 16328000 \t tot_loss 13.140770 rs_loss 0.405269 vs_loss 1.813947 logits_loss 218.431079\n",
      "[27:900] F: 16344000 \t tot_loss 13.219226 rs_loss 0.410590 vs_loss 1.895699 logits_loss 218.258729\n",
      "[27:1000] F: 16360000 \t tot_loss 13.178275 rs_loss 0.400653 vs_loss 1.867761 logits_loss 218.197208\n",
      "[27:1100] F: 16376000 \t tot_loss 13.264124 rs_loss 0.408727 vs_loss 1.934680 logits_loss 218.414361\n",
      "[27:1200] F: 16392000 \t tot_loss 13.278830 rs_loss 0.387403 vs_loss 1.984383 logits_loss 218.140888\n",
      "[27:1300] F: 16408000 \t tot_loss 13.275162 rs_loss 0.390421 vs_loss 1.974321 logits_loss 218.208403\n",
      "[27:1400] F: 16424000 \t tot_loss 13.264960 rs_loss 0.390274 vs_loss 1.954169 logits_loss 218.410343\n",
      "[27:1500] F: 16440000 \t tot_loss 13.151496 rs_loss 0.370305 vs_loss 1.866602 logits_loss 218.291774\n",
      "[27:1600] F: 16456000 \t tot_loss 13.010003 rs_loss 0.351143 vs_loss 1.724565 logits_loss 218.685901\n",
      "[27:1700] F: 16472000 \t tot_loss 12.863784 rs_loss 0.312215 vs_loss 1.604787 logits_loss 218.935617\n",
      "[27:1800] F: 16488000 \t tot_loss 12.941501 rs_loss 0.326484 vs_loss 1.681462 logits_loss 218.671081\n",
      "[27:1900] F: 16504000 \t tot_loss 13.036456 rs_loss 0.356644 vs_loss 1.736030 logits_loss 218.875639\n",
      "[27:2000] F: 16520000 \t tot_loss 13.067858 rs_loss 0.389418 vs_loss 1.761228 logits_loss 218.344242\n",
      "[27:2100] F: 16536000 \t tot_loss 13.233035 rs_loss 0.437439 vs_loss 1.901096 logits_loss 217.890002\n",
      "[27:2200] F: 16552000 \t tot_loss 13.214558 rs_loss 0.434581 vs_loss 1.879070 logits_loss 218.018121\n",
      "[27:2300] F: 16568000 \t tot_loss 12.980903 rs_loss 0.367911 vs_loss 1.725631 logits_loss 217.747228\n",
      "[27:2400] F: 16584000 \t tot_loss 12.799811 rs_loss 0.317558 vs_loss 1.596116 logits_loss 217.722752\n",
      "[27:2500] F: 16600000 \t tot_loss 13.139224 rs_loss 0.391996 vs_loss 1.865023 logits_loss 217.644091\n",
      "[27:2600] F: 16616000 \t tot_loss 13.309971 rs_loss 0.430946 vs_loss 2.002066 logits_loss 217.539168\n",
      "[27:2700] F: 16632000 \t tot_loss 13.483124 rs_loss 0.469860 vs_loss 2.141011 logits_loss 217.445057\n",
      "[27:2800] F: 16648000 \t tot_loss 13.560407 rs_loss 0.480062 vs_loss 2.190559 logits_loss 217.795720\n",
      "[27:2900] F: 16664000 \t tot_loss 13.048746 rs_loss 0.364878 vs_loss 1.788517 logits_loss 217.907038\n",
      "[27:3000] F: 16680000 \t tot_loss 12.906529 rs_loss 0.328300 vs_loss 1.688359 logits_loss 217.797400\n",
      "[27:3100] F: 16696000 \t tot_loss 12.788626 rs_loss 0.317931 vs_loss 1.565921 logits_loss 218.095479\n",
      "[27:3200] F: 16712000 \t tot_loss 12.848567 rs_loss 0.329594 vs_loss 1.627783 logits_loss 217.823793\n",
      "[27:3300] F: 16728000 \t tot_loss 12.885291 rs_loss 0.341982 vs_loss 1.638665 logits_loss 218.092872\n",
      "[27:3400] F: 16744000 \t tot_loss 12.921191 rs_loss 0.365703 vs_loss 1.639086 logits_loss 218.328040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27:3500] F: 16760000 \t tot_loss 13.049522 rs_loss 0.390739 vs_loss 1.755784 logits_loss 218.059977\n",
      "[27:3600] F: 16776000 \t tot_loss 12.873795 rs_loss 0.349578 vs_loss 1.613630 logits_loss 218.211742\n",
      "[27:3700] F: 16792000 \t tot_loss 12.809689 rs_loss 0.332684 vs_loss 1.582941 logits_loss 217.881279\n",
      "Batch [28] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.68 (+-0.06) \n",
      "[28:0] F: 16800000 \t tot_loss 12.692814 rs_loss 0.303782 vs_loss 1.502640 logits_loss 217.727838\n",
      "[28:100] F: 16816000 \t tot_loss 12.702055 rs_loss 0.292761 vs_loss 1.506207 logits_loss 218.061720\n",
      "[28:200] F: 16832000 \t tot_loss 12.779208 rs_loss 0.309717 vs_loss 1.555731 logits_loss 218.275181\n",
      "[28:300] F: 16848000 \t tot_loss 12.816971 rs_loss 0.306769 vs_loss 1.585428 logits_loss 218.495486\n",
      "[28:400] F: 16864000 \t tot_loss 13.088636 rs_loss 0.360309 vs_loss 1.791888 logits_loss 218.728773\n",
      "[28:500] F: 16880000 \t tot_loss 13.135225 rs_loss 0.362194 vs_loss 1.828271 logits_loss 218.895190\n",
      "[28:600] F: 16896000 \t tot_loss 12.955324 rs_loss 0.319905 vs_loss 1.690715 logits_loss 218.894063\n",
      "[28:700] F: 16912000 \t tot_loss 13.012036 rs_loss 0.340413 vs_loss 1.735911 logits_loss 218.714236\n",
      "[28:800] F: 16928000 \t tot_loss 12.921092 rs_loss 0.325814 vs_loss 1.635872 logits_loss 219.188116\n",
      "[28:900] F: 16944000 \t tot_loss 12.963541 rs_loss 0.355170 vs_loss 1.670363 logits_loss 218.760153\n",
      "[28:1000] F: 16960000 \t tot_loss 13.034484 rs_loss 0.377655 vs_loss 1.730300 logits_loss 218.530580\n",
      "[28:1100] F: 16976000 \t tot_loss 13.312909 rs_loss 0.425554 vs_loss 1.945406 logits_loss 218.838986\n",
      "[28:1200] F: 16992000 \t tot_loss 13.341348 rs_loss 0.431225 vs_loss 1.990197 logits_loss 218.398518\n",
      "[28:1300] F: 17008000 \t tot_loss 13.238084 rs_loss 0.398861 vs_loss 1.926791 logits_loss 218.248638\n",
      "[28:1400] F: 17024000 \t tot_loss 13.209316 rs_loss 0.386443 vs_loss 1.896047 logits_loss 218.536524\n",
      "[28:1500] F: 17040000 \t tot_loss 12.906896 rs_loss 0.359186 vs_loss 1.632308 logits_loss 218.308024\n",
      "[28:1600] F: 17056000 \t tot_loss 13.119366 rs_loss 0.385839 vs_loss 1.818673 logits_loss 218.297071\n",
      "[28:1700] F: 17072000 \t tot_loss 13.166238 rs_loss 0.391760 vs_loss 1.853305 logits_loss 218.423450\n",
      "[28:1800] F: 17088000 \t tot_loss 13.382605 rs_loss 0.431061 vs_loss 2.030108 logits_loss 218.428722\n",
      "[28:1900] F: 17104000 \t tot_loss 13.442177 rs_loss 0.436989 vs_loss 2.087790 logits_loss 218.347969\n",
      "[28:2000] F: 17120000 \t tot_loss 13.326902 rs_loss 0.452747 vs_loss 1.947878 logits_loss 218.525550\n",
      "[28:2100] F: 17136000 \t tot_loss 13.466426 rs_loss 0.498066 vs_loss 2.051516 logits_loss 218.336885\n",
      "[28:2200] F: 17152000 \t tot_loss 13.219812 rs_loss 0.441852 vs_loss 1.875796 logits_loss 218.043294\n",
      "[28:2300] F: 17168000 \t tot_loss 13.111473 rs_loss 0.403049 vs_loss 1.813580 logits_loss 217.896877\n",
      "[28:2400] F: 17184000 \t tot_loss 12.906672 rs_loss 0.354706 vs_loss 1.671973 logits_loss 217.599851\n",
      "[28:2500] F: 17200000 \t tot_loss 12.688878 rs_loss 0.299256 vs_loss 1.517512 logits_loss 217.442191\n",
      "[28:2600] F: 17216000 \t tot_loss 12.709076 rs_loss 0.310950 vs_loss 1.535112 logits_loss 217.260255\n",
      "[28:2700] F: 17232000 \t tot_loss 12.739958 rs_loss 0.310097 vs_loss 1.570936 logits_loss 217.178505\n",
      "[28:2800] F: 17248000 \t tot_loss 12.777737 rs_loss 0.298326 vs_loss 1.612986 logits_loss 217.328498\n",
      "[28:2900] F: 17264000 \t tot_loss 12.806816 rs_loss 0.305181 vs_loss 1.619528 logits_loss 217.642139\n",
      "[28:3000] F: 17280000 \t tot_loss 12.920052 rs_loss 0.323188 vs_loss 1.704045 logits_loss 217.856382\n",
      "[28:3100] F: 17296000 \t tot_loss 12.906307 rs_loss 0.334986 vs_loss 1.668537 logits_loss 218.055680\n",
      "[28:3200] F: 17312000 \t tot_loss 12.937356 rs_loss 0.367002 vs_loss 1.680076 logits_loss 217.805571\n",
      "[28:3300] F: 17328000 \t tot_loss 12.956664 rs_loss 0.369097 vs_loss 1.700599 logits_loss 217.739359\n",
      "[28:3400] F: 17344000 \t tot_loss 12.842440 rs_loss 0.355633 vs_loss 1.583065 logits_loss 218.074849\n",
      "[28:3500] F: 17360000 \t tot_loss 12.845084 rs_loss 0.338225 vs_loss 1.608236 logits_loss 217.972451\n",
      "[28:3600] F: 17376000 \t tot_loss 12.946436 rs_loss 0.355995 vs_loss 1.700752 logits_loss 217.793756\n",
      "[28:3700] F: 17392000 \t tot_loss 12.917952 rs_loss 0.350789 vs_loss 1.670951 logits_loss 217.924248\n",
      "Batch [29] starts\n"
     ]
    }
   ],
   "source": [
    "temp = 20\n",
    "\n",
    "# Load the preset policy\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_1.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"])   \n",
    "net.train(False)\n",
    "net.share_memory()\n",
    "\n",
    "# Get the actors to write on the buffer\n",
    "\n",
    "actor_processes = []\n",
    "free_queue = mp.SimpleQueue()\n",
    "loss_stats = [deque(maxlen=400) for _ in range(4)]\n",
    "\n",
    "net.train(False)\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)\n",
    "\n",
    "# Start training loop    \n",
    "\n",
    "model.train(True)\n",
    "for epoch in range(flags.tot_epoch):    \n",
    "    print(\"Batch [%d] starts\" % epoch)\n",
    "    while(not free_queue.empty()): time.sleep(1)\n",
    "    for step in range(tot_step):\n",
    "        if step == 0: \n",
    "            test_n_step_model(1, model, flags, eps_n=100, temp=temp)\n",
    "            model.train(True)\n",
    "        \n",
    "        batch = get_batch_m(flags, buffers)\n",
    "        rs_loss, vs_loss, logits_loss = compute_loss_m(model, batch)\n",
    "        tot_loss = rs_loss + vs_loss + 0.05 * logits_loss\n",
    "        for n, l in enumerate([tot_loss, rs_loss, vs_loss, logits_loss]):\n",
    "            loss_stats[n].append(l.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"[%d:%d] F: %d \\t tot_loss %f rs_loss %f vs_loss %f logits_loss %f\" % ((\n",
    "                epoch, step, (step + epoch * tot_step) * flags.bsz * flags.unroll_len,) +\n",
    "                tuple(np.average(l) for l in loss_stats)))\n",
    "        optimizer.zero_grad()        \n",
    "        tot_loss.backward()\n",
    "        optimize_params = optimizer.param_groups[0]['params']\n",
    "        if flags.grad_norm_clipping > 0:\n",
    "            total_norm = nn.utils.clip_grad_norm_(optimize_params, flags.grad_norm_clipping)\n",
    "        optimizer.step()    \n",
    "    for m in range(flags.seq_n): free_queue.put(m)\n",
    "        \n",
    "for _ in range(flags.num_actors): free_queue.put(None)        \n",
    "for actor in actor_processes: actor.join(timeout=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5319f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab596f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_queue.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ec2114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the threads\n",
    "\n",
    "for _ in range(flags.num_actors): free_queue.put(None)  \n",
    "for actor in actor_processes: actor.join(timeout=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc08045",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model testing / debug </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6e9773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict(),},\"../models/model_3.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d28e7cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())         \n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.discounting = 0.97\n",
    "flags.device = torch.device(\"cuda\")\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/model_3.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8834194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 step planning\n",
      "Finish 500 episode: avg. return: 0.38 (+-0.09) \n",
      "Time required for 1 step planning: 57.777408\n",
      "Testing 2 step planning\n",
      "Finish 500 episode: avg. return: 0.30 (+-0.08) \n",
      "Time required for 2 step planning: 124.808867\n",
      "Testing 3 step planning\n",
      "Finish 500 episode: avg. return: -0.23 (+-0.04) \n",
      "Time required for 3 step planning: 524.462036\n"
     ]
    }
   ],
   "source": [
    "all_returns = {}\n",
    "for n in range(1,4):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step_model(n, model, flags, eps_n=500, temp=20)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "773ea44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG:25704 pyplot:296 2022-11-05 08:45:13,666] Loaded backend module://matplotlib_inline.backend_inline version unknown.\n",
      "[DEBUG:25704 pyplot:296 2022-11-05 08:45:13,668] Loaded backend module://matplotlib_inline.backend_inline version unknown.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0008e-03],\n",
      "        [ 1.1497e-01],\n",
      "        [-1.9321e-02],\n",
      "        [ 1.2748e-01],\n",
      "        [ 3.2712e-01],\n",
      "        [ 6.4268e-02],\n",
      "        [ 1.4515e-01],\n",
      "        [ 2.6107e-01],\n",
      "        [-7.0306e-02],\n",
      "        [-1.4643e-01],\n",
      "        [-3.7516e-01],\n",
      "        [-1.4747e+00],\n",
      "        [-3.3316e+00],\n",
      "        [-6.4096e+00]], device='cuda:0', grad_fn=<CatBackward0>) tensor([[ 0.2557],\n",
      "        [ 0.4175],\n",
      "        [ 0.6345],\n",
      "        [ 1.5138],\n",
      "        [ 3.6506],\n",
      "        [ 4.5284],\n",
      "        [ 5.4905],\n",
      "        [ 6.8526],\n",
      "        [ 8.5195],\n",
      "        [10.2912],\n",
      "        [14.9444],\n",
      "        [26.6298],\n",
      "        [38.7544],\n",
      "        [42.7961],\n",
      "        [44.9071]], device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25704/138873245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mcur_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode_return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_exploration_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mnew_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;31m#plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/138873245.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, model, obs, add_exploration_noise)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 reward, value, policy_logits, hidden_state = model.forward_encoded(\n\u001b[0;32m---> 65\u001b[0;31m                     parent.hidden_state, torch.tensor([[action]]).to(parent.hidden_state.device), one_hot=False)\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/3153227459.py\u001b[0m in \u001b[0;36mforward_encoded\u001b[0;34m(self, encoded, actions, one_hot)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamicModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_rvpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mr_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/3153227459.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, actions)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mOutput_rvpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25704/3153227459.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    453\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 454\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MCTS testing\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"\n",
    "    Core Monte Carlo Tree Search algorithm.\n",
    "    To decide on an action, we run N simulations, always starting at the root of\n",
    "    the search tree and traversing the tree according to the UCB formula until we\n",
    "    reach a leaf node.\n",
    "    \"\"\"\n",
    "    def __init__(self, flags, num_actions):\n",
    "        self.flags = flags\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def run(self, model, obs, add_exploration_noise,):\n",
    "        \"\"\"\n",
    "        At the root of the search tree we use the representation function to obtain a\n",
    "        hidden state given the current observation.\n",
    "        We then run a Monte Carlo Tree Search using only action sequences and the model\n",
    "        learned by the network.\n",
    "        Only supports a batch size of 1.        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            root = Node(0)\n",
    "            _, root_predicted_value, policy_logits, hidden_state = model(\n",
    "                obs[\"frame\"][0], obs[\"last_action\"], one_hot=False)\n",
    "            reward = 0.\n",
    "            root_predicted_value = root_predicted_value[-1].item()\n",
    "            policy_logits = policy_logits[-1]\n",
    "            hidden_state = hidden_state[-1]\n",
    "\n",
    "            root.expand(num_actions, reward, policy_logits, hidden_state,)\n",
    "\n",
    "            if add_exploration_noise:\n",
    "                root.add_exploration_noise(\n",
    "                    dirichlet_alpha=self.flags.root_dirichlet_alpha,\n",
    "                    exploration_fraction=self.flags.root_exploration_fraction,\n",
    "                )\n",
    "\n",
    "            min_max_stats = MinMaxStats()\n",
    "\n",
    "            max_tree_depth = 0\n",
    "            \n",
    "            #print(\"p at root:\", torch.softmax(policy_logits, dim=-1))\n",
    "            for k in range(self.flags.num_simulations): \n",
    "                \n",
    "                #print(\"=======%d iteration======\"%k)\n",
    "                node = root\n",
    "                search_path = [node]\n",
    "                current_tree_depth = 0\n",
    "\n",
    "                while node.expanded():\n",
    "                    current_tree_depth += 1                    \n",
    "                    action, node = self.select_child(node, min_max_stats)                    \n",
    "                    search_path.append(node)\n",
    "                    #print(\"action sel: %d\" % action)\n",
    "                \n",
    "                #np.set_printoptions(precision=5)\n",
    "                #for x in [\"prior_score\", \"value_score\", \"pb_c\", \"prior\", \"visit_count\"]:                    \n",
    "                #    print(x, \"\\t\", np.array([getattr(search_path[0].children[n], x) for n in range(5)]))\n",
    "\n",
    "                # Inside the search tree we use the dynamics function to obtain the next hidden\n",
    "                # state given an action and the previous hidden state\n",
    "                parent = search_path[-2]     \n",
    "                reward, value, policy_logits, hidden_state = model.forward_encoded(\n",
    "                    parent.hidden_state, torch.tensor([[action]]).to(parent.hidden_state.device), one_hot=False)\n",
    "                reward = reward[-1].item()\n",
    "                value = value[-1].item()\n",
    "                #print(\"model final output: %4f\" % value)\n",
    "                policy_logits = policy_logits[-1]\n",
    "                hidden_state = hidden_state[-1]\n",
    "                node.expand(num_actions, reward, policy_logits, hidden_state)\n",
    "                self.backpropagate(search_path, value, min_max_stats)\n",
    "                max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
    "\n",
    "            extra_info = {\n",
    "                \"max_tree_depth\": max_tree_depth,\n",
    "                \"root_predicted_value\": root_predicted_value,\n",
    "            }\n",
    "        return root, extra_info\n",
    "\n",
    "    def select_child(self, node, min_max_stats):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        max_ucb = max(\n",
    "            self.ucb_score(node, child, min_max_stats)\n",
    "            for action, child in node.children.items()\n",
    "        )\n",
    "        action = np.random.choice(\n",
    "            [\n",
    "                action\n",
    "                for action, child in node.children.items()\n",
    "                if self.ucb_score(node, child, min_max_stats) == max_ucb\n",
    "            ]\n",
    "        )\n",
    "        return action, node.children[action]\n",
    "\n",
    "    def ucb_score(self, parent, child, min_max_stats):\n",
    "        \"\"\"\n",
    "        The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
    "        \"\"\"\n",
    "        pb_c = (\n",
    "            math.log(\n",
    "                (parent.visit_count + self.flags.pb_c_base + 1) / self.flags.pb_c_base\n",
    "            )\n",
    "            + self.flags.pb_c_init\n",
    "        )\n",
    "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior\n",
    "\n",
    "        if child.visit_count > 0:\n",
    "            # Mean value Q\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward + self.flags.discounting * child.value())\n",
    "        else:\n",
    "            value_score = 0\n",
    "            \n",
    "        child.pb_c = pb_c\n",
    "        child.prior_score = prior_score\n",
    "        child.value_score = value_score\n",
    "        \n",
    "        return prior_score + value_score\n",
    "\n",
    "    def backpropagate(self, search_path, value, min_max_stats):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        #print(\"bs value: %.4f\" % value)\n",
    "        for n, node in enumerate(reversed(search_path)):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            min_max_stats.update(node.reward + self.flags.discounting * node.value())\n",
    "            value = node.reward + self.flags.discounting * value\n",
    "            #print(\"%d - val: %.4f r: %.4f\" % (n, value, node.reward))\n",
    "            #print(\"node value_sum %.4f\" % node.value_sum)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior):\n",
    "        self.visit_count = 0\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def expand(self, num_actions, reward, policy_logits, hidden_state):\n",
    "        \"\"\"\n",
    "        We expand a node using the value, reward and policy prediction obtained from the\n",
    "        neural network.\n",
    "        \"\"\"\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        policy_values = torch.softmax(policy_logits[0], dim=0).tolist()\n",
    "        for a in range(num_actions):\n",
    "            self.children[a] = Node(policy_values[a])\n",
    "\n",
    "    def add_exploration_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        \"\"\"\n",
    "        At the start of each search, we add dirichlet noise to the prior of the root to\n",
    "        encourage the search to explore new actions.\n",
    "        \"\"\"\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "class MinMaxStats:\n",
    "    \"\"\"\n",
    "    A class that holds the min-max values of the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.maximum = -float(\"inf\")\n",
    "        self.minimum = float(\"inf\")\n",
    "\n",
    "    def update(self, value):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value):\n",
    "        if self.maximum > self.minimum:\n",
    "            # We normalize only when we have set the maximum and minimum values\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value            \n",
    "\n",
    "def select_action(node, temperature):\n",
    "    \"\"\"\n",
    "    Select action according to the visit count distribution and the temperature.\n",
    "    The temperature is changed dynamically with the visit_softmax_temperature function\n",
    "    in the config.\n",
    "    \"\"\"\n",
    "    visit_counts = np.array(\n",
    "        [child.visit_count for child in node.children.values()], dtype=\"int32\"\n",
    "    )\n",
    "    actions = [action for action in node.children.keys()]\n",
    "    if temperature == 0:\n",
    "        action = actions[np.argmax(visit_counts)]\n",
    "    elif temperature == float(\"inf\"):\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        # See paper appendix Data Generation\n",
    "        visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "        visit_count_distribution = visit_count_distribution / sum(\n",
    "            visit_count_distribution\n",
    "        )\n",
    "        action = np.random.choice(actions, p=visit_count_distribution)\n",
    "    #print(\"visit_counts\", visit_counts)\n",
    "    #print(\"visit_count_distribution\", visit_count_distribution)\n",
    "    return action\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()      \n",
    "flags = parser.parse_args([])   \n",
    "\n",
    "env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)\n",
    "env = Environment(env)\n",
    "env.initial()\n",
    "obs_shape, num_actions = env.gym_env.observation_space.shape, env.gym_env.action_space.n\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args([])   \n",
    "flags.discounting = 0.97\n",
    "flags.pb_c_init = 1.25\n",
    "flags.pb_c_base = 19652\n",
    "flags.root_dirichlet_alpha = 0.25\n",
    "flags.root_exploration_fraction = 0.\n",
    "flags.num_simulations = 3\n",
    "flags.temp = 0.5\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "eps_n = 10\n",
    "eps_n_cur = 0\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "obs = env.initial()\n",
    "returns = []\n",
    "mcts = MCTS(flags, num_actions)\n",
    "obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "root, extra_info = mcts.run(model, obs, add_exploration_noise=True)   \n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "actions = torch.tensor([0, 1, 3, 4, 4, 4, 2, 4, 2, 2, 2, 1, 1, 1, 1]).long().to(flags.device).reshape(-1, 1)\n",
    "reward, value, policy_logits, hidden_state  = model(obs[\"frame\"][0], actions, one_hot=False)\n",
    "print(reward, value)\n",
    "\n",
    "while(len(returns) <= eps_n):\n",
    "    cur_returns = obs['episode_return']    \n",
    "    obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "    root, extra_info = mcts.run(model, obs, add_exploration_noise=True)    \n",
    "    new_action = select_action(root, flags.temp)    \n",
    "    #plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    #flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "    #plt.show()\n",
    "    #print(\"action selected\", new_action)\n",
    "    #print(\"===========================================\")\n",
    "    obs = env.step(torch.tensor([new_action]))\n",
    "    if torch.any(obs['done']):\n",
    "        returns.extend(cur_returns[obs['done']].numpy())\n",
    "    if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "        eps_n_cur = len(returns) + 10\n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test planning algorithm\n",
    "bsz = 1\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "obs = env.initial()\n",
    "state = obs['frame'][0].to(flags.device).clone()\n",
    "action = torch.zeros(bsz).long().to(flags.device)\n",
    "encoded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e67032",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.Tensor([4]).long().to(flags.device)\n",
    "obs = env.step(action)\n",
    "state = obs['frame'][0].to(flags.device).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c551f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "device = flags.device\n",
    "\n",
    "for _ in range(1):\n",
    "    plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "    plt.show()\n",
    "    ret = np.zeros((5, 5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            for k in range(5):\n",
    "                test_action_seq = [i,j,k]\n",
    "                test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "                old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "                rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "                ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "    print(np.max(ret), (np.max(ret) == ret).nonzero())    \n",
    "    new_action = torch.Tensor((np.max(ret) == ret).nonzero()[0]).long().to(flags.device)\n",
    "    #obs = env.step(new_action)\n",
    "    #state = obs['frame'][0].to(flags.device).clone()\n",
    "    #action = new_action            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a9ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, prob, q_ret = n_step_greedy_model(state, action, model, 3, encoded=None, temp=10.)\n",
    "print(\"action: \", action)\n",
    "print(\"prob: \", prob)\n",
    "print(\"q_ret: \", q_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_action_seq = [2,3,1]\n",
    "test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "print(\"rs\", rs)\n",
    "print(\"vs\", vs)\n",
    "print(\"logits\", logits)\n",
    "print(\"ret\", ret[i,j,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 10.\n",
    "\n",
    "bsz = state.shape[0]\n",
    "device = state.device \n",
    "num_actions = model.num_actions    \n",
    "model.train(False)\n",
    "\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "\n",
    "for act in range(num_actions):        \n",
    "    new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "    old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "    rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "    ret = rs[0] + flags.discounting * vs[1]\n",
    "    rs_act[:, act] = rs[0]\n",
    "    vs_act[:, act] = vs[1]\n",
    "    q_ret[:, act] = ret\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = flags.device\n",
    "net_state = env.clone_state()\n",
    "\n",
    "bsz = 1\n",
    "temp = 10.\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)   \n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "for act in range(num_actions):\n",
    "    obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "    obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "    ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "    rs_act[:, act] = obs['reward']\n",
    "    vs_act[:, act] = net(obs)[0]['baseline']\n",
    "    q_ret[:, act] = ret\n",
    "    env.restore_state(net_state)\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)\n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch_m(flags, buffers)\n",
    "print(torch.max(batch[\"reward\"]), (torch.max(batch[\"reward\"]) == batch[\"reward\"]).nonzero())\n",
    "print(batch[\"done\"].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG LOSS\n",
    "\n",
    "#batch = get_batch_m(flags, buffers)\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "logits = logits[:-1]\n",
    "\n",
    "target_rewards = batch['reward'][1:]\n",
    "target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "target_vs = []\n",
    "target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0]    \n",
    "\n",
    "for t in range(vs.shape[0]-1, 0, -1):\n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))\n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "# if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "# rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "# vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "# logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "done_masks = []\n",
    "done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "for t in range(vs.shape[0]):\n",
    "    done = torch.logical_or(done, batch['done'][t])\n",
    "    done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "\n",
    "# compute final loss\n",
    "huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "rs_loss = torch.sum(huberloss(rs, target_rewards) * (~done_masks).float())\n",
    "#rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "vs_loss = torch.sum(huberloss(vs[:-1], target_vs) * (~done_masks).float())\n",
    "#vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "logits_loss = compute_cross_entropy_loss(logits, target_logits, done_masks)\n",
    "\n",
    "# debug\n",
    "ind = 21\n",
    "\n",
    "target_vs = []\n",
    "target_v = vs[-1]\n",
    "for t in range(vs.shape[0]-1, 0, -1):        \n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    print(t, \n",
    "          \"reward %2f\" % batch['reward'][t,ind].item(), \n",
    "          \"bootstrap %2f\" % (target_v * (~batch['done'][t]).float())[ind].item(), \n",
    "          \"truncated %2f\" % (vs[t-1] * (batch['truncated_done'][t]).float())[ind].item(),\n",
    "          \"vs[t-1] %2f\" % vs[t-1][ind].item(),\n",
    "          \"new_targ %2f\" % new_target_v[ind].item())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))    \n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)   \n",
    "print(\"done\", batch[\"done\"][:, ind])\n",
    "print(\"done_masks\", done_masks[:, ind])\n",
    "print(\"vs: \", vs[:, ind])\n",
    "print(\"target_vs: \", target_vs[:, ind])\n",
    "print(\"reward: \", rs[:, ind])\n",
    "print(\"target_reward: \", target_rewards[:, ind])\n",
    "print(\"logits: \", logits[:, ind])\n",
    "print(\"target_logits: \", target_logits[:, ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt. version of computing loss by treading terminal state as absorbing state (as in MuZero)\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_logits = batch['policy_logits'][1:].clone()\n",
    "    target_rewards = batch['reward'][1:].clone()\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "\n",
    "    c_logits = target_logits[0]\n",
    "    c_state = batch['frame'][0]\n",
    "    for t in range(vs.shape[0]-1):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        c_logits = torch.where(done.unsqueeze(-1), c_logits, target_logits[t])\n",
    "        target_logits[t] = c_logits\n",
    "        c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][t])  \n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "    done_masks = torch.concat(done_masks, dim=0)\n",
    "    done = torch.logical_or(done, batch['done'][-1])\n",
    "    c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][-1])\n",
    "    target_rewards = target_rewards * (~done_masks).float()\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(c_state, batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * target_v\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()))\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()))\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), None)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
