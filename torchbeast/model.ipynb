{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc8a370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG:285693 __init__:275 2022-11-06 01:39:37,629] matplotlib data path: /home/sc/anaconda3/lib/python3.9/site-packages/matplotlib/mpl-data\n",
      "[DEBUG:285693 __init__:275 2022-11-06 01:39:37,631] CONFIGDIR=/home/sc/.config/matplotlib\n",
      "[DEBUG:285693 __init__:1445 2022-11-06 01:39:37,632] interactive is False\n",
      "[DEBUG:285693 __init__:1446 2022-11-06 01:39:37,632] platform is linux\n",
      "[DEBUG:285693 __init__:1447 2022-11-06 01:39:37,633] loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', '_io', 'marshal', 'posix', '_frozen_importlib_external', 'time', 'zipimport', '_codecs', 'codecs', 'encodings.aliases', 'encodings', 'encodings.utf_8', '_signal', 'encodings.latin_1', '_abc', 'abc', 'io', '__main__', '_stat', 'stat', '_collections_abc', 'genericpath', 'posixpath', 'os.path', 'os', '_sitebuiltins', '_locale', '_bootlocale', '_distutils_hack', 'types', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib', 'importlib.machinery', '_heapq', 'heapq', 'itertools', 'keyword', '_operator', 'operator', 'reprlib', '_collections', 'collections', 'collections.abc', '_functools', 'functools', 'contextlib', 'enum', '_sre', 'sre_constants', 'sre_parse', 'sre_compile', 'copyreg', 're', 'typing.io', 'typing.re', 'typing', 'importlib.abc', 'importlib.util', 'mpl_toolkits', 'sphinxcontrib', 'zope', 'site', '_weakrefset', 'weakref', 'pkgutil', 'runpy', 'ipykernel._version', '_json', 'json.scanner', 'json.decoder', 'json.encoder', 'json', 'errno', 'signal', 'threading', 'pwd', 'grp', '_posixsubprocess', 'select', 'math', 'selectors', 'subprocess', 'jupyter_client._version', '_ast', 'ast', '_opcode', 'opcode', 'dis', 'token', 'tokenize', 'linecache', 'inspect', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets.utils.descriptions', 'traitlets.traitlets', 'copy', 'traitlets.utils.decorators', 'traitlets._version', 'traitlets', 'concurrent', 'traceback', '_string', 'string', 'atexit', 'logging', 'concurrent.futures._base', 'concurrent.futures', '_socket', 'array', 'socket', '_ssl', '_struct', 'struct', 'binascii', 'base64', 'ssl', 'asyncio.constants', 'asyncio.format_helpers', 'asyncio.base_futures', 'asyncio.log', 'asyncio.coroutines', '_contextvars', 'contextvars', 'asyncio.exceptions', 'asyncio.base_tasks', '_asyncio', 'asyncio.events', 'asyncio.futures', 'asyncio.protocols', 'asyncio.transports', 'asyncio.sslproto', 'asyncio.locks', 'asyncio.tasks', 'asyncio.staggered', 'asyncio.trsock', 'asyncio.base_events', 'asyncio.runners', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.threads', 'asyncio.base_subprocess', 'asyncio.selector_events', 'asyncio.unix_events', 'asyncio', '_queue', 'queue', 'platform', '_ctypes', 'ctypes._endian', 'ctypes', 'zmq.backend.select', '_cython_0_29_30', 'cython_runtime', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.message', '_bisect', 'bisect', '_random', '_sha512', 'random', '_compat_pickle', '_pickle', 'pickle', 'zmq.constants', 'zmq.backend.cython.socket', 'zmq.backend.cython._device', 'zmq.backend.cython._poll', 'zmq.backend.cython._proxy_steerable', 'zmq.backend.cython._version', 'zmq.backend.cython.error', 'zmq.backend.cython.utils', 'zmq.backend.cython', 'zmq.backend', 'zmq.sugar.attrsettr', 'zmq._typing', 'zmq.utils', 'zmq.utils.jsonapi', 'zmq.sugar.poll', 'zmq.sugar.socket', 'zmq.sugar.context', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'zmq.sugar', 'zmq', 'zmq._future', 'zmq.asyncio', 'jupyter_client.channelsabc', '_hashlib', '_blake2', 'hashlib', 'hmac', 'pprint', '_datetime', 'datetime', 'gettext', 'argparse', 'traitlets.config.loader', 'textwrap', 'traitlets.utils.text', 'traitlets.config.configurable', 'traitlets.config.application', 'traitlets.config', 'traitlets.log', 'tornado', 'numbers', 'logging.handlers', 'html.entities', 'html', 'urllib', 'urllib.parse', 'zlib', 'tornado.speedups', 'tornado.util', 'tornado.escape', 'colorama.ansi', 'colorama.win32', 'colorama.winterm', 'colorama.ansitowin32', 'colorama.initialise', 'colorama', '_curses', 'curses', 'tornado.log', 'tornado.concurrent', 'tornado.ioloop', 'tornado.platform', 'tornado.gen', 'tornado.platform.asyncio', 'zmq.eventloop.ioloop', 'zmq.eventloop', 'zmq.eventloop.zmqstream', 'jupyter_client.adapter', 'dateutil._version', 'dateutil', '__future__', 'locale', 'calendar', 'six', '_decimal', 'decimal', 'dateutil._common', 'dateutil.relativedelta', 'six.moves', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.tz.tz', 'dateutil.tz', 'dateutil.parser._parser', 'dateutil.parser.isoparser', 'dateutil.parser', '_strptime', 'jupyter_client.jsonutil', 'jupyter_client.session', 'jupyter_client.channels', 'termios', 'getpass', 'jupyter_client.clientabc', 'fnmatch', 'glob', '_compression', '_bz2', 'bz2', '_lzma', 'lzma', 'shutil', 'tempfile', 'jupyter_core.version', 'jupyter_core', 'ntpath', 'pathlib', 'jupyter_core.paths', 'jupyter_client.localinterfaces', 'jupyter_client.utils', 'jupyter_client.connect', 'jupyter_client.client', 'jupyter_client.asynchronous.client', 'jupyter_client.asynchronous', 'jupyter_client.blocking.client', 'jupyter_client.blocking', 'jupyter_client.launcher', '_uuid', 'uuid', 'jupyter_client.managerabc', 'zipfile', 'configparser', 'entrypoints', 'jupyter_client.provisioning.provisioner_base', 'jupyter_client.provisioning.factory', 'jupyter_client.provisioning.local_provisioner', 'jupyter_client.provisioning', 'jupyter_client.kernelspec', 'jupyter_client.manager', 'jupyter_client.multikernelmanager', 'jupyter_client', 'ipykernel.connect', 'ipykernel', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'sysconfig', '_sysconfigdata__linux_x86_64-linux-gnu', 'pydoc', 'bdb', 'IPython.utils', 'IPython.utils.ipstruct', 'IPython.utils.coloransi', 'pygments', 'IPython.utils.colorable', 'IPython.utils.PyColorize', 'IPython.utils.encoding', 'IPython.utils.py3compat', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'cmd', 'codeop', 'code', 'pdb', 'IPython.core.debugger', 'IPython.core.display_trap', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'tty', 'pty', 'fcntl', 'resource', 'ptyprocess.util', 'ptyprocess.ptyprocess', 'ptyprocess', 'pexpect.spawnbase', 'pexpect.pty_spawn', 'pexpect.run', 'pexpect', 'shlex', 'IPython.utils._process_common', 'IPython.utils._process_posix', 'IPython.utils.process', 'IPython.utils.decorators', 'IPython.utils.path', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.core.ultratb', 'IPython.utils._sysinfo', 'IPython.utils.sysinfo', 'IPython.core.crashhandler', 'IPython.utils.importstring', 'IPython.paths', 'IPython.core.profiledir', 'IPython.core.application', 'IPython.terminal', 'IPython.core.compilerop', 'IPython.core.error', 'IPython.utils.text', 'IPython.core.magic_arguments', 'getopt', 'mimetypes', 'IPython.core.display', 'IPython.core.page', 'IPython.lib.security', 'IPython.lib', 'IPython.lib.pretty', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.util', 'pygments.lexers', 'pygments.filter', 'pygments.token', 'pygments.filters', 'pygments.regexopt', 'pygments.lexer', 'pygments.unistring', 'pygments.lexers.python', 'pygments.formatters._mapping', 'pygments.formatters', 'pygments.styles', 'pygments.formatter', 'pygments.formatters.html', 'IPython.core.oinspect', 'IPython.core.inputtransformer2', 'decorator', 'IPython.core.magic', 'pickleshare', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.prefilter', 'IPython.core.alias', 'IPython.core.builtin_trap', 'backcall.backcall', 'backcall', 'IPython.core.events', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.utils.sentinel', 'IPython.core.formatters', '_sqlite3', 'sqlite3.dbapi2', 'sqlite3', 'IPython.core.history', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.lib.display', 'IPython.display', 'IPython.utils.capture', 'IPython.utils.io', 'IPython.core.hooks', 'IPython.utils.strdispatch', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.core.interactiveshell', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop.utils', 'prompt_toolkit.eventloop.async_generator', 'wcwidth.table_wide', 'wcwidth.table_zero', 'wcwidth.unicode_versions', 'wcwidth.wcwidth', 'wcwidth', 'prompt_toolkit.utils', 'prompt_toolkit.eventloop.inputhook', 'prompt_toolkit.eventloop', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.clipboard', 'prompt_toolkit.cache', 'prompt_toolkit.enums', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.app', 'prompt_toolkit.filters.cli', 'prompt_toolkit.filters.utils', 'prompt_toolkit.filters', 'prompt_toolkit.document', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.data_structures', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.pygments', 'colorsys', 'prompt_toolkit.styles.style_transformation', 'prompt_toolkit.styles', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.base', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output', 'prompt_toolkit.output.vt100', 'prompt_toolkit.mouse_events', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.formatted_text.ansi', 'xml', 'xml.dom.domreg', 'xml.dom', 'xml.dom.minicompat', 'xml.dom.NodeFilter', 'xml.dom.xmlbuilder', 'xml.dom.minidom', 'prompt_toolkit.formatted_text.html', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.formatted_text', 'prompt_toolkit.completion.base', 'prompt_toolkit.completion.deduplicate', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.completion.nested', 'prompt_toolkit.completion', 'prompt_toolkit.history', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.search', 'prompt_toolkit.validation', 'prompt_toolkit.buffer', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.lexers', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.controls', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.layout.screen', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.layout.scrollable_pane', 'prompt_toolkit.layout', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.renderer', 'prompt_toolkit.application.application', 'prompt_toolkit.application.dummy', 'prompt_toolkit.application', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.widgets', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.shortcuts', 'prompt_toolkit', 'prompt_toolkit.patch_stdout', 'pygments.style', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'parso.utils', 'parso.tree', 'parso.python', 'parso.python.token', 'parso.python.tokenize', 'parso.pgen2.grammar_parser', 'parso.pgen2.generator', 'parso.pgen2', 'parso.parser', 'parso._compatibility', 'difflib', 'parso.python.prefix', 'parso.python.tree', 'parso.python.parser', 'parso.python.diff', 'gc', 'parso.cache', 'parso.normalizer', 'parso.python.errors', 'parso.python.pep8', 'parso.file_io', 'parso.grammar', 'parso', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.file_io', 'jedi.inference.cache', 'jedi.inference.helpers', 'jedi.inference.utils', 'jedi.inference.base_value', 'jedi.inference.sys_path', 'jedi.inference.recursion', 'jedi.inference.flow_analysis', 'jedi.common', 'jedi.inference.lazy_value', 'jedi.inference.docstrings', 'jedi.plugins', 'jedi.inference.names', 'jedi.inference.filters', 'jedi.inference.compiled.getattr_static', 'jedi.inference.compiled.access', 'jedi.inference.signature', 'jedi.inference.context', 'jedi.inference.compiled.value', 'jedi.inference.compiled', 'jedi.inference.analysis', 'jedi.inference.gradual', 'jedi.inference.value.module', 'jedi.inference.value.dynamic_arrays', 'jedi.inference.value.iterable', 'jedi.inference.arguments', 'jedi.inference.parser_cache', 'jedi.inference.gradual.generics', 'jedi.inference.value.function', 'jedi.inference.value.klass', 'jedi.inference.value.instance', 'jedi.inference.value', 'jedi.inference.gradual.base', 'jedi.inference.gradual.type_var', 'jedi.inference.gradual.typing', 'jedi.inference.gradual.stub_value', 'jedi.inference.gradual.typeshed', 'jedi._compatibility', 'jedi.inference.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.inference.compiled.subprocess', 'jedi.inference.imports', 'jedi.inference.param', 'jedi.inference.gradual.annotation', 'jedi.inference.value.decorator', 'jedi.inference.syntax_tree', 'jedi.inference', 'jedi.inference.gradual.conversion', 'jedi.inference.compiled.mixed', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.keywords', 'jedi.api.completion_cache', 'jedi.api.helpers', 'jedi.api.classes', 'jedi.api.interpreter', 'jedi.api.strings', 'jedi.api.file_name', 'jedi.inference.docstring_utils', 'jedi.api.completion', 'filecmp', 'jedi.api.environment', 'jedi.inference.references', 'jedi.api.project', 'jedi.api.errors', 'jedi.api.refactoring', 'jedi.api.refactoring.extract', 'jedi.inference.gradual.utils', 'jedi.api', 'jedi.plugins.stdlib', 'jedi.plugins.flask', 'jedi.plugins.pytest', 'jedi.plugins.django', 'jedi.plugins.registry', 'jedi', 'IPython.core.completer', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'concurrent.futures.thread', 'IPython.terminal.debugger', 'IPython.lib.clipboard', 'IPython.terminal.magics', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.interactiveshell', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'email', 'http', 'email.errors', 'email.quoprimime', 'email.base64mime', 'quopri', 'email.encoders', 'email.charset', 'email.header', 'email._parseaddr', 'email.utils', 'email._policybase', 'email.feedparser', 'email.parser', 'uu', 'email._encoded_words', 'email.iterators', 'email.message', 'http.client', 'urllib.response', 'urllib.error', 'urllib.request', 'IPython.core.magics.code', 'IPython.core.magics.config', 'IPython.core.magics.display', 'timeit', '_lsprof', 'profile', 'cProfile', 'dataclasses', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.execution', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.pylabtools', 'IPython.core.magics.pylab', 'IPython.lib.backgroundjobs', 'IPython.core.magics.script', 'IPython.core.magics', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.terminal.ipapp', 'IPython.terminal.embed', 'IPython.utils.frame', 'IPython', 'ipykernel.control', 'ipykernel.heartbeat', 'ipykernel.iostream', 'IPython.utils.tokenutil', 'ipykernel.jsonutil', 'psutil._common', 'psutil._compat', 'psutil._psposix', 'psutil._psutil_linux', 'psutil._psutil_posix', 'psutil._pslinux', 'psutil', 'tornado.locks', 'tornado.queues', 'ipykernel.kernelbase', 'ipykernel.comm.comm', 'ipykernel.comm.manager', 'ipykernel.comm', 'ipykernel.compiler', 'debugpy._version', 'debugpy.common', 'debugpy.common.json', 'debugpy.common.fmt', 'debugpy.common.compat', 'debugpy', 'debugpy._vendored._util', 'debugpy._vendored', '_pydevd_bundle', 'encodings.ascii', 'stringprep', 'encodings.idna', '_pydevd_bundle.pydevd_vm_type', '_pydev_imps', 'xmlrpc', 'xml.parsers', 'pyexpat.errors', 'pyexpat.model', 'pyexpat', 'xml.parsers.expat.model', 'xml.parsers.expat.errors', 'xml.parsers.expat', 'gzip', 'xmlrpc.client', 'socketserver', 'http.server', 'xmlrpc.server', '_pydev_imps._pydev_saved_modules', '_pydevd_bundle.pydevd_constants', '_pydev_bundle', '_pydev_runfiles', '_pydevd_frame_eval', 'pydev_ipython', 'pydevd_concurrency_analyser', 'plistlib', 'pkg_resources.extern', 'pkg_resources._vendor', 'pkg_resources._vendor.jaraco', 'pkg_resources.extern.jaraco', 'importlib._common', 'importlib.resources', 'pkg_resources._vendor.more_itertools.recipes', 'pkg_resources._vendor.more_itertools.more', 'pkg_resources._vendor.more_itertools', 'pkg_resources.extern.more_itertools', 'pkg_resources.extern.jaraco.functools', 'pkg_resources.extern.jaraco.context', 'pkg_resources.extern.jaraco.text', 'pkg_resources._vendor.appdirs', 'pkg_resources.extern.appdirs', 'pkg_resources._vendor.packaging.__about__', 'pkg_resources._vendor.packaging', 'pkg_resources.extern.packaging', 'pkg_resources.extern.packaging._structures', 'pkg_resources.extern.packaging.version', 'pkg_resources._vendor.packaging._manylinux', 'pkg_resources._vendor.packaging._musllinux', 'pkg_resources.extern.packaging.tags', 'pkg_resources.extern.packaging.utils', 'pkg_resources.extern.packaging.specifiers', 'pkg_resources._vendor.pyparsing.util', 'pkg_resources._vendor.pyparsing.unicode', 'pkg_resources._vendor.pyparsing.exceptions', 'pkg_resources._vendor.pyparsing.actions', 'pkg_resources._vendor.pyparsing.results', 'pkg_resources._vendor.pyparsing.core', 'pkg_resources._vendor.pyparsing.helpers', 'pkg_resources._vendor.pyparsing.testing', 'pkg_resources._vendor.pyparsing.common', 'pkg_resources._vendor.pyparsing', 'pkg_resources.extern.pyparsing', 'pkg_resources.extern.packaging.markers', 'pkg_resources.extern.packaging.requirements', 'pkg_resources', 'pydevd_plugins', '_pydev_bundle.pydev_log', '_pydev_bundle._pydev_filesystem_encoding', '_pydevd_bundle.pydevd_comm_constants', 'pydevd_file_utils', '_pydev_imps._pydev_execfile', '_pydevd_bundle.pydevd_exec2', '_pydev_bundle.pydev_imports', '_pydev_bundle.pydev_is_thread_alive', '_pydev_bundle.pydev_override', 'pydevd_plugins.extensions', '_pydevd_bundle.pydevd_extension_utils', '_pydevd_bundle.pydevd_frame_utils', '_pydevd_bundle.pydevd_filtering', '_pydevd_bundle.pydevd_io', '_pydevd_bundle.pydevd_utils', '_pydev_bundle._pydev_tipper_common', '_pydev_bundle._pydev_imports_tipper', '_pydev_bundle._pydev_calltip_util', '_pydevd_bundle.pydevd_safe_repr', '_pydevd_bundle.pydevd_resolver', '_pydevd_bundle.pydevd_extension_api', '_pydevd_bundle.pydevd_xml', '_pydevd_bundle.pydevd_dont_trace', '_pydevd_frame_eval.vendored', '_pydevd_frame_eval.vendored.bytecode.flags', '_pydevd_frame_eval.vendored.bytecode.instr', '_pydevd_frame_eval.vendored.bytecode.bytecode', '_pydevd_frame_eval.vendored.bytecode.concrete', '_pydevd_frame_eval.vendored.bytecode.cfg', '_pydevd_frame_eval.vendored.bytecode', '_pydevd_bundle.pydevd_bytecode_utils', '_pydevd_bundle.pydevd_cython', '_pydevd_bundle.pydevd_cython_wrapper', '_pydevd_bundle.pydevd_additional_thread_info', '_pydevd_bundle.pydevd_thread_lifecycle', '_pydevd_bundle.pydevd_save_locals', '_pydevd_bundle.pydevd_defaults', '_pydev_bundle.pydev_monkey', 'pydevd_tracing', '_pydevd_bundle.pydevd_daemon_thread', '_pydevd_bundle.pydevd_timeout', '_pydevd_bundle.pydevd_vars', '_pydev_bundle.pydev_console_utils', '_pydevd_bundle.pydevd_import_class', '_pydevd_bundle.pydevd_breakpoints', '_pydevd_bundle.pydevd_custom_frames', '_pydevd_bundle.pydevd_dont_trace_files', '_pydevd_bundle.pydevd_net_command', '_pydev_bundle.pydev_umd', 'pydevconsole', '_pydev_bundle._pydev_completer', '_pydevd_bundle.pydevd_net_command_factory_xml', '_pydevd_bundle.pydevd_frame', '_pydevd_bundle.pydevd_additional_thread_info_regular', '_pydevd_bundle.pydevd_trace_dispatch', '_pydevd_frame_eval.pydevd_frame_eval_main', '_pydevd_bundle.pydevd_source_mapping', 'pydevd_concurrency_analyser.pydevd_thread_wrappers', 'pydevd_concurrency_analyser.pydevd_concurrency_logger', '_pydevd_bundle._debug_adapter', '_pydevd_bundle._debug_adapter.pydevd_schema_log', '_pydevd_bundle._debug_adapter.pydevd_base_schema', '_pydevd_bundle._debug_adapter.pydevd_schema', '_pydevd_bundle.pydevd_reload', '_pydev_bundle.fsnotify', '_pydevd_bundle.pydevd_console', '_pydevd_bundle.pydevd_comm', '_pydevd_bundle.pydevd_net_command_factory_json', '_pydevd_bundle.pydevd_collect_bytecode_info', '_pydevd_bundle.pydevd_api', '_pydevd_bundle.pydevd_json_debug_options', '_pydevd_bundle.pydevd_process_net_command_json', '_pydevd_bundle.pydevd_traceproperty', '_pydevd_bundle.pydevd_process_net_command', '_pydevd_bundle.pydevd_suspended_frames', '_pydevd_bundle.pydevd_trace_api', 'pydevd_plugins.django_debug', 'pydevd_plugins.jinja2_debug', '_pydevd_bundle.pydevd_plugin_utils', 'pydevd_plugins.extensions.types', 'pydevd_plugins.extensions.types.pydevd_helpers', 'pydevd_plugins.extensions.types.pydevd_plugin_numpy_types', 'pydevd_plugins.extensions.types.pydevd_plugins_django_form_str', 'pydevd', 'debugpy._vendored.force_pydevd', 'debugpy.server', 'debugpy.adapter', 'debugpy.common.timestamp', 'debugpy.common.util', 'debugpy.common.log', 'debugpy.common.sockets', 'debugpy.server.api', 'ipykernel.debugger', 'packaging.__about__', 'packaging', 'packaging._structures', 'packaging.version', 'ipykernel.eventloops', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.zmqshell', 'ipykernel.ipkernel', 'ipykernel.parentpoller', 'ipykernel.kernelapp', 'faulthandler', 'IPython.core.completerlib', 'storemagic', 'torch._utils', 'torch._utils_internal', 'torch.version', 'torch.torch_version', 'torch._six', 'torch._C._onnx', 'torch._C._jit', 'torch._C._jit_tree_views', 'torch._C._te', 'torch._C._nvfuser', 'torch._C._monitor', 'torch._C._functorch', 'torch._C._profiler', 'torch._C.cpp', 'torch._C.cpp.nn', 'torch._C._lazy', 'torch._C._lazy_ts_backend', 'torch._C._itt', 'torch._C._cudart', 'torch._C._nvtx', 'torch._C._cudnn', 'torch._C._verbose', 'torch._C', 'torch._C._fft', 'torch._C._linalg', 'torch._C._nested', 'torch._C._nn', 'torch._C._return_types', 'torch._C._sparse', 'torch._C._special', 'torch.utils.throughput_benchmark', 'torch.utils._crash_handler', 'torch.utils.cpp_backtrace', 'torch.utils', 'torch.utils.hooks', 'torch._namedtensor_internals', 'torch.overrides', 'torch._tensor', 'torch.types', 'numpy._globals', 'numpy.__config__', 'numpy._version', 'mkl._mklinit', 'mkl._py_mkl_service', 'mkl', 'numpy._distributor_init', 'numpy.version', 'numpy.core._multiarray_umath', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.compat', 'numpy.core.overrides', 'numpy.core.multiarray', 'numpy.core.umath', 'numpy.core._string_helpers', 'numpy.core._dtype', 'numpy.core._type_aliases', 'numpy.core.numerictypes', 'numpy.core._exceptions', 'numpy.core._methods', 'numpy.core.fromnumeric', 'numpy.core.shape_base', 'numpy.core._ufunc_config', 'numpy.core.arrayprint', 'numpy.core._asarray', 'numpy.core.numeric', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.einsumfunc', 'numpy.core._multiarray_tests', 'numpy.core._add_newdocs', 'numpy.core._add_newdocs_scalars', 'numpy.core._dtype_ctypes', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.core', 'numpy.lib.mixins', 'numpy.lib.ufunclike', 'numpy.lib.type_check', 'numpy.lib.scimath', 'numpy.lib.stride_tricks', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.linalg.linalg', 'numpy.linalg', 'numpy.matrixlib.defmatrix', 'numpy.matrixlib', 'numpy.lib.histograms', 'numpy.lib.function_base', 'numpy.lib.index_tricks', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.npyio', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.lib', 'numpy.fft._pocketfft_internal', 'numpy.fft._pocketfft', 'numpy.fft.helper', 'numpy.fft', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.polynomial', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.polynomial', 'numpy.random._common', 'secrets', 'numpy.random.bit_generator', 'numpy.random._bounded_integers', 'numpy.random._mt19937', 'numpy.random.mtrand', 'numpy.random._philox', 'numpy.random._pcg64', 'numpy.random._sfc64', 'numpy.random._generator', 'numpy.random._pickle', 'numpy.random', 'numpy.ctypeslib', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.ma', 'numpy', 'torch.storage', 'torch.random', 'tarfile', 'torch._sources', 'typing_extensions', 'torch._weights_only_unpickler', 'torch.serialization', 'torch._tensor_str', 'torch.amp.autocast_mode', 'torch.amp', 'torch.cuda._utils', 'torch.cuda.graphs', 'torch.cuda.streams', 'torch.cuda._memory_viz', 'torch.cuda.memory', 'torch.cuda.random', 'torch.cuda.sparse', 'torch.cuda.profiler', 'torch.cuda.nvtx', 'torch.cuda.amp.autocast_mode', 'torch.cuda.amp.common', 'torch.cuda.amp.grad_scaler', 'torch.cuda.amp', 'torch.cuda.jiterator', 'torch.cuda', 'torch.sparse', 'torch.backends', 'torch.backends.opt_einsum', 'torch.nn.parameter', 'torch.nn.modules.module', 'torch._VF', 'torch._torch_docs', 'torch._C._distributed_c10d', 'torch.distributed.constants', 'torch.distributed.rendezvous', 'torch.distributed.distributed_c10d', 'torch.distributed.remote_device', 'torch.distributed', 'torch._C._distributed_rpc', 'torch.futures', 'torch.distributed.rpc.internal', 'torch.distributed.rpc.constants', 'torch.distributed.rpc._utils', 'torch.distributed.rpc.api', 'torch.distributed.rpc.backend_registry', 'torch.distributed.rpc.functions', 'torch._C._distributed_autograd', 'torch.distributed.autograd', 'torch.distributed.rpc.options', 'torch.autograd.variable', 'torch.autograd.function', 'cmath', 'torch.testing._comparison', 'torch.testing._creation', 'torch.testing._legacy', 'torch.testing._deprecated', 'torch.testing', 'torch.utils._pytree', 'torch._vmap_internals', 'torch.autograd.gradcheck', 'torch.autograd.grad_mode', 'torch.autograd.anomaly_mode', 'torch.autograd.forward_ad', 'torch.autograd.functional', 'torch.autograd.graph', 'torch._C._autograd', 'torch.autograd.profiler_util', 'torch.autograd.profiler', 'torch.autograd', 'torch.autograd.profiler_legacy', 'torch.distributed.rpc.server_process_global_profiler', 'torch.distributed.rpc', 'pickletools', 'torch.package._digraph', 'torch.package._importlib', 'torch.package._mangling', 'torch.package.importer', 'torch.package._package_pickler', 'torch.package._stdlib', 'torch.package.find_file_dependencies', 'torch.package.glob_group', 'torch.package.package_exporter', 'torch.package.analyze.find_first_use_of_broken_modules', 'torch.package.analyze.trace_dependencies', 'torch.package.analyze', 'torch.package.analyze.is_from_package', 'torch.package.file_structure_representation', 'torch.package._directory_reader', 'torch.package._package_unpickler', 'torch.package.package_importer', 'torch.package', 'torch._jit_internal', 'torch.nn._reduction', 'torch.nn.modules.utils', 'torch.nn.grad', 'torch.nn.functional', 'torch.nn.init', 'torch.nn.modules.lazy', 'torch.nn.modules.linear', 'torch.nn.common_types', 'torch.nn.modules.conv', 'torch.nn.modules.activation', 'torch.nn.modules.distance', 'torch.nn.modules.loss', 'torch.nn.modules.container', 'torch.nn.modules.pooling', 'torch.nn.modules._functions', 'torch.nn.modules.batchnorm', 'torch.nn.modules.instancenorm', 'torch.nn.modules.normalization', 'torch.nn.modules.dropout', 'torch.nn.modules.padding', 'torch.nn.modules.sparse', 'torch.nn.utils.rnn', 'torch.nn.utils.clip_grad', 'torch.nn.utils.weight_norm', 'torch.nn.utils.convert_parameters', 'torch.nn.utils.spectral_norm', 'torch.nn.utils.fusion', 'torch.nn.utils.memory_format', 'torch.nn.utils.parametrize', 'torch.nn.utils.parametrizations', 'torch.nn.utils.init', 'torch.nn.utils.stateless', 'torch.nn.utils', 'torch.nn.modules.rnn', 'torch.nn.modules.pixelshuffle', 'torch.nn.modules.upsampling', 'torch.nn.modules.fold', 'torch.nn.modules.adaptive', 'torch.nn.modules.transformer', 'torch.nn.modules.flatten', 'torch.nn.modules.channelshuffle', 'torch.nn.modules', 'torch.nn.parallel.parallel_apply', 'torch.cuda.nccl', 'torch.nn.parallel.comm', 'torch.nn.parallel.replicate', 'torch.nn.parallel._functions', 'torch.nn.parallel.scatter_gather', 'torch.nn.parallel.data_parallel', 'torch.distributed.algorithms.join', 'torch.distributed.algorithms', 'torch.distributed.utils', 'torch.nn.parallel._replicated_tensor_ddp_utils', 'torch.nn.parallel.distributed', 'torch.nn.parallel', 'torch.nn', 'torch._linalg_utils', 'torch._lowrank', 'torch.functional', 'torch.cpu.amp.autocast_mode', 'torch.cpu.amp', 'torch.cpu', 'torch.fft', 'torch.nested', 'torch.optim.optimizer', 'torch.optim.adadelta', 'torch.optim.adagrad', 'torch.optim.adam', 'torch.optim.adamw', 'torch.optim.adamax', 'torch.optim.asgd', 'torch.optim.nadam', 'torch.optim.radam', 'torch.optim.rmsprop', 'torch.optim.rprop', 'torch.optim.sgd', 'torch.optim._functional', 'torch.optim.sparse_adam', 'torch.optim.lbfgs', 'torch.optim.lr_scheduler', 'torch.optim.swa_utils', 'torch.optim', 'torch.optim._multi_tensor', 'multiprocessing.process', 'multiprocessing.reduction', 'multiprocessing.context', '__mp_main__', 'multiprocessing', 'multiprocessing.util', 'multiprocessing.resource_sharer', 'torch.multiprocessing.reductions', '_multiprocessing', 'multiprocessing.connection', 'torch.multiprocessing.spawn', 'torch.multiprocessing', 'torch.special', 'torch.utils.backcompat', 'torch.onnx._deprecation', 'torch.onnx._constants', 'torch.onnx._internal', 'torch.onnx._internal.diagnostics.infra.sarif._property_bag', 'torch.onnx._internal.diagnostics.infra.sarif._address', 'torch.onnx._internal.diagnostics.infra.sarif._multiformat_message_string', 'torch.onnx._internal.diagnostics.infra.sarif._artifact_content', 'torch.onnx._internal.diagnostics.infra.sarif._message', 'torch.onnx._internal.diagnostics.infra.sarif._artifact_location', 'torch.onnx._internal.diagnostics.infra.sarif._artifact', 'torch.onnx._internal.diagnostics.infra.sarif._region', 'torch.onnx._internal.diagnostics.infra.sarif._replacement', 'torch.onnx._internal.diagnostics.infra.sarif._artifact_change', 'torch.onnx._internal.diagnostics.infra.sarif._rectangle', 'torch.onnx._internal.diagnostics.infra.sarif._attachment', 'torch.onnx._internal.diagnostics.infra.sarif._location_relationship', 'torch.onnx._internal.diagnostics.infra.sarif._logical_location', 'torch.onnx._internal.diagnostics.infra.sarif._physical_location', 'torch.onnx._internal.diagnostics.infra.sarif._location', 'torch.onnx._internal.diagnostics.infra.sarif._tool_component_reference', 'torch.onnx._internal.diagnostics.infra.sarif._reporting_descriptor_reference', 'torch.onnx._internal.diagnostics.infra.sarif._stack_frame', 'torch.onnx._internal.diagnostics.infra.sarif._stack', 'torch.onnx._internal.diagnostics.infra.sarif._web_request', 'torch.onnx._internal.diagnostics.infra.sarif._web_response', 'torch.onnx._internal.diagnostics.infra.sarif._thread_flow_location', 'torch.onnx._internal.diagnostics.infra.sarif._thread_flow', 'torch.onnx._internal.diagnostics.infra.sarif._code_flow', 'torch.onnx._internal.diagnostics.infra.sarif._reporting_configuration', 'torch.onnx._internal.diagnostics.infra.sarif._configuration_override', 'torch.onnx._internal.diagnostics.infra.sarif._exception', 'torch.onnx._internal.diagnostics.infra.sarif._notification', 'torch.onnx._internal.diagnostics.infra.sarif._invocation', 'torch.onnx._internal.diagnostics.infra.sarif._reporting_descriptor_relationship', 'torch.onnx._internal.diagnostics.infra.sarif._reporting_descriptor', 'torch.onnx._internal.diagnostics.infra.sarif._translation_metadata', 'torch.onnx._internal.diagnostics.infra.sarif._tool_component', 'torch.onnx._internal.diagnostics.infra.sarif._tool', 'torch.onnx._internal.diagnostics.infra.sarif._conversion', 'torch.onnx._internal.diagnostics.infra.sarif._edge', 'torch.onnx._internal.diagnostics.infra.sarif._edge_traversal', 'torch.onnx._internal.diagnostics.infra.sarif._node', 'torch.onnx._internal.diagnostics.infra.sarif._graph', 'torch.onnx._internal.diagnostics.infra.sarif._fix', 'torch.onnx._internal.diagnostics.infra.sarif._graph_traversal', 'torch.onnx._internal.diagnostics.infra.sarif._result_provenance', 'torch.onnx._internal.diagnostics.infra.sarif._suppression', 'torch.onnx._internal.diagnostics.infra.sarif._result', 'torch.onnx._internal.diagnostics.infra.sarif._external_properties', 'torch.onnx._internal.diagnostics.infra.sarif._external_property_file_reference', 'torch.onnx._internal.diagnostics.infra.sarif._external_property_file_references', 'torch.onnx._internal.diagnostics.infra.sarif._run_automation_details', 'torch.onnx._internal.diagnostics.infra.sarif._special_locations', 'torch.onnx._internal.diagnostics.infra.sarif._version_control_details', 'torch.onnx._internal.diagnostics.infra.sarif._run', 'torch.onnx._internal.diagnostics.infra.sarif._sarif_log', 'torch.onnx._internal.diagnostics.infra.sarif', 'torch.onnx._internal.diagnostics.infra.formatter', 'torch.onnx._internal.diagnostics.infra._infra', 'torch.onnx._internal.diagnostics.infra.sarif.version', 'torch.onnx._internal.diagnostics.infra.engine', 'torch.onnx._internal.diagnostics.infra', 'torch.onnx._internal.diagnostics._rules', 'torch.onnx._internal.diagnostics._diagnostic', 'torch.onnx._internal.diagnostics', 'torch.onnx.errors', 'torch.jit._dataclass_impls', 'torch.jit._monkeytype_config', 'torch.jit._state', 'torch._ops', 'torch.jit.annotations', 'torch.jit.frontend', 'torch.backends.cudnn', 'torch.jit._builtins', 'torch.jit._check', 'torch.jit._recursive', 'torch.jit._fuser', 'torch.jit._serialization', 'torch._classes', 'torch.jit._script', 'torch.jit._trace', 'torch.jit._async', 'torch.jit._decomposition_utils', 'torch.jit._freeze', 'torch.jit._ir_utils', 'torch.jit', 'torch.onnx._exporter_states', 'torch.onnx._globals', 'torch.onnx._internal._beartype', 'torch.onnx._internal.jit_utils', 'torch.onnx._internal.registration', 'torch.onnx.utils', 'torch.onnx._patch_torch', 'torch.onnx._type_utils', 'torch.onnx.symbolic_helper', 'torch.onnx.symbolic_opset9', 'torch.onnx.symbolic_caffe2', 'torch.onnx.symbolic_opset7', 'torch.onnx.symbolic_opset8', 'torch.onnx.symbolic_opset10', 'torch.onnx.symbolic_opset11', 'torch.onnx.symbolic_opset12', 'torch.onnx.symbolic_opset13', 'torch.onnx.symbolic_opset14', 'torch.onnx.symbolic_opset15', 'torch.onnx.symbolic_opset16', 'torch.onnx.symbolic_opset17', 'torch.onnx', 'torch.linalg', 'tqdm._monitor', 'tqdm._tqdm_pandas', 'tqdm.utils', 'tqdm.std', 'tqdm._dist_ver', 'tqdm.version', 'tqdm.cli', 'tqdm.gui', 'tqdm', 'ipywidgets._version', 'ipython_genutils._version', 'ipython_genutils', 'ipython_genutils.encoding', 'ipython_genutils.py3compat', 'ipywidgets.widgets.widget', 'ipywidgets.widgets.util', 'ipywidgets.widgets.trait_types', 'ipywidgets.widgets.widget_layout', 'ipywidgets.widgets.widget_style', 'ipywidgets.widgets.domwidget', 'ipywidgets.widgets.valuewidget', 'ipywidgets.widgets.widget_core', 'ipywidgets.widgets.widget_description', 'ipywidgets.widgets.widget_bool', 'ipywidgets.widgets.widget_button', 'ipywidgets.widgets.docutils', 'ipywidgets.widgets.widget_box', 'ipywidgets.widgets.widget_int', 'ipywidgets.widgets.widget_float', 'ipywidgets.widgets.widget_color', 'ipywidgets.widgets.widget_date', 'ipywidgets.widgets.widget_output', 'ipywidgets.widgets.widget_selection', 'ipywidgets.widgets.widget_selectioncontainer', 'ipywidgets.widgets.widget_string', 'ipywidgets.widgets.widget_controller', 'ipywidgets.widgets.interaction', 'ipywidgets.widgets.widget_link', 'ipywidgets.widgets.widget_media', 'ipywidgets.widgets.widget_templates', 'ipywidgets.widgets.widget_upload', 'ipywidgets.widgets', 'ipywidgets', 'tqdm.notebook', 'tqdm.autonotebook', 'tqdm.asyncio', 'tqdm.auto', 'torch.hub', 'torch.distributions.constraints', 'torch.distributions.utils', 'torch.distributions.distribution', 'torch.distributions.exp_family', 'torch.distributions.bernoulli', 'torch.distributions.dirichlet', 'torch.distributions.beta', 'torch.distributions.binomial', 'torch.distributions.categorical', 'torch.distributions.cauchy', 'torch.distributions.gamma', 'torch.distributions.chi2', 'torch.distributions.transforms', 'torch.distributions.constraint_registry', 'torch.distributions.continuous_bernoulli', 'torch.distributions.exponential', 'torch.distributions.fishersnedecor', 'torch.distributions.geometric', 'torch.distributions.uniform', 'torch.distributions.independent', 'torch.distributions.transformed_distribution', 'torch.distributions.gumbel', 'torch.distributions.half_cauchy', 'torch.distributions.normal', 'torch.distributions.half_normal', 'torch.distributions.laplace', 'torch.distributions.multivariate_normal', 'torch.distributions.lowrank_multivariate_normal', 'torch.distributions.one_hot_categorical', 'torch.distributions.pareto', 'torch.distributions.poisson', 'torch.distributions.kl', 'torch.distributions.kumaraswamy', 'torch.distributions.lkj_cholesky', 'torch.distributions.log_normal', 'torch.distributions.logistic_normal', 'torch.distributions.mixture_same_family', 'torch.distributions.multinomial', 'torch.distributions.negative_binomial', 'torch.distributions.relaxed_bernoulli', 'torch.distributions.relaxed_categorical', 'torch.distributions.studentT', 'torch.distributions.von_mises', 'torch.distributions.weibull', 'torch.distributions.wishart', 'torch.distributions', 'torch.backends.cuda', 'torch.backends.mps', 'torch.backends.mkl', 'torch.backends.mkldnn', 'torch.backends.openmp', 'torch.backends.quantized', 'torch.utils.data.sampler', 'torch.utils.data.dataset', 'torch.utils.data.datapipes._hook_iterator', 'torch.utils.data.datapipes._typing', 'torch.utils.data.datapipes.utils', 'torch.utils.data._utils.signal_handling', 'torch.utils.data._utils.worker', 'torch.utils.data._utils.pin_memory', 'torch.utils.data._utils.collate', 'torch.utils.data._utils.fetch', 'torch.utils.data._utils', 'dill.info', '_pyio', 'dill.settings', 'dill._dill', 'dill.source', 'dill.temp', 'dill.pointers', 'dill.detect', 'dill.objtypes', 'dill', 'torch.utils.data._utils.serialization', 'torch.utils.data.datapipes.utils.common', 'torch.utils.data.datapipes.datapipe', 'torch.utils.data.datapipes.iter.utils', 'torch.utils.data.datapipes._decorator', 'torch.utils.data.datapipes.dataframe.dataframe_wrapper', 'torch.utils.data.datapipes.dataframe.structures', 'torch.utils.data.datapipes.dataframe.dataframes', 'torch.utils.data.datapipes.dataframe.datapipes', 'torch.utils.data.datapipes.dataframe', 'torch.utils.data.datapipes.iter.callable', 'torch.utils.data.datapipes.iter.combinatorics', 'torch.utils.data.datapipes.iter.combining', 'torch.utils.data.datapipes.iter.filelister', 'torch.utils.data.datapipes.iter.fileopener', 'torch.utils.data.datapipes.iter.grouping', 'torch.utils.data.datapipes.utils.decoder', 'torch.utils.data.datapipes.iter.routeddecoder', 'torch.utils.data.datapipes.iter.selecting', 'torch.utils.data.datapipes.iter.streamreader', 'torch.utils.data.datapipes.iter', 'torch.utils.data.datapipes.map.callable', 'torch.utils.data.datapipes.map.combinatorics', 'torch.utils.data.datapipes.map.combining', 'torch.utils.data.datapipes.map.grouping', 'torch.utils.data.datapipes.map.utils', 'torch.utils.data.datapipes.map', 'torch.utils.data.datapipes', 'torch.utils.data.graph', 'torch.utils.data.graph_settings', 'torch.utils.data.dataloader', 'torch.utils.data.distributed', 'torch.utils.data.backward_compatibility', 'torch.utils.data.communication.eventloop', 'torch.utils.data.communication.iter', 'torch.utils.data.communication.map', 'torch.utils.data.communication.messages', 'torch.utils.data.communication.protocol', 'torch.utils.data.communication.queue', 'torch.utils.data.communication', 'torch.utils.data.dataloader_experimental', 'torch.utils.data', 'torch.__config__', 'torch.__future__', 'torch.profiler.profiler', 'torch.profiler.itt', 'torch.profiler', 'torch.ao', 'torch.ao.nn', 'torch.ao.nn.intrinsic.modules.fused', 'torch.ao.nn.intrinsic.modules', 'torch.ao.nn.intrinsic', 'torch.nn.intrinsic.modules.fused', 'torch.nn.intrinsic.modules', 'torch.nn.intrinsic', 'torch.ao.nn.quantizable.modules.activation', 'torch.ao.nn.quantizable.modules.rnn', 'torch.ao.nn.quantizable.modules', 'torch.ao.nn.quantizable', 'torch.nn.quantizable.modules', 'torch.nn.quantizable', 'torch.ao.nn.quantized.modules.activation', 'torch.ao.nn.quantized.modules.dropout', 'torch.ao.nn.quantized.modules.batchnorm', 'torch.ao.nn.quantized.modules.normalization', 'torch.ao.nn.qat.modules.linear', 'torch.ao.nn.qat.modules.conv', 'torch.ao.nn.qat.modules.embedding_ops', 'torch.ao.nn.qat.modules', 'torch.ao.nn.qat', 'torch.nn.intrinsic.qat.modules.linear_relu', 'torch.nn.intrinsic.qat.modules.linear_fused', 'torch.nn.intrinsic.qat.modules.conv_fused', 'torch.nn.intrinsic.qat.modules', 'torch.nn.intrinsic.qat', 'torch.ao.nn.quantized.modules.utils', 'torch.ao.nn.quantized.modules.conv', 'torch.ao.nn.quantized.modules.linear', 'torch.ao.nn.quantized.modules.embedding_ops', 'torch.ao.nn.quantized.modules.rnn', 'torch.ao.nn.quantized.modules.functional_modules', 'torch.ao.nn.quantized.modules', 'torch.ao.nn.quantized.functional', 'torch.ao.nn.quantized', 'torch.ao.nn.quantized.dynamic.modules.linear', 'torch.ao.nn.quantized.dynamic.modules.rnn', 'torch.ao.nn.quantized.dynamic.modules.conv', 'torch.ao.nn.quantized.dynamic.modules', 'torch.ao.nn.quantized.dynamic', 'torch.nn.quantized.dynamic', 'torch.nn.quantized.functional', 'torch.nn.quantized.modules', 'torch.nn.quantized', 'torch.ao.nn.qat.dynamic.modules.linear', 'torch.ao.nn.qat.dynamic.modules', 'torch.ao.nn.qat.dynamic', 'torch.nn.qat.dynamic.modules.linear', 'torch.nn.qat.dynamic.modules', 'torch.nn.qat.dynamic', 'torch.nn.qat.modules.conv', 'torch.nn.qat.modules.embedding_ops', 'torch.nn.qat.modules.linear', 'torch.nn.qat.modules', 'torch.nn.qat', 'torch._tensor_docs', 'torch._storage_docs', 'torch.ao.quantization.quant_type', 'torch.ao.quantization.utils', 'torch.ao.quantization.observer', 'torch.ao.quantization.fake_quantize', 'torch.ao.quantization.fuser_method_mappings', 'torch.ao.quantization.fuse_modules', 'torch.ao.quantization.qconfig', 'torch.ao.quantization.qconfig_mapping', 'torch.nn.intrinsic.quantized.modules.linear_relu', 'torch.nn.intrinsic.quantized.modules.conv_relu', 'torch.nn.intrinsic.quantized.modules.bn_relu', 'torch.nn.intrinsic.quantized.modules', 'torch.nn.intrinsic.quantized', 'torch.nn.intrinsic.quantized.dynamic.modules.linear_relu', 'torch.nn.intrinsic.quantized.dynamic.modules', 'torch.nn.intrinsic.quantized.dynamic', 'torch.ao.nn.quantized.reference.modules.utils', 'torch.ao.nn.quantized.reference.modules.linear', 'torch.ao.nn.quantized.reference.modules.conv', 'torch.ao.nn.quantized.reference.modules.rnn', 'torch.ao.nn.quantized.reference.modules.sparse', 'torch.ao.nn.quantized.reference.modules', 'torch.ao.nn.quantized.reference', 'torch.ao.nn.sparse.quantized.linear', 'torch.ao.nn.sparse.quantized.utils', 'torch.ao.nn.sparse.quantized.dynamic.linear', 'torch.ao.nn.sparse.quantized.dynamic', 'torch.ao.nn.sparse.quantized', 'torch.ao.nn.sparse', 'torch.ao.quantization.stubs', 'torch.ao.quantization.quantization_mappings', 'torch.ao.quantization.quantize', 'torch.ao.quantization.quantize_jit', 'torch.ao.quantization', 'torch.quantization.quantize', 'torch.quantization.observer', 'torch.quantization.qconfig', 'torch.quantization.fake_quantize', 'torch.quantization.fuser_method_mappings', 'torch.quantization.fuse_modules', 'torch.quantization.stubs', 'torch.quantization.quant_type', 'torch.quantization.quantize_jit', 'torch.quantization.quantization_mappings', 'torch.quantization', 'torch.quasirandom', 'torch.multiprocessing._atfork', 'torch._lobpcg', 'torch.utils.dlpack', 'torch.masked.maskedtensor.core', 'torch.masked.maskedtensor.binary', 'torch.masked.maskedtensor.passthrough', 'torch.masked.maskedtensor.creation', 'torch.masked.maskedtensor.reductions', 'torch.masked.maskedtensor.unary', 'torch.masked.maskedtensor', 'torch.masked._docs', 'torch.masked._ops', 'torch.masked', 'torch.return_types', 'torch.library', 'torch._prims_common', 'torch._prims_common.wrappers', 'torch._prims.nvfuser_prims', 'torch.utils._mode_utils', 'torch._subclasses.meta_utils', 'torch.fx._compatibility', 'torch.fx.immutable_collections', 'torch.fx.operator_schemas', 'torch.fx.node', 'torch.fx._pytree', 'torch.fx.graph', 'torch.fx.graph_module', 'torch.fx.traceback', 'torch.fx.proxy', 'torch.fx._symbolic_trace', 'torch.fx.interpreter', 'torch.fx.subgraph_rewriter', 'torch.fx', 'torch.utils._python_dispatch', 'torch._subclasses.fake_tensor', 'torch._subclasses.fake_utils', 'torch._subclasses', 'torch._prims', 'torch._decomp.decompositions', 'torch._decomp', 'torch._refs.fft', 'torch._refs.linalg', 'torch._refs.nn', 'torch._refs.nn.functional', 'torch._refs.special', 'torch._refs', 'torch._meta_registrations', 'torch', 'torchbeast', 'torchbeast.core', 'torchbeast.core.environment', 'gym.error', 'gym.version', 'gym.utils.colorize', 'gym.utils.ezpickle', 'gym.utils', 'gym.utils.closer', 'gym.core', 'gym.utils.seeding', 'gym.spaces.space', 'gym.logger', 'gym.spaces.box', 'gym.spaces.discrete', 'gym.spaces.multi_discrete', 'gym.spaces.multi_binary', 'gym.spaces.tuple', 'gym.spaces.dict', 'gym.spaces.utils', 'gym.spaces', '_csv', 'csv', 'importlib.metadata', 'gym.envs.registration', 'ale_py._ale_py', 'ale_py', 'zipp', 'importlib_resources._compat', 'importlib_resources.abc', 'importlib_resources._common', 'importlib_resources._legacy', 'importlib_resources', 'importlib_metadata._functools', 'importlib_metadata._text', 'importlib_metadata._adapters', 'importlib_metadata._compat', 'importlib_metadata._meta', 'importlib_metadata._collections', 'importlib_metadata._itertools', 'importlib_metadata', 'ale_py.roms.utils', 'importlib_resources._adapters', 'importlib_resources._itertools', 'importlib_resources.readers', 'urllib3.packages', 'urllib3.packages.six', 'urllib3.packages.six.moves', 'urllib3.packages.six.moves.http_client', 'urllib3.exceptions', 'urllib3._version', 'urllib3.contrib', 'urllib3.contrib._appengine_environ', 'urllib3.util.wait', 'urllib3.util.connection', '_cffi_backend', '_brotli.lib', '_brotli', 'brotli._brotli', 'brotli.brotli', 'brotli', 'urllib3.util.request', 'urllib3.util.response', 'urllib3.util.retry', 'urllib3.util.url', 'urllib3.util.ssltransport', 'urllib3.util.ssl_', 'urllib3.util.timeout', 'urllib3.util', 'urllib3.util.proxy', 'urllib3._collections', 'ipaddress', 'urllib3.util.ssl_match_hostname', 'urllib3.connection', 'urllib3.fields', 'urllib3.filepost', 'urllib3.packages.six.moves.urllib', 'urllib3.packages.six.moves.urllib.parse', 'urllib3.request', 'urllib3.response', 'urllib3.util.queue', 'urllib3.connectionpool', 'urllib3.poolmanager', 'urllib3', 'chardet.enums', 'chardet.charsetprober', 'chardet.charsetgroupprober', 'chardet.codingstatemachine', 'chardet.escsm', 'chardet.escprober', 'chardet.latin1prober', 'chardet.mbcssm', 'chardet.utf8prober', 'chardet.mbcharsetprober', 'chardet.euctwfreq', 'chardet.euckrfreq', 'chardet.gb2312freq', 'chardet.big5freq', 'chardet.jisfreq', 'chardet.chardistribution', 'chardet.jpcntx', 'chardet.sjisprober', 'chardet.eucjpprober', 'chardet.gb2312prober', 'chardet.euckrprober', 'chardet.cp949prober', 'chardet.big5prober', 'chardet.euctwprober', 'chardet.mbcsgroupprober', 'chardet.hebrewprober', 'chardet.sbcharsetprober', 'chardet.langbulgarianmodel', 'chardet.langgreekmodel', 'chardet.langhebrewmodel', 'chardet.langrussianmodel', 'chardet.langthaimodel', 'chardet.langturkishmodel', 'chardet.sbcsgroupprober', 'chardet.universaldetector', 'chardet.version', 'chardet', 'http.cookiejar', 'http.cookies', 'requests.compat', 'requests.exceptions', 'charset_normalizer.constant', '_multibytecodec', 'charset_normalizer.utils', 'charset_normalizer.md', 'charset_normalizer.models', 'charset_normalizer.assets', 'charset_normalizer.cd', 'charset_normalizer.api', 'charset_normalizer.legacy', 'charset_normalizer.version', 'charset_normalizer', 'requests.packages.urllib3.packages', 'requests.packages.urllib3.packages.six', 'requests.packages.urllib3.packages.six.moves', 'requests.packages.urllib3.packages.six.moves.http_client', 'requests.packages.urllib3.exceptions', 'requests.packages.urllib3._version', 'requests.packages.urllib3.contrib', 'requests.packages.urllib3.contrib._appengine_environ', 'requests.packages.urllib3.util.wait', 'requests.packages.urllib3.util.connection', 'requests.packages.urllib3.util.request', 'requests.packages.urllib3.util.response', 'requests.packages.urllib3.util.retry', 'requests.packages.urllib3.util.url', 'requests.packages.urllib3.util.ssltransport', 'requests.packages.urllib3.util.ssl_', 'requests.packages.urllib3.util.timeout', 'requests.packages.urllib3.util', 'requests.packages.urllib3.util.proxy', 'requests.packages.urllib3._collections', 'requests.packages.urllib3.util.ssl_match_hostname', 'requests.packages.urllib3.connection', 'requests.packages.urllib3.fields', 'requests.packages.urllib3.filepost', 'requests.packages.urllib3.packages.six.moves.urllib', 'requests.packages.urllib3.packages.six.moves.urllib.parse', 'requests.packages.urllib3.request', 'requests.packages.urllib3.response', 'requests.packages.urllib3.util.queue', 'requests.packages.urllib3.connectionpool', 'requests.packages.urllib3.poolmanager', 'requests.packages.urllib3', 'idna.package_data', 'idna.idnadata', 'idna.intranges', 'idna.core', 'idna', 'requests.packages.idna.package_data', 'requests.packages.idna.idnadata', 'requests.packages.idna.intranges', 'requests.packages.idna.core', 'requests.packages.idna', 'requests.packages.chardet', 'requests.packages', 'certifi.core', 'certifi', 'requests.certs', 'requests.__version__', 'requests._internal_utils', 'requests.cookies', 'requests.structures', 'requests.utils', 'requests.auth', 'requests.hooks', 'requests.status_codes', 'requests.models', 'socks', 'urllib3.contrib.socks', 'requests.adapters', 'requests.sessions', 'requests.api', 'requests', 'click._compat', 'click.globals', 'click.utils', 'click.exceptions', 'click.types', 'click._unicodefun', 'click.parser', 'click.formatting', 'click.termui', 'click.core', 'click.decorators', 'click', 'AutoROM.AutoROM', 'AutoROM', 'AutoROM.roms', 'ale_py.roms', 'ale_py.gym', 'gym.envs', 'gym.vector.utils.misc', 'gym.vector.utils.spaces', 'gym.vector.utils.numpy_utils', 'gym.vector.utils.shared_memory', 'gym.vector.utils', 'gym.vector.vector_env', 'gym.vector.async_vector_env', 'gym.vector.sync_vector_env', 'gym.vector', 'gym.wrappers.monitoring', 'gym.utils.atomic_write', 'gym.utils.json_utils', 'gym.wrappers.monitoring.stats_recorder', 'setuptools._distutils', 'distutils.debug', 'distutils.errors', 'distutils.fancy_getopt', 'distutils.dep_util', 'distutils.log', 'distutils.spawn', 'distutils.util', 'distutils.dist', 'distutils.dir_util', 'distutils.file_util', 'distutils.archive_util', 'distutils.cmd', 'distutils.config', 'distutils.extension', 'distutils.core', '_distutils_hack.override', 'setuptools._deprecation_warning', 'setuptools.version', 'distutils.filelist', 'setuptools.monkey', 'setuptools.extension', 'distutils.command', 'setuptools.extern', 'setuptools._vendor', 'setuptools._vendor.packaging.__about__', 'setuptools._vendor.packaging', 'setuptools.extern.packaging', 'setuptools._vendor.ordered_set', 'setuptools.extern.ordered_set', 'setuptools._vendor.more_itertools.recipes', 'setuptools._vendor.more_itertools.more', 'setuptools._vendor.more_itertools', 'setuptools.extern.more_itertools', 'setuptools._vendor.zipp', 'setuptools._vendor.importlib_metadata._functools', 'setuptools._vendor.importlib_metadata._text', 'setuptools._vendor.importlib_metadata._adapters', 'setuptools._vendor.importlib_metadata._compat', 'setuptools._vendor.importlib_metadata._meta', 'setuptools._vendor.importlib_metadata._collections', 'setuptools._vendor.importlib_metadata._itertools', 'setuptools._vendor.importlib_metadata', 'setuptools.extern.importlib_metadata', 'setuptools._importlib', 'distutils.command.bdist', 'setuptools.command', 'setuptools.windows_support', 'setuptools.extern.packaging._structures', 'setuptools.extern.packaging.version', 'setuptools._vendor.packaging._manylinux', 'setuptools._vendor.packaging._musllinux', 'setuptools.extern.packaging.tags', 'setuptools.extern.packaging.utils', 'setuptools.extern.packaging.specifiers', 'setuptools.config.expand', 'setuptools.config.setupcfg', 'setuptools.config', 'setuptools.errors', 'email._header_value_parser', 'email.headerregistry', 'setuptools.config._apply_pyprojecttoml', 'setuptools.config.pyprojecttoml', 'setuptools.discovery', 'setuptools._vendor.jaraco', 'setuptools.extern.jaraco', 'setuptools.extern.jaraco.functools', 'setuptools.extern.jaraco.context', 'setuptools.extern.jaraco.text', 'setuptools._reqs', 'setuptools._itertools', 'setuptools._entry_points', 'setuptools.dist', 'setuptools.py34compat', 'setuptools._imp', 'setuptools.depends', 'setuptools.logging', 'distutils.ccompiler', 'setuptools.msvc', 'setuptools', 'distutils', 'distutils.version', 'gym.wrappers.monitoring.video_recorder', 'gym.wrappers.monitor', 'gym.wrappers.time_limit', 'gym.wrappers.filter_observation', 'cv2.load_config_py3', 'cv2.version', 'cv2.Error', 'cv2.cuda', 'cv2.detail', 'cv2.dnn', 'cv2.fisheye', 'cv2.flann', 'cv2.gapi.core', 'cv2.gapi.core.cpu', 'cv2.gapi.core.fluid', 'cv2.gapi.core.ocl', 'cv2.gapi.ie', 'cv2.gapi.ie.detail', 'cv2.gapi.oak', 'cv2.gapi.onnx', 'cv2.gapi.own', 'cv2.gapi.own.detail', 'cv2.gapi.render', 'cv2.gapi.render.ocv', 'cv2.gapi.streaming', 'cv2.gapi.video', 'cv2.gapi.wip', 'cv2.gapi.wip.draw', 'cv2.gapi.wip.gst', 'cv2.gapi.wip.onevpl', 'cv2.ipp', 'cv2.ml', 'cv2.ocl', 'cv2.ogl', 'cv2.parallel', 'cv2.samples', 'cv2.segmentation', 'cv2.utils.fs', 'cv2.utils.nested', 'cv2.videoio_registry', 'cv2.qt', 'cv2.misc.version', 'cv2.misc', 'cv2.gapi', 'cv2.data', 'cv2.utils', 'cv2.mat_wrapper', 'cv2', 'gym.wrappers.atari_preprocessing', 'gym.wrappers.time_aware_observation', 'gym.wrappers.rescale_action', 'gym.wrappers.flatten_observation', 'gym.wrappers.gray_scale_observation', 'gym.wrappers.frame_stack', 'gym.wrappers.transform_observation', 'gym.wrappers.transform_reward', 'gym.wrappers.resize_observation', 'gym.wrappers.clip_action', 'gym.wrappers.record_episode_statistics', 'gym.wrappers.normalize', 'gym.wrappers.record_video', 'gym.wrappers', 'gym', 'torchbeast.atari_wrappers', 'torchbeast.core.file_writer', 'torchbeast.core.prof', 'torchbeast.core.vtrace', 'torchbeast.transformer_rnn', 'torchbeast.resnet', 'gym_sokoban', 'torchbeast.train', 'torchbeast.base', 'matplotlib', 'matplotlib._api.deprecation', 'matplotlib._api', 'matplotlib._version', 'matplotlib._c_internal_utils', 'matplotlib.cbook', 'matplotlib.docstring', 'PIL._version', 'PIL', 'defusedxml.common', 'defusedxml', 'xml.etree', 'xml.etree.ElementPath', '_elementtree', 'xml.etree.ElementTree', 'defusedxml.ElementTree', 'PIL.ImageMode', 'PIL.TiffTags', 'PIL._binary', 'PIL._deprecate', 'PIL._util', 'PIL._imaging', 'cffi.lock', 'cffi.error', 'cffi.model', 'cffi.api', 'cffi', 'PIL.Image', 'PIL.ImageChops', 'PIL.ImageFile', 'PIL.GimpGradientFile', 'PIL.GimpPaletteFile', 'PIL.ImageColor', 'PIL.PaletteFile', 'PIL.ImagePalette', 'PIL.ImageSequence', 'PIL.PngImagePlugin', 'matplotlib._path', 'matplotlib.bezier', 'matplotlib.path', 'matplotlib.transforms', 'matplotlib.ticker', 'matplotlib.scale', 'matplotlib._color_data', 'matplotlib.colors', 'pyparsing.util', 'pyparsing.unicode', 'pyparsing.exceptions', 'pyparsing.actions', 'pyparsing.results', 'pyparsing.core', 'pyparsing.helpers', 'pyparsing.testing', 'pyparsing.common', 'pyparsing', 'matplotlib.fontconfig_pattern', 'matplotlib._enums', 'cycler', 'matplotlib.rcsetup', 'matplotlib.ft2font', 'kiwisolver._cext', 'kiwisolver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG:285693 __init__:275 2022-11-06 01:39:37,699] CACHEDIR=/home/sc/.cache/matplotlib\n",
      "[DEBUG:285693 font_manager:1439 2022-11-06 01:39:37,700] Using fontManager instance from /home/sc/.cache/matplotlib/fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core.environment import Environment, Vec_Environment\n",
    "from torchbeast.atari_wrappers import SokobanWrapper\n",
    "from torchbeast.base import BaseNet\n",
    "from torchbeast.train import create_env\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "def get_param(net, name=None):\n",
    "    keys = []\n",
    "    for (k, v) in actor_wrapper.model.named_parameters(): \n",
    "        if name is None:\n",
    "            print(k)\n",
    "        else:\n",
    "            if name == k: return v\n",
    "        keys.append(k)\n",
    "    return keys        \n",
    "\n",
    "def n_step_greedy(env, net, n, temp=10.):    \n",
    "    if isinstance(env, Vec_Environment):\n",
    "        num_actions = env.gym_env.action_space[0].n\n",
    "        bsz = len(env.gym_env.envs)\n",
    "    else:\n",
    "        num_actions = env.gym_env.action_space.n\n",
    "        bsz = 1\n",
    "\n",
    "    q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "    state = env.clone_state()\n",
    "\n",
    "    for act in range(num_actions):\n",
    "        obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "        \n",
    "        if n > 1:\n",
    "            action, prob, sub_q_ret = n_step_greedy(env, net, n-1)\n",
    "            ret = obs['reward'] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] * (~obs['done']).float()\n",
    "        else:\n",
    "            ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "\n",
    "        q_ret[:, act] = ret\n",
    "        env.restore_state(state)\n",
    "    \n",
    "    prob = F.softmax(temp*q_ret, dim=1)\n",
    "    action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893df32e",
   "metadata": {},
   "source": [
    "<font size=\"5\">Testing planning algo. for perfect model with bootstrapped values</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bf8c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Synchronous version of testing \n",
    "\n",
    "def test_n_step(n, net, env, temp=10.):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    returns = []\n",
    "    obs = env.initial()\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}\n",
    "        net_out, core_state = net(obs)            \n",
    "        if n == 0:\n",
    "            action = net_out[\"action\"][0]\n",
    "        else:\n",
    "            action, _, _ = n_step_greedy(env, net, n, temp)\n",
    "        obs = env.step(action)\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns\n",
    "\n",
    "bsz = 16    \n",
    "eps_n = 500\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# create environments\n",
    "\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "num_actions = env.gym_env.action_space[0].n\n",
    "\n",
    "# import the net\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "flags.discounting = 0.97\n",
    "temp = 5\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=num_actions, flags=flags)  \n",
    "net = net.to(device)\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_2.tar\", map_location=\"cuda\")\n",
    "#checkpoint = torch.load(\"/home/schk/RS/thinker/logs/base/torchbeast-20221105-033530/model.tar\", map_location=\"cuda\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "# initialize net\n",
    "\n",
    "core_state = net.initial_state(batch_size=bsz)\n",
    "core_state = tuple(v.to(device) for v in core_state)\n",
    "net.train(False)\n",
    "\n",
    "all_returns = {}\n",
    "for n in range(2,3):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step(n, net, env, temp)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ffb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous version of testing \n",
    "\n",
    "def act_m(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    returns: Manager().list,\n",
    "    eps_n: int,\n",
    "    n: int,\n",
    "    temp: float,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        net_out, unused_state = net(env_output, agent_state)      \n",
    "        while True:            \n",
    "            if len(returns) >= eps_n: break\n",
    "            with torch.no_grad():\n",
    "                net_out, agent_state = net(env_output, agent_state)                            \n",
    "            if n == 0:\n",
    "                action = net_out[\"action\"]\n",
    "            else:\n",
    "                action, _, _ = n_step_greedy(env, net, n, temp)            \n",
    "            env_output = env.step(action)           \n",
    "            if env_output['done']: returns.append(ret)\n",
    "            ret = env_output['episode_return'].item()        \n",
    "        #logging.info(\"Actor %i end\", actor_index)\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "def asy_test_n_step(n, net, flags, temp):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    mp.set_sharing_strategy('file_system')\n",
    "    net.share_memory()\n",
    "    ctx = mp.get_context()        \n",
    "    returns = Manager().list()\n",
    "\n",
    "    actor_processes = []\n",
    "    for i in range(flags.num_actors):\n",
    "        actor = ctx.Process(target=act_m, args=(flags, i, net, returns, eps_n, n, temp),)\n",
    "        actor.start()\n",
    "        actor_processes.append(actor)    \n",
    "\n",
    "    for actor in actor_processes:\n",
    "        actor.join()    \n",
    "\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f)\" % (len(returns),\n",
    "                    np.average(returns), np.std(returns) / np.sqrt(len(returns)),))        \n",
    "    return returns        \n",
    "        \n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.discounting = 0.97     \n",
    "flags.num_actors = 32\n",
    "bsz = 1\n",
    "eps_n = 500\n",
    "temp = 5\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "net = net.to(\"cpu\")\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_2.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "all_returns = {}\n",
    "for n in range(4):\n",
    "    t = time.time()\n",
    "    all_returns[n] = asy_test_n_step(n, net, flags, temp)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abac9a",
   "metadata": {},
   "source": [
    "Results (base_1.tar):\n",
    "    \n",
    "Testing 0 step planning <br>\n",
    "Finish 512 episode: avg. return: 0.12 (+-0.06) <br>\n",
    "Testing 1 step planning <br>\n",
    "Finish 502 episode: avg. return: 0.61 (+-0.04) <br>\n",
    "Testing 2 step planning <br>\n",
    "Finish 501 episode: avg. return: 0.92 (+-0.04) <br>\n",
    "Testing 3 step planning <br>\n",
    "Finish 501 episode: avg. return: 1.01 (+-0.04) <br>\n",
    "\n",
    "Results (base_2.tar):\n",
    "Testing 0 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.27 (+-0.04) <br>\n",
    "Time required for 0 step planning: 12.629324 <br>\n",
    "Testing 1 step planning <br>\n",
    "Finish 502 episode: avg. return: 0.51 (+-0.04) <br>\n",
    "Time required for 1 step planning: 74.194364 <br>\n",
    "Testing 2 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.74 (+-0.04) <br>\n",
    "Time required for 2 step planning: 339.732901 <br>\n",
    "Testing 3 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.76 (+-0.04) <br>\n",
    "Time required for 3 step planning: 1695.472523 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097bd18",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model Training Phase</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "968043ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data for learning model [RUN]\n",
    "\n",
    "Buffers = typing.Dict[str, typing.List[torch.Tensor]]\n",
    "\n",
    "def create_buffers_m(flags, obs_shape, num_actions) -> Buffers:\n",
    "    \n",
    "    seq_len = flags.seq_len\n",
    "    seq_n = flags.seq_n\n",
    "    specs = dict(\n",
    "        frame=dict(size=(seq_len + 1, *obs_shape), dtype=torch.uint8),\n",
    "        reward=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        truncated_done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        episode_return=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        episode_step=dict(size=(seq_len + 1,), dtype=torch.int32),\n",
    "        policy_logits=dict(size=(seq_len + 1, num_actions), dtype=torch.float32),\n",
    "        baseline=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        last_action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        reg_loss=dict(size=(seq_len + 1,), dtype=torch.float32)\n",
    "    )\n",
    "    buffers: Buffers = {key: [] for key in specs}\n",
    "    for _ in range(seq_n):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "            \n",
    "    return buffers\n",
    "\n",
    "def gen_data(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    buffers: Buffers,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = net(env_output, agent_state)     \n",
    "        \n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break         \n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:\n",
    "                buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                buffers[key][index][0, ...] = agent_output[key]\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.seq_len):\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = net(env_output, agent_state)\n",
    "                env_output = env.step(agent_output[\"action\"])\n",
    "                for key in env_output:\n",
    "                    buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "        \n",
    "\n",
    "# Models\n",
    "\n",
    "DOWNSCALE_C = 2\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation,\n",
    "        groups=groups, bias=False, dilation=dilation,)\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, inplanes, outplanes=None):\n",
    "        super().__init__()\n",
    "        if outplanes is None: outplanes = inplanes \n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        self.conv1 = conv3x3(inplanes, inplanes)\n",
    "        self.bn1 = norm_layer(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(inplanes, outplanes)\n",
    "        self.bn2 = norm_layer(outplanes)\n",
    "        self.skip_conv = (outplanes != inplanes)\n",
    "        if outplanes != inplanes:\n",
    "            self.conv3 = conv1x1(inplanes, outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.skip_conv:\n",
    "            out += self.conv3(identity)\n",
    "        else:\n",
    "            out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class FrameEncoder(nn.Module):    \n",
    "    def __init__(self, num_actions, frame_channels=3, type_nn=0):\n",
    "        self.num_actions = num_actions\n",
    "        super(FrameEncoder, self).__init__() \n",
    "        \n",
    "        if type_nn == 0:\n",
    "            n_block = 1\n",
    "        elif type_nn == 1:\n",
    "            n_block = 2\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=frame_channels+num_actions, out_channels=128//DOWNSCALE_C, kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=128//DOWNSCALE_C) for i in range(n_block)]) # Deep: 2 blocks here\n",
    "        self.res1 = torch.nn.Sequential(*res)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128//DOWNSCALE_C, out_channels=256//DOWNSCALE_C, \n",
    "                               kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(n_block)]) # Deep: 3 blocks here\n",
    "        self.res2 = torch.nn.Sequential(*res)\n",
    "        self.avg1 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(n_block)]) # Deep: 3 blocks here\n",
    "        self.res3 = torch.nn.Sequential(*res)\n",
    "        self.avg2 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x, actions):        \n",
    "        # input shape: B, C, H, W        \n",
    "        # action shape: B \n",
    "        \n",
    "        x = x.float() / 255.0    \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "        x = torch.concat([x, actions], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.res1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.res2(x)\n",
    "        x = self.avg1(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.avg2(x)\n",
    "        return x\n",
    "    \n",
    "class DynamicModel(nn.Module):\n",
    "    def __init__(self, num_actions, inplanes=256, type_nn=0):        \n",
    "        super(DynamicModel, self).__init__()\n",
    "        self.type_nn = type_nn\n",
    "        if type_nn == 0:\n",
    "            res = nn.ModuleList([ResBlock(inplanes=inplanes+num_actions, outplanes=inplanes)] + [\n",
    "                    ResBlock(inplanes=inplanes) for i in range(4)]) \n",
    "        elif type_nn == 1:                      \n",
    "            res = nn.ModuleList([ResBlock(inplanes=inplanes) for i in range(15)] + [\n",
    "                    ResBlock(inplanes=inplanes, outplanes=inplanes*num_actions)])\n",
    "\n",
    "        \n",
    "        self.res = torch.nn.Sequential(*res)\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def forward(self, x, actions):              \n",
    "        bsz, c, h, w = x.shape\n",
    "        if self.training:\n",
    "            x.register_hook(lambda grad: grad * 0.5)\n",
    "        if self.type_nn == 0:\n",
    "            actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "            x = torch.concat([x, actions], dim=1)\n",
    "            out = self.res(x)\n",
    "        elif self.type_nn == 1:            \n",
    "            res_out = self.res(x).view(bsz, self.num_actions, c, h, w)        \n",
    "            actions = actions.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            out = torch.sum(actions * res_out, dim=1)\n",
    "        return out\n",
    "    \n",
    "class Output_rvpi(nn.Module):   \n",
    "    def __init__(self, num_actions, input_shape):         \n",
    "        super(Output_rvpi, self).__init__()        \n",
    "        c, h, w = input_shape\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=c//2, kernel_size=3, padding='same') \n",
    "        self.conv2 = nn.Conv2d(in_channels=c//2, out_channels=c//4, kernel_size=3, padding='same') \n",
    "        fc_in = h * w * (c // 4)\n",
    "        self.fc_r = nn.Linear(fc_in, 1) \n",
    "        self.fc_v = nn.Linear(fc_in, 1) \n",
    "        self.fc_logits = nn.Linear(fc_in, num_actions)         \n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        r, v, logits = self.fc_r(x), self.fc_v(x), self.fc_logits(x)\n",
    "        return r, v, logits\n",
    "\n",
    "class Model(nn.Module):    \n",
    "    def __init__(self, flags, obs_shape, num_actions):        \n",
    "        super(Model, self).__init__()      \n",
    "        self.flags = flags\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions          \n",
    "        self.type_nn = flags.model_type_nn # type_nn: type of neural network for the model; 0 for small, 1 for large\n",
    "        self.frameEncoder = FrameEncoder(num_actions=num_actions, frame_channels=obs_shape[0], type_nn=self.type_nn)\n",
    "        self.dynamicModel = DynamicModel(num_actions=num_actions, inplanes=256//DOWNSCALE_C, type_nn=self.type_nn)\n",
    "        self.output_rvpi = Output_rvpi(num_actions=num_actions, input_shape=(256//DOWNSCALE_C, \n",
    "                      obs_shape[1]//16, obs_shape[1]//16))\n",
    "        \n",
    "    def forward(self, x, actions, one_hot=False):\n",
    "        # Input\n",
    "        # x: frames with shape (B, C, H, W), in the form of s_t\n",
    "        # actions: action (int64) with shape (k+1, B), in the form of a_{t-1}, a_{t}, a_{t+1}, .. a_{t+k-1}\n",
    "        # Output\n",
    "        # reward: predicted reward with shape (k, B), in the form of r_{t+1}, r_{t+2}, ..., r_{t+k}\n",
    "        # value: predicted value with shape (k+1, B), in the form of v_{t}, v_{t+1}, v_{t+2}, ..., v_{t+k}\n",
    "        # policy: predicted policy with shape (k+1, B), in the form of pi_{t}, pi_{t+1}, pi_{t+2}, ..., pi_{t+k}\n",
    "        # encoded: encoded states with shape (k+1, B), in the form of z_t, z_{t+1}, z_{t+2}, ..., z_{t+k}\n",
    "        # Recall the transition notation: s_t, a_t, r_{t+1}, s_{t+1}, ...\n",
    "        \n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        encoded = self.frameEncoder(x, actions[0])\n",
    "        return self.forward_encoded(encoded, actions[1:], one_hot=True)\n",
    "    \n",
    "    def forward_encoded(self, encoded, actions, one_hot=False):\n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        \n",
    "        r, v, logits = self.output_rvpi(encoded)\n",
    "        r_list, v_list, logits_list = [], [v.squeeze(-1).unsqueeze(0)], [logits.unsqueeze(0)]\n",
    "        encoded_list = [encoded.unsqueeze(0)]\n",
    "        \n",
    "        for k in range(actions.shape[0]):            \n",
    "            encoded = self.dynamicModel(encoded, actions[k])\n",
    "            r, v, logits = self.output_rvpi(encoded)\n",
    "            r_list.append(r.squeeze(-1).unsqueeze(0))\n",
    "            v_list.append(v.squeeze(-1).unsqueeze(0))\n",
    "            logits_list.append(logits.unsqueeze(0))\n",
    "            encoded_list.append(encoded.unsqueeze(0))        \n",
    "        \n",
    "        if len(r_list) > 0:\n",
    "            rs = torch.concat(r_list, dim=0)\n",
    "        else:\n",
    "            rs = None\n",
    "            \n",
    "        vs = torch.concat(v_list, dim=0)\n",
    "        logits = torch.concat(logits_list, dim=0)\n",
    "        encodeds = torch.concat(encoded_list, dim=0)        \n",
    "        \n",
    "        return rs, vs, logits, encodeds\n",
    "\n",
    "#model = Model(flags, (3, 80, 80), num_actions=5)\n",
    "#rs, vs, logits = model(torch.rand(16, 3, 80, 80), torch.ones(8, 16).long())\n",
    "\n",
    "# functions for training models\n",
    "\n",
    "def get_batch_m(flags, buffers: Buffers):\n",
    "    batch_indices = np.random.randint(flags.seq_n, size=flags.bsz)\n",
    "    time_indices = np.random.randint(flags.seq_len - flags.unroll_len, size=flags.bsz)\n",
    "    batch = {key: torch.stack([buffers[key][m][time_indices[n]:time_indices[n]+flags.unroll_len+1] \n",
    "                          for n, m in enumerate(batch_indices)], dim=1) for key in buffers}\n",
    "    batch = {k: t.to(device=flags.device, non_blocking=True) for k, t in batch.items()}\n",
    "    return batch\n",
    "\n",
    "def compute_cross_entropy_loss(logits, target_logits, mask):\n",
    "    target_policy = F.softmax(target_logits, dim=-1)\n",
    "    log_policy = F.log_softmax(logits, dim=-1)\n",
    "    return -torch.sum(target_policy * log_policy * (~mask).float().unsqueeze(-1))\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_rewards = batch['reward'][1:]\n",
    "    target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float())# +\n",
    "                           #vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "    # if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "    # rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "    # vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "    # logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "    for t in range(vs.shape[0]):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "    done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    #rs_loss = torch.sum(huberloss(rs, target_rewards.detach()) * (~done_masks).float())\n",
    "    rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~done_masks).float())\n",
    "    #vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()) * (~done_masks).float())\n",
    "    vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), done_masks)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss\n",
    "\n",
    "# n_step_greedy for testing\n",
    "\n",
    "def n_step_greedy_model(state, action, model, n, encoded=None, temp=20.): \n",
    "    \n",
    "    # Either input state, action (S_t, A_{t-1}) or the encoded Z_t\n",
    "    # state / encoded in the shape of (B, C, H, W)\n",
    "    # action in the shape of (B)    \n",
    "    with torch.no_grad():    \n",
    "      bsz = state.shape[0] if encoded is None else encoded.shape[0]\n",
    "      device = state.device if encoded is None else encoded.device\n",
    "      num_actions = model.num_actions    \n",
    "\n",
    "      q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "\n",
    "      for act in range(num_actions):        \n",
    "          new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "          if encoded is None:            \n",
    "              old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "              rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "          else:\n",
    "              rs, vs, logits, encodeds = model.forward_encoded(encoded, new_action.unsqueeze(0))\n",
    "\n",
    "          if n > 1:\n",
    "              action, prob, sub_q_ret = n_step_greedy_model(state=None, action=None, \n",
    "                         model=model, n=n-1, encoded=encodeds[1])\n",
    "              ret = rs[0] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] \n",
    "          else:\n",
    "              ret = rs[0] + flags.discounting * vs[1]\n",
    "          q_ret[:, act] = ret\n",
    "\n",
    "      prob = F.softmax(temp*q_ret, dim=1)\n",
    "      action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret        \n",
    "   \n",
    "#n_step_greedy_model(batch['frame'][0], batch['action'][0], model, 4)  \n",
    "\n",
    "def test_n_step_model(n, model, flags, eps_n=100, temp=20.):    \n",
    "    \n",
    "    print(\"Testing %d step planning\" % n) \n",
    "    \n",
    "    bsz = 100\n",
    "    env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "    env = Vec_Environment(env, bsz)\n",
    "    num_actions = env.gym_env.action_space[0].n\n",
    "    \n",
    "    model.train(False)\n",
    "    returns = []\n",
    "    \n",
    "    obs = env.initial()\n",
    "    action = torch.zeros(bsz).long().to(flags.device)\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "        new_action, _, _ = n_step_greedy_model(obs['frame'][0], action, model, n, None, temp)        \n",
    "        obs = env.step(new_action)\n",
    "        action = new_action\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            #print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            #    np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    returns = returns[:eps_n]\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6fa3769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer created successfully.\n",
      "model size:  6905607\n",
      "frameEncoder.conv1.weight 4608\n",
      "frameEncoder.conv1.bias 64\n",
      "frameEncoder.res1.0.conv1.weight 36864\n",
      "frameEncoder.res1.0.bn1.weight 64\n",
      "frameEncoder.res1.0.bn1.bias 64\n",
      "frameEncoder.res1.0.conv2.weight 36864\n",
      "frameEncoder.res1.0.bn2.weight 64\n",
      "frameEncoder.res1.0.bn2.bias 64\n",
      "frameEncoder.res1.1.conv1.weight 36864\n",
      "frameEncoder.res1.1.bn1.weight 64\n",
      "frameEncoder.res1.1.bn1.bias 64\n",
      "frameEncoder.res1.1.conv2.weight 36864\n",
      "frameEncoder.res1.1.bn2.weight 64\n",
      "frameEncoder.res1.1.bn2.bias 64\n",
      "frameEncoder.conv2.weight 73728\n",
      "frameEncoder.conv2.bias 128\n",
      "frameEncoder.res2.0.conv1.weight 147456\n",
      "frameEncoder.res2.0.bn1.weight 128\n",
      "frameEncoder.res2.0.bn1.bias 128\n",
      "frameEncoder.res2.0.conv2.weight 147456\n",
      "frameEncoder.res2.0.bn2.weight 128\n",
      "frameEncoder.res2.0.bn2.bias 128\n",
      "frameEncoder.res2.1.conv1.weight 147456\n",
      "frameEncoder.res2.1.bn1.weight 128\n",
      "frameEncoder.res2.1.bn1.bias 128\n",
      "frameEncoder.res2.1.conv2.weight 147456\n",
      "frameEncoder.res2.1.bn2.weight 128\n",
      "frameEncoder.res2.1.bn2.bias 128\n",
      "frameEncoder.res3.0.conv1.weight 147456\n",
      "frameEncoder.res3.0.bn1.weight 128\n",
      "frameEncoder.res3.0.bn1.bias 128\n",
      "frameEncoder.res3.0.conv2.weight 147456\n",
      "frameEncoder.res3.0.bn2.weight 128\n",
      "frameEncoder.res3.0.bn2.bias 128\n",
      "frameEncoder.res3.1.conv1.weight 147456\n",
      "frameEncoder.res3.1.bn1.weight 128\n",
      "frameEncoder.res3.1.bn1.bias 128\n",
      "frameEncoder.res3.1.conv2.weight 147456\n",
      "frameEncoder.res3.1.bn2.weight 128\n",
      "frameEncoder.res3.1.bn2.bias 128\n",
      "dynamicModel.res.0.conv1.weight 147456\n",
      "dynamicModel.res.0.bn1.weight 128\n",
      "dynamicModel.res.0.bn1.bias 128\n",
      "dynamicModel.res.0.conv2.weight 147456\n",
      "dynamicModel.res.0.bn2.weight 128\n",
      "dynamicModel.res.0.bn2.bias 128\n",
      "dynamicModel.res.1.conv1.weight 147456\n",
      "dynamicModel.res.1.bn1.weight 128\n",
      "dynamicModel.res.1.bn1.bias 128\n",
      "dynamicModel.res.1.conv2.weight 147456\n",
      "dynamicModel.res.1.bn2.weight 128\n",
      "dynamicModel.res.1.bn2.bias 128\n",
      "dynamicModel.res.2.conv1.weight 147456\n",
      "dynamicModel.res.2.bn1.weight 128\n",
      "dynamicModel.res.2.bn1.bias 128\n",
      "dynamicModel.res.2.conv2.weight 147456\n",
      "dynamicModel.res.2.bn2.weight 128\n",
      "dynamicModel.res.2.bn2.bias 128\n",
      "dynamicModel.res.3.conv1.weight 147456\n",
      "dynamicModel.res.3.bn1.weight 128\n",
      "dynamicModel.res.3.bn1.bias 128\n",
      "dynamicModel.res.3.conv2.weight 147456\n",
      "dynamicModel.res.3.bn2.weight 128\n",
      "dynamicModel.res.3.bn2.bias 128\n",
      "dynamicModel.res.4.conv1.weight 147456\n",
      "dynamicModel.res.4.bn1.weight 128\n",
      "dynamicModel.res.4.bn1.bias 128\n",
      "dynamicModel.res.4.conv2.weight 147456\n",
      "dynamicModel.res.4.bn2.weight 128\n",
      "dynamicModel.res.4.bn2.bias 128\n",
      "dynamicModel.res.5.conv1.weight 147456\n",
      "dynamicModel.res.5.bn1.weight 128\n",
      "dynamicModel.res.5.bn1.bias 128\n",
      "dynamicModel.res.5.conv2.weight 147456\n",
      "dynamicModel.res.5.bn2.weight 128\n",
      "dynamicModel.res.5.bn2.bias 128\n",
      "dynamicModel.res.6.conv1.weight 147456\n",
      "dynamicModel.res.6.bn1.weight 128\n",
      "dynamicModel.res.6.bn1.bias 128\n",
      "dynamicModel.res.6.conv2.weight 147456\n",
      "dynamicModel.res.6.bn2.weight 128\n",
      "dynamicModel.res.6.bn2.bias 128\n",
      "dynamicModel.res.7.conv1.weight 147456\n",
      "dynamicModel.res.7.bn1.weight 128\n",
      "dynamicModel.res.7.bn1.bias 128\n",
      "dynamicModel.res.7.conv2.weight 147456\n",
      "dynamicModel.res.7.bn2.weight 128\n",
      "dynamicModel.res.7.bn2.bias 128\n",
      "dynamicModel.res.8.conv1.weight 147456\n",
      "dynamicModel.res.8.bn1.weight 128\n",
      "dynamicModel.res.8.bn1.bias 128\n",
      "dynamicModel.res.8.conv2.weight 147456\n",
      "dynamicModel.res.8.bn2.weight 128\n",
      "dynamicModel.res.8.bn2.bias 128\n",
      "dynamicModel.res.9.conv1.weight 147456\n",
      "dynamicModel.res.9.bn1.weight 128\n",
      "dynamicModel.res.9.bn1.bias 128\n",
      "dynamicModel.res.9.conv2.weight 147456\n",
      "dynamicModel.res.9.bn2.weight 128\n",
      "dynamicModel.res.9.bn2.bias 128\n",
      "dynamicModel.res.10.conv1.weight 147456\n",
      "dynamicModel.res.10.bn1.weight 128\n",
      "dynamicModel.res.10.bn1.bias 128\n",
      "dynamicModel.res.10.conv2.weight 147456\n",
      "dynamicModel.res.10.bn2.weight 128\n",
      "dynamicModel.res.10.bn2.bias 128\n",
      "dynamicModel.res.11.conv1.weight 147456\n",
      "dynamicModel.res.11.bn1.weight 128\n",
      "dynamicModel.res.11.bn1.bias 128\n",
      "dynamicModel.res.11.conv2.weight 147456\n",
      "dynamicModel.res.11.bn2.weight 128\n",
      "dynamicModel.res.11.bn2.bias 128\n",
      "dynamicModel.res.12.conv1.weight 147456\n",
      "dynamicModel.res.12.bn1.weight 128\n",
      "dynamicModel.res.12.bn1.bias 128\n",
      "dynamicModel.res.12.conv2.weight 147456\n",
      "dynamicModel.res.12.bn2.weight 128\n",
      "dynamicModel.res.12.bn2.bias 128\n",
      "dynamicModel.res.13.conv1.weight 147456\n",
      "dynamicModel.res.13.bn1.weight 128\n",
      "dynamicModel.res.13.bn1.bias 128\n",
      "dynamicModel.res.13.conv2.weight 147456\n",
      "dynamicModel.res.13.bn2.weight 128\n",
      "dynamicModel.res.13.bn2.bias 128\n",
      "dynamicModel.res.14.conv1.weight 147456\n",
      "dynamicModel.res.14.bn1.weight 128\n",
      "dynamicModel.res.14.bn1.bias 128\n",
      "dynamicModel.res.14.conv2.weight 147456\n",
      "dynamicModel.res.14.bn2.weight 128\n",
      "dynamicModel.res.14.bn2.bias 128\n",
      "dynamicModel.res.15.conv1.weight 147456\n",
      "dynamicModel.res.15.bn1.weight 128\n",
      "dynamicModel.res.15.bn1.bias 128\n",
      "dynamicModel.res.15.conv2.weight 737280\n",
      "dynamicModel.res.15.bn2.weight 640\n",
      "dynamicModel.res.15.bn2.bias 640\n",
      "dynamicModel.res.15.conv3.weight 81920\n",
      "output_rvpi.conv1.weight 73728\n",
      "output_rvpi.conv1.bias 64\n",
      "output_rvpi.conv2.weight 18432\n",
      "output_rvpi.conv2.bias 32\n",
      "output_rvpi.fc_r.weight 800\n",
      "output_rvpi.fc_r.bias 1\n",
      "output_rvpi.fc_v.weight 800\n",
      "output_rvpi.fc_v.bias 1\n",
      "output_rvpi.fc_logits.weight 4000\n",
      "output_rvpi.fc_logits.bias 5\n"
     ]
    }
   ],
   "source": [
    "# Start training models\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())       \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.bsz = 64\n",
    "flags.unroll_len = 5\n",
    "flags.num_actors = 32\n",
    "flags.seq_n = 1000\n",
    "flags.seq_len = 200\n",
    "flags.learning_rate = 0.0001\n",
    "flags.loop_batch_n = 3\n",
    "flags.discounting = 0.97\n",
    "flags.tot_epoch = 10000\n",
    "flags.grad_norm_clipping = 60\n",
    "\n",
    "flags.model_type_nn = 1\n",
    "\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "# Create buffer for actors to write\n",
    "\n",
    "mp.set_sharing_strategy('file_system')\n",
    "ctx = mp.get_context()        \n",
    "\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "buffers = create_buffers_m(flags, obs_shape, num_actions)\n",
    "print(\"Buffer created successfully.\")\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "\n",
    "env = create_env(flags)\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=flags.learning_rate)\n",
    "\n",
    "print(\"model size: \", sum(p.numel() for p in model.parameters()))\n",
    "for k, v in model.named_parameters(): print(k, v.numel())    \n",
    "    \n",
    "tot_step = int(flags.loop_batch_n * flags.seq_n * flags.seq_len / flags.bsz / flags.unroll_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "301892ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1095814\n",
      "Batch [0] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.72 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.58 (+-0.15) \n",
      "[0:0] F: 0 \t tot_loss 9.398491 rs_loss 0.195361 vs_loss 4.774497 logits_loss 442.863281\n",
      "[0:100] F: 32000 \t tot_loss 13.375923 rs_loss 1.491374 vs_loss 7.435394 logits_loss 444.915540\n",
      "[0:200] F: 64000 \t tot_loss 17.217728 rs_loss 1.610392 vs_loss 11.151394 logits_loss 445.594250\n",
      "[0:300] F: 96000 \t tot_loss 16.869749 rs_loss 1.582948 vs_loss 10.826971 logits_loss 445.982990\n",
      "[0:400] F: 128000 \t tot_loss 16.696534 rs_loss 1.403860 vs_loss 10.832572 logits_loss 446.010267\n",
      "[0:500] F: 160000 \t tot_loss 21.215325 rs_loss 1.629517 vs_loss 15.118014 logits_loss 446.779423\n",
      "[0:600] F: 192000 \t tot_loss 20.476190 rs_loss 1.573477 vs_loss 14.431993 logits_loss 447.071967\n",
      "[0:700] F: 224000 \t tot_loss 20.692786 rs_loss 1.337962 vs_loss 14.883223 logits_loss 447.160041\n",
      "[0:800] F: 256000 \t tot_loss 20.481714 rs_loss 1.340234 vs_loss 14.669386 logits_loss 447.209349\n",
      "[0:900] F: 288000 \t tot_loss 17.188033 rs_loss 1.108266 vs_loss 11.610246 logits_loss 446.952092\n",
      "[0:1000] F: 320000 \t tot_loss 15.353849 rs_loss 0.863244 vs_loss 10.022473 logits_loss 446.813172\n",
      "[0:1100] F: 352000 \t tot_loss 15.233250 rs_loss 0.833816 vs_loss 9.930675 logits_loss 446.875929\n",
      "[0:1200] F: 384000 \t tot_loss 16.249834 rs_loss 1.215087 vs_loss 10.567831 logits_loss 446.691663\n",
      "[0:1300] F: 416000 \t tot_loss 15.345640 rs_loss 0.974183 vs_loss 9.903296 logits_loss 446.816208\n",
      "[0:1400] F: 448000 \t tot_loss 15.329780 rs_loss 0.967147 vs_loss 9.891815 logits_loss 447.081751\n",
      "[0:1500] F: 480000 \t tot_loss 14.739421 rs_loss 0.975529 vs_loss 9.298812 logits_loss 446.507999\n",
      "[0:1600] F: 512000 \t tot_loss 13.284437 rs_loss 0.513242 vs_loss 8.302263 logits_loss 446.893209\n",
      "[0:1700] F: 544000 \t tot_loss 14.041747 rs_loss 0.756101 vs_loss 8.820074 logits_loss 446.557191\n",
      "[0:1800] F: 576000 \t tot_loss 15.985468 rs_loss 1.057334 vs_loss 10.464249 logits_loss 446.388468\n",
      "Batch [1] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.21 (+-0.27) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.55 (+-0.07) \n",
      "[1:0] F: 600000 \t tot_loss 15.032549 rs_loss 1.057566 vs_loss 9.507692 logits_loss 446.729025\n",
      "[1:100] F: 632000 \t tot_loss 16.011589 rs_loss 1.216976 vs_loss 10.329418 logits_loss 446.519494\n",
      "[1:200] F: 664000 \t tot_loss 15.620244 rs_loss 1.141137 vs_loss 10.011520 logits_loss 446.758763\n",
      "[1:300] F: 696000 \t tot_loss 18.911946 rs_loss 1.868213 vs_loss 12.578574 logits_loss 446.515941\n",
      "[1:400] F: 728000 \t tot_loss 19.643500 rs_loss 1.847883 vs_loss 13.331720 logits_loss 446.389653\n",
      "[1:500] F: 760000 \t tot_loss 20.001377 rs_loss 2.056888 vs_loss 13.480739 logits_loss 446.374967\n",
      "[1:600] F: 792000 \t tot_loss 20.878808 rs_loss 2.280998 vs_loss 14.135584 logits_loss 446.222593\n",
      "[1:700] F: 824000 \t tot_loss 19.361024 rs_loss 1.892520 vs_loss 13.009652 logits_loss 445.885272\n",
      "[1:800] F: 856000 \t tot_loss 16.885901 rs_loss 1.610482 vs_loss 10.819651 logits_loss 445.576768\n",
      "[1:900] F: 888000 \t tot_loss 16.546113 rs_loss 1.614879 vs_loss 10.473006 logits_loss 445.822851\n",
      "[1:1000] F: 920000 \t tot_loss 15.873138 rs_loss 1.242854 vs_loss 10.175927 logits_loss 445.435716\n",
      "[1:1100] F: 952000 \t tot_loss 13.953723 rs_loss 0.897998 vs_loss 8.601432 logits_loss 445.429313\n",
      "[1:1200] F: 984000 \t tot_loss 13.753243 rs_loss 0.899878 vs_loss 8.396705 logits_loss 445.666063\n",
      "[1:1300] F: 1016000 \t tot_loss 12.785649 rs_loss 0.589342 vs_loss 7.744276 logits_loss 445.203113\n",
      "[1:1400] F: 1048000 \t tot_loss 12.428831 rs_loss 0.588579 vs_loss 7.380725 logits_loss 445.952725\n",
      "[1:1500] F: 1080000 \t tot_loss 13.116466 rs_loss 0.611931 vs_loss 8.039953 logits_loss 446.458251\n",
      "[1:1600] F: 1112000 \t tot_loss 14.538919 rs_loss 0.823332 vs_loss 9.250040 logits_loss 446.554670\n",
      "[1:1700] F: 1144000 \t tot_loss 13.856480 rs_loss 0.760846 vs_loss 8.633802 logits_loss 446.183176\n",
      "[1:1800] F: 1176000 \t tot_loss 13.242011 rs_loss 0.742482 vs_loss 8.038751 logits_loss 446.077906\n",
      "Batch [2] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.98 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.62 (+-0.15) \n",
      "[2:0] F: 1200000 \t tot_loss 13.589906 rs_loss 0.747747 vs_loss 8.384657 logits_loss 445.750227\n",
      "[2:100] F: 1232000 \t tot_loss 12.078333 rs_loss 0.500559 vs_loss 7.122558 logits_loss 445.521562\n",
      "[2:200] F: 1264000 \t tot_loss 16.486827 rs_loss 1.348535 vs_loss 10.683886 logits_loss 445.440670\n",
      "[2:300] F: 1296000 \t tot_loss 16.694076 rs_loss 1.312191 vs_loss 10.932674 logits_loss 444.921032\n",
      "[2:400] F: 1328000 \t tot_loss 16.298545 rs_loss 1.273259 vs_loss 10.579242 logits_loss 444.604459\n",
      "[2:500] F: 1360000 \t tot_loss 18.221630 rs_loss 1.437449 vs_loss 12.341796 logits_loss 444.238552\n",
      "[2:600] F: 1392000 \t tot_loss 14.734601 rs_loss 0.700037 vs_loss 9.590034 logits_loss 444.453005\n",
      "[2:700] F: 1424000 \t tot_loss 15.634584 rs_loss 0.835919 vs_loss 10.355831 logits_loss 444.283360\n",
      "[2:800] F: 1456000 \t tot_loss 15.448496 rs_loss 0.857009 vs_loss 10.146189 logits_loss 444.529858\n",
      "[2:900] F: 1488000 \t tot_loss 14.054846 rs_loss 0.741954 vs_loss 8.859407 logits_loss 445.348481\n",
      "[2:1000] F: 1520000 \t tot_loss 14.064065 rs_loss 0.999105 vs_loss 8.611737 logits_loss 445.322306\n",
      "[2:1100] F: 1552000 \t tot_loss 14.332609 rs_loss 0.870215 vs_loss 9.008518 logits_loss 445.387617\n",
      "[2:1200] F: 1584000 \t tot_loss 15.042015 rs_loss 1.192610 vs_loss 9.393898 logits_loss 445.550714\n",
      "[2:1300] F: 1616000 \t tot_loss 15.233417 rs_loss 1.260885 vs_loss 9.520305 logits_loss 445.222675\n",
      "[2:1400] F: 1648000 \t tot_loss 15.688528 rs_loss 1.354808 vs_loss 9.880312 logits_loss 445.340833\n",
      "[2:1500] F: 1680000 \t tot_loss 15.737802 rs_loss 1.577583 vs_loss 9.703605 logits_loss 445.661404\n",
      "[2:1600] F: 1712000 \t tot_loss 15.080656 rs_loss 1.255460 vs_loss 9.369753 logits_loss 445.544306\n",
      "[2:1700] F: 1744000 \t tot_loss 14.792623 rs_loss 1.132185 vs_loss 9.208988 logits_loss 445.145092\n",
      "[2:1800] F: 1776000 \t tot_loss 15.106255 rs_loss 0.700115 vs_loss 9.957375 logits_loss 444.876513\n",
      "Batch [3] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.75 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.06 (+-0.23) \n",
      "[3:0] F: 1800000 \t tot_loss 13.930871 rs_loss 0.496335 vs_loss 8.988066 logits_loss 444.647072\n",
      "[3:100] F: 1832000 \t tot_loss 14.912731 rs_loss 0.527714 vs_loss 9.944820 logits_loss 444.019696\n",
      "[3:200] F: 1864000 \t tot_loss 16.599275 rs_loss 0.558115 vs_loss 11.600372 logits_loss 444.078794\n",
      "[3:300] F: 1896000 \t tot_loss 16.313801 rs_loss 0.791396 vs_loss 11.083453 logits_loss 443.895252\n",
      "[3:400] F: 1928000 \t tot_loss 20.638036 rs_loss 1.494208 vs_loss 14.699927 logits_loss 444.390076\n",
      "[3:500] F: 1960000 \t tot_loss 20.920842 rs_loss 1.749920 vs_loss 14.722527 logits_loss 444.839484\n",
      "[3:600] F: 1992000 \t tot_loss 21.053328 rs_loss 1.973348 vs_loss 14.636051 logits_loss 444.392927\n",
      "[3:700] F: 2024000 \t tot_loss 23.031040 rs_loss 2.375816 vs_loss 16.209623 logits_loss 444.560121\n",
      "[3:800] F: 2056000 \t tot_loss 19.757281 rs_loss 1.737920 vs_loss 13.574717 logits_loss 444.464392\n",
      "[3:900] F: 2088000 \t tot_loss 20.841390 rs_loss 1.909667 vs_loss 14.488002 logits_loss 444.372140\n",
      "[3:1000] F: 2120000 \t tot_loss 20.425729 rs_loss 1.775456 vs_loss 14.203058 logits_loss 444.721490\n",
      "[3:1100] F: 2152000 \t tot_loss 18.911241 rs_loss 1.614716 vs_loss 12.852470 logits_loss 444.405460\n",
      "[3:1200] F: 2184000 \t tot_loss 20.181846 rs_loss 1.865011 vs_loss 13.869611 logits_loss 444.722497\n",
      "[3:1300] F: 2216000 \t tot_loss 24.032850 rs_loss 2.198845 vs_loss 17.385018 logits_loss 444.898734\n",
      "[3:1400] F: 2248000 \t tot_loss 25.720063 rs_loss 2.332956 vs_loss 18.944364 logits_loss 444.274401\n",
      "[3:1500] F: 2280000 \t tot_loss 24.972870 rs_loss 2.133282 vs_loss 18.396253 logits_loss 444.333584\n",
      "[3:1600] F: 2312000 \t tot_loss 23.945588 rs_loss 1.850421 vs_loss 17.650851 logits_loss 444.431635\n",
      "[3:1700] F: 2344000 \t tot_loss 20.439893 rs_loss 1.689257 vs_loss 14.307737 logits_loss 444.289919\n",
      "[3:1800] F: 2376000 \t tot_loss 20.725833 rs_loss 1.842804 vs_loss 14.433875 logits_loss 444.915288\n",
      "Batch [4] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.00 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.69 (+-0.20) \n",
      "[4:0] F: 2400000 \t tot_loss 21.797124 rs_loss 2.010332 vs_loss 15.340137 logits_loss 444.665572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4:100] F: 2432000 \t tot_loss 22.043298 rs_loss 2.056556 vs_loss 15.539055 logits_loss 444.768738\n",
      "[4:200] F: 2464000 \t tot_loss 21.739563 rs_loss 1.877954 vs_loss 15.411787 logits_loss 444.982181\n",
      "[4:300] F: 2496000 \t tot_loss 21.313017 rs_loss 2.091234 vs_loss 14.769619 logits_loss 445.216346\n",
      "[4:400] F: 2528000 \t tot_loss 18.016853 rs_loss 1.327983 vs_loss 12.231220 logits_loss 445.764984\n",
      "[4:500] F: 2560000 \t tot_loss 18.719354 rs_loss 1.603701 vs_loss 12.654410 logits_loss 446.124201\n",
      "[4:600] F: 2592000 \t tot_loss 17.909904 rs_loss 1.657528 vs_loss 11.792423 logits_loss 445.995277\n",
      "[4:700] F: 2624000 \t tot_loss 17.333489 rs_loss 1.322830 vs_loss 11.549471 logits_loss 446.118799\n",
      "[4:800] F: 2656000 \t tot_loss 18.287805 rs_loss 1.451660 vs_loss 12.380371 logits_loss 445.577464\n",
      "[4:900] F: 2688000 \t tot_loss 19.371993 rs_loss 1.703586 vs_loss 13.217405 logits_loss 445.100170\n",
      "[4:1000] F: 2720000 \t tot_loss 18.082279 rs_loss 1.432121 vs_loss 12.196819 logits_loss 445.333897\n",
      "[4:1100] F: 2752000 \t tot_loss 18.504462 rs_loss 1.790866 vs_loss 12.259924 logits_loss 445.367082\n",
      "[4:1200] F: 2784000 \t tot_loss 18.246931 rs_loss 1.630524 vs_loss 12.157809 logits_loss 445.859744\n",
      "[4:1300] F: 2816000 \t tot_loss 16.148813 rs_loss 1.058805 vs_loss 10.630063 logits_loss 445.994561\n",
      "[4:1400] F: 2848000 \t tot_loss 17.342646 rs_loss 1.413072 vs_loss 11.471153 logits_loss 445.842173\n",
      "[4:1500] F: 2880000 \t tot_loss 14.982581 rs_loss 0.832192 vs_loss 9.692714 logits_loss 445.767464\n",
      "[4:1600] F: 2912000 \t tot_loss 15.901108 rs_loss 1.100800 vs_loss 10.344835 logits_loss 445.547290\n",
      "[4:1700] F: 2944000 \t tot_loss 17.037485 rs_loss 1.341753 vs_loss 11.239309 logits_loss 445.642296\n",
      "[4:1800] F: 2976000 \t tot_loss 16.037631 rs_loss 0.960034 vs_loss 10.623435 logits_loss 445.416181\n",
      "Batch [5] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.12 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.46 (+-0.32) \n",
      "[5:0] F: 3000000 \t tot_loss 16.145869 rs_loss 1.014736 vs_loss 10.672743 logits_loss 445.839090\n",
      "[5:100] F: 3032000 \t tot_loss 17.757527 rs_loss 1.281182 vs_loss 12.022688 logits_loss 445.365751\n",
      "[5:200] F: 3064000 \t tot_loss 16.110328 rs_loss 1.017352 vs_loss 10.646424 logits_loss 444.655191\n",
      "[5:300] F: 3096000 \t tot_loss 16.137801 rs_loss 1.134806 vs_loss 10.558550 logits_loss 444.444442\n",
      "[5:400] F: 3128000 \t tot_loss 17.447709 rs_loss 1.160600 vs_loss 11.851399 logits_loss 443.571023\n",
      "[5:500] F: 3160000 \t tot_loss 16.486103 rs_loss 0.925293 vs_loss 11.126233 logits_loss 443.457782\n",
      "[5:600] F: 3192000 \t tot_loss 15.398410 rs_loss 0.901518 vs_loss 10.057392 logits_loss 443.950024\n",
      "[5:700] F: 3224000 \t tot_loss 16.030740 rs_loss 0.933314 vs_loss 10.662935 logits_loss 443.449104\n",
      "[5:800] F: 3256000 \t tot_loss 15.463541 rs_loss 0.935430 vs_loss 10.093371 logits_loss 443.473923\n",
      "[5:900] F: 3288000 \t tot_loss 16.050351 rs_loss 1.084247 vs_loss 10.527854 logits_loss 443.825054\n",
      "[5:1000] F: 3320000 \t tot_loss 16.300081 rs_loss 0.920401 vs_loss 10.945262 logits_loss 443.441843\n",
      "[5:1100] F: 3352000 \t tot_loss 17.403987 rs_loss 1.032427 vs_loss 11.932159 logits_loss 443.940059\n",
      "[5:1200] F: 3384000 \t tot_loss 19.363586 rs_loss 1.460093 vs_loss 13.467186 logits_loss 443.630735\n",
      "[5:1300] F: 3416000 \t tot_loss 20.643263 rs_loss 1.577239 vs_loss 14.634230 logits_loss 443.179503\n",
      "[5:1400] F: 3448000 \t tot_loss 21.443974 rs_loss 1.624223 vs_loss 15.388686 logits_loss 443.106517\n",
      "[5:1500] F: 3480000 \t tot_loss 20.919786 rs_loss 1.400982 vs_loss 15.087897 logits_loss 443.090818\n",
      "[5:1600] F: 3512000 \t tot_loss 20.604323 rs_loss 1.239319 vs_loss 14.929922 logits_loss 443.508271\n",
      "[5:1700] F: 3544000 \t tot_loss 21.996882 rs_loss 1.785074 vs_loss 15.774660 logits_loss 443.714893\n",
      "[5:1800] F: 3576000 \t tot_loss 21.708543 rs_loss 1.986583 vs_loss 15.283821 logits_loss 443.813917\n",
      "Batch [6] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.00 (+-0.24) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.97 (+-0.20) \n",
      "[6:0] F: 3600000 \t tot_loss 23.268097 rs_loss 2.421715 vs_loss 16.414546 logits_loss 443.183560\n",
      "[6:100] F: 3632000 \t tot_loss 23.069982 rs_loss 2.644393 vs_loss 15.996047 logits_loss 442.954246\n",
      "[6:200] F: 3664000 \t tot_loss 21.678493 rs_loss 2.341271 vs_loss 14.901200 logits_loss 443.602227\n",
      "[6:300] F: 3696000 \t tot_loss 19.374530 rs_loss 1.868724 vs_loss 13.069047 logits_loss 443.675930\n",
      "[6:400] F: 3728000 \t tot_loss 17.757108 rs_loss 1.599598 vs_loss 11.712553 logits_loss 444.495701\n",
      "[6:500] F: 3760000 \t tot_loss 17.147311 rs_loss 1.335614 vs_loss 11.364801 logits_loss 444.689670\n",
      "[6:600] F: 3792000 \t tot_loss 15.624075 rs_loss 1.124336 vs_loss 10.054476 logits_loss 444.526372\n",
      "[6:700] F: 3824000 \t tot_loss 17.682926 rs_loss 1.441089 vs_loss 11.796374 logits_loss 444.546353\n",
      "[6:800] F: 3856000 \t tot_loss 18.378068 rs_loss 1.429090 vs_loss 12.507793 logits_loss 444.118491\n",
      "[6:900] F: 3888000 \t tot_loss 18.202332 rs_loss 1.472591 vs_loss 12.291221 logits_loss 443.852060\n",
      "[6:1000] F: 3920000 \t tot_loss 19.435422 rs_loss 1.431575 vs_loss 13.567272 logits_loss 443.657524\n",
      "[6:1100] F: 3952000 \t tot_loss 17.518457 rs_loss 1.096117 vs_loss 11.987914 logits_loss 443.442712\n",
      "[6:1200] F: 3984000 \t tot_loss 15.486434 rs_loss 0.873296 vs_loss 10.174361 logits_loss 443.877666\n",
      "[6:1300] F: 4016000 \t tot_loss 15.391579 rs_loss 0.935860 vs_loss 10.018175 logits_loss 443.754389\n",
      "[6:1400] F: 4048000 \t tot_loss 15.239616 rs_loss 0.995974 vs_loss 9.800167 logits_loss 444.347452\n",
      "[6:1500] F: 4080000 \t tot_loss 17.075600 rs_loss 1.473133 vs_loss 11.156178 logits_loss 444.628939\n",
      "[6:1600] F: 4112000 \t tot_loss 17.924514 rs_loss 1.454761 vs_loss 12.025075 logits_loss 444.467860\n",
      "[6:1700] F: 4144000 \t tot_loss 17.357142 rs_loss 1.333181 vs_loss 11.578424 logits_loss 444.553717\n",
      "[6:1800] F: 4176000 \t tot_loss 16.194398 rs_loss 1.286557 vs_loss 10.464622 logits_loss 444.321965\n",
      "Batch [7] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.08 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.18 (+-0.27) \n",
      "[7:0] F: 4200000 \t tot_loss 16.242246 rs_loss 1.284976 vs_loss 10.518687 logits_loss 443.858231\n",
      "[7:100] F: 4232000 \t tot_loss 15.501802 rs_loss 0.983652 vs_loss 10.076733 logits_loss 444.141682\n",
      "[7:200] F: 4264000 \t tot_loss 16.447172 rs_loss 1.073168 vs_loss 10.924240 logits_loss 444.976404\n",
      "[7:300] F: 4296000 \t tot_loss 16.999736 rs_loss 1.304482 vs_loss 11.239206 logits_loss 445.604790\n",
      "[7:400] F: 4328000 \t tot_loss 18.923128 rs_loss 1.463336 vs_loss 12.994555 logits_loss 446.523770\n",
      "[7:500] F: 4360000 \t tot_loss 19.998202 rs_loss 1.800941 vs_loss 13.736096 logits_loss 446.116512\n",
      "[7:600] F: 4392000 \t tot_loss 17.443916 rs_loss 1.459417 vs_loss 11.526434 logits_loss 445.806421\n",
      "[7:700] F: 4424000 \t tot_loss 19.081910 rs_loss 1.368892 vs_loss 13.257076 logits_loss 445.594139\n",
      "[7:800] F: 4456000 \t tot_loss 15.897398 rs_loss 0.950618 vs_loss 10.498221 logits_loss 444.855954\n",
      "[7:900] F: 4488000 \t tot_loss 16.102394 rs_loss 0.637457 vs_loss 11.014563 logits_loss 445.037375\n",
      "[7:1000] F: 4520000 \t tot_loss 17.809319 rs_loss 0.944741 vs_loss 12.415464 logits_loss 444.911449\n",
      "[7:1100] F: 4552000 \t tot_loss 16.639303 rs_loss 0.961875 vs_loss 11.228375 logits_loss 444.905343\n",
      "[7:1200] F: 4584000 \t tot_loss 20.592367 rs_loss 1.507794 vs_loss 14.629176 logits_loss 445.539724\n",
      "[7:1300] F: 4616000 \t tot_loss 18.871556 rs_loss 1.453655 vs_loss 12.958890 logits_loss 445.901114\n",
      "[7:1400] F: 4648000 \t tot_loss 20.252929 rs_loss 1.658080 vs_loss 14.131848 logits_loss 446.300122\n",
      "[7:1500] F: 4680000 \t tot_loss 19.953792 rs_loss 1.565818 vs_loss 13.919746 logits_loss 446.822791\n",
      "[7:1600] F: 4712000 \t tot_loss 16.293892 rs_loss 1.268018 vs_loss 10.561497 logits_loss 446.437662\n",
      "[7:1700] F: 4744000 \t tot_loss 16.556076 rs_loss 1.292152 vs_loss 10.804992 logits_loss 445.893151\n",
      "[7:1800] F: 4776000 \t tot_loss 13.785990 rs_loss 0.810744 vs_loss 8.520003 logits_loss 445.524414\n",
      "Batch [8] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.91 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.96 (+-0.27) \n",
      "[8:0] F: 4800000 \t tot_loss 14.227756 rs_loss 0.947925 vs_loss 8.827979 logits_loss 445.185210\n",
      "[8:100] F: 4832000 \t tot_loss 15.423852 rs_loss 0.940824 vs_loss 10.032178 logits_loss 445.085042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8:200] F: 4864000 \t tot_loss 17.261416 rs_loss 1.157718 vs_loss 11.651554 logits_loss 445.214406\n",
      "[8:300] F: 4896000 \t tot_loss 18.287397 rs_loss 1.367892 vs_loss 12.469827 logits_loss 444.967877\n",
      "[8:400] F: 4928000 \t tot_loss 18.264929 rs_loss 1.452715 vs_loss 12.365178 logits_loss 444.703622\n",
      "[8:500] F: 4960000 \t tot_loss 17.112293 rs_loss 1.215900 vs_loss 11.451975 logits_loss 444.441848\n",
      "[8:600] F: 4992000 \t tot_loss 16.388406 rs_loss 1.333411 vs_loss 10.613973 logits_loss 444.102142\n",
      "[8:700] F: 5024000 \t tot_loss 15.187745 rs_loss 1.260408 vs_loss 9.488771 logits_loss 443.856612\n",
      "[8:800] F: 5056000 \t tot_loss 16.768452 rs_loss 1.450645 vs_loss 10.879671 logits_loss 443.813593\n",
      "[8:900] F: 5088000 \t tot_loss 16.810241 rs_loss 1.448875 vs_loss 10.919629 logits_loss 444.173790\n",
      "[8:1000] F: 5120000 \t tot_loss 16.593635 rs_loss 1.383812 vs_loss 10.763246 logits_loss 444.657722\n",
      "[8:1100] F: 5152000 \t tot_loss 19.684774 rs_loss 1.451960 vs_loss 13.784214 logits_loss 444.860028\n",
      "[8:1200] F: 5184000 \t tot_loss 17.167366 rs_loss 0.976229 vs_loss 11.743133 logits_loss 444.800462\n",
      "[8:1300] F: 5216000 \t tot_loss 17.575539 rs_loss 1.344574 vs_loss 11.783773 logits_loss 444.719170\n",
      "[8:1400] F: 5248000 \t tot_loss 18.311240 rs_loss 1.355735 vs_loss 12.513364 logits_loss 444.214069\n",
      "[8:1500] F: 5280000 \t tot_loss 15.634263 rs_loss 1.229955 vs_loss 9.965542 logits_loss 443.876497\n",
      "[8:1600] F: 5312000 \t tot_loss 15.277657 rs_loss 1.184514 vs_loss 9.655404 logits_loss 443.773954\n",
      "[8:1700] F: 5344000 \t tot_loss 15.106478 rs_loss 1.019734 vs_loss 9.652213 logits_loss 443.453123\n",
      "[8:1800] F: 5376000 \t tot_loss 14.838768 rs_loss 0.731466 vs_loss 9.671234 logits_loss 443.606800\n",
      "Batch [9] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.48 (+-0.31) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.95 (+-0.23) \n",
      "[9:0] F: 5400000 \t tot_loss 15.274980 rs_loss 0.874831 vs_loss 9.956859 logits_loss 444.328964\n",
      "[9:100] F: 5432000 \t tot_loss 15.609366 rs_loss 0.885316 vs_loss 10.282955 logits_loss 444.109530\n",
      "[9:200] F: 5464000 \t tot_loss 15.991827 rs_loss 1.000629 vs_loss 10.548418 logits_loss 444.277999\n",
      "[9:300] F: 5496000 \t tot_loss 15.261662 rs_loss 1.127665 vs_loss 9.693140 logits_loss 444.085681\n",
      "[9:400] F: 5528000 \t tot_loss 16.003753 rs_loss 1.221514 vs_loss 10.344705 logits_loss 443.753331\n",
      "[9:500] F: 5560000 \t tot_loss 16.053246 rs_loss 1.203620 vs_loss 10.407572 logits_loss 444.205278\n",
      "[9:600] F: 5592000 \t tot_loss 16.331577 rs_loss 1.262477 vs_loss 10.624070 logits_loss 444.503007\n",
      "[9:700] F: 5624000 \t tot_loss 17.236633 rs_loss 1.398601 vs_loss 11.391183 logits_loss 444.684956\n",
      "[9:800] F: 5656000 \t tot_loss 14.764698 rs_loss 1.067121 vs_loss 9.252252 logits_loss 444.532462\n",
      "[9:900] F: 5688000 \t tot_loss 15.076103 rs_loss 1.076732 vs_loss 9.559012 logits_loss 444.035875\n",
      "[9:1000] F: 5720000 \t tot_loss 15.045686 rs_loss 1.022266 vs_loss 9.579949 logits_loss 444.347077\n",
      "[9:1100] F: 5752000 \t tot_loss 14.335565 rs_loss 0.909821 vs_loss 8.983101 logits_loss 444.264304\n",
      "[9:1200] F: 5784000 \t tot_loss 15.033437 rs_loss 1.023978 vs_loss 9.560417 logits_loss 444.904262\n",
      "[9:1300] F: 5816000 \t tot_loss 14.992897 rs_loss 1.053593 vs_loss 9.492287 logits_loss 444.701738\n",
      "[9:1400] F: 5848000 \t tot_loss 14.673455 rs_loss 1.090082 vs_loss 9.134013 logits_loss 444.936014\n",
      "[9:1500] F: 5880000 \t tot_loss 14.231202 rs_loss 1.058141 vs_loss 8.724319 logits_loss 444.874224\n",
      "[9:1600] F: 5912000 \t tot_loss 13.575893 rs_loss 0.932713 vs_loss 8.196173 logits_loss 444.700712\n",
      "[9:1700] F: 5944000 \t tot_loss 12.928503 rs_loss 0.871997 vs_loss 7.607076 logits_loss 444.943022\n",
      "[9:1800] F: 5976000 \t tot_loss 14.296909 rs_loss 1.407674 vs_loss 8.447964 logits_loss 444.127141\n",
      "Batch [10] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.88 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.75 (+-0.20) \n",
      "[10:0] F: 6000000 \t tot_loss 14.245644 rs_loss 1.271363 vs_loss 8.529782 logits_loss 444.449957\n",
      "[10:100] F: 6032000 \t tot_loss 14.868330 rs_loss 1.309115 vs_loss 9.111544 logits_loss 444.767189\n",
      "[10:200] F: 6064000 \t tot_loss 17.112894 rs_loss 1.760894 vs_loss 10.898539 logits_loss 445.346091\n",
      "[10:300] F: 6096000 \t tot_loss 14.504925 rs_loss 0.913874 vs_loss 9.131563 logits_loss 445.948814\n",
      "[10:400] F: 6128000 \t tot_loss 15.699957 rs_loss 0.927446 vs_loss 10.305321 logits_loss 446.719015\n",
      "[10:500] F: 6160000 \t tot_loss 16.701640 rs_loss 0.880295 vs_loss 11.348657 logits_loss 447.268818\n",
      "[10:600] F: 6192000 \t tot_loss 15.956799 rs_loss 0.490647 vs_loss 10.999035 logits_loss 446.711747\n",
      "[10:700] F: 6224000 \t tot_loss 16.867923 rs_loss 0.498556 vs_loss 11.900602 logits_loss 446.876538\n",
      "[10:800] F: 6256000 \t tot_loss 15.490571 rs_loss 0.459577 vs_loss 10.562800 logits_loss 446.819486\n",
      "[10:900] F: 6288000 \t tot_loss 14.320651 rs_loss 0.543872 vs_loss 9.314829 logits_loss 446.194971\n",
      "[10:1000] F: 6320000 \t tot_loss 14.667598 rs_loss 0.808983 vs_loss 9.393189 logits_loss 446.542595\n",
      "[10:1100] F: 6352000 \t tot_loss 14.031983 rs_loss 0.810393 vs_loss 8.755817 logits_loss 446.577289\n",
      "[10:1200] F: 6384000 \t tot_loss 15.539514 rs_loss 0.921716 vs_loss 10.151781 logits_loss 446.601654\n",
      "[10:1300] F: 6416000 \t tot_loss 16.647253 rs_loss 0.878548 vs_loss 11.301181 logits_loss 446.752404\n",
      "[10:1400] F: 6448000 \t tot_loss 15.601244 rs_loss 0.773239 vs_loss 10.363105 logits_loss 446.489947\n",
      "[10:1500] F: 6480000 \t tot_loss 16.840488 rs_loss 1.092520 vs_loss 11.279081 logits_loss 446.888744\n",
      "[10:1600] F: 6512000 \t tot_loss 16.151539 rs_loss 1.143991 vs_loss 10.541642 logits_loss 446.590657\n",
      "[10:1700] F: 6544000 \t tot_loss 14.856956 rs_loss 1.158957 vs_loss 9.231362 logits_loss 446.663785\n",
      "[10:1800] F: 6576000 \t tot_loss 17.750335 rs_loss 1.286944 vs_loss 11.998945 logits_loss 446.444541\n",
      "Batch [11] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.26 (+-0.26) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.91 (+-0.20) \n",
      "[11:0] F: 6600000 \t tot_loss 16.734306 rs_loss 0.997837 vs_loss 11.276683 logits_loss 445.978641\n",
      "[11:100] F: 6632000 \t tot_loss 18.130912 rs_loss 1.315768 vs_loss 12.355106 logits_loss 446.003816\n",
      "[11:200] F: 6664000 \t tot_loss 16.958213 rs_loss 1.164613 vs_loss 11.335409 logits_loss 445.819106\n",
      "[11:300] F: 6696000 \t tot_loss 13.911267 rs_loss 0.876612 vs_loss 8.576000 logits_loss 445.865558\n",
      "[11:400] F: 6728000 \t tot_loss 13.553530 rs_loss 0.829202 vs_loss 8.271105 logits_loss 445.322387\n",
      "[11:500] F: 6760000 \t tot_loss 13.947736 rs_loss 0.515764 vs_loss 8.982231 logits_loss 444.974142\n",
      "[11:600] F: 6792000 \t tot_loss 16.362034 rs_loss 0.813324 vs_loss 11.100957 logits_loss 444.775360\n",
      "[11:700] F: 6824000 \t tot_loss 16.782159 rs_loss 0.888190 vs_loss 11.448325 logits_loss 444.564348\n",
      "[11:800] F: 6856000 \t tot_loss 16.433572 rs_loss 0.902381 vs_loss 11.087182 logits_loss 444.400934\n",
      "[11:900] F: 6888000 \t tot_loss 15.306750 rs_loss 0.848746 vs_loss 10.013153 logits_loss 444.485154\n",
      "[11:1000] F: 6920000 \t tot_loss 13.144198 rs_loss 0.523281 vs_loss 8.170547 logits_loss 445.037021\n",
      "[11:1100] F: 6952000 \t tot_loss 14.390623 rs_loss 0.597137 vs_loss 9.344936 logits_loss 444.854973\n",
      "[11:1200] F: 6984000 \t tot_loss 16.335398 rs_loss 0.898746 vs_loss 10.984827 logits_loss 445.182524\n",
      "[11:1300] F: 7016000 \t tot_loss 18.822019 rs_loss 1.673459 vs_loss 12.699136 logits_loss 444.942354\n",
      "[11:1400] F: 7048000 \t tot_loss 19.184574 rs_loss 1.761314 vs_loss 12.978143 logits_loss 444.511710\n",
      "[11:1500] F: 7080000 \t tot_loss 18.210163 rs_loss 1.627770 vs_loss 12.132167 logits_loss 445.022533\n",
      "[11:1600] F: 7112000 \t tot_loss 16.679790 rs_loss 1.328041 vs_loss 10.897732 logits_loss 445.401786\n",
      "[11:1700] F: 7144000 \t tot_loss 17.426733 rs_loss 1.036156 vs_loss 11.932720 logits_loss 445.785641\n",
      "[11:1800] F: 7176000 \t tot_loss 20.233768 rs_loss 1.447864 vs_loss 14.331422 logits_loss 445.448196\n",
      "Batch [12] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.38 (+-0.29) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.71 (+-0.20) \n",
      "[12:0] F: 7200000 \t tot_loss 21.149272 rs_loss 1.699420 vs_loss 14.998545 logits_loss 445.130698\n",
      "[12:100] F: 7232000 \t tot_loss 24.156494 rs_loss 2.493414 vs_loss 17.218416 logits_loss 444.466434\n",
      "[12:200] F: 7264000 \t tot_loss 21.556141 rs_loss 2.089937 vs_loss 15.027497 logits_loss 443.870685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:300] F: 7296000 \t tot_loss 20.149824 rs_loss 1.892716 vs_loss 13.819561 logits_loss 443.754663\n",
      "[12:400] F: 7328000 \t tot_loss 20.549679 rs_loss 2.083116 vs_loss 14.029286 logits_loss 443.727820\n",
      "[12:500] F: 7360000 \t tot_loss 18.088189 rs_loss 1.278421 vs_loss 12.375892 logits_loss 443.387593\n",
      "[12:600] F: 7392000 \t tot_loss 19.216534 rs_loss 1.494177 vs_loss 13.286142 logits_loss 443.621575\n",
      "[12:700] F: 7424000 \t tot_loss 19.596708 rs_loss 1.530317 vs_loss 13.628876 logits_loss 443.751483\n",
      "[12:800] F: 7456000 \t tot_loss 17.253484 rs_loss 1.063472 vs_loss 11.754202 logits_loss 443.581077\n",
      "[12:900] F: 7488000 \t tot_loss 17.912873 rs_loss 1.072749 vs_loss 12.397231 logits_loss 444.289341\n",
      "[12:1000] F: 7520000 \t tot_loss 15.553484 rs_loss 0.801156 vs_loss 10.313832 logits_loss 443.849551\n",
      "[12:1100] F: 7552000 \t tot_loss 13.871137 rs_loss 0.757775 vs_loss 8.677126 logits_loss 443.623536\n",
      "[12:1200] F: 7584000 \t tot_loss 17.498812 rs_loss 1.241835 vs_loss 11.816131 logits_loss 444.084690\n",
      "[12:1300] F: 7616000 \t tot_loss 16.261975 rs_loss 1.227318 vs_loss 10.594262 logits_loss 444.039456\n",
      "[12:1400] F: 7648000 \t tot_loss 18.631949 rs_loss 1.712508 vs_loss 12.478961 logits_loss 444.047947\n",
      "[12:1500] F: 7680000 \t tot_loss 19.331939 rs_loss 1.708445 vs_loss 13.185800 logits_loss 443.769368\n",
      "[12:1600] F: 7712000 \t tot_loss 19.966180 rs_loss 1.939647 vs_loss 13.595026 logits_loss 443.150630\n",
      "[12:1700] F: 7744000 \t tot_loss 20.700875 rs_loss 2.180711 vs_loss 14.090551 logits_loss 442.961354\n",
      "[12:1800] F: 7776000 \t tot_loss 19.872897 rs_loss 1.716536 vs_loss 13.727896 logits_loss 442.846573\n",
      "Batch [13] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.33 (+-0.30) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.22 (+-0.30) \n",
      "[13:0] F: 7800000 \t tot_loss 18.203982 rs_loss 1.473874 vs_loss 12.299359 logits_loss 443.074968\n",
      "[13:100] F: 7832000 \t tot_loss 14.313560 rs_loss 0.901468 vs_loss 8.975585 logits_loss 443.650702\n",
      "[13:200] F: 7864000 \t tot_loss 15.517546 rs_loss 1.145795 vs_loss 9.932917 logits_loss 443.883467\n",
      "[13:300] F: 7896000 \t tot_loss 15.433229 rs_loss 1.374905 vs_loss 9.617892 logits_loss 444.043220\n",
      "[13:400] F: 7928000 \t tot_loss 15.277467 rs_loss 1.441075 vs_loss 9.394938 logits_loss 444.145345\n",
      "[13:500] F: 7960000 \t tot_loss 17.433046 rs_loss 1.897529 vs_loss 11.096113 logits_loss 443.940522\n",
      "[13:600] F: 7992000 \t tot_loss 15.823115 rs_loss 1.644185 vs_loss 9.741533 logits_loss 443.739779\n",
      "[13:700] F: 8024000 \t tot_loss 15.497602 rs_loss 1.505037 vs_loss 9.553406 logits_loss 443.915920\n",
      "[13:800] F: 8056000 \t tot_loss 16.345792 rs_loss 1.425726 vs_loss 10.475221 logits_loss 444.484475\n",
      "[13:900] F: 8088000 \t tot_loss 15.478824 rs_loss 1.095098 vs_loss 9.938964 logits_loss 444.476127\n",
      "[13:1000] F: 8120000 \t tot_loss 15.534045 rs_loss 0.975910 vs_loss 10.115186 logits_loss 444.294945\n",
      "[13:1100] F: 8152000 \t tot_loss 15.584632 rs_loss 1.078053 vs_loss 10.069881 logits_loss 443.669885\n",
      "[13:1200] F: 8184000 \t tot_loss 16.665293 rs_loss 1.432433 vs_loss 10.801858 logits_loss 443.100188\n",
      "[13:1300] F: 8216000 \t tot_loss 14.796880 rs_loss 1.118729 vs_loss 9.245542 logits_loss 443.260898\n",
      "[13:1400] F: 8248000 \t tot_loss 15.593875 rs_loss 1.262609 vs_loss 9.893998 logits_loss 443.726799\n",
      "[13:1500] F: 8280000 \t tot_loss 16.043329 rs_loss 1.266205 vs_loss 10.335874 logits_loss 444.124936\n",
      "[13:1600] F: 8312000 \t tot_loss 14.746579 rs_loss 0.950768 vs_loss 9.351093 logits_loss 444.471751\n",
      "[13:1700] F: 8344000 \t tot_loss 14.987641 rs_loss 0.939227 vs_loss 9.600014 logits_loss 444.839972\n",
      "[13:1800] F: 8376000 \t tot_loss 13.926062 rs_loss 0.734677 vs_loss 8.745855 logits_loss 444.553012\n",
      "Batch [14] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.62 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.89 (+-0.20) \n",
      "[14:0] F: 8400000 \t tot_loss 17.455683 rs_loss 1.253434 vs_loss 11.760037 logits_loss 444.221269\n",
      "[14:100] F: 8432000 \t tot_loss 17.062332 rs_loss 1.242692 vs_loss 11.377750 logits_loss 444.189131\n",
      "[14:200] F: 8464000 \t tot_loss 20.084317 rs_loss 1.779433 vs_loss 13.868940 logits_loss 443.594492\n",
      "[14:300] F: 8496000 \t tot_loss 21.578791 rs_loss 1.981690 vs_loss 15.168552 logits_loss 442.854842\n",
      "[14:400] F: 8528000 \t tot_loss 18.049702 rs_loss 1.592328 vs_loss 12.026151 logits_loss 443.122302\n",
      "[14:500] F: 8560000 \t tot_loss 18.866845 rs_loss 1.823290 vs_loss 12.609323 logits_loss 443.423243\n",
      "[14:600] F: 8592000 \t tot_loss 16.563393 rs_loss 1.554665 vs_loss 10.573397 logits_loss 443.533127\n",
      "[14:700] F: 8624000 \t tot_loss 14.235014 rs_loss 1.282700 vs_loss 8.511030 logits_loss 444.128469\n",
      "[14:800] F: 8656000 \t tot_loss 14.608721 rs_loss 1.176476 vs_loss 8.988716 logits_loss 444.352940\n",
      "[14:900] F: 8688000 \t tot_loss 14.431230 rs_loss 1.114039 vs_loss 8.879690 logits_loss 443.750160\n",
      "[14:1000] F: 8720000 \t tot_loss 13.999467 rs_loss 0.874458 vs_loss 8.685283 logits_loss 443.972539\n",
      "[14:1100] F: 8752000 \t tot_loss 14.993623 rs_loss 0.870338 vs_loss 9.679896 logits_loss 444.338909\n",
      "[14:1200] F: 8784000 \t tot_loss 14.632255 rs_loss 0.905754 vs_loss 9.278113 logits_loss 444.838810\n",
      "[14:1300] F: 8816000 \t tot_loss 16.422590 rs_loss 1.278758 vs_loss 10.694601 logits_loss 444.923022\n",
      "[14:1400] F: 8848000 \t tot_loss 16.371384 rs_loss 1.268451 vs_loss 10.659367 logits_loss 444.356565\n",
      "[14:1500] F: 8880000 \t tot_loss 16.146925 rs_loss 1.463693 vs_loss 10.249256 logits_loss 443.397622\n",
      "[14:1600] F: 8912000 \t tot_loss 14.472363 rs_loss 1.183937 vs_loss 8.851579 logits_loss 443.684675\n",
      "[14:1700] F: 8944000 \t tot_loss 16.443600 rs_loss 1.390390 vs_loss 10.613153 logits_loss 444.005629\n",
      "[14:1800] F: 8976000 \t tot_loss 18.108772 rs_loss 1.850759 vs_loss 11.812028 logits_loss 444.598484\n",
      "Batch [15] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.07 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.25 (+-0.26) \n",
      "[15:0] F: 9000000 \t tot_loss 19.586139 rs_loss 2.049366 vs_loss 13.086351 logits_loss 445.042172\n",
      "[15:100] F: 9032000 \t tot_loss 19.996291 rs_loss 2.155235 vs_loss 13.388223 logits_loss 445.283241\n",
      "[15:200] F: 9064000 \t tot_loss 17.295251 rs_loss 1.862628 vs_loss 10.982151 logits_loss 445.047232\n",
      "[15:300] F: 9096000 \t tot_loss 14.339060 rs_loss 1.320054 vs_loss 8.565852 logits_loss 445.315454\n",
      "[15:400] F: 9128000 \t tot_loss 11.902591 rs_loss 0.788640 vs_loss 6.653583 logits_loss 446.036877\n",
      "[15:500] F: 9160000 \t tot_loss 12.899129 rs_loss 0.956975 vs_loss 7.483817 logits_loss 445.833707\n",
      "[15:600] F: 9192000 \t tot_loss 12.343064 rs_loss 0.789782 vs_loss 7.088988 logits_loss 446.429424\n",
      "[15:700] F: 9224000 \t tot_loss 12.434898 rs_loss 0.758249 vs_loss 7.215436 logits_loss 446.121283\n",
      "[15:800] F: 9256000 \t tot_loss 12.441910 rs_loss 0.729569 vs_loss 7.252254 logits_loss 446.008705\n",
      "[15:900] F: 9288000 \t tot_loss 12.421894 rs_loss 0.706842 vs_loss 7.250013 logits_loss 446.503927\n",
      "[15:1000] F: 9320000 \t tot_loss 12.738549 rs_loss 0.753060 vs_loss 7.526396 logits_loss 445.909293\n",
      "[15:1100] F: 9352000 \t tot_loss 12.702670 rs_loss 0.806583 vs_loss 7.433805 logits_loss 446.228233\n",
      "[15:1200] F: 9384000 \t tot_loss 14.420769 rs_loss 1.254304 vs_loss 8.699472 logits_loss 446.699327\n",
      "[15:1300] F: 9416000 \t tot_loss 14.362465 rs_loss 1.209605 vs_loss 8.689040 logits_loss 446.382016\n",
      "[15:1400] F: 9448000 \t tot_loss 16.475228 rs_loss 1.328307 vs_loss 10.682294 logits_loss 446.462658\n",
      "[15:1500] F: 9480000 \t tot_loss 17.446112 rs_loss 1.293681 vs_loss 11.686462 logits_loss 446.596860\n",
      "[15:1600] F: 9512000 \t tot_loss 15.414407 rs_loss 0.801083 vs_loss 10.152594 logits_loss 446.073036\n",
      "[15:1700] F: 9544000 \t tot_loss 14.534353 rs_loss 0.585959 vs_loss 9.489170 logits_loss 445.922474\n",
      "[15:1800] F: 9576000 \t tot_loss 14.441517 rs_loss 0.769133 vs_loss 9.212498 logits_loss 445.988581\n",
      "Batch [16] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.93 (+-0.19) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.88 (+-0.23) \n",
      "[16:0] F: 9600000 \t tot_loss 14.250157 rs_loss 0.800896 vs_loss 8.988827 logits_loss 446.043388\n",
      "[16:100] F: 9632000 \t tot_loss 13.466573 rs_loss 0.809385 vs_loss 8.203211 logits_loss 445.397697\n",
      "[16:200] F: 9664000 \t tot_loss 15.775005 rs_loss 1.034773 vs_loss 10.293047 logits_loss 444.718511\n",
      "[16:300] F: 9696000 \t tot_loss 17.661037 rs_loss 1.357116 vs_loss 11.857774 logits_loss 444.614651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:400] F: 9728000 \t tot_loss 15.862046 rs_loss 0.957397 vs_loss 10.469704 logits_loss 443.494552\n",
      "[16:500] F: 9760000 \t tot_loss 16.341551 rs_loss 0.988442 vs_loss 10.916370 logits_loss 443.673836\n",
      "[16:600] F: 9792000 \t tot_loss 15.459333 rs_loss 0.829924 vs_loss 10.189698 logits_loss 443.971120\n",
      "[16:700] F: 9824000 \t tot_loss 13.560743 rs_loss 0.522828 vs_loss 8.602854 logits_loss 443.506204\n",
      "[16:800] F: 9856000 \t tot_loss 13.262928 rs_loss 0.561934 vs_loss 8.265752 logits_loss 443.524235\n",
      "[16:900] F: 9888000 \t tot_loss 14.633675 rs_loss 0.788739 vs_loss 9.410243 logits_loss 443.469303\n",
      "[16:1000] F: 9920000 \t tot_loss 13.747763 rs_loss 0.754477 vs_loss 8.560747 logits_loss 443.253942\n",
      "[16:1100] F: 9952000 \t tot_loss 14.038305 rs_loss 0.761355 vs_loss 8.837389 logits_loss 443.956110\n",
      "[16:1200] F: 9984000 \t tot_loss 14.042110 rs_loss 0.741323 vs_loss 8.857599 logits_loss 444.318883\n",
      "[16:1300] F: 10016000 \t tot_loss 12.923177 rs_loss 0.644109 vs_loss 7.834534 logits_loss 444.453405\n",
      "[16:1400] F: 10048000 \t tot_loss 13.573670 rs_loss 0.634188 vs_loss 8.492935 logits_loss 444.654674\n",
      "[16:1500] F: 10080000 \t tot_loss 14.939615 rs_loss 0.654780 vs_loss 9.845890 logits_loss 443.894545\n",
      "[16:1600] F: 10112000 \t tot_loss 14.863822 rs_loss 0.654333 vs_loss 9.775247 logits_loss 443.424192\n",
      "[16:1700] F: 10144000 \t tot_loss 14.859957 rs_loss 0.563206 vs_loss 9.861675 logits_loss 443.507626\n",
      "[16:1800] F: 10176000 \t tot_loss 14.101815 rs_loss 0.533984 vs_loss 9.139273 logits_loss 442.855845\n",
      "Batch [17] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.11 (+-0.27) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.98 (+-0.20) \n",
      "[17:0] F: 10200000 \t tot_loss 12.383663 rs_loss 0.530858 vs_loss 7.420955 logits_loss 443.184896\n",
      "[17:100] F: 10232000 \t tot_loss 13.069064 rs_loss 0.694793 vs_loss 7.939411 logits_loss 443.486031\n",
      "[17:200] F: 10264000 \t tot_loss 17.075399 rs_loss 1.343369 vs_loss 11.294808 logits_loss 443.722262\n",
      "[17:300] F: 10296000 \t tot_loss 16.065087 rs_loss 1.289258 vs_loss 10.338157 logits_loss 443.767154\n",
      "[17:400] F: 10328000 \t tot_loss 17.760882 rs_loss 1.535239 vs_loss 11.784151 logits_loss 444.149290\n",
      "[17:500] F: 10360000 \t tot_loss 17.727550 rs_loss 1.381143 vs_loss 11.905424 logits_loss 444.098263\n",
      "[17:600] F: 10392000 \t tot_loss 18.324878 rs_loss 1.640677 vs_loss 12.239938 logits_loss 444.426300\n",
      "[17:700] F: 10424000 \t tot_loss 18.399538 rs_loss 1.626771 vs_loss 12.324495 logits_loss 444.827223\n",
      "[17:800] F: 10456000 \t tot_loss 17.103693 rs_loss 1.368791 vs_loss 11.291873 logits_loss 444.302936\n",
      "[17:900] F: 10488000 \t tot_loss 18.094497 rs_loss 1.591179 vs_loss 12.054623 logits_loss 444.869581\n",
      "[17:1000] F: 10520000 \t tot_loss 13.722000 rs_loss 0.742880 vs_loss 8.533154 logits_loss 444.596583\n",
      "[17:1100] F: 10552000 \t tot_loss 16.816112 rs_loss 1.132393 vs_loss 11.241807 logits_loss 444.191174\n",
      "[17:1200] F: 10584000 \t tot_loss 16.814601 rs_loss 1.117513 vs_loss 11.251734 logits_loss 444.535383\n",
      "[17:1300] F: 10616000 \t tot_loss 16.347120 rs_loss 0.894229 vs_loss 11.009488 logits_loss 444.340238\n",
      "[17:1400] F: 10648000 \t tot_loss 16.194670 rs_loss 0.857016 vs_loss 10.894556 logits_loss 444.309871\n",
      "[17:1500] F: 10680000 \t tot_loss 13.302764 rs_loss 0.470858 vs_loss 8.378852 logits_loss 445.305382\n",
      "[17:1600] F: 10712000 \t tot_loss 13.561987 rs_loss 0.650188 vs_loss 8.455458 logits_loss 445.634194\n",
      "[17:1700] F: 10744000 \t tot_loss 12.792662 rs_loss 0.657073 vs_loss 7.679917 logits_loss 445.567242\n",
      "[17:1800] F: 10776000 \t tot_loss 13.536570 rs_loss 0.683858 vs_loss 8.395152 logits_loss 445.756077\n",
      "Batch [18] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.85 (+-0.19) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.71 (+-0.20) \n",
      "[18:0] F: 10800000 \t tot_loss 13.761542 rs_loss 0.679282 vs_loss 8.630881 logits_loss 445.137924\n",
      "[18:100] F: 10832000 \t tot_loss 13.641979 rs_loss 0.564237 vs_loss 8.630781 logits_loss 444.696068\n",
      "[18:200] F: 10864000 \t tot_loss 14.281325 rs_loss 0.835630 vs_loss 9.001865 logits_loss 444.382976\n",
      "[18:300] F: 10896000 \t tot_loss 15.548226 rs_loss 1.091998 vs_loss 10.012356 logits_loss 444.387248\n",
      "[18:400] F: 10928000 \t tot_loss 15.936441 rs_loss 1.322823 vs_loss 10.166973 logits_loss 444.664520\n",
      "[18:500] F: 10960000 \t tot_loss 17.490910 rs_loss 1.588334 vs_loss 11.454371 logits_loss 444.820461\n",
      "[18:600] F: 10992000 \t tot_loss 18.258752 rs_loss 1.751671 vs_loss 12.062720 logits_loss 444.436064\n",
      "[18:700] F: 11024000 \t tot_loss 18.763521 rs_loss 1.910803 vs_loss 12.407977 logits_loss 444.474048\n",
      "[18:800] F: 11056000 \t tot_loss 19.304446 rs_loss 1.946030 vs_loss 12.918919 logits_loss 443.949688\n",
      "[18:900] F: 11088000 \t tot_loss 18.144889 rs_loss 1.760833 vs_loss 11.947944 logits_loss 443.611197\n",
      "[18:1000] F: 11120000 \t tot_loss 15.939161 rs_loss 1.290899 vs_loss 10.210166 logits_loss 443.809625\n",
      "[18:1100] F: 11152000 \t tot_loss 13.280673 rs_loss 0.826834 vs_loss 8.015466 logits_loss 443.837255\n",
      "[18:1200] F: 11184000 \t tot_loss 12.860089 rs_loss 0.787122 vs_loss 7.631515 logits_loss 444.145194\n",
      "[18:1300] F: 11216000 \t tot_loss 14.190301 rs_loss 1.296286 vs_loss 8.450565 logits_loss 444.345085\n",
      "[18:1400] F: 11248000 \t tot_loss 14.741187 rs_loss 1.317887 vs_loss 8.978137 logits_loss 444.516293\n",
      "[18:1500] F: 11280000 \t tot_loss 15.458950 rs_loss 1.560658 vs_loss 9.451656 logits_loss 444.663641\n",
      "[18:1600] F: 11312000 \t tot_loss 17.514193 rs_loss 1.746707 vs_loss 11.315650 logits_loss 445.183638\n",
      "[18:1700] F: 11344000 \t tot_loss 17.513662 rs_loss 1.452328 vs_loss 11.609119 logits_loss 445.221484\n",
      "[18:1800] F: 11376000 \t tot_loss 18.916834 rs_loss 1.679544 vs_loss 12.783487 logits_loss 445.380304\n",
      "Batch [19] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.01 (+-0.24) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.86 (+-0.23) \n",
      "[19:0] F: 11400000 \t tot_loss 18.482026 rs_loss 1.483213 vs_loss 12.541884 logits_loss 445.692934\n",
      "[19:100] F: 11432000 \t tot_loss 16.374919 rs_loss 1.226413 vs_loss 10.697984 logits_loss 445.052239\n",
      "[19:200] F: 11464000 \t tot_loss 15.823604 rs_loss 1.236260 vs_loss 10.141171 logits_loss 444.617206\n",
      "[19:300] F: 11496000 \t tot_loss 15.393076 rs_loss 1.256276 vs_loss 9.692584 logits_loss 444.421626\n",
      "[19:400] F: 11528000 \t tot_loss 13.968957 rs_loss 1.050034 vs_loss 8.477281 logits_loss 444.164246\n",
      "[19:500] F: 11560000 \t tot_loss 14.011811 rs_loss 1.055244 vs_loss 8.517068 logits_loss 443.949891\n",
      "[19:600] F: 11592000 \t tot_loss 12.791455 rs_loss 0.785787 vs_loss 7.563904 logits_loss 444.176292\n",
      "[19:700] F: 11624000 \t tot_loss 12.755672 rs_loss 0.806800 vs_loss 7.506660 logits_loss 444.221243\n",
      "[19:800] F: 11656000 \t tot_loss 12.641151 rs_loss 0.765170 vs_loss 7.432287 logits_loss 444.369521\n",
      "[19:900] F: 11688000 \t tot_loss 12.567885 rs_loss 0.770277 vs_loss 7.352794 logits_loss 444.481470\n",
      "[19:1000] F: 11720000 \t tot_loss 13.710940 rs_loss 0.964461 vs_loss 8.300332 logits_loss 444.614630\n",
      "[19:1100] F: 11752000 \t tot_loss 13.918445 rs_loss 0.970862 vs_loss 8.502354 logits_loss 444.523006\n",
      "[19:1200] F: 11784000 \t tot_loss 15.723312 rs_loss 1.005468 vs_loss 10.273790 logits_loss 444.405434\n",
      "[19:1300] F: 11816000 \t tot_loss 15.490467 rs_loss 1.001089 vs_loss 10.044743 logits_loss 444.463567\n",
      "[19:1400] F: 11848000 \t tot_loss 14.525705 rs_loss 0.806426 vs_loss 9.271894 logits_loss 444.738526\n",
      "[19:1500] F: 11880000 \t tot_loss 14.769633 rs_loss 0.747321 vs_loss 9.580625 logits_loss 444.168768\n",
      "[19:1600] F: 11912000 \t tot_loss 13.112255 rs_loss 0.890045 vs_loss 7.778568 logits_loss 444.364225\n",
      "[19:1700] F: 11944000 \t tot_loss 13.418772 rs_loss 0.898700 vs_loss 8.076282 logits_loss 444.379006\n",
      "[19:1800] F: 11976000 \t tot_loss 14.627125 rs_loss 1.289503 vs_loss 8.892157 logits_loss 444.546433\n",
      "Batch [20] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.36 (+-0.31) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.62 (+-0.16) \n",
      "[20:0] F: 12000000 \t tot_loss 13.930903 rs_loss 1.233738 vs_loss 8.244713 logits_loss 445.245171\n",
      "[20:100] F: 12032000 \t tot_loss 13.968494 rs_loss 1.003424 vs_loss 8.516203 logits_loss 444.886795\n",
      "[20:200] F: 12064000 \t tot_loss 15.645742 rs_loss 1.212902 vs_loss 9.979269 logits_loss 445.357152\n",
      "[20:300] F: 12096000 \t tot_loss 15.374987 rs_loss 0.859915 vs_loss 10.062834 logits_loss 445.223839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:400] F: 12128000 \t tot_loss 16.174898 rs_loss 1.152071 vs_loss 10.567517 logits_loss 445.531034\n",
      "[20:500] F: 12160000 \t tot_loss 16.748454 rs_loss 1.372271 vs_loss 10.919778 logits_loss 445.640463\n",
      "[20:600] F: 12192000 \t tot_loss 15.673618 rs_loss 1.056761 vs_loss 10.160173 logits_loss 445.668401\n",
      "[20:700] F: 12224000 \t tot_loss 16.575764 rs_loss 1.283534 vs_loss 10.838039 logits_loss 445.419085\n",
      "[20:800] F: 12256000 \t tot_loss 16.472525 rs_loss 1.045582 vs_loss 10.975231 logits_loss 445.171197\n",
      "[20:900] F: 12288000 \t tot_loss 17.082335 rs_loss 0.976997 vs_loss 11.649559 logits_loss 445.577893\n",
      "[20:1000] F: 12320000 \t tot_loss 17.428450 rs_loss 0.985555 vs_loss 11.985635 logits_loss 445.726077\n",
      "[20:1100] F: 12352000 \t tot_loss 16.524490 rs_loss 0.734344 vs_loss 11.331999 logits_loss 445.814709\n",
      "[20:1200] F: 12384000 \t tot_loss 18.529669 rs_loss 0.964916 vs_loss 13.103749 logits_loss 446.100429\n",
      "[20:1300] F: 12416000 \t tot_loss 18.101684 rs_loss 0.788161 vs_loss 12.856773 logits_loss 445.675033\n",
      "[20:1400] F: 12448000 \t tot_loss 17.537857 rs_loss 0.742298 vs_loss 12.336835 logits_loss 445.872369\n",
      "[20:1500] F: 12480000 \t tot_loss 19.759614 rs_loss 1.165656 vs_loss 14.132711 logits_loss 446.124761\n",
      "[20:1600] F: 12512000 \t tot_loss 19.215982 rs_loss 0.932739 vs_loss 13.823493 logits_loss 445.975000\n",
      "[20:1700] F: 12544000 \t tot_loss 21.014310 rs_loss 1.466965 vs_loss 15.085009 logits_loss 446.233598\n",
      "[20:1800] F: 12576000 \t tot_loss 23.137839 rs_loss 1.808529 vs_loss 16.871435 logits_loss 445.787575\n",
      "Batch [21] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.98 (+-0.19) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.45 (+-0.34) \n",
      "[21:0] F: 12600000 \t tot_loss 24.005901 rs_loss 1.942504 vs_loss 17.603713 logits_loss 445.968472\n",
      "[21:100] F: 12632000 \t tot_loss 20.915648 rs_loss 1.632342 vs_loss 14.825852 logits_loss 445.745367\n",
      "[21:200] F: 12664000 \t tot_loss 16.678258 rs_loss 1.111605 vs_loss 11.111008 logits_loss 445.564483\n",
      "[21:300] F: 12696000 \t tot_loss 18.360177 rs_loss 1.718830 vs_loss 12.190765 logits_loss 445.058197\n",
      "[21:400] F: 12728000 \t tot_loss 15.924328 rs_loss 1.477470 vs_loss 9.999008 logits_loss 444.784977\n",
      "[21:500] F: 12760000 \t tot_loss 16.052010 rs_loss 1.643657 vs_loss 9.963424 logits_loss 444.492924\n",
      "[21:600] F: 12792000 \t tot_loss 16.949673 rs_loss 1.663445 vs_loss 10.843073 logits_loss 444.315547\n",
      "[21:700] F: 12824000 \t tot_loss 14.472093 rs_loss 1.042869 vs_loss 8.981513 logits_loss 444.771172\n",
      "[21:800] F: 12856000 \t tot_loss 13.889688 rs_loss 0.868663 vs_loss 8.575253 logits_loss 444.577239\n",
      "[21:900] F: 12888000 \t tot_loss 15.377999 rs_loss 0.896022 vs_loss 10.029214 logits_loss 445.276311\n",
      "[21:1000] F: 12920000 \t tot_loss 14.396088 rs_loss 0.950702 vs_loss 8.993149 logits_loss 445.223765\n",
      "[21:1100] F: 12952000 \t tot_loss 14.770098 rs_loss 0.987713 vs_loss 9.337324 logits_loss 444.506023\n",
      "[21:1200] F: 12984000 \t tot_loss 13.175100 rs_loss 0.792060 vs_loss 7.938466 logits_loss 444.457448\n",
      "[21:1300] F: 13016000 \t tot_loss 11.677647 rs_loss 0.614577 vs_loss 6.624231 logits_loss 443.883996\n",
      "[21:1400] F: 13048000 \t tot_loss 11.666898 rs_loss 0.531679 vs_loss 6.694425 logits_loss 444.079434\n",
      "[21:1500] F: 13080000 \t tot_loss 14.015873 rs_loss 1.177283 vs_loss 8.396821 logits_loss 444.176922\n",
      "[21:1600] F: 13112000 \t tot_loss 15.496308 rs_loss 1.277296 vs_loss 9.775796 logits_loss 444.321705\n",
      "[21:1700] F: 13144000 \t tot_loss 15.439865 rs_loss 1.341447 vs_loss 9.655984 logits_loss 444.243392\n",
      "[21:1800] F: 13176000 \t tot_loss 16.901729 rs_loss 1.610341 vs_loss 10.847932 logits_loss 444.345627\n",
      "Batch [22] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.00 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.02 (+-0.27) \n",
      "[22:0] F: 13200000 \t tot_loss 14.277880 rs_loss 0.929352 vs_loss 8.905743 logits_loss 444.278455\n",
      "[22:100] F: 13232000 \t tot_loss 14.063836 rs_loss 0.869359 vs_loss 8.755644 logits_loss 443.883323\n",
      "[22:200] F: 13264000 \t tot_loss 14.910992 rs_loss 0.849060 vs_loss 9.619095 logits_loss 444.283667\n",
      "[22:300] F: 13296000 \t tot_loss 13.739272 rs_loss 0.601465 vs_loss 8.691806 logits_loss 444.600103\n",
      "[22:400] F: 13328000 \t tot_loss 15.042919 rs_loss 0.684996 vs_loss 9.913474 logits_loss 444.444838\n",
      "[22:500] F: 13360000 \t tot_loss 15.094938 rs_loss 0.883557 vs_loss 9.763890 logits_loss 444.749062\n",
      "[22:600] F: 13392000 \t tot_loss 16.232938 rs_loss 0.905236 vs_loss 10.884536 logits_loss 444.316591\n",
      "[22:700] F: 13424000 \t tot_loss 16.426155 rs_loss 0.878784 vs_loss 11.105887 logits_loss 444.148395\n",
      "[22:800] F: 13456000 \t tot_loss 16.009247 rs_loss 0.832221 vs_loss 10.733047 logits_loss 444.397915\n",
      "[22:900] F: 13488000 \t tot_loss 15.224916 rs_loss 0.572696 vs_loss 10.209817 logits_loss 444.240280\n",
      "[22:1000] F: 13520000 \t tot_loss 13.821400 rs_loss 0.506289 vs_loss 8.872640 logits_loss 444.247180\n",
      "[22:1100] F: 13552000 \t tot_loss 13.977747 rs_loss 0.518572 vs_loss 9.015483 logits_loss 444.369303\n",
      "[22:1200] F: 13584000 \t tot_loss 14.878780 rs_loss 0.521558 vs_loss 9.914681 logits_loss 444.254004\n",
      "[22:1300] F: 13616000 \t tot_loss 14.345143 rs_loss 0.506586 vs_loss 9.396196 logits_loss 444.236100\n",
      "[22:1400] F: 13648000 \t tot_loss 14.859965 rs_loss 0.536978 vs_loss 9.875122 logits_loss 444.786439\n",
      "[22:1500] F: 13680000 \t tot_loss 16.318605 rs_loss 0.579020 vs_loss 11.295070 logits_loss 444.451529\n",
      "[22:1600] F: 13712000 \t tot_loss 15.817042 rs_loss 0.597801 vs_loss 10.778083 logits_loss 444.115892\n",
      "[22:1700] F: 13744000 \t tot_loss 16.190706 rs_loss 0.638736 vs_loss 11.107221 logits_loss 444.474914\n",
      "[22:1800] F: 13776000 \t tot_loss 16.417520 rs_loss 0.610044 vs_loss 11.365865 logits_loss 444.161085\n",
      "Batch [23] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.13 (+-0.29) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.14 (+-0.27) \n",
      "[23:0] F: 13800000 \t tot_loss 14.832849 rs_loss 0.604186 vs_loss 9.784340 logits_loss 444.432294\n",
      "[23:100] F: 13832000 \t tot_loss 15.778535 rs_loss 1.098795 vs_loss 10.231491 logits_loss 444.824797\n",
      "[23:200] F: 13864000 \t tot_loss 15.275609 rs_loss 1.051634 vs_loss 9.772311 logits_loss 445.166376\n",
      "[23:300] F: 13896000 \t tot_loss 15.791014 rs_loss 1.061800 vs_loss 10.276293 logits_loss 445.292207\n",
      "[23:400] F: 13928000 \t tot_loss 15.562419 rs_loss 0.992624 vs_loss 10.115601 logits_loss 445.419367\n",
      "[23:500] F: 13960000 \t tot_loss 14.350215 rs_loss 0.612473 vs_loss 9.285044 logits_loss 445.269955\n",
      "[23:600] F: 13992000 \t tot_loss 14.124006 rs_loss 0.582794 vs_loss 9.093215 logits_loss 444.799761\n",
      "[23:700] F: 14024000 \t tot_loss 12.729658 rs_loss 0.591361 vs_loss 7.689696 logits_loss 444.860142\n",
      "[23:800] F: 14056000 \t tot_loss 12.756309 rs_loss 0.629302 vs_loss 7.678219 logits_loss 444.878869\n",
      "[23:900] F: 14088000 \t tot_loss 12.099432 rs_loss 0.563769 vs_loss 7.087056 logits_loss 444.860784\n",
      "[23:1000] F: 14120000 \t tot_loss 12.109509 rs_loss 0.634364 vs_loss 7.024121 logits_loss 445.102313\n",
      "[23:1100] F: 14152000 \t tot_loss 12.162053 rs_loss 0.615517 vs_loss 7.097258 logits_loss 444.927866\n",
      "[23:1200] F: 14184000 \t tot_loss 12.257459 rs_loss 0.611145 vs_loss 7.202363 logits_loss 444.395108\n",
      "[23:1300] F: 14216000 \t tot_loss 13.221684 rs_loss 0.790380 vs_loss 7.989238 logits_loss 444.206621\n",
      "[23:1400] F: 14248000 \t tot_loss 15.448276 rs_loss 1.023771 vs_loss 9.981476 logits_loss 444.302967\n",
      "[23:1500] F: 14280000 \t tot_loss 16.376556 rs_loss 1.250232 vs_loss 10.687098 logits_loss 443.922656\n",
      "[23:1600] F: 14312000 \t tot_loss 16.058650 rs_loss 1.559054 vs_loss 10.056444 logits_loss 444.315189\n",
      "[23:1700] F: 14344000 \t tot_loss 15.409515 rs_loss 1.411871 vs_loss 9.552106 logits_loss 444.553783\n",
      "[23:1800] F: 14376000 \t tot_loss 14.727863 rs_loss 1.184435 vs_loss 9.098058 logits_loss 444.536971\n",
      "Batch [24] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.83 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.99 (+-0.24) \n",
      "[24:0] F: 14400000 \t tot_loss 14.338520 rs_loss 1.033915 vs_loss 8.858841 logits_loss 444.576370\n",
      "[24:100] F: 14432000 \t tot_loss 15.493081 rs_loss 1.122649 vs_loss 9.929731 logits_loss 444.070188\n",
      "[24:200] F: 14464000 \t tot_loss 15.982082 rs_loss 0.757985 vs_loss 10.787778 logits_loss 443.631933\n",
      "[24:300] F: 14496000 \t tot_loss 15.966976 rs_loss 0.760448 vs_loss 10.772525 logits_loss 443.400329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24:400] F: 14528000 \t tot_loss 15.762354 rs_loss 0.727878 vs_loss 10.600810 logits_loss 443.366570\n",
      "[24:500] F: 14560000 \t tot_loss 15.260044 rs_loss 0.611803 vs_loss 10.211038 logits_loss 443.720297\n",
      "[24:600] F: 14592000 \t tot_loss 17.800912 rs_loss 1.060551 vs_loss 12.303312 logits_loss 443.704897\n",
      "[24:700] F: 14624000 \t tot_loss 18.587081 rs_loss 1.576883 vs_loss 12.572245 logits_loss 443.795293\n",
      "[24:800] F: 14656000 \t tot_loss 22.822903 rs_loss 2.655522 vs_loss 15.729669 logits_loss 443.771222\n",
      "[24:900] F: 14688000 \t tot_loss 24.166384 rs_loss 2.786600 vs_loss 16.941675 logits_loss 443.810954\n",
      "[24:1000] F: 14720000 \t tot_loss 23.057169 rs_loss 2.609710 vs_loss 16.013698 logits_loss 443.376145\n",
      "[24:1100] F: 14752000 \t tot_loss 21.519476 rs_loss 1.999378 vs_loss 15.088113 logits_loss 443.198559\n",
      "[24:1200] F: 14784000 \t tot_loss 18.999277 rs_loss 1.173263 vs_loss 13.393374 logits_loss 443.264072\n",
      "[24:1300] F: 14816000 \t tot_loss 16.999544 rs_loss 1.033095 vs_loss 11.534512 logits_loss 443.193770\n",
      "[24:1400] F: 14848000 \t tot_loss 20.590271 rs_loss 1.715579 vs_loss 14.437308 logits_loss 443.738393\n",
      "[24:1500] F: 14880000 \t tot_loss 21.169992 rs_loss 1.882705 vs_loss 14.848538 logits_loss 443.874912\n",
      "[24:1600] F: 14912000 \t tot_loss 22.718198 rs_loss 1.993224 vs_loss 16.289165 logits_loss 443.580925\n",
      "[24:1700] F: 14944000 \t tot_loss 24.060926 rs_loss 2.165262 vs_loss 17.461707 logits_loss 443.395817\n",
      "[24:1800] F: 14976000 \t tot_loss 19.813620 rs_loss 1.560055 vs_loss 13.815967 logits_loss 443.759718\n",
      "Batch [25] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.91 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.93 (+-0.23) \n",
      "[25:0] F: 15000000 \t tot_loss 21.772491 rs_loss 1.767270 vs_loss 15.569500 logits_loss 443.572106\n",
      "[25:100] F: 15032000 \t tot_loss 18.084005 rs_loss 1.492464 vs_loss 12.141537 logits_loss 445.000367\n",
      "[25:200] F: 15064000 \t tot_loss 15.410238 rs_loss 1.035019 vs_loss 9.916493 logits_loss 445.872676\n",
      "[25:300] F: 15096000 \t tot_loss 16.787778 rs_loss 1.114155 vs_loss 11.208030 logits_loss 446.559234\n",
      "[25:400] F: 15128000 \t tot_loss 14.328682 rs_loss 0.833382 vs_loss 9.018313 logits_loss 447.698740\n",
      "[25:500] F: 15160000 \t tot_loss 15.867045 rs_loss 1.095617 vs_loss 10.298053 logits_loss 447.337457\n",
      "[25:600] F: 15192000 \t tot_loss 17.199933 rs_loss 1.420077 vs_loss 11.307639 logits_loss 447.221676\n",
      "[25:700] F: 15224000 \t tot_loss 15.576932 rs_loss 1.171830 vs_loss 9.931315 logits_loss 447.378690\n",
      "[25:800] F: 15256000 \t tot_loss 18.230669 rs_loss 1.626773 vs_loss 12.134774 logits_loss 446.912174\n",
      "[25:900] F: 15288000 \t tot_loss 16.225382 rs_loss 1.362455 vs_loss 10.393473 logits_loss 446.945446\n",
      "[25:1000] F: 15320000 \t tot_loss 14.349946 rs_loss 1.010215 vs_loss 8.870394 logits_loss 446.933664\n",
      "[25:1100] F: 15352000 \t tot_loss 13.787133 rs_loss 0.946260 vs_loss 8.366604 logits_loss 447.426800\n",
      "[25:1200] F: 15384000 \t tot_loss 11.950452 rs_loss 0.753637 vs_loss 6.716519 logits_loss 448.029733\n",
      "[25:1300] F: 15416000 \t tot_loss 12.676703 rs_loss 0.777561 vs_loss 7.418501 logits_loss 448.063997\n",
      "[25:1400] F: 15448000 \t tot_loss 14.704148 rs_loss 1.136738 vs_loss 9.083544 logits_loss 448.386620\n",
      "[25:1500] F: 15480000 \t tot_loss 15.719418 rs_loss 1.158821 vs_loss 10.081041 logits_loss 447.955547\n",
      "[25:1600] F: 15512000 \t tot_loss 16.698098 rs_loss 1.180826 vs_loss 11.040861 logits_loss 447.640999\n",
      "[25:1700] F: 15544000 \t tot_loss 19.817841 rs_loss 1.689067 vs_loss 13.654243 logits_loss 447.453068\n",
      "[25:1800] F: 15576000 \t tot_loss 19.641073 rs_loss 1.867545 vs_loss 13.302632 logits_loss 447.089706\n",
      "Batch [26] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.54 (+-0.31) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.19 (+-0.30) \n",
      "[26:0] F: 15600000 \t tot_loss 19.404751 rs_loss 1.894348 vs_loss 13.038731 logits_loss 447.167211\n",
      "[26:100] F: 15632000 \t tot_loss 19.695084 rs_loss 1.945625 vs_loss 13.281691 logits_loss 446.776859\n",
      "[26:200] F: 15664000 \t tot_loss 17.923570 rs_loss 1.662972 vs_loss 11.796132 logits_loss 446.446626\n",
      "[26:300] F: 15696000 \t tot_loss 16.009657 rs_loss 1.372263 vs_loss 10.182918 logits_loss 445.447587\n",
      "[26:400] F: 15728000 \t tot_loss 15.698465 rs_loss 1.362512 vs_loss 9.887738 logits_loss 444.821525\n",
      "[26:500] F: 15760000 \t tot_loss 17.308104 rs_loss 1.801237 vs_loss 11.060375 logits_loss 444.649227\n",
      "[26:600] F: 15792000 \t tot_loss 16.118477 rs_loss 1.516270 vs_loss 10.158916 logits_loss 444.328995\n",
      "[26:700] F: 15824000 \t tot_loss 16.591774 rs_loss 1.626536 vs_loss 10.522167 logits_loss 444.307077\n",
      "[26:800] F: 15856000 \t tot_loss 15.679067 rs_loss 1.121754 vs_loss 10.114020 logits_loss 444.329218\n",
      "[26:900] F: 15888000 \t tot_loss 13.316197 rs_loss 0.636103 vs_loss 8.237341 logits_loss 444.275359\n",
      "[26:1000] F: 15920000 \t tot_loss 13.736977 rs_loss 0.898100 vs_loss 8.394594 logits_loss 444.428352\n",
      "[26:1100] F: 15952000 \t tot_loss 14.304684 rs_loss 1.050293 vs_loss 8.806798 logits_loss 444.759321\n",
      "[26:1200] F: 15984000 \t tot_loss 15.536301 rs_loss 1.424317 vs_loss 9.662624 logits_loss 444.936041\n",
      "[26:1300] F: 16016000 \t tot_loss 16.942668 rs_loss 1.561348 vs_loss 10.933349 logits_loss 444.797019\n",
      "[26:1400] F: 16048000 \t tot_loss 16.865932 rs_loss 1.340881 vs_loss 11.075756 logits_loss 444.929462\n",
      "[26:1500] F: 16080000 \t tot_loss 15.680512 rs_loss 1.124436 vs_loss 10.103071 logits_loss 445.300483\n",
      "[26:1600] F: 16112000 \t tot_loss 15.068164 rs_loss 0.820156 vs_loss 9.802042 logits_loss 444.596626\n",
      "[26:1700] F: 16144000 \t tot_loss 15.161095 rs_loss 1.053445 vs_loss 9.660161 logits_loss 444.748905\n",
      "[26:1800] F: 16176000 \t tot_loss 14.698100 rs_loss 1.031947 vs_loss 9.218161 logits_loss 444.799231\n",
      "Batch [27] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.72 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.75 (+-0.20) \n",
      "[27:0] F: 16200000 \t tot_loss 14.721066 rs_loss 0.996441 vs_loss 9.282278 logits_loss 444.234741\n",
      "[27:100] F: 16232000 \t tot_loss 15.045267 rs_loss 0.989692 vs_loss 9.611122 logits_loss 444.445352\n",
      "[27:200] F: 16264000 \t tot_loss 16.189826 rs_loss 0.897373 vs_loss 10.852248 logits_loss 444.020488\n",
      "[27:300] F: 16296000 \t tot_loss 15.888024 rs_loss 0.906694 vs_loss 10.546248 logits_loss 443.508277\n",
      "[27:400] F: 16328000 \t tot_loss 16.228900 rs_loss 0.931795 vs_loss 10.862694 logits_loss 443.441174\n",
      "[27:500] F: 16360000 \t tot_loss 14.975412 rs_loss 0.935953 vs_loss 9.603224 logits_loss 443.623496\n",
      "[27:600] F: 16392000 \t tot_loss 14.064976 rs_loss 0.929309 vs_loss 8.698982 logits_loss 443.668475\n",
      "[27:700] F: 16424000 \t tot_loss 12.256734 rs_loss 0.664955 vs_loss 7.151597 logits_loss 444.018274\n",
      "[27:800] F: 16456000 \t tot_loss 12.057468 rs_loss 0.629134 vs_loss 6.984555 logits_loss 444.377830\n",
      "[27:900] F: 16488000 \t tot_loss 12.077264 rs_loss 0.521748 vs_loss 7.115651 logits_loss 443.986516\n",
      "[27:1000] F: 16520000 \t tot_loss 11.945980 rs_loss 0.487426 vs_loss 7.016861 logits_loss 444.169316\n",
      "[27:1100] F: 16552000 \t tot_loss 13.837301 rs_loss 0.768004 vs_loss 8.627654 logits_loss 444.164240\n",
      "[27:1200] F: 16584000 \t tot_loss 13.944953 rs_loss 0.786822 vs_loss 8.712925 logits_loss 444.520650\n",
      "[27:1300] F: 16616000 \t tot_loss 13.593026 rs_loss 0.793332 vs_loss 8.352552 logits_loss 444.714197\n",
      "[27:1400] F: 16648000 \t tot_loss 15.216985 rs_loss 1.044221 vs_loss 9.724540 logits_loss 444.822393\n",
      "[27:1500] F: 16680000 \t tot_loss 14.909448 rs_loss 0.772591 vs_loss 9.684096 logits_loss 445.276114\n",
      "[27:1600] F: 16712000 \t tot_loss 15.483713 rs_loss 0.754985 vs_loss 10.283970 logits_loss 444.475848\n",
      "[27:1700] F: 16744000 \t tot_loss 15.560351 rs_loss 0.789398 vs_loss 10.321488 logits_loss 444.946463\n",
      "[27:1800] F: 16776000 \t tot_loss 15.163216 rs_loss 0.863301 vs_loss 9.851842 logits_loss 444.807266\n",
      "Batch [28] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.80 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.68 (+-0.16) \n",
      "[28:0] F: 16800000 \t tot_loss 14.675144 rs_loss 0.889286 vs_loss 9.339417 logits_loss 444.644060\n",
      "[28:100] F: 16832000 \t tot_loss 14.343385 rs_loss 0.883393 vs_loss 9.015130 logits_loss 444.486143\n",
      "[28:200] F: 16864000 \t tot_loss 16.027640 rs_loss 1.125979 vs_loss 10.461133 logits_loss 444.052826\n",
      "[28:300] F: 16896000 \t tot_loss 14.649452 rs_loss 0.776189 vs_loss 9.432634 logits_loss 444.062857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28:400] F: 16928000 \t tot_loss 15.352701 rs_loss 0.724891 vs_loss 10.185083 logits_loss 444.272653\n",
      "[28:500] F: 16960000 \t tot_loss 18.299600 rs_loss 1.282773 vs_loss 12.573818 logits_loss 444.300905\n",
      "[28:600] F: 16992000 \t tot_loss 17.345650 rs_loss 1.280094 vs_loss 11.621226 logits_loss 444.432984\n",
      "[28:700] F: 17024000 \t tot_loss 18.707971 rs_loss 1.398339 vs_loss 12.865154 logits_loss 444.447890\n",
      "[28:800] F: 17056000 \t tot_loss 17.883867 rs_loss 1.454611 vs_loss 11.993010 logits_loss 443.624521\n",
      "[28:900] F: 17088000 \t tot_loss 14.951602 rs_loss 1.062149 vs_loss 9.450222 logits_loss 443.923176\n",
      "[28:1000] F: 17120000 \t tot_loss 16.304252 rs_loss 1.165747 vs_loss 10.698069 logits_loss 444.043566\n",
      "[28:1100] F: 17152000 \t tot_loss 16.403356 rs_loss 1.188718 vs_loss 10.772021 logits_loss 444.261696\n",
      "[28:1200] F: 17184000 \t tot_loss 16.560559 rs_loss 1.108846 vs_loss 11.003324 logits_loss 444.838860\n",
      "[28:1300] F: 17216000 \t tot_loss 21.257740 rs_loss 1.712202 vs_loss 15.101262 logits_loss 444.427653\n",
      "[28:1400] F: 17248000 \t tot_loss 19.314890 rs_loss 1.433620 vs_loss 13.437506 logits_loss 444.376410\n",
      "[28:1500] F: 17280000 \t tot_loss 18.759462 rs_loss 1.345851 vs_loss 12.970637 logits_loss 444.297402\n",
      "[28:1600] F: 17312000 \t tot_loss 20.761052 rs_loss 1.902640 vs_loss 14.418614 logits_loss 443.979813\n",
      "[28:1700] F: 17344000 \t tot_loss 15.871251 rs_loss 1.136416 vs_loss 10.286674 logits_loss 444.816077\n",
      "[28:1800] F: 17376000 \t tot_loss 16.094647 rs_loss 1.064609 vs_loss 10.583303 logits_loss 444.673449\n",
      "Batch [29] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.92 (+-0.24) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.94 (+-0.27) \n",
      "[29:0] F: 17400000 \t tot_loss 16.652517 rs_loss 1.100062 vs_loss 11.105916 logits_loss 444.654005\n",
      "[29:100] F: 17432000 \t tot_loss 14.298962 rs_loss 0.528405 vs_loss 9.320914 logits_loss 444.964231\n",
      "[29:200] F: 17464000 \t tot_loss 13.734272 rs_loss 0.550827 vs_loss 8.731266 logits_loss 445.217945\n",
      "[29:300] F: 17496000 \t tot_loss 14.715890 rs_loss 0.520268 vs_loss 9.736551 logits_loss 445.907063\n",
      "[29:400] F: 17528000 \t tot_loss 13.941356 rs_loss 0.462180 vs_loss 9.015583 logits_loss 446.359430\n",
      "[29:500] F: 17560000 \t tot_loss 13.691301 rs_loss 0.486227 vs_loss 8.743346 logits_loss 446.172885\n",
      "[29:600] F: 17592000 \t tot_loss 15.199502 rs_loss 0.712592 vs_loss 10.028410 logits_loss 445.850021\n",
      "[29:700] F: 17624000 \t tot_loss 15.019940 rs_loss 1.040339 vs_loss 9.522517 logits_loss 445.708426\n",
      "[29:800] F: 17656000 \t tot_loss 16.623893 rs_loss 1.350902 vs_loss 10.815639 logits_loss 445.735123\n",
      "[29:900] F: 17688000 \t tot_loss 17.762887 rs_loss 1.477941 vs_loss 11.822994 logits_loss 446.195221\n",
      "[29:1000] F: 17720000 \t tot_loss 16.206861 rs_loss 1.219047 vs_loss 10.524310 logits_loss 446.350398\n",
      "[29:1100] F: 17752000 \t tot_loss 16.008995 rs_loss 0.909629 vs_loss 10.634083 logits_loss 446.528421\n",
      "[29:1200] F: 17784000 \t tot_loss 16.443142 rs_loss 0.879768 vs_loss 11.099784 logits_loss 446.358961\n",
      "[29:1300] F: 17816000 \t tot_loss 16.633042 rs_loss 1.164182 vs_loss 11.005510 logits_loss 446.334942\n",
      "[29:1400] F: 17848000 \t tot_loss 19.691414 rs_loss 1.834386 vs_loss 13.398276 logits_loss 445.875161\n",
      "[29:1500] F: 17880000 \t tot_loss 19.221891 rs_loss 1.831704 vs_loss 12.934122 logits_loss 445.606543\n",
      "[29:1600] F: 17912000 \t tot_loss 17.332883 rs_loss 1.690189 vs_loss 11.183933 logits_loss 445.876226\n",
      "[29:1700] F: 17944000 \t tot_loss 16.504143 rs_loss 1.468520 vs_loss 10.577608 logits_loss 445.801457\n",
      "[29:1800] F: 17976000 \t tot_loss 13.776642 rs_loss 0.803054 vs_loss 8.510708 logits_loss 446.287935\n",
      "Batch [30] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.82 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.68 (+-0.21) \n",
      "[30:0] F: 18000000 \t tot_loss 15.536006 rs_loss 1.533525 vs_loss 9.542145 logits_loss 446.033544\n",
      "[30:100] F: 18032000 \t tot_loss 15.852007 rs_loss 1.679742 vs_loss 9.713150 logits_loss 445.911542\n",
      "[30:200] F: 18064000 \t tot_loss 16.637011 rs_loss 1.642724 vs_loss 10.540981 logits_loss 445.330658\n",
      "[30:300] F: 18096000 \t tot_loss 17.007507 rs_loss 1.605608 vs_loss 10.954521 logits_loss 444.737848\n",
      "[30:400] F: 18128000 \t tot_loss 15.725902 rs_loss 0.914315 vs_loss 10.367074 logits_loss 444.451337\n",
      "[30:500] F: 18160000 \t tot_loss 17.502826 rs_loss 1.299877 vs_loss 11.759652 logits_loss 444.329686\n",
      "[30:600] F: 18192000 \t tot_loss 16.032184 rs_loss 1.018688 vs_loss 10.566783 logits_loss 444.671301\n",
      "[30:700] F: 18224000 \t tot_loss 16.210873 rs_loss 1.015959 vs_loss 10.751335 logits_loss 444.357891\n",
      "[30:800] F: 18256000 \t tot_loss 17.423293 rs_loss 1.269824 vs_loss 11.711273 logits_loss 444.219686\n",
      "[30:900] F: 18288000 \t tot_loss 15.880221 rs_loss 0.921289 vs_loss 10.518331 logits_loss 444.060068\n",
      "[30:1000] F: 18320000 \t tot_loss 17.618183 rs_loss 1.239723 vs_loss 11.941343 logits_loss 443.711743\n",
      "[30:1100] F: 18352000 \t tot_loss 16.741329 rs_loss 1.236795 vs_loss 11.061076 logits_loss 444.345792\n",
      "[30:1200] F: 18384000 \t tot_loss 14.521008 rs_loss 0.943157 vs_loss 9.139877 logits_loss 443.797416\n",
      "[30:1300] F: 18416000 \t tot_loss 13.788162 rs_loss 0.777933 vs_loss 8.568738 logits_loss 444.149048\n",
      "[30:1400] F: 18448000 \t tot_loss 12.117846 rs_loss 0.474108 vs_loss 7.200868 logits_loss 444.286894\n",
      "[30:1500] F: 18480000 \t tot_loss 12.929678 rs_loss 0.531512 vs_loss 7.954634 logits_loss 444.353271\n",
      "[30:1600] F: 18512000 \t tot_loss 13.150852 rs_loss 0.514042 vs_loss 8.190803 logits_loss 444.600719\n",
      "[30:1700] F: 18544000 \t tot_loss 14.270001 rs_loss 0.594559 vs_loss 9.233788 logits_loss 444.165465\n",
      "[30:1800] F: 18576000 \t tot_loss 14.331636 rs_loss 0.605854 vs_loss 9.285089 logits_loss 444.069318\n",
      "Batch [31] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.83 (+-0.16) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.80 (+-0.15) \n",
      "[31:0] F: 18600000 \t tot_loss 15.754967 rs_loss 0.827531 vs_loss 10.491488 logits_loss 443.594791\n",
      "[31:100] F: 18632000 \t tot_loss 17.060066 rs_loss 0.825143 vs_loss 11.801044 logits_loss 443.387832\n",
      "[31:200] F: 18664000 \t tot_loss 15.459834 rs_loss 0.774855 vs_loss 10.249823 logits_loss 443.515602\n",
      "[31:300] F: 18696000 \t tot_loss 15.544960 rs_loss 0.933580 vs_loss 10.176108 logits_loss 443.527144\n",
      "[31:400] F: 18728000 \t tot_loss 17.101222 rs_loss 1.424274 vs_loss 11.236033 logits_loss 444.091586\n",
      "[31:500] F: 18760000 \t tot_loss 16.187974 rs_loss 1.582462 vs_loss 10.159296 logits_loss 444.621551\n",
      "[31:600] F: 18792000 \t tot_loss 16.714281 rs_loss 1.557603 vs_loss 10.706726 logits_loss 444.995175\n",
      "[31:700] F: 18824000 \t tot_loss 17.057315 rs_loss 1.457844 vs_loss 11.151625 logits_loss 444.784579\n",
      "[31:800] F: 18856000 \t tot_loss 14.825685 rs_loss 0.810726 vs_loss 9.569183 logits_loss 444.577601\n",
      "[31:900] F: 18888000 \t tot_loss 15.161508 rs_loss 0.922338 vs_loss 9.795783 logits_loss 444.338700\n",
      "[31:1000] F: 18920000 \t tot_loss 17.553135 rs_loss 1.478849 vs_loss 11.631076 logits_loss 444.321038\n",
      "[31:1100] F: 18952000 \t tot_loss 17.001755 rs_loss 1.415796 vs_loss 11.143106 logits_loss 444.285279\n",
      "[31:1200] F: 18984000 \t tot_loss 16.411066 rs_loss 1.530609 vs_loss 10.432589 logits_loss 444.786839\n",
      "[31:1300] F: 19016000 \t tot_loss 15.000663 rs_loss 1.242450 vs_loss 9.306128 logits_loss 445.208459\n",
      "[31:1400] F: 19048000 \t tot_loss 14.115255 rs_loss 1.020784 vs_loss 8.644480 logits_loss 444.998998\n",
      "[31:1500] F: 19080000 \t tot_loss 15.468207 rs_loss 1.525888 vs_loss 9.488305 logits_loss 445.401433\n",
      "[31:1600] F: 19112000 \t tot_loss 17.520336 rs_loss 1.729374 vs_loss 11.337010 logits_loss 445.395213\n",
      "[31:1700] F: 19144000 \t tot_loss 18.416142 rs_loss 1.731718 vs_loss 12.236761 logits_loss 444.766357\n",
      "[31:1800] F: 19176000 \t tot_loss 17.212669 rs_loss 1.415129 vs_loss 11.344794 logits_loss 445.274710\n",
      "Batch [32] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.91 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.10 (+-0.30) \n",
      "[32:0] F: 19200000 \t tot_loss 16.531492 rs_loss 0.989029 vs_loss 11.090315 logits_loss 445.214864\n",
      "[32:100] F: 19232000 \t tot_loss 17.996127 rs_loss 0.916102 vs_loss 12.632836 logits_loss 444.718906\n",
      "[32:200] F: 19264000 \t tot_loss 18.091019 rs_loss 0.910195 vs_loss 12.726974 logits_loss 445.384989\n",
      "[32:300] F: 19296000 \t tot_loss 17.950693 rs_loss 0.901720 vs_loss 12.597964 logits_loss 445.100918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32:400] F: 19328000 \t tot_loss 16.314191 rs_loss 0.816347 vs_loss 11.051511 logits_loss 444.633272\n",
      "[32:500] F: 19360000 \t tot_loss 12.512091 rs_loss 0.582966 vs_loss 7.482995 logits_loss 444.613012\n",
      "[32:600] F: 19392000 \t tot_loss 13.034643 rs_loss 0.611723 vs_loss 7.976818 logits_loss 444.610225\n",
      "[32:700] F: 19424000 \t tot_loss 14.302225 rs_loss 0.851606 vs_loss 9.004105 logits_loss 444.651374\n",
      "[32:800] F: 19456000 \t tot_loss 14.649441 rs_loss 0.933363 vs_loss 9.263650 logits_loss 445.242726\n",
      "[32:900] F: 19488000 \t tot_loss 15.306425 rs_loss 0.880452 vs_loss 9.975449 logits_loss 445.052433\n",
      "[32:1000] F: 19520000 \t tot_loss 15.627170 rs_loss 1.031646 vs_loss 10.151781 logits_loss 444.374320\n",
      "[32:1100] F: 19552000 \t tot_loss 15.161479 rs_loss 1.009922 vs_loss 9.704050 logits_loss 444.750732\n",
      "[32:1200] F: 19584000 \t tot_loss 15.967439 rs_loss 0.937731 vs_loss 10.585549 logits_loss 444.415921\n",
      "[32:1300] F: 19616000 \t tot_loss 17.159189 rs_loss 1.136083 vs_loss 11.582137 logits_loss 444.096937\n",
      "[32:1400] F: 19648000 \t tot_loss 17.216491 rs_loss 1.255342 vs_loss 11.516667 logits_loss 444.448210\n",
      "[32:1500] F: 19680000 \t tot_loss 17.724197 rs_loss 1.339287 vs_loss 11.945513 logits_loss 443.939684\n",
      "[32:1600] F: 19712000 \t tot_loss 16.623827 rs_loss 1.327533 vs_loss 10.858400 logits_loss 443.789329\n",
      "[32:1700] F: 19744000 \t tot_loss 14.992903 rs_loss 1.107149 vs_loss 9.446772 logits_loss 443.898268\n",
      "[32:1800] F: 19776000 \t tot_loss 16.442272 rs_loss 1.080109 vs_loss 10.921154 logits_loss 444.100888\n",
      "Batch [33] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.04 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.83 (+-0.20) \n",
      "[33:0] F: 19800000 \t tot_loss 15.416079 rs_loss 0.968706 vs_loss 10.006643 logits_loss 444.073057\n",
      "[33:100] F: 19832000 \t tot_loss 16.376896 rs_loss 0.963631 vs_loss 10.967804 logits_loss 444.546156\n",
      "[33:200] F: 19864000 \t tot_loss 16.535290 rs_loss 0.927718 vs_loss 11.156391 logits_loss 445.118132\n",
      "[33:300] F: 19896000 \t tot_loss 14.831646 rs_loss 0.702849 vs_loss 9.680534 logits_loss 444.826316\n",
      "[33:400] F: 19928000 \t tot_loss 17.136767 rs_loss 1.082379 vs_loss 11.601272 logits_loss 445.311638\n",
      "[33:500] F: 19960000 \t tot_loss 18.788949 rs_loss 1.153451 vs_loss 13.184416 logits_loss 445.108220\n",
      "[33:600] F: 19992000 \t tot_loss 18.522142 rs_loss 1.162252 vs_loss 12.905741 logits_loss 445.414938\n",
      "[33:700] F: 20024000 \t tot_loss 18.030032 rs_loss 1.149164 vs_loss 12.427604 logits_loss 445.326466\n",
      "[33:800] F: 20056000 \t tot_loss 16.485499 rs_loss 0.835646 vs_loss 11.198219 logits_loss 445.163336\n",
      "[33:900] F: 20088000 \t tot_loss 17.057662 rs_loss 1.343247 vs_loss 11.260984 logits_loss 445.343034\n",
      "[33:1000] F: 20120000 \t tot_loss 17.500809 rs_loss 1.436757 vs_loss 11.611849 logits_loss 445.220313\n",
      "[33:1100] F: 20152000 \t tot_loss 18.719440 rs_loss 1.383636 vs_loss 12.882344 logits_loss 445.346013\n",
      "[33:1200] F: 20184000 \t tot_loss 19.256038 rs_loss 1.641939 vs_loss 13.161593 logits_loss 445.250598\n",
      "[33:1300] F: 20216000 \t tot_loss 18.316076 rs_loss 1.385035 vs_loss 12.479493 logits_loss 445.154793\n",
      "[33:1400] F: 20248000 \t tot_loss 17.339578 rs_loss 1.278723 vs_loss 11.604214 logits_loss 445.664184\n",
      "[33:1500] F: 20280000 \t tot_loss 16.126341 rs_loss 1.309091 vs_loss 10.362702 logits_loss 445.454880\n",
      "[33:1600] F: 20312000 \t tot_loss 13.944418 rs_loss 0.793661 vs_loss 8.698255 logits_loss 445.250233\n",
      "[33:1700] F: 20344000 \t tot_loss 14.094538 rs_loss 1.076103 vs_loss 8.571569 logits_loss 444.686590\n",
      "[33:1800] F: 20376000 \t tot_loss 14.438957 rs_loss 1.086092 vs_loss 8.909666 logits_loss 444.319938\n",
      "Batch [34] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.81 (+-0.35) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.95 (+-0.27) \n",
      "[34:0] F: 20400000 \t tot_loss 14.635273 rs_loss 1.079750 vs_loss 9.112605 logits_loss 444.291876\n",
      "[34:100] F: 20432000 \t tot_loss 14.858136 rs_loss 1.089605 vs_loss 9.327860 logits_loss 444.067107\n",
      "[34:200] F: 20464000 \t tot_loss 13.129834 rs_loss 0.467859 vs_loss 8.214594 logits_loss 444.738208\n",
      "[34:300] F: 20496000 \t tot_loss 13.363747 rs_loss 0.547203 vs_loss 8.370540 logits_loss 444.600367\n",
      "[34:400] F: 20528000 \t tot_loss 13.049170 rs_loss 0.573461 vs_loss 8.028681 logits_loss 444.702815\n",
      "[34:500] F: 20560000 \t tot_loss 16.463122 rs_loss 1.169893 vs_loss 10.844709 logits_loss 444.852005\n",
      "[34:600] F: 20592000 \t tot_loss 16.002784 rs_loss 1.155344 vs_loss 10.398973 logits_loss 444.846718\n",
      "[34:700] F: 20624000 \t tot_loss 16.957522 rs_loss 1.150480 vs_loss 11.359674 logits_loss 444.736804\n",
      "[34:800] F: 20656000 \t tot_loss 17.962914 rs_loss 1.402799 vs_loss 12.113553 logits_loss 444.656273\n",
      "[34:900] F: 20688000 \t tot_loss 13.986875 rs_loss 0.767783 vs_loss 8.770829 logits_loss 444.826240\n",
      "[34:1000] F: 20720000 \t tot_loss 14.543351 rs_loss 0.973774 vs_loss 9.119355 logits_loss 445.022219\n",
      "[34:1100] F: 20752000 \t tot_loss 13.829630 rs_loss 0.935596 vs_loss 8.447479 logits_loss 444.655640\n",
      "[34:1200] F: 20784000 \t tot_loss 14.923064 rs_loss 1.168984 vs_loss 9.307367 logits_loss 444.671287\n",
      "[34:1300] F: 20816000 \t tot_loss 15.956847 rs_loss 1.450259 vs_loss 10.056727 logits_loss 444.986109\n",
      "[34:1400] F: 20848000 \t tot_loss 15.130460 rs_loss 1.232078 vs_loss 9.454444 logits_loss 444.393849\n",
      "[34:1500] F: 20880000 \t tot_loss 15.129991 rs_loss 1.455013 vs_loss 9.226537 logits_loss 444.844198\n",
      "[34:1600] F: 20912000 \t tot_loss 14.961815 rs_loss 1.230458 vs_loss 9.289670 logits_loss 444.168731\n",
      "[34:1700] F: 20944000 \t tot_loss 14.633656 rs_loss 1.058774 vs_loss 9.139164 logits_loss 443.571789\n",
      "[34:1800] F: 20976000 \t tot_loss 14.571521 rs_loss 1.057704 vs_loss 9.075195 logits_loss 443.862120\n",
      "Batch [35] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.92 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.68 (+-0.15) \n",
      "[35:0] F: 21000000 \t tot_loss 13.804731 rs_loss 0.854505 vs_loss 8.512852 logits_loss 443.737389\n",
      "[35:100] F: 21032000 \t tot_loss 13.335525 rs_loss 0.842676 vs_loss 8.053472 logits_loss 443.937676\n",
      "[35:200] F: 21064000 \t tot_loss 13.568686 rs_loss 0.870397 vs_loss 8.255747 logits_loss 444.254200\n",
      "[35:300] F: 21096000 \t tot_loss 14.183105 rs_loss 0.850952 vs_loss 8.882436 logits_loss 444.971727\n",
      "[35:400] F: 21128000 \t tot_loss 14.138209 rs_loss 0.810964 vs_loss 8.875320 logits_loss 445.192522\n",
      "[35:500] F: 21160000 \t tot_loss 12.620580 rs_loss 0.539347 vs_loss 7.623746 logits_loss 445.748676\n",
      "[35:600] F: 21192000 \t tot_loss 14.095996 rs_loss 0.538762 vs_loss 9.101235 logits_loss 445.599938\n",
      "[35:700] F: 21224000 \t tot_loss 14.118882 rs_loss 0.596153 vs_loss 9.070556 logits_loss 445.217264\n",
      "[35:800] F: 21256000 \t tot_loss 14.269481 rs_loss 0.575421 vs_loss 9.245262 logits_loss 444.879866\n",
      "[35:900] F: 21288000 \t tot_loss 16.229911 rs_loss 0.891548 vs_loss 10.888165 logits_loss 445.019823\n",
      "[35:1000] F: 21320000 \t tot_loss 15.313106 rs_loss 1.026666 vs_loss 9.833724 logits_loss 445.271649\n",
      "[35:1100] F: 21352000 \t tot_loss 14.747745 rs_loss 0.992202 vs_loss 9.304359 logits_loss 445.118419\n",
      "[35:1200] F: 21384000 \t tot_loss 16.573668 rs_loss 1.385035 vs_loss 10.735036 logits_loss 445.359835\n",
      "[35:1300] F: 21416000 \t tot_loss 15.477777 rs_loss 1.239057 vs_loss 9.790793 logits_loss 444.792807\n",
      "[35:1400] F: 21448000 \t tot_loss 17.161033 rs_loss 1.547681 vs_loss 11.167139 logits_loss 444.621308\n",
      "[35:1500] F: 21480000 \t tot_loss 18.355721 rs_loss 1.700079 vs_loss 12.206323 logits_loss 444.931904\n",
      "[35:1600] F: 21512000 \t tot_loss 17.416313 rs_loss 1.367034 vs_loss 11.601707 logits_loss 444.757239\n",
      "[35:1700] F: 21544000 \t tot_loss 17.803361 rs_loss 1.514861 vs_loss 11.838573 logits_loss 444.992747\n",
      "[35:1800] F: 21576000 \t tot_loss 17.406690 rs_loss 1.151617 vs_loss 11.802529 logits_loss 445.254454\n",
      "Batch [36] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.22 (+-0.26) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.60 (+-0.15) \n",
      "[36:0] F: 21600000 \t tot_loss 17.255392 rs_loss 1.189201 vs_loss 11.613777 logits_loss 445.241458\n",
      "[36:100] F: 21632000 \t tot_loss 19.152046 rs_loss 1.442189 vs_loss 13.258357 logits_loss 445.150034\n",
      "[36:200] F: 21664000 \t tot_loss 20.337376 rs_loss 1.876994 vs_loss 14.013491 logits_loss 444.689125\n",
      "[36:300] F: 21696000 \t tot_loss 18.377896 rs_loss 1.390427 vs_loss 12.542805 logits_loss 444.466353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36:400] F: 21728000 \t tot_loss 20.184947 rs_loss 1.514830 vs_loss 14.229224 logits_loss 444.089338\n",
      "[36:500] F: 21760000 \t tot_loss 17.231334 rs_loss 1.232329 vs_loss 11.564432 logits_loss 443.457325\n",
      "[36:600] F: 21792000 \t tot_loss 17.188200 rs_loss 1.060454 vs_loss 11.692055 logits_loss 443.569126\n",
      "[36:700] F: 21824000 \t tot_loss 17.873950 rs_loss 1.266513 vs_loss 12.175299 logits_loss 443.213876\n",
      "[36:800] F: 21856000 \t tot_loss 17.471251 rs_loss 1.023905 vs_loss 12.016686 logits_loss 443.065957\n",
      "[36:900] F: 21888000 \t tot_loss 18.919273 rs_loss 1.320837 vs_loss 13.160873 logits_loss 443.756342\n",
      "[36:1000] F: 21920000 \t tot_loss 18.238991 rs_loss 1.100449 vs_loss 12.698050 logits_loss 444.049202\n",
      "[36:1100] F: 21952000 \t tot_loss 17.341475 rs_loss 1.055285 vs_loss 11.840297 logits_loss 444.589329\n",
      "[36:1200] F: 21984000 \t tot_loss 17.238000 rs_loss 1.238882 vs_loss 11.550202 logits_loss 444.891608\n",
      "[36:1300] F: 22016000 \t tot_loss 18.970032 rs_loss 1.341468 vs_loss 13.180988 logits_loss 444.757511\n",
      "[36:1400] F: 22048000 \t tot_loss 18.524410 rs_loss 1.385383 vs_loss 12.694548 logits_loss 444.447874\n",
      "[36:1500] F: 22080000 \t tot_loss 21.114084 rs_loss 1.776410 vs_loss 14.897108 logits_loss 444.056529\n",
      "[36:1600] F: 22112000 \t tot_loss 19.764484 rs_loss 1.847009 vs_loss 13.479526 logits_loss 443.794886\n",
      "[36:1700] F: 22144000 \t tot_loss 17.274591 rs_loss 1.520607 vs_loss 11.316887 logits_loss 443.709645\n",
      "[36:1800] F: 22176000 \t tot_loss 18.190977 rs_loss 1.589561 vs_loss 12.163159 logits_loss 443.825701\n",
      "Batch [37] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.05 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.47 (+-0.08) \n",
      "[37:0] F: 22200000 \t tot_loss 17.255187 rs_loss 1.358407 vs_loss 11.452448 logits_loss 444.433204\n",
      "[37:100] F: 22232000 \t tot_loss 16.369020 rs_loss 1.267737 vs_loss 10.654536 logits_loss 444.674737\n",
      "[37:200] F: 22264000 \t tot_loss 15.611050 rs_loss 1.171613 vs_loss 9.991559 logits_loss 444.787798\n",
      "[37:300] F: 22296000 \t tot_loss 16.786770 rs_loss 1.482681 vs_loss 10.855519 logits_loss 444.856936\n",
      "[37:400] F: 22328000 \t tot_loss 15.833746 rs_loss 1.450716 vs_loss 9.936638 logits_loss 444.639186\n",
      "[37:500] F: 22360000 \t tot_loss 20.029889 rs_loss 1.935490 vs_loss 13.652659 logits_loss 444.174006\n",
      "[37:600] F: 22392000 \t tot_loss 21.339050 rs_loss 1.958476 vs_loss 14.934271 logits_loss 444.630405\n",
      "[37:700] F: 22424000 \t tot_loss 23.238821 rs_loss 2.159788 vs_loss 16.634382 logits_loss 444.465169\n",
      "[37:800] F: 22456000 \t tot_loss 22.046319 rs_loss 1.961814 vs_loss 15.637851 logits_loss 444.665437\n",
      "[37:900] F: 22488000 \t tot_loss 17.872809 rs_loss 1.335034 vs_loss 12.087847 logits_loss 444.992741\n",
      "[37:1000] F: 22520000 \t tot_loss 16.617906 rs_loss 1.339742 vs_loss 10.833087 logits_loss 444.507639\n",
      "[37:1100] F: 22552000 \t tot_loss 15.510176 rs_loss 1.070517 vs_loss 9.994419 logits_loss 444.524065\n",
      "[37:1200] F: 22584000 \t tot_loss 16.087364 rs_loss 1.180680 vs_loss 10.466011 logits_loss 444.067341\n",
      "[37:1300] F: 22616000 \t tot_loss 18.843953 rs_loss 1.843975 vs_loss 12.561674 logits_loss 443.830359\n",
      "[37:1400] F: 22648000 \t tot_loss 20.969589 rs_loss 2.032176 vs_loss 14.497557 logits_loss 443.985661\n",
      "[37:1500] F: 22680000 \t tot_loss 19.966919 rs_loss 2.067937 vs_loss 13.460591 logits_loss 443.839105\n",
      "[37:1600] F: 22712000 \t tot_loss 21.348208 rs_loss 2.135580 vs_loss 14.765149 logits_loss 444.747951\n",
      "[37:1700] F: 22744000 \t tot_loss 17.820534 rs_loss 1.213641 vs_loss 12.161121 logits_loss 444.577212\n",
      "[37:1800] F: 22776000 \t tot_loss 17.077151 rs_loss 1.093381 vs_loss 11.535435 logits_loss 444.833552\n",
      "Batch [38] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.63 (+-0.09) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.25 (+-0.29) \n",
      "[38:0] F: 22800000 \t tot_loss 17.311652 rs_loss 1.059001 vs_loss 11.799652 logits_loss 445.299817\n",
      "[38:100] F: 22832000 \t tot_loss 17.437487 rs_loss 1.087788 vs_loss 11.896511 logits_loss 445.318815\n",
      "[38:200] F: 22864000 \t tot_loss 19.724870 rs_loss 1.289336 vs_loss 13.978859 logits_loss 445.667497\n",
      "[38:300] F: 22896000 \t tot_loss 19.618210 rs_loss 1.483272 vs_loss 13.675436 logits_loss 445.950180\n",
      "[38:400] F: 22928000 \t tot_loss 18.103094 rs_loss 1.242042 vs_loss 12.404167 logits_loss 445.688500\n",
      "[38:500] F: 22960000 \t tot_loss 18.212945 rs_loss 1.309421 vs_loss 12.447893 logits_loss 445.563115\n",
      "[38:600] F: 22992000 \t tot_loss 15.675929 rs_loss 1.275078 vs_loss 9.941605 logits_loss 445.924546\n",
      "[38:700] F: 23024000 \t tot_loss 15.370450 rs_loss 1.009190 vs_loss 9.905445 logits_loss 445.581471\n",
      "[38:800] F: 23056000 \t tot_loss 17.255055 rs_loss 1.280777 vs_loss 11.517010 logits_loss 445.726757\n",
      "[38:900] F: 23088000 \t tot_loss 16.413618 rs_loss 1.194200 vs_loss 10.760166 logits_loss 445.925171\n",
      "[38:1000] F: 23120000 \t tot_loss 15.829239 rs_loss 0.939544 vs_loss 10.429064 logits_loss 446.063032\n",
      "[38:1100] F: 23152000 \t tot_loss 16.803527 rs_loss 1.166018 vs_loss 11.174856 logits_loss 446.265228\n",
      "[38:1200] F: 23184000 \t tot_loss 15.800278 rs_loss 0.916157 vs_loss 10.423375 logits_loss 446.074594\n",
      "[38:1300] F: 23216000 \t tot_loss 17.757313 rs_loss 1.195331 vs_loss 12.101350 logits_loss 446.063234\n",
      "[38:1400] F: 23248000 \t tot_loss 19.850095 rs_loss 1.423680 vs_loss 13.968565 logits_loss 445.785004\n",
      "[38:1500] F: 23280000 \t tot_loss 18.018696 rs_loss 1.197063 vs_loss 12.366607 logits_loss 445.502680\n",
      "[38:1600] F: 23312000 \t tot_loss 20.087047 rs_loss 1.591330 vs_loss 14.037472 logits_loss 445.824551\n",
      "[38:1700] F: 23344000 \t tot_loss 19.736640 rs_loss 1.614341 vs_loss 13.664725 logits_loss 445.757401\n",
      "[38:1800] F: 23376000 \t tot_loss 20.559557 rs_loss 1.420963 vs_loss 14.683989 logits_loss 445.460498\n",
      "Batch [39] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.99 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.05 (+-0.27) \n",
      "[39:0] F: 23400000 \t tot_loss 21.720783 rs_loss 1.429381 vs_loss 15.840147 logits_loss 445.125442\n",
      "[39:100] F: 23432000 \t tot_loss 19.197329 rs_loss 1.039069 vs_loss 13.709771 logits_loss 444.848824\n",
      "[39:200] F: 23464000 \t tot_loss 18.434243 rs_loss 1.169122 vs_loss 12.819065 logits_loss 444.605633\n",
      "[39:300] F: 23496000 \t tot_loss 15.610769 rs_loss 0.958643 vs_loss 10.212237 logits_loss 443.988952\n",
      "[39:400] F: 23528000 \t tot_loss 13.711390 rs_loss 0.939188 vs_loss 8.331463 logits_loss 444.073843\n",
      "[39:500] F: 23560000 \t tot_loss 16.966526 rs_loss 1.583650 vs_loss 10.945000 logits_loss 443.787650\n",
      "[39:600] F: 23592000 \t tot_loss 15.701567 rs_loss 1.092428 vs_loss 10.168057 logits_loss 444.108147\n",
      "[39:700] F: 23624000 \t tot_loss 15.237691 rs_loss 1.120648 vs_loss 9.673202 logits_loss 444.384177\n",
      "[39:800] F: 23656000 \t tot_loss 15.154348 rs_loss 1.081241 vs_loss 9.630926 logits_loss 444.218122\n",
      "[39:900] F: 23688000 \t tot_loss 13.639929 rs_loss 0.867390 vs_loss 8.333558 logits_loss 443.898092\n",
      "[39:1000] F: 23720000 \t tot_loss 14.853673 rs_loss 1.184751 vs_loss 9.229990 logits_loss 443.893203\n",
      "[39:1100] F: 23752000 \t tot_loss 15.696213 rs_loss 1.408355 vs_loss 9.842264 logits_loss 444.559349\n",
      "[39:1200] F: 23784000 \t tot_loss 18.994393 rs_loss 2.242988 vs_loss 12.302900 logits_loss 444.850548\n",
      "[39:1300] F: 23816000 \t tot_loss 18.909192 rs_loss 1.925057 vs_loss 12.526542 logits_loss 445.759364\n",
      "[39:1400] F: 23848000 \t tot_loss 17.442420 rs_loss 1.608441 vs_loss 11.374389 logits_loss 445.959100\n",
      "[39:1500] F: 23880000 \t tot_loss 17.029945 rs_loss 1.485534 vs_loss 11.088128 logits_loss 445.628301\n",
      "[39:1600] F: 23912000 \t tot_loss 13.767232 rs_loss 0.663901 vs_loss 8.646959 logits_loss 445.637182\n",
      "[39:1700] F: 23944000 \t tot_loss 12.006830 rs_loss 0.532560 vs_loss 7.022527 logits_loss 445.174287\n",
      "[39:1800] F: 23976000 \t tot_loss 12.957912 rs_loss 1.036471 vs_loss 7.473592 logits_loss 444.784898\n",
      "Batch [40] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.18 (+-0.29) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.89 (+-0.27) \n",
      "[40:0] F: 24000000 \t tot_loss 12.710813 rs_loss 0.943799 vs_loss 7.319472 logits_loss 444.754248\n",
      "[40:100] F: 24032000 \t tot_loss 14.509225 rs_loss 1.472195 vs_loss 8.590586 logits_loss 444.644393\n",
      "[40:200] F: 24064000 \t tot_loss 16.118736 rs_loss 1.614582 vs_loss 10.055019 logits_loss 444.913567\n",
      "[40:300] F: 24096000 \t tot_loss 17.338263 rs_loss 1.587767 vs_loss 11.299198 logits_loss 445.129878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40:400] F: 24128000 \t tot_loss 17.098542 rs_loss 1.563343 vs_loss 11.083530 logits_loss 445.166889\n",
      "[40:500] F: 24160000 \t tot_loss 15.994621 rs_loss 1.136698 vs_loss 10.404624 logits_loss 445.329817\n",
      "[40:600] F: 24192000 \t tot_loss 15.825994 rs_loss 1.127228 vs_loss 10.246408 logits_loss 445.235761\n",
      "[40:700] F: 24224000 \t tot_loss 13.545900 rs_loss 0.678463 vs_loss 8.412867 logits_loss 445.457012\n",
      "[40:800] F: 24256000 \t tot_loss 15.296952 rs_loss 0.959322 vs_loss 9.880718 logits_loss 445.691252\n",
      "[40:900] F: 24288000 \t tot_loss 14.773010 rs_loss 0.914497 vs_loss 9.404090 logits_loss 445.442350\n",
      "[40:1000] F: 24320000 \t tot_loss 16.128346 rs_loss 1.154443 vs_loss 10.518090 logits_loss 445.581352\n",
      "[40:1100] F: 24352000 \t tot_loss 15.814594 rs_loss 1.170501 vs_loss 10.192050 logits_loss 445.204196\n",
      "[40:1200] F: 24384000 \t tot_loss 15.199416 rs_loss 0.925333 vs_loss 9.825508 logits_loss 444.857608\n",
      "[40:1300] F: 24416000 \t tot_loss 16.908102 rs_loss 1.108527 vs_loss 11.349614 logits_loss 444.996075\n",
      "[40:1400] F: 24448000 \t tot_loss 14.530002 rs_loss 0.778247 vs_loss 9.304115 logits_loss 444.763976\n",
      "[40:1500] F: 24480000 \t tot_loss 16.569763 rs_loss 1.173281 vs_loss 10.947269 logits_loss 444.921247\n",
      "[40:1600] F: 24512000 \t tot_loss 17.346662 rs_loss 1.449776 vs_loss 11.446060 logits_loss 445.082576\n",
      "[40:1700] F: 24544000 \t tot_loss 17.593441 rs_loss 1.403826 vs_loss 11.737898 logits_loss 445.171623\n",
      "[40:1800] F: 24576000 \t tot_loss 17.482499 rs_loss 1.393956 vs_loss 11.630768 logits_loss 445.777522\n",
      "Batch [41] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.34 (+-0.29) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.87 (+-0.20) \n",
      "[41:0] F: 24600000 \t tot_loss 16.045830 rs_loss 0.967746 vs_loss 10.624090 logits_loss 445.399457\n",
      "[41:100] F: 24632000 \t tot_loss 17.278646 rs_loss 1.090605 vs_loss 11.736063 logits_loss 445.197822\n",
      "[41:200] F: 24664000 \t tot_loss 16.772287 rs_loss 1.038871 vs_loss 11.277858 logits_loss 445.555858\n",
      "[41:300] F: 24696000 \t tot_loss 18.329204 rs_loss 1.274351 vs_loss 12.596409 logits_loss 445.844423\n",
      "[41:400] F: 24728000 \t tot_loss 18.161039 rs_loss 1.264116 vs_loss 12.432006 logits_loss 446.491625\n",
      "[41:500] F: 24760000 \t tot_loss 15.625990 rs_loss 0.928450 vs_loss 10.230833 logits_loss 446.670760\n",
      "[41:600] F: 24792000 \t tot_loss 15.581021 rs_loss 0.956677 vs_loss 10.161293 logits_loss 446.305130\n",
      "[41:700] F: 24824000 \t tot_loss 14.184897 rs_loss 0.542387 vs_loss 9.185272 logits_loss 445.723728\n",
      "[41:800] F: 24856000 \t tot_loss 15.062942 rs_loss 0.830765 vs_loss 9.778830 logits_loss 445.334665\n",
      "[41:900] F: 24888000 \t tot_loss 18.536418 rs_loss 1.255641 vs_loss 12.828148 logits_loss 445.262878\n",
      "[41:1000] F: 24920000 \t tot_loss 19.367333 rs_loss 1.282739 vs_loss 13.628183 logits_loss 445.641188\n",
      "[41:1100] F: 24952000 \t tot_loss 20.217555 rs_loss 1.407465 vs_loss 14.352044 logits_loss 445.804704\n",
      "[41:1200] F: 24984000 \t tot_loss 19.595770 rs_loss 1.119737 vs_loss 14.013705 logits_loss 446.232787\n",
      "[41:1300] F: 25016000 \t tot_loss 16.642821 rs_loss 0.754072 vs_loss 11.430500 logits_loss 445.824916\n",
      "[41:1400] F: 25048000 \t tot_loss 17.395406 rs_loss 0.873012 vs_loss 12.068312 logits_loss 445.408207\n",
      "[41:1500] F: 25080000 \t tot_loss 16.267315 rs_loss 0.792224 vs_loss 11.021361 logits_loss 445.373045\n",
      "[41:1600] F: 25112000 \t tot_loss 19.531474 rs_loss 1.414459 vs_loss 13.667632 logits_loss 444.938393\n",
      "[41:1700] F: 25144000 \t tot_loss 20.736046 rs_loss 1.308159 vs_loss 14.973255 logits_loss 445.463208\n",
      "[41:1800] F: 25176000 \t tot_loss 18.892452 rs_loss 1.171341 vs_loss 13.257156 logits_loss 446.395481\n",
      "Batch [42] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.92 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.80 (+-0.20) \n",
      "[42:0] F: 25200000 \t tot_loss 19.615878 rs_loss 1.286756 vs_loss 13.864897 logits_loss 446.422593\n",
      "[42:100] F: 25232000 \t tot_loss 18.074198 rs_loss 1.210930 vs_loss 12.403459 logits_loss 445.980880\n",
      "[42:200] F: 25264000 \t tot_loss 18.552869 rs_loss 1.344211 vs_loss 12.753719 logits_loss 445.493913\n",
      "[42:300] F: 25296000 \t tot_loss 18.312149 rs_loss 1.665604 vs_loss 12.202378 logits_loss 444.416679\n",
      "[42:400] F: 25328000 \t tot_loss 17.748150 rs_loss 1.564745 vs_loss 11.740749 logits_loss 444.265591\n",
      "[42:500] F: 25360000 \t tot_loss 15.149718 rs_loss 1.148304 vs_loss 9.558681 logits_loss 444.273267\n",
      "[42:600] F: 25392000 \t tot_loss 14.330229 rs_loss 1.254299 vs_loss 8.631555 logits_loss 444.437538\n",
      "[42:700] F: 25424000 \t tot_loss 13.630710 rs_loss 0.917252 vs_loss 8.266952 logits_loss 444.650560\n",
      "[42:800] F: 25456000 \t tot_loss 13.602511 rs_loss 0.973210 vs_loss 8.185997 logits_loss 444.330359\n",
      "[42:900] F: 25488000 \t tot_loss 13.854022 rs_loss 0.958539 vs_loss 8.449351 logits_loss 444.613189\n",
      "[42:1000] F: 25520000 \t tot_loss 13.327387 rs_loss 0.814317 vs_loss 8.065033 logits_loss 444.803667\n",
      "[42:1100] F: 25552000 \t tot_loss 13.530195 rs_loss 0.824360 vs_loss 8.263438 logits_loss 444.239622\n",
      "[42:1200] F: 25584000 \t tot_loss 12.840960 rs_loss 0.712314 vs_loss 7.689264 logits_loss 443.938197\n",
      "[42:1300] F: 25616000 \t tot_loss 13.686834 rs_loss 0.776801 vs_loss 8.473136 logits_loss 443.689786\n",
      "[42:1400] F: 25648000 \t tot_loss 13.773779 rs_loss 0.739933 vs_loss 8.599229 logits_loss 443.461719\n",
      "[42:1500] F: 25680000 \t tot_loss 13.626274 rs_loss 0.769901 vs_loss 8.423882 logits_loss 443.249170\n",
      "[42:1600] F: 25712000 \t tot_loss 15.270869 rs_loss 1.229260 vs_loss 9.609736 logits_loss 443.187277\n",
      "[42:1700] F: 25744000 \t tot_loss 14.331339 rs_loss 1.176863 vs_loss 8.723247 logits_loss 443.122872\n",
      "[42:1800] F: 25776000 \t tot_loss 13.471602 rs_loss 1.024607 vs_loss 8.013443 logits_loss 443.355226\n",
      "Batch [43] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.91 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.14 (+-0.30) \n",
      "[43:0] F: 25800000 \t tot_loss 13.568462 rs_loss 1.232126 vs_loss 7.898216 logits_loss 443.811940\n",
      "[43:100] F: 25832000 \t tot_loss 13.975801 rs_loss 0.998625 vs_loss 8.535190 logits_loss 444.198673\n",
      "[43:200] F: 25864000 \t tot_loss 14.522064 rs_loss 1.079215 vs_loss 8.998721 logits_loss 444.412754\n",
      "[43:300] F: 25896000 \t tot_loss 15.691086 rs_loss 1.291108 vs_loss 9.959282 logits_loss 444.069628\n",
      "[43:400] F: 25928000 \t tot_loss 15.839020 rs_loss 1.046095 vs_loss 10.353403 logits_loss 443.952194\n",
      "[43:500] F: 25960000 \t tot_loss 16.492841 rs_loss 1.263877 vs_loss 10.789868 logits_loss 443.909647\n",
      "[43:600] F: 25992000 \t tot_loss 16.720894 rs_loss 1.183133 vs_loss 11.098360 logits_loss 443.940070\n",
      "[43:700] F: 26024000 \t tot_loss 16.415398 rs_loss 1.044430 vs_loss 10.933949 logits_loss 443.701981\n",
      "[43:800] F: 26056000 \t tot_loss 15.831354 rs_loss 1.080016 vs_loss 10.317072 logits_loss 443.426635\n",
      "[43:900] F: 26088000 \t tot_loss 15.649788 rs_loss 1.022119 vs_loss 10.193284 logits_loss 443.438548\n",
      "[43:1000] F: 26120000 \t tot_loss 16.669888 rs_loss 1.253203 vs_loss 10.981815 logits_loss 443.487030\n",
      "[43:1100] F: 26152000 \t tot_loss 16.546275 rs_loss 1.308679 vs_loss 10.803202 logits_loss 443.439379\n",
      "[43:1200] F: 26184000 \t tot_loss 18.321622 rs_loss 1.347794 vs_loss 12.539350 logits_loss 443.447794\n",
      "[43:1300] F: 26216000 \t tot_loss 16.825529 rs_loss 1.126131 vs_loss 11.262306 logits_loss 443.709190\n",
      "[43:1400] F: 26248000 \t tot_loss 14.862330 rs_loss 0.849966 vs_loss 9.574971 logits_loss 443.739342\n",
      "[43:1500] F: 26280000 \t tot_loss 14.025286 rs_loss 0.702880 vs_loss 8.883493 logits_loss 443.891289\n",
      "[43:1600] F: 26312000 \t tot_loss 12.381274 rs_loss 0.647834 vs_loss 7.291818 logits_loss 444.162237\n",
      "[43:1700] F: 26344000 \t tot_loss 11.212411 rs_loss 0.504962 vs_loss 6.265912 logits_loss 444.153728\n",
      "[43:1800] F: 26376000 \t tot_loss 11.236631 rs_loss 0.533120 vs_loss 6.262641 logits_loss 444.087014\n",
      "Batch [44] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.96 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.74 (+-0.20) \n",
      "[44:0] F: 26400000 \t tot_loss 11.372276 rs_loss 0.517416 vs_loss 6.411919 logits_loss 444.294101\n",
      "[44:100] F: 26432000 \t tot_loss 11.967577 rs_loss 0.531463 vs_loss 6.991169 logits_loss 444.494523\n",
      "[44:200] F: 26464000 \t tot_loss 14.364361 rs_loss 0.884724 vs_loss 9.030863 logits_loss 444.877429\n",
      "[44:300] F: 26496000 \t tot_loss 14.926944 rs_loss 0.864045 vs_loss 9.610991 logits_loss 445.190901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44:400] F: 26528000 \t tot_loss 15.749089 rs_loss 0.832870 vs_loss 10.462105 logits_loss 445.411425\n",
      "[44:500] F: 26560000 \t tot_loss 17.730723 rs_loss 1.086768 vs_loss 12.189467 logits_loss 445.448751\n",
      "[44:600] F: 26592000 \t tot_loss 16.520607 rs_loss 0.731820 vs_loss 11.339401 logits_loss 444.938569\n",
      "[44:700] F: 26624000 \t tot_loss 17.443516 rs_loss 0.906670 vs_loss 12.088709 logits_loss 444.813592\n",
      "[44:800] F: 26656000 \t tot_loss 17.583899 rs_loss 1.213338 vs_loss 11.919337 logits_loss 445.122381\n",
      "[44:900] F: 26688000 \t tot_loss 15.394912 rs_loss 0.934092 vs_loss 10.010300 logits_loss 445.052065\n",
      "[44:1000] F: 26720000 \t tot_loss 17.231105 rs_loss 1.366534 vs_loss 11.413920 logits_loss 445.065067\n",
      "[44:1100] F: 26752000 \t tot_loss 16.085836 rs_loss 1.176039 vs_loss 10.453992 logits_loss 445.580482\n",
      "[44:1200] F: 26784000 \t tot_loss 15.976790 rs_loss 0.899864 vs_loss 10.631663 logits_loss 444.526280\n",
      "[44:1300] F: 26816000 \t tot_loss 16.756789 rs_loss 1.065927 vs_loss 11.245298 logits_loss 444.556396\n",
      "[44:1400] F: 26848000 \t tot_loss 14.857492 rs_loss 0.635929 vs_loss 9.777752 logits_loss 444.381118\n",
      "[44:1500] F: 26880000 \t tot_loss 15.838243 rs_loss 0.627528 vs_loss 10.765728 logits_loss 444.498761\n",
      "[44:1600] F: 26912000 \t tot_loss 15.373127 rs_loss 0.659346 vs_loss 10.264629 logits_loss 444.915213\n",
      "[44:1700] F: 26944000 \t tot_loss 16.127787 rs_loss 0.736823 vs_loss 10.938024 logits_loss 445.294024\n",
      "[44:1800] F: 26976000 \t tot_loss 20.357396 rs_loss 1.497532 vs_loss 14.403607 logits_loss 445.625721\n",
      "Batch [45] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.95 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.75 (+-0.20) \n",
      "[45:0] F: 27000000 \t tot_loss 20.272033 rs_loss 1.529432 vs_loss 14.285722 logits_loss 445.687929\n",
      "[45:100] F: 27032000 \t tot_loss 19.114232 rs_loss 1.488523 vs_loss 13.170536 logits_loss 445.517297\n",
      "[45:200] F: 27064000 \t tot_loss 18.434003 rs_loss 1.252777 vs_loss 12.728851 logits_loss 445.237484\n",
      "[45:300] F: 27096000 \t tot_loss 14.357097 rs_loss 0.762638 vs_loss 9.139257 logits_loss 445.520230\n",
      "[45:400] F: 27128000 \t tot_loss 14.161272 rs_loss 0.953915 vs_loss 8.758792 logits_loss 444.856490\n",
      "[45:500] F: 27160000 \t tot_loss 13.586201 rs_loss 0.948279 vs_loss 8.186544 logits_loss 445.137859\n",
      "[45:600] F: 27192000 \t tot_loss 13.316883 rs_loss 1.063478 vs_loss 7.799914 logits_loss 445.349059\n",
      "[45:700] F: 27224000 \t tot_loss 11.819505 rs_loss 0.764664 vs_loss 6.603165 logits_loss 445.167609\n",
      "[45:800] F: 27256000 \t tot_loss 11.914863 rs_loss 0.671463 vs_loss 6.787970 logits_loss 445.542962\n",
      "[45:900] F: 27288000 \t tot_loss 11.956325 rs_loss 0.691275 vs_loss 6.810972 logits_loss 445.407727\n",
      "[45:1000] F: 27320000 \t tot_loss 11.067738 rs_loss 0.550320 vs_loss 6.062604 logits_loss 445.481349\n",
      "[45:1100] F: 27352000 \t tot_loss 11.316932 rs_loss 0.536451 vs_loss 6.324049 logits_loss 445.643198\n",
      "[45:1200] F: 27384000 \t tot_loss 10.817034 rs_loss 0.430024 vs_loss 5.929256 logits_loss 445.775512\n",
      "[45:1300] F: 27416000 \t tot_loss 10.905771 rs_loss 0.409290 vs_loss 6.039194 logits_loss 445.728778\n",
      "[45:1400] F: 27448000 \t tot_loss 12.281870 rs_loss 0.700056 vs_loss 7.132072 logits_loss 444.974181\n",
      "[45:1500] F: 27480000 \t tot_loss 12.817816 rs_loss 0.841573 vs_loss 7.527337 logits_loss 444.890505\n",
      "[45:1600] F: 27512000 \t tot_loss 14.389962 rs_loss 0.887441 vs_loss 9.049163 logits_loss 445.335810\n",
      "[45:1700] F: 27544000 \t tot_loss 15.168670 rs_loss 0.879667 vs_loss 9.834202 logits_loss 445.480087\n",
      "[45:1800] F: 27576000 \t tot_loss 13.801250 rs_loss 0.629679 vs_loss 8.712267 logits_loss 445.930451\n",
      "Batch [46] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.06 (+-0.27) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.68 (+-0.20) \n",
      "[46:0] F: 27600000 \t tot_loss 13.977292 rs_loss 0.741186 vs_loss 8.778701 logits_loss 445.740466\n",
      "[46:100] F: 27632000 \t tot_loss 14.692810 rs_loss 0.725105 vs_loss 9.516840 logits_loss 445.086507\n",
      "[46:200] F: 27664000 \t tot_loss 14.278940 rs_loss 0.747780 vs_loss 9.087986 logits_loss 444.317439\n",
      "[46:300] F: 27696000 \t tot_loss 15.645998 rs_loss 1.101208 vs_loss 10.106495 logits_loss 443.829414\n",
      "[46:400] F: 27728000 \t tot_loss 15.871201 rs_loss 1.179284 vs_loss 10.258967 logits_loss 443.295005\n",
      "[46:500] F: 27760000 \t tot_loss 17.210148 rs_loss 1.781470 vs_loss 10.998101 logits_loss 443.057733\n",
      "[46:600] F: 27792000 \t tot_loss 18.740280 rs_loss 2.309461 vs_loss 11.997719 logits_loss 443.310019\n",
      "[46:700] F: 27824000 \t tot_loss 18.492890 rs_loss 1.920976 vs_loss 12.138010 logits_loss 443.390350\n",
      "[46:800] F: 27856000 \t tot_loss 18.334283 rs_loss 1.669613 vs_loss 12.230916 logits_loss 443.375413\n",
      "[46:900] F: 27888000 \t tot_loss 17.991383 rs_loss 1.631781 vs_loss 11.926162 logits_loss 443.344068\n",
      "[46:1000] F: 27920000 \t tot_loss 16.545762 rs_loss 1.348292 vs_loss 10.760955 logits_loss 443.651541\n",
      "[46:1100] F: 27952000 \t tot_loss 19.725994 rs_loss 2.035734 vs_loss 13.254654 logits_loss 443.560612\n",
      "[46:1200] F: 27984000 \t tot_loss 20.309336 rs_loss 2.225684 vs_loss 13.650039 logits_loss 443.361267\n",
      "[46:1300] F: 28016000 \t tot_loss 17.418569 rs_loss 1.617523 vs_loss 11.370600 logits_loss 443.044585\n",
      "[46:1400] F: 28048000 \t tot_loss 18.998534 rs_loss 1.858514 vs_loss 12.711291 logits_loss 442.872864\n",
      "[46:1500] F: 28080000 \t tot_loss 14.832872 rs_loss 1.212231 vs_loss 9.187269 logits_loss 443.337174\n",
      "[46:1600] F: 28112000 \t tot_loss 14.286062 rs_loss 0.968528 vs_loss 8.879260 logits_loss 443.827355\n",
      "[46:1700] F: 28144000 \t tot_loss 15.395549 rs_loss 0.997566 vs_loss 9.953528 logits_loss 444.445455\n",
      "[46:1800] F: 28176000 \t tot_loss 13.558205 rs_loss 0.513975 vs_loss 8.604406 logits_loss 443.982458\n",
      "Batch [47] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.19 (+-0.26) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.70 (+-0.15) \n",
      "[47:0] F: 28200000 \t tot_loss 16.101680 rs_loss 1.046965 vs_loss 10.617550 logits_loss 443.716474\n",
      "[47:100] F: 28232000 \t tot_loss 16.657574 rs_loss 1.076482 vs_loss 11.147874 logits_loss 443.321911\n",
      "[47:200] F: 28264000 \t tot_loss 20.317938 rs_loss 1.910629 vs_loss 13.978015 logits_loss 442.929325\n",
      "[47:300] F: 28296000 \t tot_loss 20.287804 rs_loss 2.096686 vs_loss 13.762517 logits_loss 442.860151\n",
      "[47:400] F: 28328000 \t tot_loss 20.530642 rs_loss 1.853444 vs_loss 14.247029 logits_loss 443.016975\n",
      "[47:500] F: 28360000 \t tot_loss 23.051818 rs_loss 2.136024 vs_loss 16.481659 logits_loss 443.413477\n",
      "[47:600] F: 28392000 \t tot_loss 21.510651 rs_loss 1.599222 vs_loss 15.473115 logits_loss 443.831397\n",
      "[47:700] F: 28424000 \t tot_loss 24.891063 rs_loss 1.866820 vs_loss 18.581779 logits_loss 444.246457\n",
      "[47:800] F: 28456000 \t tot_loss 24.330396 rs_loss 1.817670 vs_loss 18.069646 logits_loss 444.307952\n",
      "[47:900] F: 28488000 \t tot_loss 24.648047 rs_loss 1.963203 vs_loss 18.246109 logits_loss 443.873451\n",
      "[47:1000] F: 28520000 \t tot_loss 22.862258 rs_loss 1.942818 vs_loss 16.479886 logits_loss 443.955481\n",
      "[47:1100] F: 28552000 \t tot_loss 20.074563 rs_loss 1.583116 vs_loss 14.056566 logits_loss 443.488108\n",
      "[47:1200] F: 28584000 \t tot_loss 18.640072 rs_loss 1.302553 vs_loss 12.902052 logits_loss 443.546784\n",
      "[47:1300] F: 28616000 \t tot_loss 18.563382 rs_loss 1.139909 vs_loss 12.986796 logits_loss 443.667695\n",
      "[47:1400] F: 28648000 \t tot_loss 20.140751 rs_loss 1.170745 vs_loss 14.534837 logits_loss 443.516967\n",
      "[47:1500] F: 28680000 \t tot_loss 22.251322 rs_loss 1.343307 vs_loss 16.471431 logits_loss 443.658471\n",
      "[47:1600] F: 28712000 \t tot_loss 22.612794 rs_loss 1.377000 vs_loss 16.804420 logits_loss 443.137386\n",
      "[47:1700] F: 28744000 \t tot_loss 25.154108 rs_loss 2.101597 vs_loss 18.617397 logits_loss 443.511440\n",
      "[47:1800] F: 28776000 \t tot_loss 24.935431 rs_loss 2.128899 vs_loss 18.374990 logits_loss 443.154241\n",
      "Batch [48] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.02 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.53 (+-0.15) \n",
      "[48:0] F: 28800000 \t tot_loss 22.440744 rs_loss 1.870301 vs_loss 16.137355 logits_loss 443.308775\n",
      "[48:100] F: 28832000 \t tot_loss 21.426930 rs_loss 1.820958 vs_loss 15.166341 logits_loss 443.963085\n",
      "[48:200] F: 28864000 \t tot_loss 18.258696 rs_loss 1.395052 vs_loss 12.424196 logits_loss 443.944856\n",
      "[48:300] F: 28896000 \t tot_loss 17.357458 rs_loss 1.482729 vs_loss 11.437686 logits_loss 443.704279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48:400] F: 28928000 \t tot_loss 16.461327 rs_loss 1.486493 vs_loss 10.537164 logits_loss 443.767067\n",
      "[48:500] F: 28960000 \t tot_loss 17.595275 rs_loss 1.562693 vs_loss 11.594623 logits_loss 443.795851\n",
      "[48:600] F: 28992000 \t tot_loss 19.595181 rs_loss 1.925076 vs_loss 13.231995 logits_loss 443.811020\n",
      "[48:700] F: 29024000 \t tot_loss 16.999094 rs_loss 1.290986 vs_loss 11.265766 logits_loss 444.234200\n",
      "[48:800] F: 29056000 \t tot_loss 17.935289 rs_loss 1.255618 vs_loss 12.235699 logits_loss 444.397264\n",
      "[48:900] F: 29088000 \t tot_loss 19.789758 rs_loss 1.500613 vs_loss 13.844061 logits_loss 444.508450\n",
      "[48:1000] F: 29120000 \t tot_loss 15.691419 rs_loss 0.706155 vs_loss 10.539827 logits_loss 444.543781\n",
      "[48:1100] F: 29152000 \t tot_loss 17.366406 rs_loss 1.147648 vs_loss 11.775572 logits_loss 444.318601\n",
      "[48:1200] F: 29184000 \t tot_loss 17.575500 rs_loss 1.178597 vs_loss 11.952278 logits_loss 444.462623\n",
      "[48:1300] F: 29216000 \t tot_loss 18.467067 rs_loss 1.599898 vs_loss 12.423435 logits_loss 444.373448\n",
      "[48:1400] F: 29248000 \t tot_loss 19.046767 rs_loss 1.692230 vs_loss 12.907674 logits_loss 444.686202\n",
      "[48:1500] F: 29280000 \t tot_loss 19.155991 rs_loss 1.352875 vs_loss 13.352977 logits_loss 445.013905\n",
      "[48:1600] F: 29312000 \t tot_loss 20.900355 rs_loss 1.704108 vs_loss 14.745218 logits_loss 445.102842\n",
      "[48:1700] F: 29344000 \t tot_loss 18.679416 rs_loss 1.096229 vs_loss 13.129521 logits_loss 445.366508\n",
      "[48:1800] F: 29376000 \t tot_loss 22.571287 rs_loss 1.803915 vs_loss 16.318745 logits_loss 444.862743\n",
      "Batch [49] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.85 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.93 (+-0.28) \n",
      "[49:0] F: 29400000 \t tot_loss 21.642201 rs_loss 1.975443 vs_loss 15.220889 logits_loss 444.586938\n",
      "[49:100] F: 29432000 \t tot_loss 19.409962 rs_loss 1.701101 vs_loss 13.261395 logits_loss 444.746586\n",
      "[49:200] F: 29464000 \t tot_loss 17.961081 rs_loss 1.771859 vs_loss 11.746745 logits_loss 444.247778\n",
      "[49:300] F: 29496000 \t tot_loss 17.681397 rs_loss 2.038294 vs_loss 11.201954 logits_loss 444.114877\n",
      "[49:400] F: 29528000 \t tot_loss 16.667469 rs_loss 1.777623 vs_loss 10.444052 logits_loss 444.579354\n",
      "[49:500] F: 29560000 \t tot_loss 16.983864 rs_loss 1.798019 vs_loss 10.743790 logits_loss 444.205570\n",
      "[49:600] F: 29592000 \t tot_loss 16.657627 rs_loss 1.626744 vs_loss 10.587199 logits_loss 444.368466\n",
      "[49:700] F: 29624000 \t tot_loss 13.687131 rs_loss 0.600577 vs_loss 8.641511 logits_loss 444.504373\n",
      "[49:800] F: 29656000 \t tot_loss 14.926049 rs_loss 0.725331 vs_loss 9.756731 logits_loss 444.398735\n",
      "[49:900] F: 29688000 \t tot_loss 15.105194 rs_loss 0.743401 vs_loss 9.916862 logits_loss 444.493106\n",
      "[49:1000] F: 29720000 \t tot_loss 15.468341 rs_loss 0.941402 vs_loss 10.082894 logits_loss 444.404521\n",
      "[49:1100] F: 29752000 \t tot_loss 16.410515 rs_loss 1.146526 vs_loss 10.822535 logits_loss 444.145410\n",
      "[49:1200] F: 29784000 \t tot_loss 16.848476 rs_loss 1.342445 vs_loss 11.066108 logits_loss 443.992199\n",
      "[49:1300] F: 29816000 \t tot_loss 16.553825 rs_loss 1.305996 vs_loss 10.811692 logits_loss 443.613676\n",
      "[49:1400] F: 29848000 \t tot_loss 16.838361 rs_loss 1.288170 vs_loss 11.112934 logits_loss 443.725740\n",
      "[49:1500] F: 29880000 \t tot_loss 14.142714 rs_loss 1.028420 vs_loss 8.678374 logits_loss 443.592101\n",
      "[49:1600] F: 29912000 \t tot_loss 13.646236 rs_loss 0.790546 vs_loss 8.419654 logits_loss 443.603587\n",
      "[49:1700] F: 29944000 \t tot_loss 13.670066 rs_loss 0.813826 vs_loss 8.418631 logits_loss 443.760955\n",
      "[49:1800] F: 29976000 \t tot_loss 13.351220 rs_loss 0.817794 vs_loss 8.095903 logits_loss 443.752368\n",
      "Batch [50] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.65 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.05 (+-0.24) \n",
      "[50:0] F: 30000000 \t tot_loss 14.526080 rs_loss 0.869930 vs_loss 9.217747 logits_loss 443.840425\n",
      "[50:100] F: 30032000 \t tot_loss 15.255937 rs_loss 0.952969 vs_loss 9.869990 logits_loss 443.297709\n",
      "[50:200] F: 30064000 \t tot_loss 15.165363 rs_loss 0.973199 vs_loss 9.758524 logits_loss 443.364102\n",
      "[50:300] F: 30096000 \t tot_loss 16.997077 rs_loss 1.420133 vs_loss 11.145415 logits_loss 443.152921\n",
      "[50:400] F: 30128000 \t tot_loss 17.268863 rs_loss 1.543999 vs_loss 11.293901 logits_loss 443.096230\n",
      "[50:500] F: 30160000 \t tot_loss 17.405805 rs_loss 1.450882 vs_loss 11.523733 logits_loss 443.119039\n",
      "[50:600] F: 30192000 \t tot_loss 18.476359 rs_loss 1.659347 vs_loss 12.389943 logits_loss 442.706834\n",
      "[50:700] F: 30224000 \t tot_loss 16.365848 rs_loss 0.904152 vs_loss 11.035165 logits_loss 442.653087\n",
      "[50:800] F: 30256000 \t tot_loss 15.986220 rs_loss 1.232678 vs_loss 10.329068 logits_loss 442.447390\n",
      "[50:900] F: 30288000 \t tot_loss 14.275660 rs_loss 1.202289 vs_loss 8.644650 logits_loss 442.872107\n",
      "[50:1000] F: 30320000 \t tot_loss 13.356807 rs_loss 1.018460 vs_loss 7.905744 logits_loss 443.260283\n",
      "[50:1100] F: 30352000 \t tot_loss 15.393276 rs_loss 1.454197 vs_loss 9.509315 logits_loss 442.976415\n",
      "[50:1200] F: 30384000 \t tot_loss 16.863979 rs_loss 1.437274 vs_loss 10.993267 logits_loss 443.343854\n",
      "[50:1300] F: 30416000 \t tot_loss 18.106224 rs_loss 1.444217 vs_loss 12.226883 logits_loss 443.512363\n",
      "[50:1400] F: 30448000 \t tot_loss 18.712417 rs_loss 1.408836 vs_loss 12.873330 logits_loss 443.025170\n",
      "[50:1500] F: 30480000 \t tot_loss 18.065934 rs_loss 1.537226 vs_loss 12.094007 logits_loss 443.470136\n",
      "[50:1600] F: 30512000 \t tot_loss 15.898649 rs_loss 1.068480 vs_loss 10.395410 logits_loss 443.475983\n",
      "[50:1700] F: 30544000 \t tot_loss 16.469515 rs_loss 1.447978 vs_loss 10.594894 logits_loss 442.664358\n",
      "[50:1800] F: 30576000 \t tot_loss 17.046979 rs_loss 1.794767 vs_loss 10.823201 logits_loss 442.901024\n",
      "Batch [51] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.99 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.94 (+-0.24) \n",
      "[51:0] F: 30600000 \t tot_loss 18.615771 rs_loss 2.041987 vs_loss 12.147874 logits_loss 442.590930\n",
      "[51:100] F: 30632000 \t tot_loss 19.202390 rs_loss 1.816892 vs_loss 12.954897 logits_loss 443.060091\n",
      "[51:200] F: 30664000 \t tot_loss 21.941411 rs_loss 2.030949 vs_loss 15.470803 logits_loss 443.965818\n",
      "[51:300] F: 30696000 \t tot_loss 20.444495 rs_loss 1.839332 vs_loss 14.159802 logits_loss 444.536136\n",
      "[51:400] F: 30728000 \t tot_loss 19.174367 rs_loss 1.377650 vs_loss 13.341885 logits_loss 445.483107\n",
      "[51:500] F: 30760000 \t tot_loss 20.836080 rs_loss 1.765869 vs_loss 14.613915 logits_loss 445.629664\n",
      "[51:600] F: 30792000 \t tot_loss 18.713994 rs_loss 1.496051 vs_loss 12.761100 logits_loss 445.684375\n",
      "[51:700] F: 30824000 \t tot_loss 21.167999 rs_loss 1.741746 vs_loss 14.971413 logits_loss 445.483999\n",
      "[51:800] F: 30856000 \t tot_loss 19.075507 rs_loss 1.476966 vs_loss 13.146186 logits_loss 445.235547\n",
      "[51:900] F: 30888000 \t tot_loss 16.639792 rs_loss 1.076379 vs_loss 11.117694 logits_loss 444.571936\n",
      "[51:1000] F: 30920000 \t tot_loss 18.406117 rs_loss 1.171215 vs_loss 12.790557 logits_loss 444.434451\n",
      "[51:1100] F: 30952000 \t tot_loss 17.826838 rs_loss 0.978811 vs_loss 12.404470 logits_loss 444.355717\n",
      "[51:1200] F: 30984000 \t tot_loss 17.930005 rs_loss 1.011749 vs_loss 12.472410 logits_loss 444.584648\n",
      "[51:1300] F: 31016000 \t tot_loss 19.149605 rs_loss 1.368168 vs_loss 13.334550 logits_loss 444.688686\n",
      "[51:1400] F: 31048000 \t tot_loss 21.178947 rs_loss 1.620693 vs_loss 15.105662 logits_loss 445.259175\n",
      "[51:1500] F: 31080000 \t tot_loss 19.285575 rs_loss 1.369976 vs_loss 13.459948 logits_loss 445.565195\n",
      "[51:1600] F: 31112000 \t tot_loss 20.934195 rs_loss 1.358433 vs_loss 15.122092 logits_loss 445.367005\n",
      "[51:1700] F: 31144000 \t tot_loss 20.601457 rs_loss 1.233374 vs_loss 14.910294 logits_loss 445.778933\n",
      "[51:1800] F: 31176000 \t tot_loss 16.035203 rs_loss 0.955864 vs_loss 10.629421 logits_loss 444.991692\n",
      "Batch [52] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.80 (+-0.20) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.57 (+-0.16) \n",
      "[52:0] F: 31200000 \t tot_loss 16.825882 rs_loss 1.292333 vs_loss 11.086259 logits_loss 444.729067\n",
      "[52:100] F: 31232000 \t tot_loss 17.099182 rs_loss 1.347639 vs_loss 11.306121 logits_loss 444.542229\n",
      "[52:200] F: 31264000 \t tot_loss 15.719390 rs_loss 1.311909 vs_loss 9.969869 logits_loss 443.761232\n",
      "[52:300] F: 31296000 \t tot_loss 17.316882 rs_loss 1.409008 vs_loss 11.468685 logits_loss 443.918925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52:400] F: 31328000 \t tot_loss 17.209156 rs_loss 1.149061 vs_loss 11.624775 logits_loss 443.532075\n",
      "[52:500] F: 31360000 \t tot_loss 16.592372 rs_loss 1.107667 vs_loss 11.046839 logits_loss 443.786684\n",
      "[52:600] F: 31392000 \t tot_loss 15.781751 rs_loss 0.913542 vs_loss 10.428177 logits_loss 444.003210\n",
      "[52:700] F: 31424000 \t tot_loss 16.153050 rs_loss 1.173249 vs_loss 10.547372 logits_loss 443.242917\n",
      "[52:800] F: 31456000 \t tot_loss 19.714996 rs_loss 1.709827 vs_loss 13.571529 logits_loss 443.364024\n",
      "[52:900] F: 31488000 \t tot_loss 22.966148 rs_loss 1.958503 vs_loss 16.578982 logits_loss 442.866365\n",
      "[52:1000] F: 31520000 \t tot_loss 22.353411 rs_loss 1.897691 vs_loss 16.025698 logits_loss 443.002204\n",
      "[52:1100] F: 31552000 \t tot_loss 19.630808 rs_loss 1.301201 vs_loss 13.896917 logits_loss 443.269051\n",
      "[52:1200] F: 31584000 \t tot_loss 15.747257 rs_loss 0.726154 vs_loss 10.583091 logits_loss 443.801262\n",
      "[52:1300] F: 31616000 \t tot_loss 13.308045 rs_loss 0.723019 vs_loss 8.143277 logits_loss 444.174871\n",
      "[52:1400] F: 31648000 \t tot_loss 17.652667 rs_loss 1.505234 vs_loss 11.709122 logits_loss 443.831083\n",
      "[52:1500] F: 31680000 \t tot_loss 20.529917 rs_loss 2.103096 vs_loss 13.985395 logits_loss 444.142645\n",
      "[52:1600] F: 31712000 \t tot_loss 20.251164 rs_loss 2.166957 vs_loss 13.650241 logits_loss 443.396563\n",
      "[52:1700] F: 31744000 \t tot_loss 19.921991 rs_loss 1.892987 vs_loss 13.593842 logits_loss 443.516229\n",
      "[52:1800] F: 31776000 \t tot_loss 16.725563 rs_loss 1.260413 vs_loss 11.030843 logits_loss 443.430640\n",
      "Batch [53] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.09 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.90 (+-0.24) \n",
      "[53:0] F: 31800000 \t tot_loss 17.310788 rs_loss 1.257959 vs_loss 11.621848 logits_loss 443.098118\n",
      "[53:100] F: 31832000 \t tot_loss 15.817741 rs_loss 0.595397 vs_loss 10.785591 logits_loss 443.675291\n",
      "[53:200] F: 31864000 \t tot_loss 17.404679 rs_loss 1.076299 vs_loss 11.890950 logits_loss 443.742910\n",
      "[53:300] F: 31896000 \t tot_loss 17.901441 rs_loss 0.959213 vs_loss 12.496986 logits_loss 444.524231\n",
      "[53:400] F: 31928000 \t tot_loss 18.151568 rs_loss 0.992496 vs_loss 12.709925 logits_loss 444.914714\n",
      "[53:500] F: 31960000 \t tot_loss 18.609723 rs_loss 1.385047 vs_loss 12.773416 logits_loss 445.125984\n",
      "[53:600] F: 31992000 \t tot_loss 20.802189 rs_loss 1.641374 vs_loss 14.709546 logits_loss 445.126928\n",
      "[53:700] F: 32024000 \t tot_loss 22.054247 rs_loss 2.172529 vs_loss 15.432019 logits_loss 444.969874\n",
      "[53:800] F: 32056000 \t tot_loss 24.754982 rs_loss 2.884396 vs_loss 17.420362 logits_loss 445.022466\n",
      "[53:900] F: 32088000 \t tot_loss 23.805353 rs_loss 2.549086 vs_loss 16.807317 logits_loss 444.894999\n",
      "[53:1000] F: 32120000 \t tot_loss 19.142752 rs_loss 1.828432 vs_loss 12.862682 logits_loss 445.163798\n",
      "[53:1100] F: 32152000 \t tot_loss 17.532137 rs_loss 1.509771 vs_loss 11.572027 logits_loss 445.033979\n",
      "[53:1200] F: 32184000 \t tot_loss 15.180230 rs_loss 0.811386 vs_loss 9.915604 logits_loss 445.323957\n",
      "[53:1300] F: 32216000 \t tot_loss 14.879757 rs_loss 0.778238 vs_loss 9.645689 logits_loss 445.583016\n",
      "[53:1400] F: 32248000 \t tot_loss 15.386584 rs_loss 0.879464 vs_loss 10.051612 logits_loss 445.550848\n",
      "[53:1500] F: 32280000 \t tot_loss 14.713357 rs_loss 0.720292 vs_loss 9.538403 logits_loss 445.466169\n",
      "[53:1600] F: 32312000 \t tot_loss 13.685193 rs_loss 0.689839 vs_loss 8.546047 logits_loss 444.930682\n",
      "[53:1700] F: 32344000 \t tot_loss 14.019858 rs_loss 0.911692 vs_loss 8.659979 logits_loss 444.818659\n",
      "[53:1800] F: 32376000 \t tot_loss 13.807072 rs_loss 0.798212 vs_loss 8.558047 logits_loss 445.081327\n",
      "Batch [54] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.04 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.79 (+-0.20) \n",
      "[54:0] F: 32400000 \t tot_loss 14.392646 rs_loss 0.943639 vs_loss 8.999554 logits_loss 444.945332\n",
      "[54:100] F: 32432000 \t tot_loss 16.262851 rs_loss 0.976188 vs_loss 10.830346 logits_loss 445.631604\n",
      "[54:200] F: 32464000 \t tot_loss 16.727958 rs_loss 0.936304 vs_loss 11.337841 logits_loss 445.381391\n",
      "[54:300] F: 32496000 \t tot_loss 18.041261 rs_loss 1.177238 vs_loss 12.411219 logits_loss 445.280465\n",
      "[54:400] F: 32528000 \t tot_loss 20.837102 rs_loss 1.497402 vs_loss 14.889948 logits_loss 444.975273\n",
      "[54:500] F: 32560000 \t tot_loss 21.099429 rs_loss 1.945968 vs_loss 14.709794 logits_loss 444.366638\n",
      "[54:600] F: 32592000 \t tot_loss 21.127183 rs_loss 2.227533 vs_loss 14.457580 logits_loss 444.207049\n",
      "[54:700] F: 32624000 \t tot_loss 21.634605 rs_loss 2.259266 vs_loss 14.931197 logits_loss 444.414278\n",
      "[54:800] F: 32656000 \t tot_loss 18.680689 rs_loss 1.820606 vs_loss 12.413943 logits_loss 444.614095\n",
      "[54:900] F: 32688000 \t tot_loss 16.698287 rs_loss 1.325756 vs_loss 10.919093 logits_loss 445.343877\n",
      "[54:1000] F: 32720000 \t tot_loss 16.163791 rs_loss 1.058689 vs_loss 10.649808 logits_loss 445.529403\n",
      "[54:1100] F: 32752000 \t tot_loss 14.629961 rs_loss 0.818667 vs_loss 9.357113 logits_loss 445.418077\n",
      "[54:1200] F: 32784000 \t tot_loss 15.052641 rs_loss 0.768976 vs_loss 9.825915 logits_loss 445.775070\n",
      "[54:1300] F: 32816000 \t tot_loss 16.331495 rs_loss 0.764596 vs_loss 11.117036 logits_loss 444.986284\n",
      "[54:1400] F: 32848000 \t tot_loss 15.690246 rs_loss 0.559048 vs_loss 10.678764 logits_loss 445.243389\n",
      "[54:1500] F: 32880000 \t tot_loss 15.075532 rs_loss 0.536005 vs_loss 10.087928 logits_loss 445.159808\n",
      "[54:1600] F: 32912000 \t tot_loss 14.882230 rs_loss 0.512770 vs_loss 9.923092 logits_loss 444.636812\n",
      "[54:1700] F: 32944000 \t tot_loss 14.130116 rs_loss 0.536019 vs_loss 9.143443 logits_loss 445.065458\n",
      "[54:1800] F: 32976000 \t tot_loss 17.147204 rs_loss 0.797905 vs_loss 11.901206 logits_loss 444.809277\n",
      "Batch [55] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.03 (+-0.24) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 1.01 (+-0.24) \n",
      "[55:0] F: 33000000 \t tot_loss 18.960868 rs_loss 1.159252 vs_loss 13.355753 logits_loss 444.586304\n",
      "[55:100] F: 33032000 \t tot_loss 18.611954 rs_loss 1.141265 vs_loss 13.021435 logits_loss 444.925426\n",
      "[55:200] F: 33064000 \t tot_loss 19.068988 rs_loss 1.147566 vs_loss 13.475641 logits_loss 444.578149\n",
      "[55:300] F: 33096000 \t tot_loss 17.568354 rs_loss 1.362354 vs_loss 11.756957 logits_loss 444.904286\n",
      "[55:400] F: 33128000 \t tot_loss 16.011552 rs_loss 1.231927 vs_loss 10.326706 logits_loss 445.291893\n",
      "[55:500] F: 33160000 \t tot_loss 16.154556 rs_loss 1.260696 vs_loss 10.444856 logits_loss 444.900491\n",
      "[55:600] F: 33192000 \t tot_loss 17.325842 rs_loss 1.244915 vs_loss 11.628392 logits_loss 445.253450\n",
      "[55:700] F: 33224000 \t tot_loss 17.264270 rs_loss 1.236135 vs_loss 11.574986 logits_loss 445.314857\n",
      "[55:800] F: 33256000 \t tot_loss 16.854899 rs_loss 1.006901 vs_loss 11.398745 logits_loss 444.925345\n",
      "[55:900] F: 33288000 \t tot_loss 17.369245 rs_loss 1.003125 vs_loss 11.912535 logits_loss 445.358449\n",
      "[55:1000] F: 33320000 \t tot_loss 15.619985 rs_loss 0.984599 vs_loss 10.184470 logits_loss 445.091588\n",
      "[55:1100] F: 33352000 \t tot_loss 13.097278 rs_loss 0.504698 vs_loss 8.146032 logits_loss 444.654801\n",
      "[55:1200] F: 33384000 \t tot_loss 12.619542 rs_loss 0.511577 vs_loss 7.657441 logits_loss 445.052349\n",
      "[55:1300] F: 33416000 \t tot_loss 11.503600 rs_loss 0.570478 vs_loss 6.485225 logits_loss 444.789789\n",
      "[55:1400] F: 33448000 \t tot_loss 12.324801 rs_loss 0.802273 vs_loss 7.068674 logits_loss 445.385383\n",
      "[55:1500] F: 33480000 \t tot_loss 12.523593 rs_loss 0.783003 vs_loss 7.283530 logits_loss 445.705957\n",
      "[55:1600] F: 33512000 \t tot_loss 12.603854 rs_loss 0.761941 vs_loss 7.390754 logits_loss 445.115959\n",
      "[55:1700] F: 33544000 \t tot_loss 13.677971 rs_loss 0.771614 vs_loss 8.453074 logits_loss 445.328317\n",
      "[55:1800] F: 33576000 \t tot_loss 13.960664 rs_loss 0.532305 vs_loss 8.978431 logits_loss 444.992764\n",
      "Batch [56] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.05 (+-0.23) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.65 (+-0.16) \n",
      "[56:0] F: 33600000 \t tot_loss 13.939800 rs_loss 0.540528 vs_loss 8.948265 logits_loss 445.100596\n",
      "[56:100] F: 33632000 \t tot_loss 15.732452 rs_loss 0.859236 vs_loss 10.423846 logits_loss 444.936960\n",
      "[56:200] F: 33664000 \t tot_loss 16.284105 rs_loss 1.088194 vs_loss 10.744984 logits_loss 445.092738\n",
      "[56:300] F: 33696000 \t tot_loss 16.673479 rs_loss 1.165343 vs_loss 11.062728 logits_loss 444.540788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56:400] F: 33728000 \t tot_loss 17.221692 rs_loss 1.209293 vs_loss 11.578455 logits_loss 443.394369\n",
      "[56:500] F: 33760000 \t tot_loss 16.389727 rs_loss 0.889977 vs_loss 11.065575 logits_loss 443.417567\n",
      "[56:600] F: 33792000 \t tot_loss 16.402409 rs_loss 0.713105 vs_loss 11.256800 logits_loss 443.250407\n",
      "[56:700] F: 33824000 \t tot_loss 16.246397 rs_loss 0.866974 vs_loss 10.945891 logits_loss 443.353181\n",
      "[56:800] F: 33856000 \t tot_loss 18.205771 rs_loss 1.311719 vs_loss 12.457706 logits_loss 443.634605\n",
      "[56:900] F: 33888000 \t tot_loss 17.526187 rs_loss 1.302061 vs_loss 11.791025 logits_loss 443.310066\n",
      "[56:1000] F: 33920000 \t tot_loss 16.119561 rs_loss 1.247869 vs_loss 10.440743 logits_loss 443.094968\n",
      "[56:1100] F: 33952000 \t tot_loss 15.386701 rs_loss 1.048329 vs_loss 9.907979 logits_loss 443.039315\n",
      "[56:1200] F: 33984000 \t tot_loss 14.650480 rs_loss 0.882679 vs_loss 9.332319 logits_loss 443.548226\n",
      "[56:1300] F: 34016000 \t tot_loss 15.005664 rs_loss 1.161068 vs_loss 9.408510 logits_loss 443.608580\n",
      "[56:1400] F: 34048000 \t tot_loss 16.858188 rs_loss 1.338898 vs_loss 11.087727 logits_loss 443.156234\n",
      "[56:1500] F: 34080000 \t tot_loss 17.195281 rs_loss 1.342947 vs_loss 11.421833 logits_loss 443.050068\n",
      "[56:1600] F: 34112000 \t tot_loss 16.021174 rs_loss 1.031481 vs_loss 10.565430 logits_loss 442.426282\n",
      "[56:1700] F: 34144000 \t tot_loss 15.306702 rs_loss 0.793449 vs_loss 10.088417 logits_loss 442.483654\n",
      "[56:1800] F: 34176000 \t tot_loss 13.943633 rs_loss 0.869931 vs_loss 8.644939 logits_loss 442.876415\n",
      "Batch [57] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.94 (+-0.24) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.55 (+-0.09) \n",
      "[57:0] F: 34200000 \t tot_loss 13.368444 rs_loss 0.871219 vs_loss 8.066674 logits_loss 443.055133\n",
      "[57:100] F: 34232000 \t tot_loss 12.026392 rs_loss 0.834225 vs_loss 6.754895 logits_loss 443.727146\n",
      "[57:200] F: 34264000 \t tot_loss 11.934094 rs_loss 0.818690 vs_loss 6.672588 logits_loss 444.281587\n",
      "[57:300] F: 34296000 \t tot_loss 11.480814 rs_loss 0.725075 vs_loss 6.308467 logits_loss 444.727148\n",
      "[57:400] F: 34328000 \t tot_loss 11.183015 rs_loss 0.445274 vs_loss 6.286666 logits_loss 445.107505\n",
      "[57:500] F: 34360000 \t tot_loss 11.136607 rs_loss 0.444778 vs_loss 6.239681 logits_loss 445.214767\n",
      "[57:600] F: 34392000 \t tot_loss 13.089449 rs_loss 0.805613 vs_loss 7.827701 logits_loss 445.613457\n",
      "[57:700] F: 34424000 \t tot_loss 13.094753 rs_loss 0.832707 vs_loss 7.804477 logits_loss 445.756951\n",
      "[57:800] F: 34456000 \t tot_loss 15.043065 rs_loss 1.141289 vs_loss 9.442591 logits_loss 445.918497\n",
      "[57:900] F: 34488000 \t tot_loss 18.104662 rs_loss 1.694841 vs_loss 11.951351 logits_loss 445.846940\n",
      "[57:1000] F: 34520000 \t tot_loss 16.214323 rs_loss 1.334560 vs_loss 10.422738 logits_loss 445.702527\n",
      "[57:1100] F: 34552000 \t tot_loss 17.362831 rs_loss 1.640996 vs_loss 11.262405 logits_loss 445.942998\n",
      "[57:1200] F: 34584000 \t tot_loss 17.029121 rs_loss 1.624553 vs_loss 10.946970 logits_loss 445.759879\n",
      "[57:1300] F: 34616000 \t tot_loss 15.557744 rs_loss 1.347199 vs_loss 9.752865 logits_loss 445.767921\n",
      "[57:1400] F: 34648000 \t tot_loss 15.347863 rs_loss 1.345769 vs_loss 9.541387 logits_loss 446.070787\n",
      "[57:1500] F: 34680000 \t tot_loss 14.510026 rs_loss 1.054647 vs_loss 8.997630 logits_loss 445.774852\n",
      "[57:1600] F: 34712000 \t tot_loss 14.138978 rs_loss 1.014617 vs_loss 8.668917 logits_loss 445.544366\n",
      "[57:1700] F: 34744000 \t tot_loss 14.138455 rs_loss 0.833898 vs_loss 8.847591 logits_loss 445.696557\n",
      "[57:1800] F: 34776000 \t tot_loss 13.970227 rs_loss 0.851831 vs_loss 8.665713 logits_loss 445.268268\n",
      "Batch [58] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.49 (+-0.08) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.64 (+-0.15) \n",
      "[58:0] F: 34800000 \t tot_loss 13.633582 rs_loss 0.801169 vs_loss 8.381200 logits_loss 445.121295\n",
      "[58:100] F: 34832000 \t tot_loss 14.685149 rs_loss 0.600066 vs_loss 9.629321 logits_loss 445.576317\n",
      "[58:200] F: 34864000 \t tot_loss 13.106863 rs_loss 0.474269 vs_loss 8.187158 logits_loss 444.543549\n",
      "[58:300] F: 34896000 \t tot_loss 13.347829 rs_loss 0.485872 vs_loss 8.416766 logits_loss 444.519094\n",
      "[58:400] F: 34928000 \t tot_loss 13.866524 rs_loss 0.482092 vs_loss 8.945117 logits_loss 443.931412\n",
      "[58:500] F: 34960000 \t tot_loss 12.271008 rs_loss 0.445947 vs_loss 7.384598 logits_loss 444.046254\n",
      "[58:600] F: 34992000 \t tot_loss 13.826268 rs_loss 0.796794 vs_loss 8.587629 logits_loss 444.184571\n",
      "[58:700] F: 35024000 \t tot_loss 13.970218 rs_loss 0.743886 vs_loss 8.785761 logits_loss 444.057117\n",
      "[58:800] F: 35056000 \t tot_loss 14.510834 rs_loss 0.764035 vs_loss 9.308442 logits_loss 443.835669\n",
      "[58:900] F: 35088000 \t tot_loss 13.493792 rs_loss 0.728148 vs_loss 8.327497 logits_loss 443.814759\n",
      "[58:1000] F: 35120000 \t tot_loss 12.151744 rs_loss 0.391329 vs_loss 7.319416 logits_loss 444.099929\n",
      "[58:1100] F: 35152000 \t tot_loss 14.322722 rs_loss 0.482086 vs_loss 9.404555 logits_loss 443.607998\n",
      "[58:1200] F: 35184000 \t tot_loss 13.578661 rs_loss 0.539218 vs_loss 8.601138 logits_loss 443.830481\n",
      "[58:1300] F: 35216000 \t tot_loss 14.165432 rs_loss 0.577871 vs_loss 9.153406 logits_loss 443.415592\n",
      "[58:1400] F: 35248000 \t tot_loss 15.196995 rs_loss 0.578052 vs_loss 10.186110 logits_loss 443.283244\n",
      "[58:1500] F: 35280000 \t tot_loss 15.050686 rs_loss 0.538728 vs_loss 10.077199 logits_loss 443.475889\n",
      "[58:1600] F: 35312000 \t tot_loss 15.697426 rs_loss 0.799203 vs_loss 10.460770 logits_loss 443.745320\n",
      "[58:1700] F: 35344000 \t tot_loss 16.622370 rs_loss 1.084047 vs_loss 11.104052 logits_loss 443.427127\n",
      "[58:1800] F: 35376000 \t tot_loss 16.454289 rs_loss 1.090831 vs_loss 10.934597 logits_loss 442.886086\n",
      "Batch [59] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.31 (+-0.29) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.69 (+-0.16) \n",
      "[59:0] F: 35400000 \t tot_loss 15.281740 rs_loss 1.097372 vs_loss 9.752173 logits_loss 443.219531\n",
      "[59:100] F: 35432000 \t tot_loss 14.603405 rs_loss 0.895574 vs_loss 9.278426 logits_loss 442.940558\n",
      "[59:200] F: 35464000 \t tot_loss 16.041538 rs_loss 0.812539 vs_loss 10.793171 logits_loss 443.582805\n",
      "[59:300] F: 35496000 \t tot_loss 16.601898 rs_loss 0.805184 vs_loss 11.355105 logits_loss 444.160924\n",
      "[59:400] F: 35528000 \t tot_loss 21.263110 rs_loss 1.516710 vs_loss 15.303133 logits_loss 444.326712\n",
      "[59:500] F: 35560000 \t tot_loss 23.449298 rs_loss 1.660423 vs_loss 17.347116 logits_loss 444.175847\n",
      "[59:600] F: 35592000 \t tot_loss 22.268447 rs_loss 1.696591 vs_loss 16.127228 logits_loss 444.462900\n",
      "[59:700] F: 35624000 \t tot_loss 21.380677 rs_loss 1.782752 vs_loss 15.152358 logits_loss 444.556812\n",
      "[59:800] F: 35656000 \t tot_loss 18.632611 rs_loss 1.053311 vs_loss 13.132923 logits_loss 444.637777\n",
      "[59:900] F: 35688000 \t tot_loss 17.220567 rs_loss 1.036251 vs_loss 11.735196 logits_loss 444.912076\n",
      "[59:1000] F: 35720000 \t tot_loss 17.251778 rs_loss 0.834956 vs_loss 11.969208 logits_loss 444.761348\n",
      "[59:1100] F: 35752000 \t tot_loss 18.391540 rs_loss 1.083675 vs_loss 12.860412 logits_loss 444.745391\n",
      "[59:1200] F: 35784000 \t tot_loss 18.204944 rs_loss 1.326917 vs_loss 12.434936 logits_loss 444.309095\n",
      "[59:1300] F: 35816000 \t tot_loss 17.983177 rs_loss 1.476527 vs_loss 12.063655 logits_loss 444.299508\n",
      "[59:1400] F: 35848000 \t tot_loss 20.583779 rs_loss 1.677758 vs_loss 14.463265 logits_loss 444.275572\n",
      "[59:1500] F: 35880000 \t tot_loss 21.525474 rs_loss 1.651234 vs_loss 15.431779 logits_loss 444.246131\n",
      "[59:1600] F: 35912000 \t tot_loss 20.908939 rs_loss 1.458746 vs_loss 15.004922 logits_loss 444.527171\n",
      "[59:1700] F: 35944000 \t tot_loss 22.852945 rs_loss 1.308080 vs_loss 17.093873 logits_loss 445.099281\n",
      "[59:1800] F: 35976000 \t tot_loss 20.972892 rs_loss 1.347500 vs_loss 15.172102 logits_loss 445.329072\n",
      "Batch [60] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.78 (+-0.15) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.77 (+-0.20) \n",
      "[60:0] F: 36000000 \t tot_loss 18.909000 rs_loss 1.064508 vs_loss 13.389841 logits_loss 445.465188\n",
      "[60:100] F: 36032000 \t tot_loss 17.518824 rs_loss 1.050408 vs_loss 12.013344 logits_loss 445.507224\n",
      "[60:200] F: 36064000 \t tot_loss 14.162383 rs_loss 0.921927 vs_loss 8.792499 logits_loss 444.795732\n",
      "[60:300] F: 36096000 \t tot_loss 15.902690 rs_loss 1.284982 vs_loss 10.174068 logits_loss 444.363960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60:400] F: 36128000 \t tot_loss 16.889969 rs_loss 1.192242 vs_loss 11.258858 logits_loss 443.886889\n",
      "[60:500] F: 36160000 \t tot_loss 18.840314 rs_loss 1.377463 vs_loss 13.028182 logits_loss 443.466962\n",
      "[60:600] F: 36192000 \t tot_loss 24.872948 rs_loss 2.446608 vs_loss 17.991282 logits_loss 443.505827\n",
      "[60:700] F: 36224000 \t tot_loss 22.909071 rs_loss 2.067535 vs_loss 16.404380 logits_loss 443.715636\n",
      "[60:800] F: 36256000 \t tot_loss 21.770005 rs_loss 2.363401 vs_loss 14.967427 logits_loss 443.917614\n",
      "[60:900] F: 36288000 \t tot_loss 19.286440 rs_loss 2.223943 vs_loss 12.618132 logits_loss 444.436495\n",
      "[60:1000] F: 36320000 \t tot_loss 13.672459 rs_loss 1.061396 vs_loss 8.160742 logits_loss 445.032125\n",
      "[60:1100] F: 36352000 \t tot_loss 13.683898 rs_loss 1.061285 vs_loss 8.174947 logits_loss 444.766708\n",
      "[60:1200] F: 36384000 \t tot_loss 12.347315 rs_loss 0.678781 vs_loss 7.214723 logits_loss 445.381096\n",
      "[60:1300] F: 36416000 \t tot_loss 15.623554 rs_loss 0.972157 vs_loss 10.201909 logits_loss 444.948817\n",
      "[60:1400] F: 36448000 \t tot_loss 16.089145 rs_loss 0.929832 vs_loss 10.712077 logits_loss 444.723579\n",
      "[60:1500] F: 36480000 \t tot_loss 16.336235 rs_loss 0.915945 vs_loss 10.968894 logits_loss 445.139704\n",
      "[60:1600] F: 36512000 \t tot_loss 17.918891 rs_loss 1.110417 vs_loss 12.370653 logits_loss 443.782116\n",
      "[60:1700] F: 36544000 \t tot_loss 15.810773 rs_loss 1.029006 vs_loss 10.340718 logits_loss 444.104831\n",
      "[60:1800] F: 36576000 \t tot_loss 18.557955 rs_loss 1.647470 vs_loss 12.473031 logits_loss 443.745512\n",
      "Batch [61] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.02 (+-0.24) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.69 (+-0.20) \n",
      "[61:0] F: 36600000 \t tot_loss 19.381627 rs_loss 1.604896 vs_loss 13.343219 logits_loss 443.351200\n",
      "[61:100] F: 36632000 \t tot_loss 19.419629 rs_loss 1.393690 vs_loss 13.588453 logits_loss 443.748609\n",
      "[61:200] F: 36664000 \t tot_loss 18.620856 rs_loss 1.097905 vs_loss 13.087907 logits_loss 443.504448\n",
      "[61:300] F: 36696000 \t tot_loss 16.663758 rs_loss 0.791332 vs_loss 11.433730 logits_loss 443.869622\n",
      "[61:400] F: 36728000 \t tot_loss 15.251966 rs_loss 0.752561 vs_loss 10.062000 logits_loss 443.740533\n",
      "[61:500] F: 36760000 \t tot_loss 14.849483 rs_loss 0.934947 vs_loss 9.479878 logits_loss 443.465843\n",
      "[61:600] F: 36792000 \t tot_loss 17.999026 rs_loss 1.402519 vs_loss 12.157613 logits_loss 443.889448\n",
      "[61:700] F: 36824000 \t tot_loss 20.449814 rs_loss 1.682610 vs_loss 14.331584 logits_loss 443.562046\n",
      "[61:800] F: 36856000 \t tot_loss 19.708021 rs_loss 1.744053 vs_loss 13.529146 logits_loss 443.482153\n",
      "[61:900] F: 36888000 \t tot_loss 20.969057 rs_loss 1.829231 vs_loss 14.697339 logits_loss 444.248624\n",
      "[61:1000] F: 36920000 \t tot_loss 19.628302 rs_loss 1.684725 vs_loss 13.502252 logits_loss 444.132433\n",
      "[61:1100] F: 36952000 \t tot_loss 18.652980 rs_loss 1.384947 vs_loss 12.827800 logits_loss 444.023288\n",
      "[61:1200] F: 36984000 \t tot_loss 19.174045 rs_loss 1.330478 vs_loss 13.396795 logits_loss 444.677298\n",
      "[61:1300] F: 37016000 \t tot_loss 19.565605 rs_loss 1.591773 vs_loss 13.528749 logits_loss 444.508251\n",
      "[61:1400] F: 37048000 \t tot_loss 21.487754 rs_loss 1.668622 vs_loss 15.368827 logits_loss 445.030480\n",
      "[61:1500] F: 37080000 \t tot_loss 21.366775 rs_loss 1.911960 vs_loss 15.007893 logits_loss 444.692201\n",
      "[61:1600] F: 37112000 \t tot_loss 22.201348 rs_loss 1.974160 vs_loss 15.783797 logits_loss 444.339112\n",
      "[61:1700] F: 37144000 \t tot_loss 19.886526 rs_loss 1.554979 vs_loss 13.889789 logits_loss 444.175803\n",
      "[61:1800] F: 37176000 \t tot_loss 16.592623 rs_loss 1.397588 vs_loss 10.756127 logits_loss 443.890776\n",
      "Batch [62] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 0.83 (+-0.14) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.86 (+-0.24) \n",
      "[62:0] F: 37200000 \t tot_loss 15.580465 rs_loss 1.335434 vs_loss 9.808180 logits_loss 443.685062\n",
      "[62:100] F: 37232000 \t tot_loss 13.529997 rs_loss 1.103950 vs_loss 7.986611 logits_loss 443.943537\n",
      "[62:200] F: 37264000 \t tot_loss 14.404619 rs_loss 1.149805 vs_loss 8.815317 logits_loss 443.949680\n",
      "[62:300] F: 37296000 \t tot_loss 16.907884 rs_loss 1.625707 vs_loss 10.846125 logits_loss 443.605202\n",
      "[62:400] F: 37328000 \t tot_loss 17.279531 rs_loss 1.681394 vs_loss 11.155980 logits_loss 444.215775\n",
      "[62:500] F: 37360000 \t tot_loss 16.888614 rs_loss 1.649711 vs_loss 10.799532 logits_loss 443.936997\n",
      "[62:600] F: 37392000 \t tot_loss 18.090536 rs_loss 1.893710 vs_loss 11.764306 logits_loss 443.252061\n",
      "[62:700] F: 37424000 \t tot_loss 16.235986 rs_loss 1.462476 vs_loss 10.340865 logits_loss 443.264576\n",
      "[62:800] F: 37456000 \t tot_loss 15.662221 rs_loss 1.088557 vs_loss 10.141331 logits_loss 443.233318\n",
      "[62:900] F: 37488000 \t tot_loss 16.619229 rs_loss 1.142058 vs_loss 11.045191 logits_loss 443.197998\n",
      "[62:1000] F: 37520000 \t tot_loss 14.542805 rs_loss 0.791440 vs_loss 9.314479 logits_loss 443.688534\n",
      "[62:1100] F: 37552000 \t tot_loss 14.361364 rs_loss 0.775202 vs_loss 9.148965 logits_loss 443.719712\n",
      "[62:1200] F: 37584000 \t tot_loss 12.976252 rs_loss 0.598269 vs_loss 7.941930 logits_loss 443.605344\n",
      "[62:1300] F: 37616000 \t tot_loss 13.901118 rs_loss 0.807879 vs_loss 8.656919 logits_loss 443.632014\n",
      "[62:1400] F: 37648000 \t tot_loss 15.539115 rs_loss 0.992225 vs_loss 10.108218 logits_loss 443.867191\n",
      "[62:1500] F: 37680000 \t tot_loss 15.756739 rs_loss 1.139906 vs_loss 10.174172 logits_loss 444.266094\n",
      "[62:1600] F: 37712000 \t tot_loss 18.783217 rs_loss 1.670553 vs_loss 12.671205 logits_loss 444.145893\n",
      "[62:1700] F: 37744000 \t tot_loss 19.293220 rs_loss 1.749178 vs_loss 13.102813 logits_loss 444.122829\n",
      "[62:1800] F: 37776000 \t tot_loss 18.045376 rs_loss 1.482816 vs_loss 12.123998 logits_loss 443.856181\n",
      "Batch [63] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: 1.16 (+-0.27) \n",
      "Testing 2 step planning\n",
      "Finish 100 episode: avg. return: 0.79 (+-0.20) \n",
      "[63:0] F: 37800000 \t tot_loss 17.311323 rs_loss 1.299979 vs_loss 11.576142 logits_loss 443.520243\n",
      "[63:100] F: 37832000 \t tot_loss 16.026703 rs_loss 1.093930 vs_loss 10.495863 logits_loss 443.691053\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_285693/853069653.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mrs_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtot_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvs_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogits_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_285693/459829411.py\u001b[0m in \u001b[0;36mget_batch_m\u001b[0;34m(flags, buffers)\u001b[0m\n\u001b[1;32m    266\u001b[0m     batch = {key: torch.stack([buffers[key][m][time_indices[n]:time_indices[n]+flags.unroll_len+1] \n\u001b[1;32m    267\u001b[0m                           for n, m in enumerate(batch_indices)], dim=1) for key in buffers}\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_285693/459829411.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    266\u001b[0m     batch = {key: torch.stack([buffers[key][m][time_indices[n]:time_indices[n]+flags.unroll_len+1] \n\u001b[1;32m    267\u001b[0m                           for n, m in enumerate(batch_indices)], dim=1) for key in buffers}\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp = 20\n",
    "\n",
    "# Load the preset policy\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "checkpoint = torch.load(\"/home/sc/RS/thinker/models/base_1.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"])   \n",
    "net.train(False)\n",
    "net.share_memory()\n",
    "\n",
    "# Get the actors to write on the buffer\n",
    "\n",
    "actor_processes = []\n",
    "free_queue = mp.SimpleQueue()\n",
    "loss_stats = [deque(maxlen=400) for _ in range(4)]\n",
    "\n",
    "net.train(False)\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)\n",
    "\n",
    "# Start training loop    \n",
    "\n",
    "model.train(True)\n",
    "for epoch in range(flags.tot_epoch):    \n",
    "    print(\"Batch [%d] starts\" % epoch)\n",
    "    while(not free_queue.empty()): time.sleep(1)\n",
    "    for step in range(tot_step):\n",
    "        if step == 0: \n",
    "            test_n_step_model(1, model, flags, eps_n=100, temp=temp)\n",
    "            test_n_step_model(2, model, flags, eps_n=100, temp=temp)\n",
    "            model.train(True)\n",
    "        \n",
    "        batch = get_batch_m(flags, buffers)\n",
    "        rs_loss, vs_loss, logits_loss = compute_loss_m(model, batch)\n",
    "        tot_loss = rs_loss + vs_loss + 0.01 * logits_loss\n",
    "        for n, l in enumerate([tot_loss, rs_loss, vs_loss, logits_loss]):\n",
    "            loss_stats[n].append(l.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"[%d:%d] F: %d \\t tot_loss %f rs_loss %f vs_loss %f logits_loss %f\" % ((\n",
    "                epoch, step, (step + epoch * tot_step) * flags.bsz * flags.unroll_len,) +\n",
    "                tuple(np.average(l) for l in loss_stats)))\n",
    "        optimizer.zero_grad()        \n",
    "        tot_loss.backward()\n",
    "        optimize_params = optimizer.param_groups[0]['params']\n",
    "        if flags.grad_norm_clipping > 0:\n",
    "            total_norm = nn.utils.clip_grad_norm_(optimize_params, flags.grad_norm_clipping)\n",
    "        optimizer.step()    \n",
    "    for m in range(flags.seq_n): free_queue.put(m)\n",
    "        \n",
    "for _ in range(flags.num_actors): free_queue.put(None)        \n",
    "for actor in actor_processes: actor.join(timeout=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5319f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab596f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_queue.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the threads\n",
    "\n",
    "for _ in range(flags.num_actors): free_queue.put(None)  \n",
    "for actor in actor_processes: actor.join(timeout=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc08045",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model testing / debug </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6e9773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict(),},\"../models/large_model_1.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d28e7cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model \n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#flags = parser.parse_args(\"\".split())         \n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.discounting = 0.97\n",
    "flags.device = torch.device(\"cuda\")\n",
    "flags.model_type_nn = 1\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/large_model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8834194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 step planning\n",
      "Finish 500 episode: avg. return: 0.89 (+-0.09) \n",
      "Time required for 1 step planning: 35.609551\n",
      "Testing 2 step planning\n",
      "Finish 500 episode: avg. return: 1.06 (+-0.12) \n",
      "Time required for 2 step planning: 67.660493\n",
      "Testing 3 step planning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_285693/4208054650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mall_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_n_step_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time required for %d step planning: %f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_285693/459829411.py\u001b[0m in \u001b[0;36mtest_n_step_model\u001b[0;34m(n, model, flags, eps_n, temp)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mcur_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'episode_return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mnew_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_step_greedy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frame'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_285693/459829411.py\u001b[0m in \u001b[0;36mn_step_greedy_model\u001b[0;34m(state, action, model, n, encoded, temp)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m               action, prob, sub_q_ret = n_step_greedy_model(state=None, action=None, \n\u001b[0m\u001b[1;32m    341\u001b[0m                          model=model, n=n-1, encoded=encodeds[1])\n\u001b[1;32m    342\u001b[0m               \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscounting\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_q_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_285693/459829411.py\u001b[0m in \u001b[0;36mn_step_greedy_model\u001b[0;34m(state, action, model, n, encoded, temp)\u001b[0m\n\u001b[1;32m    327\u001b[0m       \u001b[0mnum_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m       \u001b[0mq_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_returns = {}\n",
    "for n in range(1,5):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step_model(n, model, flags, eps_n=500, temp=20)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ea44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCTS testing\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"\n",
    "    Core Monte Carlo Tree Search algorithm.\n",
    "    To decide on an action, we run N simulations, always starting at the root of\n",
    "    the search tree and traversing the tree according to the UCB formula until we\n",
    "    reach a leaf node.\n",
    "    \"\"\"\n",
    "    def __init__(self, flags, num_actions):\n",
    "        self.flags = flags\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def run(self, model, obs, add_exploration_noise,):\n",
    "        \"\"\"\n",
    "        At the root of the search tree we use the representation function to obtain a\n",
    "        hidden state given the current observation.\n",
    "        We then run a Monte Carlo Tree Search using only action sequences and the model\n",
    "        learned by the network.\n",
    "        Only supports a batch size of 1.        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            root = Node(0)\n",
    "            _, root_predicted_value, policy_logits, hidden_state = model(\n",
    "                obs[\"frame\"][0], obs[\"last_action\"], one_hot=False)\n",
    "            reward = 0.\n",
    "            root_predicted_value = root_predicted_value[-1].item()\n",
    "            policy_logits = policy_logits[-1]\n",
    "            hidden_state = hidden_state[-1]\n",
    "\n",
    "            root.expand(num_actions, reward, policy_logits, hidden_state,)\n",
    "\n",
    "            if add_exploration_noise:\n",
    "                root.add_exploration_noise(\n",
    "                    dirichlet_alpha=self.flags.root_dirichlet_alpha,\n",
    "                    exploration_fraction=self.flags.root_exploration_fraction,\n",
    "                )\n",
    "\n",
    "            min_max_stats = MinMaxStats()\n",
    "\n",
    "            max_tree_depth = 0\n",
    "            \n",
    "            #print(\"p at root:\", torch.softmax(policy_logits, dim=-1))\n",
    "            for k in range(self.flags.num_simulations): \n",
    "                \n",
    "                #print(\"=======%d iteration======\"%k)\n",
    "                node = root\n",
    "                search_path = [node]\n",
    "                current_tree_depth = 0\n",
    "\n",
    "                while node.expanded():\n",
    "                    current_tree_depth += 1                    \n",
    "                    action, node = self.select_child(node, min_max_stats)                    \n",
    "                    search_path.append(node)\n",
    "                    #print(\"action sel: %d\" % action)\n",
    "                \n",
    "                #np.set_printoptions(precision=5)\n",
    "                #for x in [\"prior_score\", \"value_score\", \"pb_c\", \"prior\", \"visit_count\"]:                    \n",
    "                #    print(x, \"\\t\", np.array([getattr(search_path[0].children[n], x) for n in range(5)]))\n",
    "\n",
    "                # Inside the search tree we use the dynamics function to obtain the next hidden\n",
    "                # state given an action and the previous hidden state\n",
    "                parent = search_path[-2]     \n",
    "                reward, value, policy_logits, hidden_state = model.forward_encoded(\n",
    "                    parent.hidden_state, torch.tensor([[action]]).to(parent.hidden_state.device), one_hot=False)\n",
    "                reward = reward[-1].item()\n",
    "                value = value[-1].item()\n",
    "                #print(\"model final output: %4f\" % value)\n",
    "                policy_logits = policy_logits[-1]\n",
    "                hidden_state = hidden_state[-1]\n",
    "                node.expand(num_actions, reward, policy_logits, hidden_state)\n",
    "                self.backpropagate(search_path, value, min_max_stats)\n",
    "                max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
    "\n",
    "            extra_info = {\n",
    "                \"max_tree_depth\": max_tree_depth,\n",
    "                \"root_predicted_value\": root_predicted_value,\n",
    "            }\n",
    "        return root, extra_info\n",
    "\n",
    "    def select_child(self, node, min_max_stats):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        max_ucb = max(\n",
    "            self.ucb_score(node, child, min_max_stats)\n",
    "            for action, child in node.children.items()\n",
    "        )\n",
    "        action = np.random.choice(\n",
    "            [\n",
    "                action\n",
    "                for action, child in node.children.items()\n",
    "                if self.ucb_score(node, child, min_max_stats) == max_ucb\n",
    "            ]\n",
    "        )\n",
    "        return action, node.children[action]\n",
    "\n",
    "    def ucb_score(self, parent, child, min_max_stats):\n",
    "        \"\"\"\n",
    "        The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
    "        \"\"\"\n",
    "        pb_c = (\n",
    "            math.log(\n",
    "                (parent.visit_count + self.flags.pb_c_base + 1) / self.flags.pb_c_base\n",
    "            )\n",
    "            + self.flags.pb_c_init\n",
    "        )\n",
    "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior\n",
    "\n",
    "        if child.visit_count > 0:\n",
    "            # Mean value Q\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward + self.flags.discounting * child.value())\n",
    "        else:\n",
    "            value_score = 0\n",
    "            \n",
    "        child.pb_c = pb_c\n",
    "        child.prior_score = prior_score\n",
    "        child.value_score = value_score\n",
    "        \n",
    "        return prior_score + value_score\n",
    "\n",
    "    def backpropagate(self, search_path, value, min_max_stats):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        #print(\"bs value: %.4f\" % value)\n",
    "        for n, node in enumerate(reversed(search_path)):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            min_max_stats.update(node.reward + self.flags.discounting * node.value())\n",
    "            value = node.reward + self.flags.discounting * value\n",
    "            #print(\"%d - val: %.4f r: %.4f\" % (n, value, node.reward))\n",
    "            #print(\"node value_sum %.4f\" % node.value_sum)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior):\n",
    "        self.visit_count = 0\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def expand(self, num_actions, reward, policy_logits, hidden_state):\n",
    "        \"\"\"\n",
    "        We expand a node using the value, reward and policy prediction obtained from the\n",
    "        neural network.\n",
    "        \"\"\"\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        policy_values = torch.softmax(policy_logits[0], dim=0).tolist()\n",
    "        for a in range(num_actions):\n",
    "            self.children[a] = Node(policy_values[a])\n",
    "\n",
    "    def add_exploration_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        \"\"\"\n",
    "        At the start of each search, we add dirichlet noise to the prior of the root to\n",
    "        encourage the search to explore new actions.\n",
    "        \"\"\"\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "class MinMaxStats:\n",
    "    \"\"\"\n",
    "    A class that holds the min-max values of the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.maximum = -float(\"inf\")\n",
    "        self.minimum = float(\"inf\")\n",
    "\n",
    "    def update(self, value):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value):\n",
    "        if self.maximum > self.minimum:\n",
    "            # We normalize only when we have set the maximum and minimum values\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value            \n",
    "\n",
    "def select_action(node, temperature):\n",
    "    \"\"\"\n",
    "    Select action according to the visit count distribution and the temperature.\n",
    "    The temperature is changed dynamically with the visit_softmax_temperature function\n",
    "    in the config.\n",
    "    \"\"\"\n",
    "    visit_counts = np.array(\n",
    "        [child.visit_count for child in node.children.values()], dtype=\"int32\"\n",
    "    )\n",
    "    actions = [action for action in node.children.keys()]\n",
    "    if temperature == 0:\n",
    "        action = actions[np.argmax(visit_counts)]\n",
    "    elif temperature == float(\"inf\"):\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        # See paper appendix Data Generation\n",
    "        visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "        visit_count_distribution = visit_count_distribution / sum(\n",
    "            visit_count_distribution\n",
    "        )\n",
    "        action = np.random.choice(actions, p=visit_count_distribution)\n",
    "    #print(\"visit_counts\", visit_counts)\n",
    "    #print(\"visit_count_distribution\", visit_count_distribution)\n",
    "    return action\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()      \n",
    "flags = parser.parse_args([])   \n",
    "\n",
    "env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)\n",
    "env = Environment(env)\n",
    "env.initial()\n",
    "obs_shape, num_actions = env.gym_env.observation_space.shape, env.gym_env.action_space.n\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args([])   \n",
    "flags.discounting = 0.97\n",
    "flags.pb_c_init = 1.25\n",
    "flags.pb_c_base = 19652\n",
    "flags.root_dirichlet_alpha = 0.25\n",
    "flags.root_exploration_fraction = 0.\n",
    "flags.num_simulations = 3\n",
    "flags.temp = 0.5\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "eps_n = 10\n",
    "eps_n_cur = 0\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "obs = env.initial()\n",
    "returns = []\n",
    "mcts = MCTS(flags, num_actions)\n",
    "obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "root, extra_info = mcts.run(model, obs, add_exploration_noise=True)   \n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "actions = torch.tensor([0, 1, 3, 4, 4, 4, 2, 4, 2, 2, 2, 1, 1, 1, 1]).long().to(flags.device).reshape(-1, 1)\n",
    "reward, value, policy_logits, hidden_state  = model(obs[\"frame\"][0], actions, one_hot=False)\n",
    "print(reward, value)\n",
    "\n",
    "while(len(returns) <= eps_n):\n",
    "    cur_returns = obs['episode_return']    \n",
    "    obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "    root, extra_info = mcts.run(model, obs, add_exploration_noise=True)    \n",
    "    new_action = select_action(root, flags.temp)    \n",
    "    #plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    #flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "    #plt.show()\n",
    "    #print(\"action selected\", new_action)\n",
    "    #print(\"===========================================\")\n",
    "    obs = env.step(torch.tensor([new_action]))\n",
    "    if torch.any(obs['done']):\n",
    "        returns.extend(cur_returns[obs['done']].numpy())\n",
    "    if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "        eps_n_cur = len(returns) + 10\n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test planning algorithm\n",
    "bsz = 1\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "obs = env.initial()\n",
    "state = obs['frame'][0].to(flags.device).clone()\n",
    "action = torch.zeros(bsz).long().to(flags.device)\n",
    "encoded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e67032",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.Tensor([4]).long().to(flags.device)\n",
    "obs = env.step(action)\n",
    "state = obs['frame'][0].to(flags.device).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c551f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "device = flags.device\n",
    "\n",
    "for _ in range(1):\n",
    "    plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "    plt.show()\n",
    "    ret = np.zeros((5, 5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            for k in range(5):\n",
    "                test_action_seq = [i,j,k]\n",
    "                test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "                old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "                rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "                ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "    print(np.max(ret), (np.max(ret) == ret).nonzero())    \n",
    "    new_action = torch.Tensor((np.max(ret) == ret).nonzero()[0]).long().to(flags.device)\n",
    "    #obs = env.step(new_action)\n",
    "    #state = obs['frame'][0].to(flags.device).clone()\n",
    "    #action = new_action            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a9ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, prob, q_ret = n_step_greedy_model(state, action, model, 3, encoded=None, temp=10.)\n",
    "print(\"action: \", action)\n",
    "print(\"prob: \", prob)\n",
    "print(\"q_ret: \", q_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_action_seq = [2,3,1]\n",
    "test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "print(\"rs\", rs)\n",
    "print(\"vs\", vs)\n",
    "print(\"logits\", logits)\n",
    "print(\"ret\", ret[i,j,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 10.\n",
    "\n",
    "bsz = state.shape[0]\n",
    "device = state.device \n",
    "num_actions = model.num_actions    \n",
    "model.train(False)\n",
    "\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "\n",
    "for act in range(num_actions):        \n",
    "    new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "    old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "    rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "    ret = rs[0] + flags.discounting * vs[1]\n",
    "    rs_act[:, act] = rs[0]\n",
    "    vs_act[:, act] = vs[1]\n",
    "    q_ret[:, act] = ret\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = flags.device\n",
    "net_state = env.clone_state()\n",
    "\n",
    "bsz = 1\n",
    "temp = 10.\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)   \n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "for act in range(num_actions):\n",
    "    obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "    obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "    ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "    rs_act[:, act] = obs['reward']\n",
    "    vs_act[:, act] = net(obs)[0]['baseline']\n",
    "    q_ret[:, act] = ret\n",
    "    env.restore_state(net_state)\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)\n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch_m(flags, buffers)\n",
    "print(torch.max(batch[\"reward\"]), (torch.max(batch[\"reward\"]) == batch[\"reward\"]).nonzero())\n",
    "print(batch[\"done\"].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG LOSS\n",
    "\n",
    "#batch = get_batch_m(flags, buffers)\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "logits = logits[:-1]\n",
    "\n",
    "target_rewards = batch['reward'][1:]\n",
    "target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "target_vs = []\n",
    "target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0]    \n",
    "\n",
    "for t in range(vs.shape[0]-1, 0, -1):\n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))\n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "# if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "# rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "# vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "# logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "done_masks = []\n",
    "done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "for t in range(vs.shape[0]):\n",
    "    done = torch.logical_or(done, batch['done'][t])\n",
    "    done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "\n",
    "# compute final loss\n",
    "huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "#rs_loss = torch.sum(huberloss(rs, target_rewards) * (~done_masks).float())\n",
    "rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~done_masks).float())\n",
    "#vs_loss = torch.sum(huberloss(vs[:-1], target_vs) * (~done_masks).float())\n",
    "vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~done_masks).float())\n",
    "logits_loss = compute_cross_entropy_loss(logits, target_logits, done_masks)\n",
    "\n",
    "# debug\n",
    "ind = 10\n",
    "\n",
    "target_vs = []\n",
    "target_v = vs[-1]\n",
    "for t in range(vs.shape[0]-1, 0, -1):        \n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    print(t, \n",
    "          \"reward %2f\" % batch['reward'][t,ind].item(), \n",
    "          \"bootstrap %2f\" % (target_v * (~batch['done'][t]).float())[ind].item(), \n",
    "          \"truncated %2f\" % (vs[t-1] * (batch['truncated_done'][t]).float())[ind].item(),\n",
    "          \"vs[t-1] %2f\" % vs[t-1][ind].item(),\n",
    "          \"new_targ %2f\" % new_target_v[ind].item())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))    \n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)   \n",
    "print(\"done\", batch[\"done\"][:, ind])\n",
    "print(\"done_masks\", done_masks[:, ind])\n",
    "print(\"vs: \", vs[:, ind])\n",
    "print(\"target_vs: \", target_vs[:, ind])\n",
    "print(\"reward: \", rs[:, ind])\n",
    "print(\"target_reward: \", target_rewards[:, ind])\n",
    "print(\"logits: \", logits[:, ind])\n",
    "print(\"target_logits: \", target_logits[:, ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_m(model, batch):\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_rewards = batch['reward'][1:]\n",
    "    target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float())# +\n",
    "                           #vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "    # if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "    # rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "    # vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "    # logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "    for t in range(vs.shape[0]):\n",
    "        done = torch.logical_or(done, batch['done'][t])\n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "    done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()) * (~done_masks).float())\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()) * (~done_masks).float())\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), done_masks)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss\n",
    "\n",
    "# alt. version of computing loss by treading terminal state as absorbing state (as in MuZero)\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_logits = batch['policy_logits'][1:].clone()\n",
    "    target_rewards = batch['reward'][1:].clone()\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "\n",
    "    c_logits = target_logits[0]\n",
    "    c_state = batch['frame'][0]\n",
    "    for t in range(vs.shape[0]-1):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        c_logits = torch.where(done.unsqueeze(-1), c_logits, target_logits[t])\n",
    "        target_logits[t] = c_logits\n",
    "        c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][t])  \n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "    done_masks = torch.concat(done_masks, dim=0)\n",
    "    done = torch.logical_or(done, batch['done'][-1])\n",
    "    c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][-1])\n",
    "    target_rewards = target_rewards * (~done_masks).float()\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(c_state, batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * target_v\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()))\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()))\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), None)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd360a98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
