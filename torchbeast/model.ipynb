{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core.environment import Environment, Vec_Environment\n",
    "from torchbeast.atari_wrappers import SokobanWrapper\n",
    "from torchbeast.base import BaseNet\n",
    "from torchbeast.train import create_env\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "def get_param(net, name=None):\n",
    "    keys = []\n",
    "    for (k, v) in actor_wrapper.model.named_parameters(): \n",
    "        if name is None:\n",
    "            print(k)\n",
    "        else:\n",
    "            if name == k: return v\n",
    "        keys.append(k)\n",
    "    return keys        \n",
    "\n",
    "def n_step_greedy(env, net, n, temp=10.):    \n",
    "    if isinstance(env, Vec_Environment):\n",
    "        num_actions = env.gym_env.action_space[0].n\n",
    "        bsz = len(env.gym_env.envs)\n",
    "    else:\n",
    "        num_actions = env.gym_env.action_space.n\n",
    "        bsz = 1\n",
    "\n",
    "    q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "    state = env.clone_state()\n",
    "\n",
    "    for act in range(num_actions):\n",
    "        obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "        \n",
    "        if n > 1:\n",
    "            action, prob, sub_q_ret = n_step_greedy(env, net, n-1)\n",
    "            ret = obs['reward'] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] * (~obs['done']).float()\n",
    "        else:\n",
    "            ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "\n",
    "        q_ret[:, act] = ret\n",
    "        env.restore_state(state)\n",
    "    \n",
    "    prob = F.softmax(temp*q_ret, dim=1)\n",
    "    action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893df32e",
   "metadata": {},
   "source": [
    "<font size=\"5\">Testing planning algo. for perfect model with bootstrapped values</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bf8c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Synchronous version of testing \n",
    "\n",
    "def test_n_step(n, net, env, temp=10.):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    returns = []\n",
    "    obs = env.initial()\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}\n",
    "        net_out, core_state = net(obs)            \n",
    "        if n == 0:\n",
    "            action = net_out[\"action\"][0]\n",
    "        else:\n",
    "            action, _, _ = n_step_greedy(env, net, n, temp)\n",
    "        obs = env.step(action)\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns\n",
    "\n",
    "bsz = 16    \n",
    "eps_n = 500\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# create environments\n",
    "\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "num_actions = env.gym_env.action_space[0].n\n",
    "\n",
    "# import the net\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "flags.discounting = 0.97\n",
    "temp = 5\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=num_actions, flags=flags)  \n",
    "net = net.to(device)\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_2.tar\", map_location=\"cuda\")\n",
    "#checkpoint = torch.load(\"/home/schk/RS/thinker/logs/base/torchbeast-20221105-033530/model.tar\", map_location=\"cuda\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "# initialize net\n",
    "\n",
    "core_state = net.initial_state(batch_size=bsz)\n",
    "core_state = tuple(v.to(device) for v in core_state)\n",
    "net.train(False)\n",
    "\n",
    "all_returns = {}\n",
    "for n in range(2,3):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step(n, net, env, temp)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ffb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous version of testing \n",
    "\n",
    "def act_m(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    returns: Manager().list,\n",
    "    eps_n: int,\n",
    "    n: int,\n",
    "    temp: float,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        net_out, unused_state = net(env_output, agent_state)      \n",
    "        while True:            \n",
    "            if len(returns) >= eps_n: break\n",
    "            with torch.no_grad():\n",
    "                net_out, agent_state = net(env_output, agent_state)                            \n",
    "            if n == 0:\n",
    "                action = net_out[\"action\"]\n",
    "            else:\n",
    "                action, _, _ = n_step_greedy(env, net, n, temp)            \n",
    "            env_output = env.step(action)           \n",
    "            if env_output['done']: returns.append(ret)\n",
    "            ret = env_output['episode_return'].item()        \n",
    "        #logging.info(\"Actor %i end\", actor_index)\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "def asy_test_n_step(n, net, flags, temp):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    mp.set_sharing_strategy('file_system')\n",
    "    net.share_memory()\n",
    "    ctx = mp.get_context()        \n",
    "    returns = Manager().list()\n",
    "\n",
    "    actor_processes = []\n",
    "    for i in range(flags.num_actors):\n",
    "        actor = ctx.Process(target=act_m, args=(flags, i, net, returns, eps_n, n, temp),)\n",
    "        actor.start()\n",
    "        actor_processes.append(actor)    \n",
    "\n",
    "    for actor in actor_processes:\n",
    "        actor.join()    \n",
    "\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f)\" % (len(returns),\n",
    "                    np.average(returns), np.std(returns) / np.sqrt(len(returns)),))        \n",
    "    return returns        \n",
    "        \n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.discounting = 0.97     \n",
    "flags.num_actors = 32\n",
    "bsz = 1\n",
    "eps_n = 500\n",
    "temp = 5\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "net = net.to(\"cpu\")\n",
    "checkpoint = torch.load(\"/home/schk/RS/thinker/models/base_2.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "all_returns = {}\n",
    "for n in range(4):\n",
    "    t = time.time()\n",
    "    all_returns[n] = asy_test_n_step(n, net, flags, temp)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abac9a",
   "metadata": {},
   "source": [
    "Results (base_1.tar):\n",
    "    \n",
    "Testing 0 step planning <br>\n",
    "Finish 512 episode: avg. return: 0.12 (+-0.06) <br>\n",
    "Testing 1 step planning <br>\n",
    "Finish 502 episode: avg. return: 0.61 (+-0.04) <br>\n",
    "Testing 2 step planning <br>\n",
    "Finish 501 episode: avg. return: 0.92 (+-0.04) <br>\n",
    "Testing 3 step planning <br>\n",
    "Finish 501 episode: avg. return: 1.01 (+-0.04) <br>\n",
    "\n",
    "Results (base_2.tar):\n",
    "Testing 0 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.27 (+-0.04) <br>\n",
    "Time required for 0 step planning: 12.629324 <br>\n",
    "Testing 1 step planning <br>\n",
    "Finish 502 episode: avg. return: 0.51 (+-0.04) <br>\n",
    "Time required for 1 step planning: 74.194364 <br>\n",
    "Testing 2 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.74 (+-0.04) <br>\n",
    "Time required for 2 step planning: 339.732901 <br>\n",
    "Testing 3 step planning <br>\n",
    "Finish 500 episode: avg. return: 0.76 (+-0.04) <br>\n",
    "Time required for 3 step planning: 1695.472523 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097bd18",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model Training Phase</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "968043ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data for learning model [RUN]\n",
    "\n",
    "Buffers = typing.Dict[str, typing.List[torch.Tensor]]\n",
    "\n",
    "def create_buffers_m(flags, obs_shape, num_actions) -> Buffers:\n",
    "    \n",
    "    seq_len = flags.seq_len\n",
    "    seq_n = flags.seq_n\n",
    "    specs = dict(\n",
    "        frame=dict(size=(seq_len + 1, *obs_shape), dtype=torch.uint8),\n",
    "        reward=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        truncated_done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        episode_return=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        episode_step=dict(size=(seq_len + 1,), dtype=torch.int32),\n",
    "        policy_logits=dict(size=(seq_len + 1, num_actions), dtype=torch.float32),\n",
    "        baseline=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        last_action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        reg_loss=dict(size=(seq_len + 1,), dtype=torch.float32)\n",
    "    )\n",
    "    buffers: Buffers = {key: [] for key in specs}\n",
    "    for _ in range(seq_n):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "            \n",
    "    return buffers\n",
    "\n",
    "def gen_data(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    buffers: Buffers,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = net(env_output, agent_state)     \n",
    "        \n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break         \n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:\n",
    "                buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                buffers[key][index][0, ...] = agent_output[key]\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.seq_len):\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = net(env_output, agent_state)\n",
    "                env_output = env.step(agent_output[\"action\"])\n",
    "                for key in env_output:\n",
    "                    buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "        \n",
    "\n",
    "# Models\n",
    "\n",
    "DOWNSCALE_C = 2\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation,\n",
    "        groups=groups, bias=False, dilation=dilation,)\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, inplanes, outplanes=None):\n",
    "        super().__init__()\n",
    "        if outplanes is None: outplanes = inplanes \n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        self.conv1 = conv3x3(inplanes, inplanes)\n",
    "        self.bn1 = norm_layer(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(inplanes, outplanes)\n",
    "        self.bn2 = norm_layer(outplanes)\n",
    "        self.skip_conv = (outplanes != inplanes)\n",
    "        if outplanes != inplanes:\n",
    "            self.conv3 = conv1x1(inplanes, outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.skip_conv:\n",
    "            out += self.conv3(identity)\n",
    "        else:\n",
    "            out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class FrameEncoder(nn.Module):    \n",
    "    def __init__(self, num_actions, frame_channels=3):\n",
    "        self.num_actions = num_actions\n",
    "        super(FrameEncoder, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=frame_channels+num_actions, out_channels=128//DOWNSCALE_C, kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=128//DOWNSCALE_C) for i in range(2)]) # Deep: 2 blocks here\n",
    "        self.res1 = torch.nn.Sequential(*res)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128//DOWNSCALE_C, out_channels=256//DOWNSCALE_C, \n",
    "                               kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(2)]) # Deep: 3 blocks here\n",
    "        self.res2 = torch.nn.Sequential(*res)\n",
    "        self.avg1 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(2)]) # Deep: 3 blocks here\n",
    "        self.res3 = torch.nn.Sequential(*res)\n",
    "        self.avg2 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x, actions):        \n",
    "        # input shape: B, C, H, W        \n",
    "        # action shape: B \n",
    "        \n",
    "        x = x.float() / 255.0    \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "        x = torch.concat([x, actions], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.res1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.res2(x)\n",
    "        x = self.avg1(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.avg2(x)\n",
    "        return x\n",
    "    \n",
    "class DynamicModel(nn.Module):\n",
    "    def __init__(self, num_actions, inplanes=256):        \n",
    "        super(DynamicModel, self).__init__()\n",
    "        res = nn.ModuleList([ResBlock(inplanes=inplanes) for i in range(15)] + \n",
    "                            [ResBlock(inplanes=inplanes, outplanes=inplanes*num_actions)]) # Deep: 15 blocks here\n",
    "        \n",
    "        self.res = torch.nn.Sequential(*res)\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def forward(self, x, actions):      \n",
    "        bsz, c, h, w = x.shape\n",
    "        res_out = self.res(x).view(bsz, self.num_actions, c, h, w)        \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        act_res_out = torch.sum(actions * res_out, dim=1)\n",
    "        return act_res_out\n",
    "    \n",
    "class DynamicModel_(nn.Module):\n",
    "    def __init__(self, num_actions, inplanes=256):        \n",
    "        super(DynamicModel, self).__init__()\n",
    "        res = nn.ModuleList([ResBlock(inplanes=inplanes+num_actions, outplanes=inplanes)] + [\n",
    "            ResBlock(inplanes=inplanes) for i in range(8)]) # Deep: 15 blocks here\n",
    "        self.res = torch.nn.Sequential(*res)\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def forward(self, x, actions):    \n",
    "        x.register_hook(lambda grad: grad * 0.5)\n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "        x = torch.concat([x, actions], dim=1)\n",
    "        return self.res(x)\n",
    "    \n",
    "class Output_rvpi(nn.Module):   \n",
    "    def __init__(self, num_actions, input_shape):         \n",
    "        super(Output_rvpi, self).__init__()        \n",
    "        c, h, w = input_shape\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=c//2, kernel_size=3, padding='same') \n",
    "        self.conv2 = nn.Conv2d(in_channels=c//2, out_channels=c//4, kernel_size=3, padding='same') \n",
    "        fc_in = h * w * (c // 4)\n",
    "        self.fc_r = nn.Linear(fc_in, 1) \n",
    "        self.fc_v = nn.Linear(fc_in, 1) \n",
    "        self.fc_logits = nn.Linear(fc_in, num_actions)         \n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        r, v, logits = self.fc_r(x), self.fc_v(x), self.fc_logits(x)\n",
    "        return r, v, logits\n",
    "\n",
    "class Model(nn.Module):    \n",
    "    def __init__(self, flags, obs_shape, num_actions):\n",
    "        super(Model, self).__init__()      \n",
    "        self.flags = flags\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions          \n",
    "        self.frameEncoder = FrameEncoder(num_actions=num_actions, frame_channels=obs_shape[0])\n",
    "        self.dynamicModel = DynamicModel(num_actions=num_actions, inplanes=256//DOWNSCALE_C)\n",
    "        self.output_rvpi = Output_rvpi(num_actions=num_actions, input_shape=(256//DOWNSCALE_C, \n",
    "                      obs_shape[1]//16, obs_shape[1]//16))\n",
    "        \n",
    "    def forward(self, x, actions, one_hot=False):\n",
    "        # Input\n",
    "        # x: frames with shape (B, C, H, W), in the form of s_t\n",
    "        # actions: action (int64) with shape (k+1, B), in the form of a_{t-1}, a_{t}, a_{t+1}, .. a_{t+k-1}\n",
    "        # Output\n",
    "        # reward: predicted reward with shape (k, B), in the form of r_{t+1}, r_{t+2}, ..., r_{t+k}\n",
    "        # value: predicted value with shape (k+1, B), in the form of v_{t}, v_{t+1}, v_{t+2}, ..., v_{t+k}\n",
    "        # policy: predicted policy with shape (k+1, B), in the form of pi_{t}, pi_{t+1}, pi_{t+2}, ..., pi_{t+k}\n",
    "        # encoded: encoded states with shape (k+1, B), in the form of z_t, z_{t+1}, z_{t+2}, ..., z_{t+k}\n",
    "        # Recall the transition notation: s_t, a_t, r_{t+1}, s_{t+1}, ...\n",
    "        \n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        encoded = self.frameEncoder(x, actions[0])\n",
    "        return self.forward_encoded(encoded, actions[1:], one_hot=True)\n",
    "    \n",
    "    def forward_encoded(self, encoded, actions, one_hot=False):\n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        \n",
    "        r, v, logits = self.output_rvpi(encoded)\n",
    "        r_list, v_list, logits_list = [], [v.squeeze(-1).unsqueeze(0)], [logits.unsqueeze(0)]\n",
    "        encoded_list = [encoded.unsqueeze(0)]\n",
    "        \n",
    "        for k in range(actions.shape[0]):            \n",
    "            encoded = self.dynamicModel(encoded, actions[k])\n",
    "            r, v, logits = self.output_rvpi(encoded)\n",
    "            r_list.append(r.squeeze(-1).unsqueeze(0))\n",
    "            v_list.append(v.squeeze(-1).unsqueeze(0))\n",
    "            logits_list.append(logits.unsqueeze(0))\n",
    "            encoded_list.append(encoded.unsqueeze(0))        \n",
    "        \n",
    "        if len(r_list) > 0:\n",
    "            rs = torch.concat(r_list, dim=0)\n",
    "        else:\n",
    "            rs = None\n",
    "            \n",
    "        vs = torch.concat(v_list, dim=0)\n",
    "        logits = torch.concat(logits_list, dim=0)\n",
    "        encodeds = torch.concat(encoded_list, dim=0)        \n",
    "        \n",
    "        return rs, vs, logits, encodeds\n",
    "\n",
    "#model = Model(flags, (3, 80, 80), num_actions=5)\n",
    "#rs, vs, logits = model(torch.rand(16, 3, 80, 80), torch.ones(8, 16).long())\n",
    "\n",
    "# functions for training models\n",
    "\n",
    "def get_batch_m(flags, buffers: Buffers):\n",
    "    batch_indices = np.random.randint(flags.seq_n, size=flags.bsz)\n",
    "    time_indices = np.random.randint(flags.seq_len - flags.unroll_len, size=flags.bsz)\n",
    "    batch = {key: torch.stack([buffers[key][m][time_indices[n]:time_indices[n]+flags.unroll_len+1] \n",
    "                          for n, m in enumerate(batch_indices)], dim=1) for key in buffers}\n",
    "    batch = {k: t.to(device=flags.device, non_blocking=True) for k, t in batch.items()}\n",
    "    return batch\n",
    "\n",
    "def compute_cross_entropy_loss(logits, target_logits, mask):\n",
    "    target_policy = F.softmax(target_logits, dim=-1)\n",
    "    log_policy = F.log_softmax(logits, dim=-1)\n",
    "    return -torch.sum(target_policy * log_policy * (~mask).float().unsqueeze(-1))\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_rewards = batch['reward'][1:]\n",
    "    target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float())# +\n",
    "                           #vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "    # if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "    # rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "    # vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "    # logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "    for t in range(vs.shape[0]):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "    done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    #rs_loss = torch.sum(huberloss(rs, target_rewards.detach()) * (~done_masks).float())\n",
    "    rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    #vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()) * (~done_masks).float())\n",
    "    vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), done_masks)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss\n",
    "\n",
    "# n_step_greedy for testing\n",
    "\n",
    "def n_step_greedy_model(state, action, model, n, encoded=None, temp=20.): \n",
    "    \n",
    "    # Either input state, action (S_t, A_{t-1}) or the encoded Z_t\n",
    "    # state / encoded in the shape of (B, C, H, W)\n",
    "    # action in the shape of (B)    \n",
    "    with torch.no_grad():    \n",
    "      bsz = state.shape[0] if encoded is None else encoded.shape[0]\n",
    "      device = state.device if encoded is None else encoded.device\n",
    "      num_actions = model.num_actions    \n",
    "\n",
    "      q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "\n",
    "      for act in range(num_actions):        \n",
    "          new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "          if encoded is None:            \n",
    "              old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "              rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "          else:\n",
    "              rs, vs, logits, encodeds = model.forward_encoded(encoded, new_action.unsqueeze(0))\n",
    "\n",
    "          if n > 1:\n",
    "              action, prob, sub_q_ret = n_step_greedy_model(state=None, action=None, \n",
    "                         model=model, n=n-1, encoded=encodeds[1])\n",
    "              ret = rs[0] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] \n",
    "          else:\n",
    "              ret = rs[0] + flags.discounting * vs[1]\n",
    "          q_ret[:, act] = ret\n",
    "\n",
    "      prob = F.softmax(temp*q_ret, dim=1)\n",
    "      action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret        \n",
    "   \n",
    "#n_step_greedy_model(batch['frame'][0], batch['action'][0], model, 4)  \n",
    "\n",
    "def test_n_step_model(n, model, flags, eps_n=100, temp=20.):    \n",
    "    \n",
    "    print(\"Testing %d step planning\" % n) \n",
    "    \n",
    "    bsz = 100\n",
    "    env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "    env = Vec_Environment(env, bsz)\n",
    "    num_actions = env.gym_env.action_space[0].n\n",
    "    \n",
    "    model.train(False)\n",
    "    returns = []\n",
    "    \n",
    "    obs = env.initial()\n",
    "    action = torch.zeros(bsz).long().to(flags.device)\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "        new_action, _, _ = n_step_greedy_model(obs['frame'][0], action, model, n, None, temp)        \n",
    "        obs = env.step(new_action)\n",
    "        action = new_action\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            #print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            #    np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    returns = returns[:eps_n]\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6fa3769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffer created successfully.\n",
      "model size:  6905607\n",
      "frameEncoder.conv1.weight 4608\n",
      "frameEncoder.conv1.bias 64\n",
      "frameEncoder.res1.0.conv1.weight 36864\n",
      "frameEncoder.res1.0.bn1.weight 64\n",
      "frameEncoder.res1.0.bn1.bias 64\n",
      "frameEncoder.res1.0.conv2.weight 36864\n",
      "frameEncoder.res1.0.bn2.weight 64\n",
      "frameEncoder.res1.0.bn2.bias 64\n",
      "frameEncoder.res1.1.conv1.weight 36864\n",
      "frameEncoder.res1.1.bn1.weight 64\n",
      "frameEncoder.res1.1.bn1.bias 64\n",
      "frameEncoder.res1.1.conv2.weight 36864\n",
      "frameEncoder.res1.1.bn2.weight 64\n",
      "frameEncoder.res1.1.bn2.bias 64\n",
      "frameEncoder.conv2.weight 73728\n",
      "frameEncoder.conv2.bias 128\n",
      "frameEncoder.res2.0.conv1.weight 147456\n",
      "frameEncoder.res2.0.bn1.weight 128\n",
      "frameEncoder.res2.0.bn1.bias 128\n",
      "frameEncoder.res2.0.conv2.weight 147456\n",
      "frameEncoder.res2.0.bn2.weight 128\n",
      "frameEncoder.res2.0.bn2.bias 128\n",
      "frameEncoder.res2.1.conv1.weight 147456\n",
      "frameEncoder.res2.1.bn1.weight 128\n",
      "frameEncoder.res2.1.bn1.bias 128\n",
      "frameEncoder.res2.1.conv2.weight 147456\n",
      "frameEncoder.res2.1.bn2.weight 128\n",
      "frameEncoder.res2.1.bn2.bias 128\n",
      "frameEncoder.res3.0.conv1.weight 147456\n",
      "frameEncoder.res3.0.bn1.weight 128\n",
      "frameEncoder.res3.0.bn1.bias 128\n",
      "frameEncoder.res3.0.conv2.weight 147456\n",
      "frameEncoder.res3.0.bn2.weight 128\n",
      "frameEncoder.res3.0.bn2.bias 128\n",
      "frameEncoder.res3.1.conv1.weight 147456\n",
      "frameEncoder.res3.1.bn1.weight 128\n",
      "frameEncoder.res3.1.bn1.bias 128\n",
      "frameEncoder.res3.1.conv2.weight 147456\n",
      "frameEncoder.res3.1.bn2.weight 128\n",
      "frameEncoder.res3.1.bn2.bias 128\n",
      "dynamicModel.res.0.conv1.weight 147456\n",
      "dynamicModel.res.0.bn1.weight 128\n",
      "dynamicModel.res.0.bn1.bias 128\n",
      "dynamicModel.res.0.conv2.weight 147456\n",
      "dynamicModel.res.0.bn2.weight 128\n",
      "dynamicModel.res.0.bn2.bias 128\n",
      "dynamicModel.res.1.conv1.weight 147456\n",
      "dynamicModel.res.1.bn1.weight 128\n",
      "dynamicModel.res.1.bn1.bias 128\n",
      "dynamicModel.res.1.conv2.weight 147456\n",
      "dynamicModel.res.1.bn2.weight 128\n",
      "dynamicModel.res.1.bn2.bias 128\n",
      "dynamicModel.res.2.conv1.weight 147456\n",
      "dynamicModel.res.2.bn1.weight 128\n",
      "dynamicModel.res.2.bn1.bias 128\n",
      "dynamicModel.res.2.conv2.weight 147456\n",
      "dynamicModel.res.2.bn2.weight 128\n",
      "dynamicModel.res.2.bn2.bias 128\n",
      "dynamicModel.res.3.conv1.weight 147456\n",
      "dynamicModel.res.3.bn1.weight 128\n",
      "dynamicModel.res.3.bn1.bias 128\n",
      "dynamicModel.res.3.conv2.weight 147456\n",
      "dynamicModel.res.3.bn2.weight 128\n",
      "dynamicModel.res.3.bn2.bias 128\n",
      "dynamicModel.res.4.conv1.weight 147456\n",
      "dynamicModel.res.4.bn1.weight 128\n",
      "dynamicModel.res.4.bn1.bias 128\n",
      "dynamicModel.res.4.conv2.weight 147456\n",
      "dynamicModel.res.4.bn2.weight 128\n",
      "dynamicModel.res.4.bn2.bias 128\n",
      "dynamicModel.res.5.conv1.weight 147456\n",
      "dynamicModel.res.5.bn1.weight 128\n",
      "dynamicModel.res.5.bn1.bias 128\n",
      "dynamicModel.res.5.conv2.weight 147456\n",
      "dynamicModel.res.5.bn2.weight 128\n",
      "dynamicModel.res.5.bn2.bias 128\n",
      "dynamicModel.res.6.conv1.weight 147456\n",
      "dynamicModel.res.6.bn1.weight 128\n",
      "dynamicModel.res.6.bn1.bias 128\n",
      "dynamicModel.res.6.conv2.weight 147456\n",
      "dynamicModel.res.6.bn2.weight 128\n",
      "dynamicModel.res.6.bn2.bias 128\n",
      "dynamicModel.res.7.conv1.weight 147456\n",
      "dynamicModel.res.7.bn1.weight 128\n",
      "dynamicModel.res.7.bn1.bias 128\n",
      "dynamicModel.res.7.conv2.weight 147456\n",
      "dynamicModel.res.7.bn2.weight 128\n",
      "dynamicModel.res.7.bn2.bias 128\n",
      "dynamicModel.res.8.conv1.weight 147456\n",
      "dynamicModel.res.8.bn1.weight 128\n",
      "dynamicModel.res.8.bn1.bias 128\n",
      "dynamicModel.res.8.conv2.weight 147456\n",
      "dynamicModel.res.8.bn2.weight 128\n",
      "dynamicModel.res.8.bn2.bias 128\n",
      "dynamicModel.res.9.conv1.weight 147456\n",
      "dynamicModel.res.9.bn1.weight 128\n",
      "dynamicModel.res.9.bn1.bias 128\n",
      "dynamicModel.res.9.conv2.weight 147456\n",
      "dynamicModel.res.9.bn2.weight 128\n",
      "dynamicModel.res.9.bn2.bias 128\n",
      "dynamicModel.res.10.conv1.weight 147456\n",
      "dynamicModel.res.10.bn1.weight 128\n",
      "dynamicModel.res.10.bn1.bias 128\n",
      "dynamicModel.res.10.conv2.weight 147456\n",
      "dynamicModel.res.10.bn2.weight 128\n",
      "dynamicModel.res.10.bn2.bias 128\n",
      "dynamicModel.res.11.conv1.weight 147456\n",
      "dynamicModel.res.11.bn1.weight 128\n",
      "dynamicModel.res.11.bn1.bias 128\n",
      "dynamicModel.res.11.conv2.weight 147456\n",
      "dynamicModel.res.11.bn2.weight 128\n",
      "dynamicModel.res.11.bn2.bias 128\n",
      "dynamicModel.res.12.conv1.weight 147456\n",
      "dynamicModel.res.12.bn1.weight 128\n",
      "dynamicModel.res.12.bn1.bias 128\n",
      "dynamicModel.res.12.conv2.weight 147456\n",
      "dynamicModel.res.12.bn2.weight 128\n",
      "dynamicModel.res.12.bn2.bias 128\n",
      "dynamicModel.res.13.conv1.weight 147456\n",
      "dynamicModel.res.13.bn1.weight 128\n",
      "dynamicModel.res.13.bn1.bias 128\n",
      "dynamicModel.res.13.conv2.weight 147456\n",
      "dynamicModel.res.13.bn2.weight 128\n",
      "dynamicModel.res.13.bn2.bias 128\n",
      "dynamicModel.res.14.conv1.weight 147456\n",
      "dynamicModel.res.14.bn1.weight 128\n",
      "dynamicModel.res.14.bn1.bias 128\n",
      "dynamicModel.res.14.conv2.weight 147456\n",
      "dynamicModel.res.14.bn2.weight 128\n",
      "dynamicModel.res.14.bn2.bias 128\n",
      "dynamicModel.res.15.conv1.weight 147456\n",
      "dynamicModel.res.15.bn1.weight 128\n",
      "dynamicModel.res.15.bn1.bias 128\n",
      "dynamicModel.res.15.conv2.weight 737280\n",
      "dynamicModel.res.15.bn2.weight 640\n",
      "dynamicModel.res.15.bn2.bias 640\n",
      "dynamicModel.res.15.conv3.weight 81920\n",
      "output_rvpi.conv1.weight 73728\n",
      "output_rvpi.conv1.bias 64\n",
      "output_rvpi.conv2.weight 18432\n",
      "output_rvpi.conv2.bias 32\n",
      "output_rvpi.fc_r.weight 800\n",
      "output_rvpi.fc_r.bias 1\n",
      "output_rvpi.fc_v.weight 800\n",
      "output_rvpi.fc_v.bias 1\n",
      "output_rvpi.fc_logits.weight 4000\n",
      "output_rvpi.fc_logits.bias 5\n"
     ]
    }
   ],
   "source": [
    "# Start training models\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())       \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.bsz = 64\n",
    "flags.unroll_len = 5\n",
    "flags.num_actors = 32\n",
    "flags.seq_n = 1000\n",
    "flags.seq_len = 200\n",
    "flags.learning_rate = 0.0001\n",
    "flags.loop_batch_n = 3\n",
    "flags.discounting = 0.97\n",
    "flags.tot_epoch = 10000\n",
    "flags.grad_norm_clipping = 60\n",
    "\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "# Create buffer for actors to write\n",
    "\n",
    "mp.set_sharing_strategy('file_system')\n",
    "ctx = mp.get_context()        \n",
    "\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "buffers = create_buffers_m(flags, obs_shape, num_actions)\n",
    "print(\"Buffer created successfully.\")\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "\n",
    "env = create_env(flags)\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=flags.learning_rate)\n",
    "\n",
    "print(\"model size: \", sum(p.numel() for p in model.parameters()))\n",
    "for k, v in model.named_parameters(): print(k, v.numel())    \n",
    "    \n",
    "tot_step = int(flags.loop_batch_n * flags.seq_n * flags.seq_len / flags.bsz / flags.unroll_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "301892ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size:  1095814\n",
      "Batch [0] starts\n",
      "Testing 1 step planning\n",
      "Finish 100 episode: avg. return: -0.80 (+-0.06) \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r_logit_done_masks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_265057/809179272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mrs_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss_m\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mtot_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvs_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.01\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlogits_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtot_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvs_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_265057/4074779718.py\u001b[0m in \u001b[0;36mcompute_loss_m\u001b[0;34m(model, batch)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0mhuberloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHuberLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;31m#rs_loss = torch.sum(huberloss(rs, target_rewards.detach()) * (~done_masks).float())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0mrs_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget_rewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mr_logit_done_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;31m#vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()) * (~done_masks).float())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0mvs_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget_vs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mv_done_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'r_logit_done_masks' is not defined"
     ]
    }
   ],
   "source": [
    "temp = 20\n",
    "\n",
    "# Load the preset policy\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "checkpoint = torch.load(\"/home/sc/RS/thinker/models/base_1.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"])   \n",
    "net.train(False)\n",
    "net.share_memory()\n",
    "\n",
    "# Get the actors to write on the buffer\n",
    "\n",
    "actor_processes = []\n",
    "free_queue = mp.SimpleQueue()\n",
    "loss_stats = [deque(maxlen=400) for _ in range(4)]\n",
    "\n",
    "net.train(False)\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)\n",
    "\n",
    "# Start training loop    \n",
    "\n",
    "model.train(True)\n",
    "for epoch in range(flags.tot_epoch):    \n",
    "    print(\"Batch [%d] starts\" % epoch)\n",
    "    while(not free_queue.empty()): time.sleep(1)\n",
    "    for step in range(tot_step):\n",
    "        if step == 0: \n",
    "            test_n_step_model(1, model, flags, eps_n=100, temp=temp)\n",
    "            model.train(True)\n",
    "        \n",
    "        batch = get_batch_m(flags, buffers)\n",
    "        rs_loss, vs_loss, logits_loss = compute_loss_m(model, batch)\n",
    "        tot_loss = rs_loss + vs_loss + 0.01 * logits_loss\n",
    "        for n, l in enumerate([tot_loss, rs_loss, vs_loss, logits_loss]):\n",
    "            loss_stats[n].append(l.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"[%d:%d] F: %d \\t tot_loss %f rs_loss %f vs_loss %f logits_loss %f\" % ((\n",
    "                epoch, step, (step + epoch * tot_step) * flags.bsz * flags.unroll_len,) +\n",
    "                tuple(np.average(l) for l in loss_stats)))\n",
    "        optimizer.zero_grad()        \n",
    "        tot_loss.backward()\n",
    "        optimize_params = optimizer.param_groups[0]['params']\n",
    "        if flags.grad_norm_clipping > 0:\n",
    "            total_norm = nn.utils.clip_grad_norm_(optimize_params, flags.grad_norm_clipping)\n",
    "        optimizer.step()    \n",
    "    for m in range(flags.seq_n): free_queue.put(m)\n",
    "        \n",
    "for _ in range(flags.num_actors): free_queue.put(None)        \n",
    "for actor in actor_processes: actor.join(timeout=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab596f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_queue.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the threads\n",
    "\n",
    "for _ in range(flags.num_actors): free_queue.put(None)  \n",
    "for actor in actor_processes: actor.join(timeout=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc08045",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model testing / debug </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict(),},\"../models/model_3.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())         \n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.discounting = 0.97\n",
    "flags.device = torch.device(\"cuda\")\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/model_3.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8834194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_returns = {}\n",
    "for n in range(1,4):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step_model(n, model, flags, eps_n=500, temp=20)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ea44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCTS testing\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"\n",
    "    Core Monte Carlo Tree Search algorithm.\n",
    "    To decide on an action, we run N simulations, always starting at the root of\n",
    "    the search tree and traversing the tree according to the UCB formula until we\n",
    "    reach a leaf node.\n",
    "    \"\"\"\n",
    "    def __init__(self, flags, num_actions):\n",
    "        self.flags = flags\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def run(self, model, obs, add_exploration_noise,):\n",
    "        \"\"\"\n",
    "        At the root of the search tree we use the representation function to obtain a\n",
    "        hidden state given the current observation.\n",
    "        We then run a Monte Carlo Tree Search using only action sequences and the model\n",
    "        learned by the network.\n",
    "        Only supports a batch size of 1.        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            root = Node(0)\n",
    "            _, root_predicted_value, policy_logits, hidden_state = model(\n",
    "                obs[\"frame\"][0], obs[\"last_action\"], one_hot=False)\n",
    "            reward = 0.\n",
    "            root_predicted_value = root_predicted_value[-1].item()\n",
    "            policy_logits = policy_logits[-1]\n",
    "            hidden_state = hidden_state[-1]\n",
    "\n",
    "            root.expand(num_actions, reward, policy_logits, hidden_state,)\n",
    "\n",
    "            if add_exploration_noise:\n",
    "                root.add_exploration_noise(\n",
    "                    dirichlet_alpha=self.flags.root_dirichlet_alpha,\n",
    "                    exploration_fraction=self.flags.root_exploration_fraction,\n",
    "                )\n",
    "\n",
    "            min_max_stats = MinMaxStats()\n",
    "\n",
    "            max_tree_depth = 0\n",
    "            \n",
    "            #print(\"p at root:\", torch.softmax(policy_logits, dim=-1))\n",
    "            for k in range(self.flags.num_simulations): \n",
    "                \n",
    "                #print(\"=======%d iteration======\"%k)\n",
    "                node = root\n",
    "                search_path = [node]\n",
    "                current_tree_depth = 0\n",
    "\n",
    "                while node.expanded():\n",
    "                    current_tree_depth += 1                    \n",
    "                    action, node = self.select_child(node, min_max_stats)                    \n",
    "                    search_path.append(node)\n",
    "                    #print(\"action sel: %d\" % action)\n",
    "                \n",
    "                #np.set_printoptions(precision=5)\n",
    "                #for x in [\"prior_score\", \"value_score\", \"pb_c\", \"prior\", \"visit_count\"]:                    \n",
    "                #    print(x, \"\\t\", np.array([getattr(search_path[0].children[n], x) for n in range(5)]))\n",
    "\n",
    "                # Inside the search tree we use the dynamics function to obtain the next hidden\n",
    "                # state given an action and the previous hidden state\n",
    "                parent = search_path[-2]     \n",
    "                reward, value, policy_logits, hidden_state = model.forward_encoded(\n",
    "                    parent.hidden_state, torch.tensor([[action]]).to(parent.hidden_state.device), one_hot=False)\n",
    "                reward = reward[-1].item()\n",
    "                value = value[-1].item()\n",
    "                #print(\"model final output: %4f\" % value)\n",
    "                policy_logits = policy_logits[-1]\n",
    "                hidden_state = hidden_state[-1]\n",
    "                node.expand(num_actions, reward, policy_logits, hidden_state)\n",
    "                self.backpropagate(search_path, value, min_max_stats)\n",
    "                max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
    "\n",
    "            extra_info = {\n",
    "                \"max_tree_depth\": max_tree_depth,\n",
    "                \"root_predicted_value\": root_predicted_value,\n",
    "            }\n",
    "        return root, extra_info\n",
    "\n",
    "    def select_child(self, node, min_max_stats):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        max_ucb = max(\n",
    "            self.ucb_score(node, child, min_max_stats)\n",
    "            for action, child in node.children.items()\n",
    "        )\n",
    "        action = np.random.choice(\n",
    "            [\n",
    "                action\n",
    "                for action, child in node.children.items()\n",
    "                if self.ucb_score(node, child, min_max_stats) == max_ucb\n",
    "            ]\n",
    "        )\n",
    "        return action, node.children[action]\n",
    "\n",
    "    def ucb_score(self, parent, child, min_max_stats):\n",
    "        \"\"\"\n",
    "        The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
    "        \"\"\"\n",
    "        pb_c = (\n",
    "            math.log(\n",
    "                (parent.visit_count + self.flags.pb_c_base + 1) / self.flags.pb_c_base\n",
    "            )\n",
    "            + self.flags.pb_c_init\n",
    "        )\n",
    "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior\n",
    "\n",
    "        if child.visit_count > 0:\n",
    "            # Mean value Q\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward + self.flags.discounting * child.value())\n",
    "        else:\n",
    "            value_score = 0\n",
    "            \n",
    "        child.pb_c = pb_c\n",
    "        child.prior_score = prior_score\n",
    "        child.value_score = value_score\n",
    "        \n",
    "        return prior_score + value_score\n",
    "\n",
    "    def backpropagate(self, search_path, value, min_max_stats):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        #print(\"bs value: %.4f\" % value)\n",
    "        for n, node in enumerate(reversed(search_path)):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            min_max_stats.update(node.reward + self.flags.discounting * node.value())\n",
    "            value = node.reward + self.flags.discounting * value\n",
    "            #print(\"%d - val: %.4f r: %.4f\" % (n, value, node.reward))\n",
    "            #print(\"node value_sum %.4f\" % node.value_sum)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior):\n",
    "        self.visit_count = 0\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def expand(self, num_actions, reward, policy_logits, hidden_state):\n",
    "        \"\"\"\n",
    "        We expand a node using the value, reward and policy prediction obtained from the\n",
    "        neural network.\n",
    "        \"\"\"\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        policy_values = torch.softmax(policy_logits[0], dim=0).tolist()\n",
    "        for a in range(num_actions):\n",
    "            self.children[a] = Node(policy_values[a])\n",
    "\n",
    "    def add_exploration_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        \"\"\"\n",
    "        At the start of each search, we add dirichlet noise to the prior of the root to\n",
    "        encourage the search to explore new actions.\n",
    "        \"\"\"\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "class MinMaxStats:\n",
    "    \"\"\"\n",
    "    A class that holds the min-max values of the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.maximum = -float(\"inf\")\n",
    "        self.minimum = float(\"inf\")\n",
    "\n",
    "    def update(self, value):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value):\n",
    "        if self.maximum > self.minimum:\n",
    "            # We normalize only when we have set the maximum and minimum values\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value            \n",
    "\n",
    "def select_action(node, temperature):\n",
    "    \"\"\"\n",
    "    Select action according to the visit count distribution and the temperature.\n",
    "    The temperature is changed dynamically with the visit_softmax_temperature function\n",
    "    in the config.\n",
    "    \"\"\"\n",
    "    visit_counts = np.array(\n",
    "        [child.visit_count for child in node.children.values()], dtype=\"int32\"\n",
    "    )\n",
    "    actions = [action for action in node.children.keys()]\n",
    "    if temperature == 0:\n",
    "        action = actions[np.argmax(visit_counts)]\n",
    "    elif temperature == float(\"inf\"):\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        # See paper appendix Data Generation\n",
    "        visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "        visit_count_distribution = visit_count_distribution / sum(\n",
    "            visit_count_distribution\n",
    "        )\n",
    "        action = np.random.choice(actions, p=visit_count_distribution)\n",
    "    #print(\"visit_counts\", visit_counts)\n",
    "    #print(\"visit_count_distribution\", visit_count_distribution)\n",
    "    return action\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()      \n",
    "flags = parser.parse_args([])   \n",
    "\n",
    "env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)\n",
    "env = Environment(env)\n",
    "env.initial()\n",
    "obs_shape, num_actions = env.gym_env.observation_space.shape, env.gym_env.action_space.n\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args([])   \n",
    "flags.discounting = 0.97\n",
    "flags.pb_c_init = 1.25\n",
    "flags.pb_c_base = 19652\n",
    "flags.root_dirichlet_alpha = 0.25\n",
    "flags.root_exploration_fraction = 0.\n",
    "flags.num_simulations = 3\n",
    "flags.temp = 0.5\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "eps_n = 10\n",
    "eps_n_cur = 0\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "obs = env.initial()\n",
    "returns = []\n",
    "mcts = MCTS(flags, num_actions)\n",
    "obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "root, extra_info = mcts.run(model, obs, add_exploration_noise=True)   \n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "actions = torch.tensor([0, 1, 3, 4, 4, 4, 2, 4, 2, 2, 2, 1, 1, 1, 1]).long().to(flags.device).reshape(-1, 1)\n",
    "reward, value, policy_logits, hidden_state  = model(obs[\"frame\"][0], actions, one_hot=False)\n",
    "print(reward, value)\n",
    "\n",
    "while(len(returns) <= eps_n):\n",
    "    cur_returns = obs['episode_return']    \n",
    "    obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "    root, extra_info = mcts.run(model, obs, add_exploration_noise=True)    \n",
    "    new_action = select_action(root, flags.temp)    \n",
    "    #plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    #flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "    #plt.show()\n",
    "    #print(\"action selected\", new_action)\n",
    "    #print(\"===========================================\")\n",
    "    obs = env.step(torch.tensor([new_action]))\n",
    "    if torch.any(obs['done']):\n",
    "        returns.extend(cur_returns[obs['done']].numpy())\n",
    "    if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "        eps_n_cur = len(returns) + 10\n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test planning algorithm\n",
    "bsz = 1\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "obs = env.initial()\n",
    "state = obs['frame'][0].to(flags.device).clone()\n",
    "action = torch.zeros(bsz).long().to(flags.device)\n",
    "encoded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e67032",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.Tensor([4]).long().to(flags.device)\n",
    "obs = env.step(action)\n",
    "state = obs['frame'][0].to(flags.device).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c551f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "device = flags.device\n",
    "\n",
    "for _ in range(1):\n",
    "    plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "    plt.show()\n",
    "    ret = np.zeros((5, 5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            for k in range(5):\n",
    "                test_action_seq = [i,j,k]\n",
    "                test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "                old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "                rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "                ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "    print(np.max(ret), (np.max(ret) == ret).nonzero())    \n",
    "    new_action = torch.Tensor((np.max(ret) == ret).nonzero()[0]).long().to(flags.device)\n",
    "    #obs = env.step(new_action)\n",
    "    #state = obs['frame'][0].to(flags.device).clone()\n",
    "    #action = new_action            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a9ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, prob, q_ret = n_step_greedy_model(state, action, model, 3, encoded=None, temp=10.)\n",
    "print(\"action: \", action)\n",
    "print(\"prob: \", prob)\n",
    "print(\"q_ret: \", q_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_action_seq = [2,3,1]\n",
    "test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "print(\"rs\", rs)\n",
    "print(\"vs\", vs)\n",
    "print(\"logits\", logits)\n",
    "print(\"ret\", ret[i,j,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 10.\n",
    "\n",
    "bsz = state.shape[0]\n",
    "device = state.device \n",
    "num_actions = model.num_actions    \n",
    "model.train(False)\n",
    "\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "\n",
    "for act in range(num_actions):        \n",
    "    new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "    old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "    rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "    ret = rs[0] + flags.discounting * vs[1]\n",
    "    rs_act[:, act] = rs[0]\n",
    "    vs_act[:, act] = vs[1]\n",
    "    q_ret[:, act] = ret\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = flags.device\n",
    "net_state = env.clone_state()\n",
    "\n",
    "bsz = 1\n",
    "temp = 10.\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)   \n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "for act in range(num_actions):\n",
    "    obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "    obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "    ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "    rs_act[:, act] = obs['reward']\n",
    "    vs_act[:, act] = net(obs)[0]['baseline']\n",
    "    q_ret[:, act] = ret\n",
    "    env.restore_state(net_state)\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)\n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd08cd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9900, device='cuda:0') tensor([[ 0, 47],\n",
      "        [ 0, 51],\n",
      "        [ 2, 10],\n",
      "        [ 3,  8]], device='cuda:0')\n",
      "tensor([[ 2, 58],\n",
      "        [ 3, 22],\n",
      "        [ 3, 59],\n",
      "        [ 5, 33]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch = get_batch_m(flags, buffers)\n",
    "print(torch.max(batch[\"reward\"]), (torch.max(batch[\"reward\"]) == batch[\"reward\"]).nonzero())\n",
    "print(batch[\"done\"].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7e953ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 reward -0.010000 bootstrap 0.124231 truncated 0.000000 vs[t-1] 0.097474 new_targ 0.110504\n",
      "4 reward -0.010000 bootstrap 0.110504 truncated 0.000000 vs[t-1] 0.141802 new_targ 0.097189\n",
      "3 reward -0.010000 bootstrap 0.097189 truncated 0.000000 vs[t-1] 0.173684 new_targ 0.084273\n",
      "2 reward 0.990000 bootstrap 0.084273 truncated 0.000000 vs[t-1] 0.312329 new_targ 1.071745\n",
      "1 reward -0.010000 bootstrap 1.071745 truncated 0.000000 vs[t-1] 0.832461 new_targ 1.029593\n",
      "done tensor([False, False, False, False, False, False], device='cuda:0')\n",
      "done_masks tensor([False, False, False, False, False], device='cuda:0')\n",
      "vs:  tensor([0.8325, 0.3123, 0.1737, 0.1418, 0.0975, 0.1242], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "target_vs:  tensor([1.0296, 1.0717, 0.0843, 0.0972, 0.1105], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "reward:  tensor([0.0007, 0.1490, 0.1359, 0.0955, 0.0116], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "target_reward:  tensor([-0.0100,  0.9900, -0.0100, -0.0100, -0.0100], device='cuda:0')\n",
      "logits:  tensor([[-0.9673, -0.4577, -1.0373, -1.4926,  0.0846],\n",
      "        [-0.4168, -0.4886,  0.2400, -0.1134, -0.0797],\n",
      "        [-0.4296, -0.3781,  0.1268,  0.5917, -0.6968],\n",
      "        [-0.5089, -0.5806,  0.2383,  0.1876, -0.2986],\n",
      "        [-0.5534, -0.3538,  0.2360, -0.0058, -0.2222]], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "target_logits:  tensor([[-0.2980,  0.9509, -1.0657, -1.0807,  1.0567],\n",
      "        [-0.2980,  0.9509, -1.0657, -1.0807,  1.0567],\n",
      "        [ 0.3034,  0.5813, -0.9270,  1.9539, -2.7967],\n",
      "        [ 0.0158,  0.3151, -0.7212,  1.4481, -1.7363],\n",
      "        [-0.3098,  1.2413, -1.4106,  0.0531, -0.2152]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# DEBUG LOSS\n",
    "\n",
    "#batch = get_batch_m(flags, buffers)\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "logits = logits[:-1]\n",
    "\n",
    "target_rewards = batch['reward'][1:]\n",
    "target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "target_vs = []\n",
    "target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0]    \n",
    "\n",
    "for t in range(vs.shape[0]-1, 0, -1):\n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))\n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "# if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "# rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "# vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "# logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "done_masks = []\n",
    "done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "for t in range(vs.shape[0]):\n",
    "    done = torch.logical_or(done, batch['done'][t])\n",
    "    done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "\n",
    "# compute final loss\n",
    "huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "rs_loss = torch.sum(huberloss(rs, target_rewards) * (~done_masks).float())\n",
    "#rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "vs_loss = torch.sum(huberloss(vs[:-1], target_vs) * (~done_masks).float())\n",
    "#vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "logits_loss = compute_cross_entropy_loss(logits, target_logits, done_masks)\n",
    "\n",
    "# debug\n",
    "ind = 10\n",
    "\n",
    "target_vs = []\n",
    "target_v = vs[-1]\n",
    "for t in range(vs.shape[0]-1, 0, -1):        \n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    print(t, \n",
    "          \"reward %2f\" % batch['reward'][t,ind].item(), \n",
    "          \"bootstrap %2f\" % (target_v * (~batch['done'][t]).float())[ind].item(), \n",
    "          \"truncated %2f\" % (vs[t-1] * (batch['truncated_done'][t]).float())[ind].item(),\n",
    "          \"vs[t-1] %2f\" % vs[t-1][ind].item(),\n",
    "          \"new_targ %2f\" % new_target_v[ind].item())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))    \n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)   \n",
    "print(\"done\", batch[\"done\"][:, ind])\n",
    "print(\"done_masks\", done_masks[:, ind])\n",
    "print(\"vs: \", vs[:, ind])\n",
    "print(\"target_vs: \", target_vs[:, ind])\n",
    "print(\"reward: \", rs[:, ind])\n",
    "print(\"target_reward: \", target_rewards[:, ind])\n",
    "print(\"logits: \", logits[:, ind])\n",
    "print(\"target_logits: \", target_logits[:, ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_m(model, batch):\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_rewards = batch['reward'][1:]\n",
    "    target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float())# +\n",
    "                           #vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "    # if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "    # rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "    # vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "    # logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "    for t in range(vs.shape[0]):\n",
    "        done = torch.logical_or(done, batch['done'][t])\n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "    done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()) * (~done_masks).float())\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()) * (~done_masks).float())\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), done_masks)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss\n",
    "\n",
    "# alt. version of computing loss by treading terminal state as absorbing state (as in MuZero)\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_logits = batch['policy_logits'][1:].clone()\n",
    "    target_rewards = batch['reward'][1:].clone()\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "\n",
    "    c_logits = target_logits[0]\n",
    "    c_state = batch['frame'][0]\n",
    "    for t in range(vs.shape[0]-1):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        c_logits = torch.where(done.unsqueeze(-1), c_logits, target_logits[t])\n",
    "        target_logits[t] = c_logits\n",
    "        c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][t])  \n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "    done_masks = torch.concat(done_masks, dim=0)\n",
    "    done = torch.logical_or(done, batch['done'][-1])\n",
    "    c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][-1])\n",
    "    target_rewards = target_rewards * (~done_masks).float()\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(c_state, batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * target_v\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()))\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()))\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), None)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
