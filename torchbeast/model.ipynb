{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fc8a370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG:58738 __init__:275 2022-11-02 22:00:51,431] matplotlib data path: /home/sc/anaconda3/lib/python3.9/site-packages/matplotlib/mpl-data\n",
      "[DEBUG:58738 __init__:275 2022-11-02 22:00:51,436] CONFIGDIR=/home/sc/.config/matplotlib\n",
      "[DEBUG:58738 __init__:1445 2022-11-02 22:00:51,438] interactive is False\n",
      "[DEBUG:58738 __init__:1446 2022-11-02 22:00:51,440] platform is linux\n",
      "[DEBUG:58738 __init__:1447 2022-11-02 22:00:51,440] loaded modules: ['sys', 'builtins', '_frozen_importlib', '_imp', '_thread', '_warnings', '_weakref', '_io', 'marshal', 'posix', '_frozen_importlib_external', 'time', 'zipimport', '_codecs', 'codecs', 'encodings.aliases', 'encodings', 'encodings.utf_8', '_signal', 'encodings.latin_1', '_abc', 'abc', 'io', '__main__', '_stat', 'stat', '_collections_abc', 'genericpath', 'posixpath', 'os.path', 'os', '_sitebuiltins', '_locale', '_bootlocale', '_distutils_hack', 'types', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib', 'importlib.machinery', '_heapq', 'heapq', 'itertools', 'keyword', '_operator', 'operator', 'reprlib', '_collections', 'collections', 'collections.abc', '_functools', 'functools', 'contextlib', 'enum', '_sre', 'sre_constants', 'sre_parse', 'sre_compile', 'copyreg', 're', 'typing.io', 'typing.re', 'typing', 'importlib.abc', 'importlib.util', 'mpl_toolkits', 'sphinxcontrib', 'zope', 'site', '_weakrefset', 'weakref', 'pkgutil', 'runpy', 'ipykernel._version', '_json', 'json.scanner', 'json.decoder', 'json.encoder', 'json', 'errno', 'signal', 'threading', 'pwd', 'grp', '_posixsubprocess', 'select', 'math', 'selectors', 'subprocess', 'jupyter_client._version', '_ast', 'ast', '_opcode', 'opcode', 'dis', 'token', 'tokenize', 'linecache', 'inspect', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets.utils.descriptions', 'traitlets.traitlets', 'copy', 'traitlets.utils.decorators', 'traitlets._version', 'traitlets', 'concurrent', 'traceback', '_string', 'string', 'atexit', 'logging', 'concurrent.futures._base', 'concurrent.futures', '_socket', 'array', 'socket', '_ssl', '_struct', 'struct', 'binascii', 'base64', 'ssl', 'asyncio.constants', 'asyncio.format_helpers', 'asyncio.base_futures', 'asyncio.log', 'asyncio.coroutines', '_contextvars', 'contextvars', 'asyncio.exceptions', 'asyncio.base_tasks', '_asyncio', 'asyncio.events', 'asyncio.futures', 'asyncio.protocols', 'asyncio.transports', 'asyncio.sslproto', 'asyncio.locks', 'asyncio.tasks', 'asyncio.staggered', 'asyncio.trsock', 'asyncio.base_events', 'asyncio.runners', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.threads', 'asyncio.base_subprocess', 'asyncio.selector_events', 'asyncio.unix_events', 'asyncio', '_queue', 'queue', 'platform', '_ctypes', 'ctypes._endian', 'ctypes', 'zmq.backend.select', '_cython_0_29_30', 'cython_runtime', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.message', '_bisect', 'bisect', '_random', '_sha512', 'random', '_compat_pickle', '_pickle', 'pickle', 'zmq.constants', 'zmq.backend.cython.socket', 'zmq.backend.cython._device', 'zmq.backend.cython._poll', 'zmq.backend.cython._proxy_steerable', 'zmq.backend.cython._version', 'zmq.backend.cython.error', 'zmq.backend.cython.utils', 'zmq.backend.cython', 'zmq.backend', 'zmq.sugar.attrsettr', 'zmq._typing', 'zmq.utils', 'zmq.utils.jsonapi', 'zmq.sugar.poll', 'zmq.sugar.socket', 'zmq.sugar.context', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'zmq.sugar', 'zmq', 'zmq._future', 'zmq.asyncio', 'jupyter_client.channelsabc', '_hashlib', '_blake2', 'hashlib', 'hmac', 'pprint', '_datetime', 'datetime', 'gettext', 'argparse', 'traitlets.config.loader', 'textwrap', 'traitlets.utils.text', 'traitlets.config.configurable', 'traitlets.config.application', 'traitlets.config', 'traitlets.log', 'tornado', 'numbers', 'logging.handlers', 'html.entities', 'html', 'urllib', 'urllib.parse', 'zlib', 'tornado.speedups', 'tornado.util', 'tornado.escape', 'colorama.ansi', 'colorama.win32', 'colorama.winterm', 'colorama.ansitowin32', 'colorama.initialise', 'colorama', '_curses', 'curses', 'tornado.log', 'tornado.concurrent', 'tornado.ioloop', 'tornado.platform', 'tornado.gen', 'tornado.platform.asyncio', 'zmq.eventloop.ioloop', 'zmq.eventloop', 'zmq.eventloop.zmqstream', 'jupyter_client.adapter', 'dateutil._version', 'dateutil', '__future__', 'locale', 'calendar', 'six', '_decimal', 'decimal', 'dateutil._common', 'dateutil.relativedelta', 'six.moves', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.tz.tz', 'dateutil.tz', 'dateutil.parser._parser', 'dateutil.parser.isoparser', 'dateutil.parser', '_strptime', 'jupyter_client.jsonutil', 'jupyter_client.session', 'jupyter_client.channels', 'termios', 'getpass', 'jupyter_client.clientabc', 'fnmatch', 'glob', '_compression', '_bz2', 'bz2', '_lzma', 'lzma', 'shutil', 'tempfile', 'jupyter_core.version', 'jupyter_core', 'ntpath', 'pathlib', 'jupyter_core.paths', 'jupyter_client.localinterfaces', 'jupyter_client.utils', 'jupyter_client.connect', 'jupyter_client.client', 'jupyter_client.asynchronous.client', 'jupyter_client.asynchronous', 'jupyter_client.blocking.client', 'jupyter_client.blocking', 'jupyter_client.launcher', '_uuid', 'uuid', 'jupyter_client.managerabc', 'zipfile', 'configparser', 'entrypoints', 'jupyter_client.provisioning.provisioner_base', 'jupyter_client.provisioning.factory', 'jupyter_client.provisioning.local_provisioner', 'jupyter_client.provisioning', 'jupyter_client.kernelspec', 'jupyter_client.manager', 'jupyter_client.multikernelmanager', 'jupyter_client', 'ipykernel.connect', 'ipykernel', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'sysconfig', '_sysconfigdata__linux_x86_64-linux-gnu', 'pydoc', 'bdb', 'IPython.utils', 'IPython.utils.ipstruct', 'IPython.utils.coloransi', 'pygments', 'IPython.utils.colorable', 'IPython.utils.PyColorize', 'IPython.utils.encoding', 'IPython.utils.py3compat', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'cmd', 'codeop', 'code', 'pdb', 'IPython.core.debugger', 'IPython.core.display_trap', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'tty', 'pty', 'fcntl', 'resource', 'ptyprocess.util', 'ptyprocess.ptyprocess', 'ptyprocess', 'pexpect.spawnbase', 'pexpect.pty_spawn', 'pexpect.run', 'pexpect', 'shlex', 'IPython.utils._process_common', 'IPython.utils._process_posix', 'IPython.utils.process', 'IPython.utils.decorators', 'IPython.utils.path', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.core.ultratb', 'IPython.utils._sysinfo', 'IPython.utils.sysinfo', 'IPython.core.crashhandler', 'IPython.utils.importstring', 'IPython.paths', 'IPython.core.profiledir', 'IPython.core.application', 'IPython.terminal', 'IPython.core.compilerop', 'IPython.core.error', 'IPython.utils.text', 'IPython.core.magic_arguments', 'getopt', 'mimetypes', 'IPython.core.display', 'IPython.core.page', 'IPython.lib.security', 'IPython.lib', 'IPython.lib.pretty', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.util', 'pygments.lexers', 'pygments.filter', 'pygments.token', 'pygments.filters', 'pygments.regexopt', 'pygments.lexer', 'pygments.unistring', 'pygments.lexers.python', 'pygments.formatters._mapping', 'pygments.formatters', 'pygments.styles', 'pygments.formatter', 'pygments.formatters.html', 'IPython.core.oinspect', 'IPython.core.inputtransformer2', 'decorator', 'IPython.core.magic', 'pickleshare', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.prefilter', 'IPython.core.alias', 'IPython.core.builtin_trap', 'backcall.backcall', 'backcall', 'IPython.core.events', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.utils.sentinel', 'IPython.core.formatters', '_sqlite3', 'sqlite3.dbapi2', 'sqlite3', 'IPython.core.history', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.lib.display', 'IPython.display', 'IPython.utils.capture', 'IPython.utils.io', 'IPython.core.hooks', 'IPython.utils.strdispatch', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.core.interactiveshell', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop.utils', 'prompt_toolkit.eventloop.async_generator', 'wcwidth.table_wide', 'wcwidth.table_zero', 'wcwidth.unicode_versions', 'wcwidth.wcwidth', 'wcwidth', 'prompt_toolkit.utils', 'prompt_toolkit.eventloop.inputhook', 'prompt_toolkit.eventloop', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.clipboard', 'prompt_toolkit.cache', 'prompt_toolkit.enums', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.app', 'prompt_toolkit.filters.cli', 'prompt_toolkit.filters.utils', 'prompt_toolkit.filters', 'prompt_toolkit.document', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.data_structures', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.pygments', 'colorsys', 'prompt_toolkit.styles.style_transformation', 'prompt_toolkit.styles', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.base', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output', 'prompt_toolkit.output.vt100', 'prompt_toolkit.mouse_events', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.formatted_text.ansi', 'xml', 'xml.dom.domreg', 'xml.dom', 'xml.dom.minicompat', 'xml.dom.NodeFilter', 'xml.dom.xmlbuilder', 'xml.dom.minidom', 'prompt_toolkit.formatted_text.html', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.formatted_text', 'prompt_toolkit.completion.base', 'prompt_toolkit.completion.deduplicate', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.completion.fuzzy_completer', 'prompt_toolkit.completion.nested', 'prompt_toolkit.completion', 'prompt_toolkit.history', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.search', 'prompt_toolkit.validation', 'prompt_toolkit.buffer', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.lexers', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.controls', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.layout.screen', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.layout.scrollable_pane', 'prompt_toolkit.layout', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.renderer', 'prompt_toolkit.application.application', 'prompt_toolkit.application.dummy', 'prompt_toolkit.application', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.widgets', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.shortcuts', 'prompt_toolkit', 'prompt_toolkit.patch_stdout', 'pygments.style', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'parso.utils', 'parso.tree', 'parso.python', 'parso.python.token', 'parso.python.tokenize', 'parso.pgen2.grammar_parser', 'parso.pgen2.generator', 'parso.pgen2', 'parso.parser', 'parso._compatibility', 'difflib', 'parso.python.prefix', 'parso.python.tree', 'parso.python.parser', 'parso.python.diff', 'gc', 'parso.cache', 'parso.normalizer', 'parso.python.errors', 'parso.python.pep8', 'parso.file_io', 'parso.grammar', 'parso', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.file_io', 'jedi.inference.cache', 'jedi.inference.helpers', 'jedi.inference.utils', 'jedi.inference.base_value', 'jedi.inference.sys_path', 'jedi.inference.recursion', 'jedi.inference.flow_analysis', 'jedi.common', 'jedi.inference.lazy_value', 'jedi.inference.docstrings', 'jedi.plugins', 'jedi.inference.names', 'jedi.inference.filters', 'jedi.inference.compiled.getattr_static', 'jedi.inference.compiled.access', 'jedi.inference.signature', 'jedi.inference.context', 'jedi.inference.compiled.value', 'jedi.inference.compiled', 'jedi.inference.analysis', 'jedi.inference.gradual', 'jedi.inference.value.module', 'jedi.inference.value.dynamic_arrays', 'jedi.inference.value.iterable', 'jedi.inference.arguments', 'jedi.inference.parser_cache', 'jedi.inference.gradual.generics', 'jedi.inference.value.function', 'jedi.inference.value.klass', 'jedi.inference.value.instance', 'jedi.inference.value', 'jedi.inference.gradual.base', 'jedi.inference.gradual.type_var', 'jedi.inference.gradual.typing', 'jedi.inference.gradual.stub_value', 'jedi.inference.gradual.typeshed', 'jedi._compatibility', 'jedi.inference.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.inference.compiled.subprocess', 'jedi.inference.imports', 'jedi.inference.param', 'jedi.inference.gradual.annotation', 'jedi.inference.value.decorator', 'jedi.inference.syntax_tree', 'jedi.inference', 'jedi.inference.gradual.conversion', 'jedi.inference.compiled.mixed', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.keywords', 'jedi.api.completion_cache', 'jedi.api.helpers', 'jedi.api.classes', 'jedi.api.interpreter', 'jedi.api.strings', 'jedi.api.file_name', 'jedi.inference.docstring_utils', 'jedi.api.completion', 'filecmp', 'jedi.api.environment', 'jedi.inference.references', 'jedi.api.project', 'jedi.api.errors', 'jedi.api.refactoring', 'jedi.api.refactoring.extract', 'jedi.inference.gradual.utils', 'jedi.api', 'jedi.plugins.stdlib', 'jedi.plugins.flask', 'jedi.plugins.pytest', 'jedi.plugins.django', 'jedi.plugins.registry', 'jedi', 'IPython.core.completer', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'concurrent.futures.thread', 'IPython.terminal.debugger', 'IPython.lib.clipboard', 'IPython.terminal.magics', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.interactiveshell', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'email', 'http', 'email.errors', 'email.quoprimime', 'email.base64mime', 'quopri', 'email.encoders', 'email.charset', 'email.header', 'email._parseaddr', 'email.utils', 'email._policybase', 'email.feedparser', 'email.parser', 'uu', 'email._encoded_words', 'email.iterators', 'email.message', 'http.client', 'urllib.response', 'urllib.error', 'urllib.request', 'IPython.core.magics.code', 'IPython.core.magics.config', 'IPython.core.magics.display', 'timeit', '_lsprof', 'profile', 'cProfile', 'dataclasses', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.execution', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.packaging', 'IPython.core.pylabtools', 'IPython.core.magics.pylab', 'IPython.lib.backgroundjobs', 'IPython.core.magics.script', 'IPython.core.magics', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.terminal.ipapp', 'IPython.terminal.embed', 'IPython.utils.frame', 'IPython', 'ipykernel.control', 'ipykernel.heartbeat', 'ipykernel.iostream', 'IPython.utils.tokenutil', 'ipykernel.jsonutil', 'psutil._common', 'psutil._compat', 'psutil._psposix', 'psutil._psutil_linux', 'psutil._psutil_posix', 'psutil._pslinux', 'psutil', 'tornado.locks', 'tornado.queues', 'ipykernel.kernelbase', 'ipykernel.comm.comm', 'ipykernel.comm.manager', 'ipykernel.comm', 'ipykernel.compiler', 'debugpy._version', 'debugpy.common', 'debugpy.common.json', 'debugpy.common.fmt', 'debugpy.common.compat', 'debugpy', 'debugpy._vendored._util', 'debugpy._vendored', '_pydevd_bundle', 'encodings.ascii', 'stringprep', 'encodings.idna', '_pydevd_bundle.pydevd_vm_type', '_pydev_imps', 'xmlrpc', 'xml.parsers', 'pyexpat.errors', 'pyexpat.model', 'pyexpat', 'xml.parsers.expat.model', 'xml.parsers.expat.errors', 'xml.parsers.expat', 'gzip', 'xmlrpc.client', 'socketserver', 'http.server', 'xmlrpc.server', '_pydev_imps._pydev_saved_modules', '_pydevd_bundle.pydevd_constants', '_pydev_bundle', '_pydev_runfiles', '_pydevd_frame_eval', 'pydev_ipython', 'pydevd_concurrency_analyser', 'plistlib', 'pkg_resources.extern', 'pkg_resources._vendor', 'pkg_resources._vendor.jaraco', 'pkg_resources.extern.jaraco', 'importlib._common', 'importlib.resources', 'pkg_resources._vendor.more_itertools.recipes', 'pkg_resources._vendor.more_itertools.more', 'pkg_resources._vendor.more_itertools', 'pkg_resources.extern.more_itertools', 'pkg_resources.extern.jaraco.functools', 'pkg_resources.extern.jaraco.context', 'pkg_resources.extern.jaraco.text', 'pkg_resources._vendor.appdirs', 'pkg_resources.extern.appdirs', 'pkg_resources._vendor.packaging.__about__', 'pkg_resources._vendor.packaging', 'pkg_resources.extern.packaging', 'pkg_resources.extern.packaging._structures', 'pkg_resources.extern.packaging.version', 'pkg_resources._vendor.packaging._manylinux', 'pkg_resources._vendor.packaging._musllinux', 'pkg_resources.extern.packaging.tags', 'pkg_resources.extern.packaging.utils', 'pkg_resources.extern.packaging.specifiers', 'pkg_resources._vendor.pyparsing.util', 'pkg_resources._vendor.pyparsing.unicode', 'pkg_resources._vendor.pyparsing.exceptions', 'pkg_resources._vendor.pyparsing.actions', 'pkg_resources._vendor.pyparsing.results', 'pkg_resources._vendor.pyparsing.core', 'pkg_resources._vendor.pyparsing.helpers', 'pkg_resources._vendor.pyparsing.testing', 'pkg_resources._vendor.pyparsing.common', 'pkg_resources._vendor.pyparsing', 'pkg_resources.extern.pyparsing', 'pkg_resources.extern.packaging.markers', 'pkg_resources.extern.packaging.requirements', 'pkg_resources', 'pydevd_plugins', '_pydev_bundle.pydev_log', '_pydev_bundle._pydev_filesystem_encoding', '_pydevd_bundle.pydevd_comm_constants', 'pydevd_file_utils', '_pydev_imps._pydev_execfile', '_pydevd_bundle.pydevd_exec2', '_pydev_bundle.pydev_imports', '_pydev_bundle.pydev_is_thread_alive', '_pydev_bundle.pydev_override', 'pydevd_plugins.extensions', '_pydevd_bundle.pydevd_extension_utils', '_pydevd_bundle.pydevd_frame_utils', '_pydevd_bundle.pydevd_filtering', '_pydevd_bundle.pydevd_io', '_pydevd_bundle.pydevd_utils', '_pydev_bundle._pydev_tipper_common', '_pydev_bundle._pydev_imports_tipper', '_pydev_bundle._pydev_calltip_util', '_pydevd_bundle.pydevd_safe_repr', '_pydevd_bundle.pydevd_resolver', '_pydevd_bundle.pydevd_extension_api', '_pydevd_bundle.pydevd_xml', '_pydevd_bundle.pydevd_dont_trace', '_pydevd_frame_eval.vendored', '_pydevd_frame_eval.vendored.bytecode.flags', '_pydevd_frame_eval.vendored.bytecode.instr', '_pydevd_frame_eval.vendored.bytecode.bytecode', '_pydevd_frame_eval.vendored.bytecode.concrete', '_pydevd_frame_eval.vendored.bytecode.cfg', '_pydevd_frame_eval.vendored.bytecode', '_pydevd_bundle.pydevd_bytecode_utils', '_pydevd_bundle.pydevd_cython', '_pydevd_bundle.pydevd_cython_wrapper', '_pydevd_bundle.pydevd_additional_thread_info', '_pydevd_bundle.pydevd_thread_lifecycle', '_pydevd_bundle.pydevd_save_locals', '_pydevd_bundle.pydevd_defaults', '_pydev_bundle.pydev_monkey', 'pydevd_tracing', '_pydevd_bundle.pydevd_daemon_thread', '_pydevd_bundle.pydevd_timeout', '_pydevd_bundle.pydevd_vars', '_pydev_bundle.pydev_console_utils', '_pydevd_bundle.pydevd_import_class', '_pydevd_bundle.pydevd_breakpoints', '_pydevd_bundle.pydevd_custom_frames', '_pydevd_bundle.pydevd_dont_trace_files', '_pydevd_bundle.pydevd_net_command', '_pydev_bundle.pydev_umd', 'pydevconsole', '_pydev_bundle._pydev_completer', '_pydevd_bundle.pydevd_net_command_factory_xml', '_pydevd_bundle.pydevd_frame', '_pydevd_bundle.pydevd_additional_thread_info_regular', '_pydevd_bundle.pydevd_trace_dispatch', '_pydevd_frame_eval.pydevd_frame_eval_main', '_pydevd_bundle.pydevd_source_mapping', 'pydevd_concurrency_analyser.pydevd_thread_wrappers', 'pydevd_concurrency_analyser.pydevd_concurrency_logger', '_pydevd_bundle._debug_adapter', '_pydevd_bundle._debug_adapter.pydevd_schema_log', '_pydevd_bundle._debug_adapter.pydevd_base_schema', '_pydevd_bundle._debug_adapter.pydevd_schema', '_pydevd_bundle.pydevd_reload', '_pydev_bundle.fsnotify', '_pydevd_bundle.pydevd_console', '_pydevd_bundle.pydevd_comm', '_pydevd_bundle.pydevd_net_command_factory_json', '_pydevd_bundle.pydevd_collect_bytecode_info', '_pydevd_bundle.pydevd_api', '_pydevd_bundle.pydevd_json_debug_options', '_pydevd_bundle.pydevd_process_net_command_json', '_pydevd_bundle.pydevd_traceproperty', '_pydevd_bundle.pydevd_process_net_command', '_pydevd_bundle.pydevd_suspended_frames', '_pydevd_bundle.pydevd_trace_api', 'pydevd_plugins.django_debug', 'pydevd_plugins.jinja2_debug', '_pydevd_bundle.pydevd_plugin_utils', 'pydevd_plugins.extensions.types', 'pydevd_plugins.extensions.types.pydevd_helpers', 'pydevd_plugins.extensions.types.pydevd_plugin_numpy_types', 'pydevd_plugins.extensions.types.pydevd_plugins_django_form_str', 'pydevd', 'debugpy._vendored.force_pydevd', 'debugpy.server', 'debugpy.adapter', 'debugpy.common.timestamp', 'debugpy.common.util', 'debugpy.common.log', 'debugpy.common.sockets', 'debugpy.server.api', 'ipykernel.debugger', 'packaging.__about__', 'packaging', 'packaging._structures', 'packaging.version', 'ipykernel.eventloops', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.zmqshell', 'ipykernel.ipkernel', 'ipykernel.parentpoller', 'ipykernel.kernelapp', 'faulthandler', 'IPython.core.completerlib', 'storemagic', 'torch._utils', 'torch._utils_internal', 'torch.version', 'torch.torch_version', 'torch._six', 'torch._C._onnx', 'torch._C._jit', 'torch._C._jit_tree_views', 'torch._C._te', 'torch._C._nvfuser', 'torch._C._monitor', 'torch._C._functorch', 'torch._C._profiler', 'torch._C.cpp', 'torch._C.cpp.nn', 'torch._C._lazy', 'torch._C._lazy_ts_backend', 'torch._C._itt', 'torch._C._cudart', 'torch._C._nvtx', 'torch._C._cudnn', 'torch._C._verbose', 'torch._C', 'torch._C._fft', 'torch._C._linalg', 'torch._C._nested', 'torch._C._nn', 'torch._C._return_types', 'torch._C._sparse', 'torch._C._special', 'torch.utils.throughput_benchmark', 'torch.utils._crash_handler', 'torch.utils.cpp_backtrace', 'torch.utils', 'torch.utils.hooks', 'torch._namedtensor_internals', 'torch.overrides', 'torch._tensor', 'torch.types', 'numpy._globals', 'numpy.__config__', 'numpy._version', 'mkl._mklinit', 'mkl._py_mkl_service', 'mkl', 'numpy._distributor_init', 'numpy.version', 'numpy.core._multiarray_umath', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.compat', 'numpy.core.overrides', 'numpy.core.multiarray', 'numpy.core.umath', 'numpy.core._string_helpers', 'numpy.core._dtype', 'numpy.core._type_aliases', 'numpy.core.numerictypes', 'numpy.core._exceptions', 'numpy.core._methods', 'numpy.core.fromnumeric', 'numpy.core.shape_base', 'numpy.core._ufunc_config', 'numpy.core.arrayprint', 'numpy.core._asarray', 'numpy.core.numeric', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.einsumfunc', 'numpy.core._multiarray_tests', 'numpy.core._add_newdocs', 'numpy.core._add_newdocs_scalars', 'numpy.core._dtype_ctypes', 'numpy.core._internal', 'numpy._pytesttester', 'numpy.core', 'numpy.lib.mixins', 'numpy.lib.ufunclike', 'numpy.lib.type_check', 'numpy.lib.scimath', 'numpy.lib.stride_tricks', 'numpy.lib.twodim_base', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.linalg.linalg', 'numpy.linalg', 'numpy.matrixlib.defmatrix', 'numpy.matrixlib', 'numpy.lib.histograms', 'numpy.lib.function_base', 'numpy.lib.index_tricks', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.polynomial', 'numpy.lib.utils', 'numpy.lib.arraysetops', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.npyio', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy.lib', 'numpy.fft._pocketfft_internal', 'numpy.fft._pocketfft', 'numpy.fft.helper', 'numpy.fft', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.polynomial', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.polynomial', 'numpy.random._common', 'secrets', 'numpy.random.bit_generator', 'numpy.random._bounded_integers', 'numpy.random._mt19937', 'numpy.random.mtrand', 'numpy.random._philox', 'numpy.random._pcg64', 'numpy.random._sfc64', 'numpy.random._generator', 'numpy.random._pickle', 'numpy.random', 'numpy.ctypeslib', 'numpy.ma.core', 'numpy.ma.extras', 'numpy.ma', 'numpy', 'torch.storage', 'torch.random', 'tarfile', 'torch._sources', 'typing_extensions', 'torch._weights_only_unpickler', 'torch.serialization', 'torch._tensor_str', 'torch.amp.autocast_mode', 'torch.amp', 'torch.cuda._utils', 'torch.cuda.graphs', 'torch.cuda.streams', 'torch.cuda._memory_viz', 'torch.cuda.memory', 'torch.cuda.random', 'torch.cuda.sparse', 'torch.cuda.profiler', 'torch.cuda.nvtx', 'torch.cuda.amp.autocast_mode', 'torch.cuda.amp.common', 'torch.cuda.amp.grad_scaler', 'torch.cuda.amp', 'torch.cuda.jiterator', 'torch.cuda', 'torch.sparse', 'torch.backends', 'torch.backends.opt_einsum', 'torch.nn.parameter', 'torch.nn.modules.module', 'torch._VF', 'torch._torch_docs', 'torch._C._distributed_c10d', 'torch.distributed.constants', 'torch.distributed.rendezvous', 'torch.distributed.distributed_c10d', 'torch.distributed.remote_device', 'torch.distributed', 'torch._C._distributed_rpc', 'torch.futures', 'torch.distributed.rpc.internal', 'torch.distributed.rpc.constants', 'torch.distributed.rpc._utils', 'torch.distributed.rpc.api', 'torch.distributed.rpc.backend_registry', 'torch.distributed.rpc.functions', 'torch._C._distributed_autograd', 'torch.distributed.autograd', 'torch.distributed.rpc.options', 'torch.autograd.variable', 'torch.autograd.function', 'cmath', 'torch.testing._comparison', 'torch.testing._creation', 'torch.testing._legacy', 'torch.testing._deprecated', 'torch.testing', 'torch.utils._pytree', 'torch._vmap_internals', 'torch.autograd.gradcheck', 'torch.autograd.grad_mode', 'torch.autograd.anomaly_mode', 'torch.autograd.forward_ad', 'torch.autograd.functional', 'torch.autograd.graph', 'torch._C._autograd', 'torch.autograd.profiler_util', 'torch.autograd.profiler', 'torch.autograd', 'torch.autograd.profiler_legacy', 'torch.distributed.rpc.server_process_global_profiler', 'torch.distributed.rpc', 'pickletools', 'torch.package._digraph', 'torch.package._importlib', 'torch.package._mangling', 'torch.package.importer', 'torch.package._package_pickler', 'torch.package._stdlib', 'torch.package.find_file_dependencies', 'torch.package.glob_group', 'torch.package.package_exporter', 'torch.package.analyze.find_first_use_of_broken_modules', 'torch.package.analyze.trace_dependencies', 'torch.package.analyze', 'torch.package.analyze.is_from_package', 'torch.package.file_structure_representation', 'torch.package._directory_reader', 'torch.package._package_unpickler', 'torch.package.package_importer', 'torch.package', 'torch._jit_internal', 'torch.nn._reduction', 'torch.nn.modules.utils', 'torch.nn.grad', 'torch.nn.functional', 'torch.nn.init', 'torch.nn.modules.lazy', 'torch.nn.modules.linear', 'torch.nn.common_types', 'torch.nn.modules.conv', 'torch.nn.modules.activation', 'torch.nn.modules.distance', 'torch.nn.modules.loss', 'torch.nn.modules.container', 'torch.nn.modules.pooling', 'torch.nn.modules._functions', 'torch.nn.modules.batchnorm', 'torch.nn.modules.instancenorm', 'torch.nn.modules.normalization', 'torch.nn.modules.dropout', 'torch.nn.modules.padding', 'torch.nn.modules.sparse', 'torch.nn.utils.rnn', 'torch.nn.utils.clip_grad', 'torch.nn.utils.weight_norm', 'torch.nn.utils.convert_parameters', 'torch.nn.utils.spectral_norm', 'torch.nn.utils.fusion', 'torch.nn.utils.memory_format', 'torch.nn.utils.parametrize', 'torch.nn.utils.parametrizations', 'torch.nn.utils.init', 'torch.nn.utils.stateless', 'torch.nn.utils', 'torch.nn.modules.rnn', 'torch.nn.modules.pixelshuffle', 'torch.nn.modules.upsampling', 'torch.nn.modules.fold', 'torch.nn.modules.adaptive', 'torch.nn.modules.transformer', 'torch.nn.modules.flatten', 'torch.nn.modules.channelshuffle', 'torch.nn.modules', 'torch.nn.parallel.parallel_apply', 'torch.cuda.nccl', 'torch.nn.parallel.comm', 'torch.nn.parallel.replicate', 'torch.nn.parallel._functions', 'torch.nn.parallel.scatter_gather', 'torch.nn.parallel.data_parallel', 'torch.distributed.algorithms.join', 'torch.distributed.algorithms', 'torch.distributed.utils', 'torch.nn.parallel._replicated_tensor_ddp_utils', 'torch.nn.parallel.distributed', 'torch.nn.parallel', 'torch.nn', 'torch._linalg_utils', 'torch._lowrank', 'torch.functional', 'torch.cpu.amp.autocast_mode', 'torch.cpu.amp', 'torch.cpu', 'torch.fft', 'torch.nested', 'torch.optim.optimizer', 'torch.optim.adadelta', 'torch.optim.adagrad', 'torch.optim.adam', 'torch.optim.adamw', 'torch.optim.adamax', 'torch.optim.asgd', 'torch.optim.nadam', 'torch.optim.radam', 'torch.optim.rmsprop', 'torch.optim.rprop', 'torch.optim.sgd', 'torch.optim._functional', 'torch.optim.sparse_adam', 'torch.optim.lbfgs', 'torch.optim.lr_scheduler', 'torch.optim.swa_utils', 'torch.optim', 'torch.optim._multi_tensor', 'multiprocessing.process', 'multiprocessing.reduction', 'multiprocessing.context', '__mp_main__', 'multiprocessing', 'multiprocessing.util', 'multiprocessing.resource_sharer', 'torch.multiprocessing.reductions', '_multiprocessing', 'multiprocessing.connection', 'torch.multiprocessing.spawn', 'torch.multiprocessing', 'torch.special', 'torch.utils.backcompat', 'torch.onnx._deprecation', 'torch.onnx._constants', 'torch.onnx._internal', 'torch.onnx._internal.diagnostics.infra.sarif._property_bag', 'torch.onnx._internal.diagnostics.infra.sarif._address', 'torch.onnx._internal.diagnostics.infra.sarif._multiformat_message_string', 'torch.onnx._internal.diagnostics.infra.sarif._artifact_content', 'torch.onnx._internal.diagnostics.infra.sarif._message', 'torch.onnx._internal.diagnostics.infra.sarif._artifact_location', 'torch.onnx._internal.diagnostics.infra.sarif._artifact', 'torch.onnx._internal.diagnostics.infra.sarif._region', 'torch.onnx._internal.diagnostics.infra.sarif._replacement', 'torch.onnx._internal.diagnostics.infra.sarif._artifact_change', 'torch.onnx._internal.diagnostics.infra.sarif._rectangle', 'torch.onnx._internal.diagnostics.infra.sarif._attachment', 'torch.onnx._internal.diagnostics.infra.sarif._location_relationship', 'torch.onnx._internal.diagnostics.infra.sarif._logical_location', 'torch.onnx._internal.diagnostics.infra.sarif._physical_location', 'torch.onnx._internal.diagnostics.infra.sarif._location', 'torch.onnx._internal.diagnostics.infra.sarif._tool_component_reference', 'torch.onnx._internal.diagnostics.infra.sarif._reporting_descriptor_reference', 'torch.onnx._internal.diagnostics.infra.sarif._stack_frame', 'torch.onnx._internal.diagnostics.infra.sarif._stack', 'torch.onnx._internal.diagnostics.infra.sarif._web_request', 'torch.onnx._internal.diagnostics.infra.sarif._web_response', 'torch.onnx._internal.diagnostics.infra.sarif._thread_flow_location', 'torch.onnx._internal.diagnostics.infra.sarif._thread_flow', 'torch.onnx._internal.diagnostics.infra.sarif._code_flow', 'torch.onnx._internal.diagnostics.infra.sarif._reporting_configuration', 'torch.onnx._internal.diagnostics.infra.sarif._configuration_override', 'torch.onnx._internal.diagnostics.infra.sarif._exception', 'torch.onnx._internal.diagnostics.infra.sarif._notification', 'torch.onnx._internal.diagnostics.infra.sarif._invocation', 'torch.onnx._internal.diagnostics.infra.sarif._reporting_descriptor_relationship', 'torch.onnx._internal.diagnostics.infra.sarif._reporting_descriptor', 'torch.onnx._internal.diagnostics.infra.sarif._translation_metadata', 'torch.onnx._internal.diagnostics.infra.sarif._tool_component', 'torch.onnx._internal.diagnostics.infra.sarif._tool', 'torch.onnx._internal.diagnostics.infra.sarif._conversion', 'torch.onnx._internal.diagnostics.infra.sarif._edge', 'torch.onnx._internal.diagnostics.infra.sarif._edge_traversal', 'torch.onnx._internal.diagnostics.infra.sarif._node', 'torch.onnx._internal.diagnostics.infra.sarif._graph', 'torch.onnx._internal.diagnostics.infra.sarif._fix', 'torch.onnx._internal.diagnostics.infra.sarif._graph_traversal', 'torch.onnx._internal.diagnostics.infra.sarif._result_provenance', 'torch.onnx._internal.diagnostics.infra.sarif._suppression', 'torch.onnx._internal.diagnostics.infra.sarif._result', 'torch.onnx._internal.diagnostics.infra.sarif._external_properties', 'torch.onnx._internal.diagnostics.infra.sarif._external_property_file_reference', 'torch.onnx._internal.diagnostics.infra.sarif._external_property_file_references', 'torch.onnx._internal.diagnostics.infra.sarif._run_automation_details', 'torch.onnx._internal.diagnostics.infra.sarif._special_locations', 'torch.onnx._internal.diagnostics.infra.sarif._version_control_details', 'torch.onnx._internal.diagnostics.infra.sarif._run', 'torch.onnx._internal.diagnostics.infra.sarif._sarif_log', 'torch.onnx._internal.diagnostics.infra.sarif', 'torch.onnx._internal.diagnostics.infra.formatter', 'torch.onnx._internal.diagnostics.infra._infra', 'torch.onnx._internal.diagnostics.infra.sarif.version', 'torch.onnx._internal.diagnostics.infra.engine', 'torch.onnx._internal.diagnostics.infra', 'torch.onnx._internal.diagnostics._rules', 'torch.onnx._internal.diagnostics._diagnostic', 'torch.onnx._internal.diagnostics', 'torch.onnx.errors', 'torch.jit._dataclass_impls', 'torch.jit._monkeytype_config', 'torch.jit._state', 'torch._ops', 'torch.jit.annotations', 'torch.jit.frontend', 'torch.backends.cudnn', 'torch.jit._builtins', 'torch.jit._check', 'torch.jit._recursive', 'torch.jit._fuser', 'torch.jit._serialization', 'torch._classes', 'torch.jit._script', 'torch.jit._trace', 'torch.jit._async', 'torch.jit._decomposition_utils', 'torch.jit._freeze', 'torch.jit._ir_utils', 'torch.jit', 'torch.onnx._exporter_states', 'torch.onnx._globals', 'torch.onnx._internal._beartype', 'torch.onnx._internal.jit_utils', 'torch.onnx._internal.registration', 'torch.onnx.utils', 'torch.onnx._patch_torch', 'torch.onnx._type_utils', 'torch.onnx.symbolic_helper', 'torch.onnx.symbolic_opset9', 'torch.onnx.symbolic_caffe2', 'torch.onnx.symbolic_opset7', 'torch.onnx.symbolic_opset8', 'torch.onnx.symbolic_opset10', 'torch.onnx.symbolic_opset11', 'torch.onnx.symbolic_opset12', 'torch.onnx.symbolic_opset13', 'torch.onnx.symbolic_opset14', 'torch.onnx.symbolic_opset15', 'torch.onnx.symbolic_opset16', 'torch.onnx.symbolic_opset17', 'torch.onnx', 'torch.linalg', 'tqdm._monitor', 'tqdm._tqdm_pandas', 'tqdm.utils', 'tqdm.std', 'tqdm._dist_ver', 'tqdm.version', 'tqdm.cli', 'tqdm.gui', 'tqdm', 'ipywidgets._version', 'ipython_genutils._version', 'ipython_genutils', 'ipython_genutils.encoding', 'ipython_genutils.py3compat', 'ipywidgets.widgets.widget', 'ipywidgets.widgets.util', 'ipywidgets.widgets.trait_types', 'ipywidgets.widgets.widget_layout', 'ipywidgets.widgets.widget_style', 'ipywidgets.widgets.domwidget', 'ipywidgets.widgets.valuewidget', 'ipywidgets.widgets.widget_core', 'ipywidgets.widgets.widget_description', 'ipywidgets.widgets.widget_bool', 'ipywidgets.widgets.widget_button', 'ipywidgets.widgets.docutils', 'ipywidgets.widgets.widget_box', 'ipywidgets.widgets.widget_int', 'ipywidgets.widgets.widget_float', 'ipywidgets.widgets.widget_color', 'ipywidgets.widgets.widget_date', 'ipywidgets.widgets.widget_output', 'ipywidgets.widgets.widget_selection', 'ipywidgets.widgets.widget_selectioncontainer', 'ipywidgets.widgets.widget_string', 'ipywidgets.widgets.widget_controller', 'ipywidgets.widgets.interaction', 'ipywidgets.widgets.widget_link', 'ipywidgets.widgets.widget_media', 'ipywidgets.widgets.widget_templates', 'ipywidgets.widgets.widget_upload', 'ipywidgets.widgets', 'ipywidgets', 'tqdm.notebook', 'tqdm.autonotebook', 'tqdm.asyncio', 'tqdm.auto', 'torch.hub', 'torch.distributions.constraints', 'torch.distributions.utils', 'torch.distributions.distribution', 'torch.distributions.exp_family', 'torch.distributions.bernoulli', 'torch.distributions.dirichlet', 'torch.distributions.beta', 'torch.distributions.binomial', 'torch.distributions.categorical', 'torch.distributions.cauchy', 'torch.distributions.gamma', 'torch.distributions.chi2', 'torch.distributions.transforms', 'torch.distributions.constraint_registry', 'torch.distributions.continuous_bernoulli', 'torch.distributions.exponential', 'torch.distributions.fishersnedecor', 'torch.distributions.geometric', 'torch.distributions.uniform', 'torch.distributions.independent', 'torch.distributions.transformed_distribution', 'torch.distributions.gumbel', 'torch.distributions.half_cauchy', 'torch.distributions.normal', 'torch.distributions.half_normal', 'torch.distributions.laplace', 'torch.distributions.multivariate_normal', 'torch.distributions.lowrank_multivariate_normal', 'torch.distributions.one_hot_categorical', 'torch.distributions.pareto', 'torch.distributions.poisson', 'torch.distributions.kl', 'torch.distributions.kumaraswamy', 'torch.distributions.lkj_cholesky', 'torch.distributions.log_normal', 'torch.distributions.logistic_normal', 'torch.distributions.mixture_same_family', 'torch.distributions.multinomial', 'torch.distributions.negative_binomial', 'torch.distributions.relaxed_bernoulli', 'torch.distributions.relaxed_categorical', 'torch.distributions.studentT', 'torch.distributions.von_mises', 'torch.distributions.weibull', 'torch.distributions.wishart', 'torch.distributions', 'torch.backends.cuda', 'torch.backends.mps', 'torch.backends.mkl', 'torch.backends.mkldnn', 'torch.backends.openmp', 'torch.backends.quantized', 'torch.utils.data.sampler', 'torch.utils.data.dataset', 'torch.utils.data.datapipes._hook_iterator', 'torch.utils.data.datapipes._typing', 'torch.utils.data.datapipes.utils', 'torch.utils.data._utils.signal_handling', 'torch.utils.data._utils.worker', 'torch.utils.data._utils.pin_memory', 'torch.utils.data._utils.collate', 'torch.utils.data._utils.fetch', 'torch.utils.data._utils', 'dill.info', '_pyio', 'dill.settings', 'dill._dill', 'dill.source', 'dill.temp', 'dill.pointers', 'dill.detect', 'dill.objtypes', 'dill', 'torch.utils.data._utils.serialization', 'torch.utils.data.datapipes.utils.common', 'torch.utils.data.datapipes.datapipe', 'torch.utils.data.datapipes.iter.utils', 'torch.utils.data.datapipes._decorator', 'torch.utils.data.datapipes.dataframe.dataframe_wrapper', 'torch.utils.data.datapipes.dataframe.structures', 'torch.utils.data.datapipes.dataframe.dataframes', 'torch.utils.data.datapipes.dataframe.datapipes', 'torch.utils.data.datapipes.dataframe', 'torch.utils.data.datapipes.iter.callable', 'torch.utils.data.datapipes.iter.combinatorics', 'torch.utils.data.datapipes.iter.combining', 'torch.utils.data.datapipes.iter.filelister', 'torch.utils.data.datapipes.iter.fileopener', 'torch.utils.data.datapipes.iter.grouping', 'torch.utils.data.datapipes.utils.decoder', 'torch.utils.data.datapipes.iter.routeddecoder', 'torch.utils.data.datapipes.iter.selecting', 'torch.utils.data.datapipes.iter.streamreader', 'torch.utils.data.datapipes.iter', 'torch.utils.data.datapipes.map.callable', 'torch.utils.data.datapipes.map.combinatorics', 'torch.utils.data.datapipes.map.combining', 'torch.utils.data.datapipes.map.grouping', 'torch.utils.data.datapipes.map.utils', 'torch.utils.data.datapipes.map', 'torch.utils.data.datapipes', 'torch.utils.data.graph', 'torch.utils.data.graph_settings', 'torch.utils.data.dataloader', 'torch.utils.data.distributed', 'torch.utils.data.backward_compatibility', 'torch.utils.data.communication.eventloop', 'torch.utils.data.communication.iter', 'torch.utils.data.communication.map', 'torch.utils.data.communication.messages', 'torch.utils.data.communication.protocol', 'torch.utils.data.communication.queue', 'torch.utils.data.communication', 'torch.utils.data.dataloader_experimental', 'torch.utils.data', 'torch.__config__', 'torch.__future__', 'torch.profiler.profiler', 'torch.profiler.itt', 'torch.profiler', 'torch.ao', 'torch.ao.nn', 'torch.ao.nn.intrinsic.modules.fused', 'torch.ao.nn.intrinsic.modules', 'torch.ao.nn.intrinsic', 'torch.nn.intrinsic.modules.fused', 'torch.nn.intrinsic.modules', 'torch.nn.intrinsic', 'torch.ao.nn.quantizable.modules.activation', 'torch.ao.nn.quantizable.modules.rnn', 'torch.ao.nn.quantizable.modules', 'torch.ao.nn.quantizable', 'torch.nn.quantizable.modules', 'torch.nn.quantizable', 'torch.ao.nn.quantized.modules.activation', 'torch.ao.nn.quantized.modules.dropout', 'torch.ao.nn.quantized.modules.batchnorm', 'torch.ao.nn.quantized.modules.normalization', 'torch.ao.nn.qat.modules.linear', 'torch.ao.nn.qat.modules.conv', 'torch.ao.nn.qat.modules.embedding_ops', 'torch.ao.nn.qat.modules', 'torch.ao.nn.qat', 'torch.nn.intrinsic.qat.modules.linear_relu', 'torch.nn.intrinsic.qat.modules.linear_fused', 'torch.nn.intrinsic.qat.modules.conv_fused', 'torch.nn.intrinsic.qat.modules', 'torch.nn.intrinsic.qat', 'torch.ao.nn.quantized.modules.utils', 'torch.ao.nn.quantized.modules.conv', 'torch.ao.nn.quantized.modules.linear', 'torch.ao.nn.quantized.modules.embedding_ops', 'torch.ao.nn.quantized.modules.rnn', 'torch.ao.nn.quantized.modules.functional_modules', 'torch.ao.nn.quantized.modules', 'torch.ao.nn.quantized.functional', 'torch.ao.nn.quantized', 'torch.ao.nn.quantized.dynamic.modules.linear', 'torch.ao.nn.quantized.dynamic.modules.rnn', 'torch.ao.nn.quantized.dynamic.modules.conv', 'torch.ao.nn.quantized.dynamic.modules', 'torch.ao.nn.quantized.dynamic', 'torch.nn.quantized.dynamic', 'torch.nn.quantized.functional', 'torch.nn.quantized.modules', 'torch.nn.quantized', 'torch.ao.nn.qat.dynamic.modules.linear', 'torch.ao.nn.qat.dynamic.modules', 'torch.ao.nn.qat.dynamic', 'torch.nn.qat.dynamic.modules.linear', 'torch.nn.qat.dynamic.modules', 'torch.nn.qat.dynamic', 'torch.nn.qat.modules.conv', 'torch.nn.qat.modules.embedding_ops', 'torch.nn.qat.modules.linear', 'torch.nn.qat.modules', 'torch.nn.qat', 'torch._tensor_docs', 'torch._storage_docs', 'torch.ao.quantization.quant_type', 'torch.ao.quantization.utils', 'torch.ao.quantization.observer', 'torch.ao.quantization.fake_quantize', 'torch.ao.quantization.fuser_method_mappings', 'torch.ao.quantization.fuse_modules', 'torch.ao.quantization.qconfig', 'torch.ao.quantization.qconfig_mapping', 'torch.nn.intrinsic.quantized.modules.linear_relu', 'torch.nn.intrinsic.quantized.modules.conv_relu', 'torch.nn.intrinsic.quantized.modules.bn_relu', 'torch.nn.intrinsic.quantized.modules', 'torch.nn.intrinsic.quantized', 'torch.nn.intrinsic.quantized.dynamic.modules.linear_relu', 'torch.nn.intrinsic.quantized.dynamic.modules', 'torch.nn.intrinsic.quantized.dynamic', 'torch.ao.nn.quantized.reference.modules.utils', 'torch.ao.nn.quantized.reference.modules.linear', 'torch.ao.nn.quantized.reference.modules.conv', 'torch.ao.nn.quantized.reference.modules.rnn', 'torch.ao.nn.quantized.reference.modules.sparse', 'torch.ao.nn.quantized.reference.modules', 'torch.ao.nn.quantized.reference', 'torch.ao.nn.sparse.quantized.linear', 'torch.ao.nn.sparse.quantized.utils', 'torch.ao.nn.sparse.quantized.dynamic.linear', 'torch.ao.nn.sparse.quantized.dynamic', 'torch.ao.nn.sparse.quantized', 'torch.ao.nn.sparse', 'torch.ao.quantization.stubs', 'torch.ao.quantization.quantization_mappings', 'torch.ao.quantization.quantize', 'torch.ao.quantization.quantize_jit', 'torch.ao.quantization', 'torch.quantization.quantize', 'torch.quantization.observer', 'torch.quantization.qconfig', 'torch.quantization.fake_quantize', 'torch.quantization.fuser_method_mappings', 'torch.quantization.fuse_modules', 'torch.quantization.stubs', 'torch.quantization.quant_type', 'torch.quantization.quantize_jit', 'torch.quantization.quantization_mappings', 'torch.quantization', 'torch.quasirandom', 'torch.multiprocessing._atfork', 'torch._lobpcg', 'torch.utils.dlpack', 'torch.masked.maskedtensor.core', 'torch.masked.maskedtensor.binary', 'torch.masked.maskedtensor.passthrough', 'torch.masked.maskedtensor.creation', 'torch.masked.maskedtensor.reductions', 'torch.masked.maskedtensor.unary', 'torch.masked.maskedtensor', 'torch.masked._docs', 'torch.masked._ops', 'torch.masked', 'torch.return_types', 'torch.library', 'torch._prims_common', 'torch._prims_common.wrappers', 'torch._prims.nvfuser_prims', 'torch.utils._mode_utils', 'torch._subclasses.meta_utils', 'torch.fx._compatibility', 'torch.fx.immutable_collections', 'torch.fx.operator_schemas', 'torch.fx.node', 'torch.fx._pytree', 'torch.fx.graph', 'torch.fx.graph_module', 'torch.fx.traceback', 'torch.fx.proxy', 'torch.fx._symbolic_trace', 'torch.fx.interpreter', 'torch.fx.subgraph_rewriter', 'torch.fx', 'torch.utils._python_dispatch', 'torch._subclasses.fake_tensor', 'torch._subclasses.fake_utils', 'torch._subclasses', 'torch._prims', 'torch._decomp.decompositions', 'torch._decomp', 'torch._refs.fft', 'torch._refs.linalg', 'torch._refs.nn', 'torch._refs.nn.functional', 'torch._refs.special', 'torch._refs', 'torch._meta_registrations', 'torch', 'torchbeast', 'torchbeast.core', 'torchbeast.core.environment', 'gym.error', 'gym.version', 'gym.utils.colorize', 'gym.utils.ezpickle', 'gym.utils', 'gym.utils.closer', 'gym.core', 'gym.utils.seeding', 'gym.spaces.space', 'gym.logger', 'gym.spaces.box', 'gym.spaces.discrete', 'gym.spaces.multi_discrete', 'gym.spaces.multi_binary', 'gym.spaces.tuple', 'gym.spaces.dict', 'gym.spaces.utils', 'gym.spaces', '_csv', 'csv', 'importlib.metadata', 'gym.envs.registration', 'ale_py._ale_py', 'ale_py', 'zipp', 'importlib_resources._compat', 'importlib_resources.abc', 'importlib_resources._common', 'importlib_resources._legacy', 'importlib_resources', 'importlib_metadata._functools', 'importlib_metadata._text', 'importlib_metadata._adapters', 'importlib_metadata._compat', 'importlib_metadata._meta', 'importlib_metadata._collections', 'importlib_metadata._itertools', 'importlib_metadata', 'ale_py.roms.utils', 'importlib_resources._adapters', 'importlib_resources._itertools', 'importlib_resources.readers', 'urllib3.packages', 'urllib3.packages.six', 'urllib3.packages.six.moves', 'urllib3.packages.six.moves.http_client', 'urllib3.exceptions', 'urllib3._version', 'urllib3.contrib', 'urllib3.contrib._appengine_environ', 'urllib3.util.wait', 'urllib3.util.connection', '_cffi_backend', '_brotli.lib', '_brotli', 'brotli._brotli', 'brotli.brotli', 'brotli', 'urllib3.util.request', 'urllib3.util.response', 'urllib3.util.retry', 'urllib3.util.url', 'urllib3.util.ssltransport', 'urllib3.util.ssl_', 'urllib3.util.timeout', 'urllib3.util', 'urllib3.util.proxy', 'urllib3._collections', 'ipaddress', 'urllib3.util.ssl_match_hostname', 'urllib3.connection', 'urllib3.fields', 'urllib3.filepost', 'urllib3.packages.six.moves.urllib', 'urllib3.packages.six.moves.urllib.parse', 'urllib3.request', 'urllib3.response', 'urllib3.util.queue', 'urllib3.connectionpool', 'urllib3.poolmanager', 'urllib3', 'chardet.enums', 'chardet.charsetprober', 'chardet.charsetgroupprober', 'chardet.codingstatemachine', 'chardet.escsm', 'chardet.escprober', 'chardet.latin1prober', 'chardet.mbcssm', 'chardet.utf8prober', 'chardet.mbcharsetprober', 'chardet.euctwfreq', 'chardet.euckrfreq', 'chardet.gb2312freq', 'chardet.big5freq', 'chardet.jisfreq', 'chardet.chardistribution', 'chardet.jpcntx', 'chardet.sjisprober', 'chardet.eucjpprober', 'chardet.gb2312prober', 'chardet.euckrprober', 'chardet.cp949prober', 'chardet.big5prober', 'chardet.euctwprober', 'chardet.mbcsgroupprober', 'chardet.hebrewprober', 'chardet.sbcharsetprober', 'chardet.langbulgarianmodel', 'chardet.langgreekmodel', 'chardet.langhebrewmodel', 'chardet.langrussianmodel', 'chardet.langthaimodel', 'chardet.langturkishmodel', 'chardet.sbcsgroupprober', 'chardet.universaldetector', 'chardet.version', 'chardet', 'http.cookiejar', 'http.cookies', 'requests.compat', 'requests.exceptions', 'charset_normalizer.constant', '_multibytecodec', 'charset_normalizer.utils', 'charset_normalizer.md', 'charset_normalizer.models', 'charset_normalizer.assets', 'charset_normalizer.cd', 'charset_normalizer.api', 'charset_normalizer.legacy', 'charset_normalizer.version', 'charset_normalizer', 'requests.packages.urllib3.packages', 'requests.packages.urllib3.packages.six', 'requests.packages.urllib3.packages.six.moves', 'requests.packages.urllib3.packages.six.moves.http_client', 'requests.packages.urllib3.exceptions', 'requests.packages.urllib3._version', 'requests.packages.urllib3.contrib', 'requests.packages.urllib3.contrib._appengine_environ', 'requests.packages.urllib3.util.wait', 'requests.packages.urllib3.util.connection', 'requests.packages.urllib3.util.request', 'requests.packages.urllib3.util.response', 'requests.packages.urllib3.util.retry', 'requests.packages.urllib3.util.url', 'requests.packages.urllib3.util.ssltransport', 'requests.packages.urllib3.util.ssl_', 'requests.packages.urllib3.util.timeout', 'requests.packages.urllib3.util', 'requests.packages.urllib3.util.proxy', 'requests.packages.urllib3._collections', 'requests.packages.urllib3.util.ssl_match_hostname', 'requests.packages.urllib3.connection', 'requests.packages.urllib3.fields', 'requests.packages.urllib3.filepost', 'requests.packages.urllib3.packages.six.moves.urllib', 'requests.packages.urllib3.packages.six.moves.urllib.parse', 'requests.packages.urllib3.request', 'requests.packages.urllib3.response', 'requests.packages.urllib3.util.queue', 'requests.packages.urllib3.connectionpool', 'requests.packages.urllib3.poolmanager', 'requests.packages.urllib3', 'idna.package_data', 'idna.idnadata', 'idna.intranges', 'idna.core', 'idna', 'requests.packages.idna.package_data', 'requests.packages.idna.idnadata', 'requests.packages.idna.intranges', 'requests.packages.idna.core', 'requests.packages.idna', 'requests.packages.chardet', 'requests.packages', 'certifi.core', 'certifi', 'requests.certs', 'requests.__version__', 'requests._internal_utils', 'requests.cookies', 'requests.structures', 'requests.utils', 'requests.auth', 'requests.hooks', 'requests.status_codes', 'requests.models', 'socks', 'urllib3.contrib.socks', 'requests.adapters', 'requests.sessions', 'requests.api', 'requests', 'click._compat', 'click.globals', 'click.utils', 'click.exceptions', 'click.types', 'click._unicodefun', 'click.parser', 'click.formatting', 'click.termui', 'click.core', 'click.decorators', 'click', 'AutoROM.AutoROM', 'AutoROM', 'AutoROM.roms', 'ale_py.roms', 'ale_py.gym', 'gym.envs', 'gym.vector.utils.misc', 'gym.vector.utils.spaces', 'gym.vector.utils.numpy_utils', 'gym.vector.utils.shared_memory', 'gym.vector.utils', 'gym.vector.vector_env', 'gym.vector.async_vector_env', 'gym.vector.sync_vector_env', 'gym.vector', 'gym.wrappers.monitoring', 'gym.utils.atomic_write', 'gym.utils.json_utils', 'gym.wrappers.monitoring.stats_recorder', 'setuptools._distutils', 'distutils.debug', 'distutils.errors', 'distutils.fancy_getopt', 'distutils.dep_util', 'distutils.log', 'distutils.spawn', 'distutils.util', 'distutils.dist', 'distutils.dir_util', 'distutils.file_util', 'distutils.archive_util', 'distutils.cmd', 'distutils.config', 'distutils.extension', 'distutils.core', '_distutils_hack.override', 'setuptools._deprecation_warning', 'setuptools.version', 'distutils.filelist', 'setuptools.monkey', 'setuptools.extension', 'distutils.command', 'setuptools.extern', 'setuptools._vendor', 'setuptools._vendor.packaging.__about__', 'setuptools._vendor.packaging', 'setuptools.extern.packaging', 'setuptools._vendor.ordered_set', 'setuptools.extern.ordered_set', 'setuptools._vendor.more_itertools.recipes', 'setuptools._vendor.more_itertools.more', 'setuptools._vendor.more_itertools', 'setuptools.extern.more_itertools', 'setuptools._vendor.zipp', 'setuptools._vendor.importlib_metadata._functools', 'setuptools._vendor.importlib_metadata._text', 'setuptools._vendor.importlib_metadata._adapters', 'setuptools._vendor.importlib_metadata._compat', 'setuptools._vendor.importlib_metadata._meta', 'setuptools._vendor.importlib_metadata._collections', 'setuptools._vendor.importlib_metadata._itertools', 'setuptools._vendor.importlib_metadata', 'setuptools.extern.importlib_metadata', 'setuptools._importlib', 'distutils.command.bdist', 'setuptools.command', 'setuptools.windows_support', 'setuptools.extern.packaging._structures', 'setuptools.extern.packaging.version', 'setuptools._vendor.packaging._manylinux', 'setuptools._vendor.packaging._musllinux', 'setuptools.extern.packaging.tags', 'setuptools.extern.packaging.utils', 'setuptools.extern.packaging.specifiers', 'setuptools.config.expand', 'setuptools.config.setupcfg', 'setuptools.config', 'setuptools.errors', 'email._header_value_parser', 'email.headerregistry', 'setuptools.config._apply_pyprojecttoml', 'setuptools.config.pyprojecttoml', 'setuptools.discovery', 'setuptools._vendor.jaraco', 'setuptools.extern.jaraco', 'setuptools.extern.jaraco.functools', 'setuptools.extern.jaraco.context', 'setuptools.extern.jaraco.text', 'setuptools._reqs', 'setuptools._itertools', 'setuptools._entry_points', 'setuptools.dist', 'setuptools.py34compat', 'setuptools._imp', 'setuptools.depends', 'setuptools.logging', 'distutils.ccompiler', 'setuptools.msvc', 'setuptools', 'distutils', 'distutils.version', 'gym.wrappers.monitoring.video_recorder', 'gym.wrappers.monitor', 'gym.wrappers.time_limit', 'gym.wrappers.filter_observation', 'cv2.load_config_py3', 'cv2.version', 'cv2.Error', 'cv2.cuda', 'cv2.detail', 'cv2.dnn', 'cv2.fisheye', 'cv2.flann', 'cv2.gapi.core', 'cv2.gapi.core.cpu', 'cv2.gapi.core.fluid', 'cv2.gapi.core.ocl', 'cv2.gapi.ie', 'cv2.gapi.ie.detail', 'cv2.gapi.oak', 'cv2.gapi.onnx', 'cv2.gapi.own', 'cv2.gapi.own.detail', 'cv2.gapi.render', 'cv2.gapi.render.ocv', 'cv2.gapi.streaming', 'cv2.gapi.video', 'cv2.gapi.wip', 'cv2.gapi.wip.draw', 'cv2.gapi.wip.gst', 'cv2.gapi.wip.onevpl', 'cv2.ipp', 'cv2.ml', 'cv2.ocl', 'cv2.ogl', 'cv2.parallel', 'cv2.samples', 'cv2.segmentation', 'cv2.utils.fs', 'cv2.utils.nested', 'cv2.videoio_registry', 'cv2.qt', 'cv2.misc.version', 'cv2.misc', 'cv2.gapi', 'cv2.data', 'cv2.utils', 'cv2.mat_wrapper', 'cv2', 'gym.wrappers.atari_preprocessing', 'gym.wrappers.time_aware_observation', 'gym.wrappers.rescale_action', 'gym.wrappers.flatten_observation', 'gym.wrappers.gray_scale_observation', 'gym.wrappers.frame_stack', 'gym.wrappers.transform_observation', 'gym.wrappers.transform_reward', 'gym.wrappers.resize_observation', 'gym.wrappers.clip_action', 'gym.wrappers.record_episode_statistics', 'gym.wrappers.normalize', 'gym.wrappers.record_video', 'gym.wrappers', 'gym', 'torchbeast.atari_wrappers', 'torchbeast.core.file_writer', 'torchbeast.core.prof', 'torchbeast.core.vtrace', 'torchbeast.transformer_rnn', 'torchbeast.resnet', 'gym_sokoban', 'torchbeast.train', 'torchbeast.base', 'matplotlib', 'matplotlib._api.deprecation', 'matplotlib._api', 'matplotlib._version', 'matplotlib._c_internal_utils', 'matplotlib.cbook', 'matplotlib.docstring', 'PIL._version', 'PIL', 'defusedxml.common', 'defusedxml', 'xml.etree', 'xml.etree.ElementPath', '_elementtree', 'xml.etree.ElementTree', 'defusedxml.ElementTree', 'PIL.ImageMode', 'PIL.TiffTags', 'PIL._binary', 'PIL._deprecate', 'PIL._util', 'PIL._imaging', 'cffi.lock', 'cffi.error', 'cffi.model', 'cffi.api', 'cffi', 'PIL.Image', 'PIL.ImageChops', 'PIL.ImageFile', 'PIL.GimpGradientFile', 'PIL.GimpPaletteFile', 'PIL.ImageColor', 'PIL.PaletteFile', 'PIL.ImagePalette', 'PIL.ImageSequence', 'PIL.PngImagePlugin', 'matplotlib._path', 'matplotlib.bezier', 'matplotlib.path', 'matplotlib.transforms', 'matplotlib.ticker', 'matplotlib.scale', 'matplotlib._color_data', 'matplotlib.colors', 'pyparsing.util', 'pyparsing.unicode', 'pyparsing.exceptions', 'pyparsing.actions', 'pyparsing.results', 'pyparsing.core', 'pyparsing.helpers', 'pyparsing.testing', 'pyparsing.common', 'pyparsing', 'matplotlib.fontconfig_pattern', 'matplotlib._enums', 'cycler', 'matplotlib.rcsetup', 'matplotlib.ft2font', 'kiwisolver._cext', 'kiwisolver']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG:58738 __init__:275 2022-11-02 22:00:51,644] CACHEDIR=/home/sc/.cache/matplotlib\n",
      "[DEBUG:58738 font_manager:1439 2022-11-02 22:00:51,647] Using fontManager instance from /home/sc/.cache/matplotlib/fontlist-v330.json\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import threading\n",
    "import time\n",
    "import timeit\n",
    "import traceback\n",
    "import typing\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"  # Necessary for multithreading.\n",
    "\n",
    "import torch\n",
    "from torch import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Manager\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchbeast.core.environment import Environment, Vec_Environment\n",
    "from torchbeast.atari_wrappers import SokobanWrapper\n",
    "from torchbeast.base import BaseNet\n",
    "from torchbeast.train import create_env\n",
    "\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import numpy as np\n",
    "import math\n",
    "import logging\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "def get_param(net, name=None):\n",
    "    keys = []\n",
    "    for (k, v) in actor_wrapper.model.named_parameters(): \n",
    "        if name is None:\n",
    "            print(k)\n",
    "        else:\n",
    "            if name == k: return v\n",
    "        keys.append(k)\n",
    "    return keys        \n",
    "\n",
    "def n_step_greedy(env, net, n):    \n",
    "    if isinstance(env, Vec_Environment):\n",
    "        num_actions = env.gym_env.action_space[0].n\n",
    "        bsz = len(env.gym_env.envs)\n",
    "    else:\n",
    "        num_actions = env.gym_env.action_space.n\n",
    "        bsz = 1\n",
    "\n",
    "    temp = 10.\n",
    "    q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "    state = env.clone_state()\n",
    "\n",
    "    for act in range(num_actions):\n",
    "        obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "        \n",
    "        if n > 1:\n",
    "            action, prob, sub_q_ret = n_step_greedy(env, net, n-1)\n",
    "            ret = obs['reward'] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] * (~obs['done']).float()\n",
    "        else:\n",
    "            ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "\n",
    "        q_ret[:, act] = ret\n",
    "        env.restore_state(state)\n",
    "    \n",
    "    prob = F.softmax(temp*q_ret, dim=1)\n",
    "    action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893df32e",
   "metadata": {},
   "source": [
    "<font size=\"5\">Testing planning algo. for perfect model with bootstrapped values</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51bf8c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Synchronous version of testing \n",
    "\n",
    "def test_n_step(n, net, env):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    returns = []\n",
    "    obs = env.initial()\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(device) for k, v in obs.items()}\n",
    "        net_out, core_state = net(obs)            \n",
    "        if n == 0:\n",
    "            action = net_out[\"action\"][0]\n",
    "        else:\n",
    "            action, _, _ = n_step_greedy(env, net, n)\n",
    "        obs = env.step(action)\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns\n",
    "\n",
    "bsz = 16    \n",
    "eps_n = 500\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# create environments\n",
    "\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "num_actions = env.gym_env.action_space[0].n\n",
    "\n",
    "# import the net\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "flags.discounting = 0.97\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=num_actions, flags=flags)  \n",
    "net = net.to(device)\n",
    "checkpoint = torch.load(\"../models/base_1.tar\", map_location=\"cuda\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "# initialize net\n",
    "\n",
    "core_state = net.initial_state(batch_size=bsz)\n",
    "core_state = tuple(v.to(device) for v in core_state)\n",
    "net.train(False)\n",
    "\n",
    "all_returns = {}\n",
    "for n in range(4):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step(n, net, env)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ffb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous version of testing \n",
    "\n",
    "def act_m(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    returns: Manager().list,\n",
    "    eps_n: int,\n",
    "    n: int,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        net_out, unused_state = net(env_output, agent_state)      \n",
    "        while True:            \n",
    "            if len(returns) >= eps_n: break\n",
    "            with torch.no_grad():\n",
    "                net_out, agent_state = net(env_output, agent_state)                            \n",
    "            if n == 0:\n",
    "                action = net_out[\"action\"]\n",
    "            else:\n",
    "                action, _, _ = n_step_greedy(env, net, n)            \n",
    "            env_output = env.step(action)           \n",
    "            if env_output['done']: returns.append(ret)\n",
    "            ret = env_output['episode_return'].item()        \n",
    "        #logging.info(\"Actor %i end\", actor_index)\n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "\n",
    "def asy_test_n_step(n, net, flags):\n",
    "    \n",
    "    print(\"Testing %d step planning\" % n)\n",
    "\n",
    "    mp.set_sharing_strategy('file_system')\n",
    "    net.share_memory()\n",
    "    ctx = mp.get_context()        \n",
    "    returns = Manager().list()\n",
    "\n",
    "    actor_processes = []\n",
    "    for i in range(flags.num_actors):\n",
    "        actor = ctx.Process(target=act_m, args=(flags, i, net, returns, eps_n, n),)\n",
    "        actor.start()\n",
    "        actor_processes.append(actor)    \n",
    "\n",
    "    for actor in actor_processes:\n",
    "        actor.join()    \n",
    "\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f)\" % (len(returns),\n",
    "                    np.average(returns), np.std(returns) / np.sqrt(len(returns)),))        \n",
    "    return returns        \n",
    "        \n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())   \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.discounting = 0.97     \n",
    "flags.num_actors = 32\n",
    "bsz = 1\n",
    "eps_n = 500\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "net = net.to(\"cpu\")\n",
    "checkpoint = torch.load(\"../models/base_1.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"]) \n",
    "\n",
    "all_returns = {}\n",
    "for n in range(4):\n",
    "    t = time.time()\n",
    "    all_returns[n] = asy_test_n_step(n, net, flags)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abac9a",
   "metadata": {},
   "source": [
    "Results:\n",
    "    \n",
    "Testing 0 step planning <br>\n",
    "Finish 512 episode: avg. return: 0.12 (+-0.06) <br>\n",
    "Testing 1 step planning <br>\n",
    "Finish 502 episode: avg. return: 0.61 (+-0.04) <br>\n",
    "Testing 2 step planning <br>\n",
    "Finish 501 episode: avg. return: 0.92 (+-0.04) <br>\n",
    "Testing 3 step planning <br>\n",
    "Finish 501 episode: avg. return: 1.01 (+-0.04) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097bd18",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model Training Phase</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968043ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating data for learning model [RUN]\n",
    "\n",
    "Buffers = typing.Dict[str, typing.List[torch.Tensor]]\n",
    "\n",
    "def create_buffers_m(flags, obs_shape, num_actions) -> Buffers:\n",
    "    \n",
    "    seq_len = flags.seq_len\n",
    "    seq_n = flags.seq_n\n",
    "    specs = dict(\n",
    "        frame=dict(size=(seq_len + 1, *obs_shape), dtype=torch.uint8),\n",
    "        reward=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        truncated_done=dict(size=(seq_len + 1,), dtype=torch.bool),\n",
    "        episode_return=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        episode_step=dict(size=(seq_len + 1,), dtype=torch.int32),\n",
    "        policy_logits=dict(size=(seq_len + 1, num_actions), dtype=torch.float32),\n",
    "        baseline=dict(size=(seq_len + 1,), dtype=torch.float32),\n",
    "        last_action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        action=dict(size=(seq_len + 1,), dtype=torch.int64),\n",
    "        reg_loss=dict(size=(seq_len + 1,), dtype=torch.float32)\n",
    "    )\n",
    "    buffers: Buffers = {key: [] for key in specs}\n",
    "    for _ in range(seq_n):\n",
    "        for key in buffers:\n",
    "            buffers[key].append(torch.empty(**specs[key]).share_memory_())\n",
    "            \n",
    "    return buffers\n",
    "\n",
    "def gen_data(\n",
    "    flags,\n",
    "    actor_index: int,\n",
    "    net: torch.nn.Module,\n",
    "    buffers: Buffers,\n",
    "    free_queue: mp.SimpleQueue,\n",
    "):    \n",
    "    try:    \n",
    "        #logging.info(\"Actor %i started\", actor_index)\n",
    "        gym_env = create_env(flags)\n",
    "        seed = actor_index ^ int.from_bytes(os.urandom(4), byteorder=\"little\")\n",
    "        gym_env.seed(seed)\n",
    "        env = Environment(gym_env)\n",
    "        env_output = env.initial()  \n",
    "        agent_state = net.initial_state(batch_size=1)\n",
    "        agent_output, unused_state = net(env_output, agent_state)     \n",
    "        \n",
    "        while True:\n",
    "            index = free_queue.get()\n",
    "            if index is None:\n",
    "                break         \n",
    "\n",
    "            # Write old rollout end.\n",
    "            for key in env_output:\n",
    "                buffers[key][index][0, ...] = env_output[key]\n",
    "            for key in agent_output:\n",
    "                buffers[key][index][0, ...] = agent_output[key]\n",
    "\n",
    "            # Do new rollout.\n",
    "            for t in range(flags.seq_len):\n",
    "                with torch.no_grad():\n",
    "                    agent_output, agent_state = net(env_output, agent_state)\n",
    "                env_output = env.step(agent_output[\"action\"])\n",
    "                for key in env_output:\n",
    "                    buffers[key][index][t + 1, ...] = env_output[key]\n",
    "                for key in agent_output:\n",
    "                    buffers[key][index][t + 1, ...] = agent_output[key]\n",
    "                    \n",
    "    except KeyboardInterrupt:\n",
    "        pass  # Return silently.\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exception in worker process %i\", actor_index)\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "        \n",
    "\n",
    "# Models\n",
    "\n",
    "DOWNSCALE_C = 2\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation,\n",
    "        groups=groups, bias=False, dilation=dilation,)\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(self, inplanes, outplanes=None):\n",
    "        super().__init__()\n",
    "        if outplanes is None: outplanes = inplanes \n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        self.conv1 = conv3x3(inplanes, inplanes)\n",
    "        self.bn1 = norm_layer(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(inplanes, outplanes)\n",
    "        self.bn2 = norm_layer(outplanes)\n",
    "        self.skip_conv = (outplanes != inplanes)\n",
    "        if outplanes != inplanes:\n",
    "            self.conv3 = conv1x1(inplanes, outplanes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.skip_conv:\n",
    "            out += self.conv3(identity)\n",
    "        else:\n",
    "            out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class FrameEncoder(nn.Module):    \n",
    "    def __init__(self, num_actions, frame_channels=3):\n",
    "        self.num_actions = num_actions\n",
    "        super(FrameEncoder, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(in_channels=frame_channels+num_actions, out_channels=128//DOWNSCALE_C, kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=128//DOWNSCALE_C) for i in range(1)]) # Deep: 2 blocks here\n",
    "        self.res1 = torch.nn.Sequential(*res)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128//DOWNSCALE_C, out_channels=256//DOWNSCALE_C, \n",
    "                               kernel_size=3, stride=2, padding=1) \n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(1)]) # Deep: 3 blocks here\n",
    "        self.res2 = torch.nn.Sequential(*res)\n",
    "        self.avg1 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        res = nn.ModuleList([ResBlock(inplanes=256//DOWNSCALE_C) for i in range(1)]) # Deep: 3 blocks here\n",
    "        self.res3 = torch.nn.Sequential(*res)\n",
    "        self.avg2 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, x, actions):        \n",
    "        # input shape: B, C, H, W        \n",
    "        # action shape: B \n",
    "        \n",
    "        x = x.float() / 255.0    \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "        x = torch.concat([x, actions], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.res1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.res2(x)\n",
    "        x = self.avg1(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.avg2(x)\n",
    "        return x\n",
    "    \n",
    "class DynamicModel(nn.Module):\n",
    "    def __init__(self, num_actions, inplanes=256):        \n",
    "        super(DynamicModel, self).__init__()\n",
    "        res = nn.ModuleList([ResBlock(inplanes=inplanes+num_actions, outplanes=inplanes)] + [\n",
    "            ResBlock(inplanes=inplanes) for i in range(4)]) # Deep: 15 blocks here\n",
    "        self.res = torch.nn.Sequential(*res)\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def forward(self, x, actions):      \n",
    "        actions = actions.unsqueeze(-1).unsqueeze(-1).tile([1, 1, x.shape[2], x.shape[3]])        \n",
    "        x = torch.concat([x, actions], dim=1)\n",
    "        return self.res(x)\n",
    "    \n",
    "class Output_rvpi(nn.Module):   \n",
    "    def __init__(self, num_actions, input_shape):         \n",
    "        super(Output_rvpi, self).__init__()        \n",
    "        c, h, w = input_shape\n",
    "        self.conv1 = nn.Conv2d(in_channels=c, out_channels=c//2, kernel_size=3, padding='same') \n",
    "        self.conv2 = nn.Conv2d(in_channels=c//2, out_channels=c//4, kernel_size=3, padding='same') \n",
    "        fc_in = h * w * (c // 4)\n",
    "        self.fc_r = nn.Linear(fc_in, 1) \n",
    "        self.fc_v = nn.Linear(fc_in, 1) \n",
    "        self.fc_logits = nn.Linear(fc_in, num_actions)         \n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        r, v, logits = self.fc_r(x), self.fc_v(x), self.fc_logits(x)\n",
    "        return r, v, logits\n",
    "\n",
    "class Model(nn.Module):    \n",
    "    def __init__(self, flags, obs_shape, num_actions):\n",
    "        super(Model, self).__init__()      \n",
    "        self.flags = flags\n",
    "        self.obs_shape = obs_shape\n",
    "        self.num_actions = num_actions          \n",
    "        self.frameEncoder = FrameEncoder(num_actions=num_actions, frame_channels=obs_shape[0])\n",
    "        self.dynamicModel = DynamicModel(num_actions=num_actions, inplanes=256//DOWNSCALE_C)\n",
    "        self.output_rvpi = Output_rvpi(num_actions=num_actions, input_shape=(256//DOWNSCALE_C, \n",
    "                      obs_shape[1]//16, obs_shape[1]//16))\n",
    "        \n",
    "    def forward(self, x, actions, one_hot=False):\n",
    "        # Input\n",
    "        # x: frames with shape (B, C, H, W), in the form of s_t\n",
    "        # actions: action (int64) with shape (k+1, B), in the form of a_{t-1}, a_{t}, a_{t+1}, .. a_{t+k-1}\n",
    "        # Output\n",
    "        # reward: predicted reward with shape (k, B), in the form of r_{t+1}, r_{t+2}, ..., r_{t+k}\n",
    "        # value: predicted value with shape (k+1, B), in the form of v_{t}, v_{t+1}, v_{t+2}, ..., v_{t+k}\n",
    "        # policy: predicted policy with shape (k+1, B), in the form of pi_{t}, pi_{t+1}, pi_{t+2}, ..., pi_{t+k}\n",
    "        # encoded: encoded states with shape (k+1, B), in the form of z_t, z_{t+1}, z_{t+2}, ..., z_{t+k}\n",
    "        # Recall the transition notation: s_t, a_t, r_{t+1}, s_{t+1}, ...\n",
    "        \n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        encoded = self.frameEncoder(x, actions[0])\n",
    "        return self.forward_encoded(encoded, actions[1:], one_hot=True)\n",
    "    \n",
    "    def forward_encoded(self, encoded, actions, one_hot=False):\n",
    "        if not one_hot:\n",
    "            actions = F.one_hot(actions, self.num_actions)                \n",
    "        \n",
    "        r, v, logits = self.output_rvpi(encoded)\n",
    "        r_list, v_list, logits_list = [], [v.squeeze(-1).unsqueeze(0)], [logits.unsqueeze(0)]\n",
    "        encoded_list = [encoded.unsqueeze(0)]\n",
    "        \n",
    "        for k in range(actions.shape[0]):            \n",
    "            encoded = self.dynamicModel(encoded, actions[k])\n",
    "            r, v, logits = self.output_rvpi(encoded)\n",
    "            r_list.append(r.squeeze(-1).unsqueeze(0))\n",
    "            v_list.append(v.squeeze(-1).unsqueeze(0))\n",
    "            logits_list.append(logits.unsqueeze(0))\n",
    "            encoded_list.append(encoded.unsqueeze(0))        \n",
    "        \n",
    "        if len(r_list) > 0:\n",
    "            rs = torch.concat(r_list, dim=0)\n",
    "        else:\n",
    "            rs = None\n",
    "            \n",
    "        vs = torch.concat(v_list, dim=0)\n",
    "        logits = torch.concat(logits_list, dim=0)\n",
    "        encodeds = torch.concat(encoded_list, dim=0)        \n",
    "        \n",
    "        return rs, vs, logits, encodeds\n",
    "\n",
    "#model = Model(flags, (3, 80, 80), num_actions=5)\n",
    "#rs, vs, logits = model(torch.rand(16, 3, 80, 80), torch.ones(8, 16).long())\n",
    "\n",
    "# functions for training models\n",
    "\n",
    "def get_batch_m(flags, buffers: Buffers):\n",
    "    batch_indices = np.random.randint(flags.seq_n, size=flags.bsz)\n",
    "    time_indices = np.random.randint(flags.seq_len - flags.unroll_len, size=flags.bsz)\n",
    "    batch = {key: torch.stack([buffers[key][m][time_indices[n]:time_indices[n]+flags.unroll_len+1] \n",
    "                          for n, m in enumerate(batch_indices)], dim=1) for key in buffers}\n",
    "    batch = {k: t.to(device=flags.device, non_blocking=True) for k, t in batch.items()}\n",
    "    return batch\n",
    "\n",
    "def compute_cross_entropy_loss(logits, target_logits, mask):\n",
    "    target_policy = F.softmax(target_logits, dim=-1)\n",
    "    log_policy = F.log_softmax(logits, dim=-1)\n",
    "    return -torch.sum(target_policy * log_policy * (~mask).float().unsqueeze(-1))\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_rewards = batch['reward'][1:]\n",
    "    target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float())# +\n",
    "                           #vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "    # if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "    # rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "    # vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "    # logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "    for t in range(vs.shape[0]):\n",
    "        done = torch.logical_or(done, batch['done'][t])\n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "    done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()) * (~done_masks).float())\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()) * (~done_masks).float())\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), done_masks)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss\n",
    "\n",
    "# n_step_greedy for testing\n",
    "\n",
    "def n_step_greedy_model(state, action, model, n, encoded=None, temp=20.): \n",
    "    \n",
    "    # Either input state, action (S_t, A_{t-1}) or the encoded Z_t\n",
    "    # state / encoded in the shape of (B, C, H, W)\n",
    "    # action in the shape of (B)\n",
    "    \n",
    "    bsz = state.shape[0] if encoded is None else encoded.shape[0]\n",
    "    device = state.device if encoded is None else encoded.device\n",
    "    num_actions = model.num_actions    \n",
    "    \n",
    "    q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "    \n",
    "    for act in range(num_actions):        \n",
    "        new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "        if encoded is None:            \n",
    "            old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "            rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "        else:\n",
    "            rs, vs, logits, encodeds = model.forward_encoded(encoded, new_action.unsqueeze(0))\n",
    "        \n",
    "        if n > 1:\n",
    "            action, prob, sub_q_ret = n_step_greedy_model(state=None, action=None, \n",
    "                       model=model, n=n-1, encoded=encodeds[1])\n",
    "            ret = rs[0] + flags.discounting * torch.max(sub_q_ret, dim=1)[0] \n",
    "        else:\n",
    "            ret = rs[0] + flags.discounting * vs[1]\n",
    "        q_ret[:, act] = ret\n",
    "    \n",
    "    prob = F.softmax(temp*q_ret, dim=1)\n",
    "    action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "    \n",
    "    return action, prob, q_ret        \n",
    "   \n",
    "#n_step_greedy_model(batch['frame'][0], batch['action'][0], model, 4)  \n",
    "\n",
    "def test_n_step_model(n, model, flags, eps_n=100, temp=20.):    \n",
    "    \n",
    "    print(\"Testing %d step planning\" % n) \n",
    "    \n",
    "    bsz = 100\n",
    "    env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "    env = Vec_Environment(env, bsz)\n",
    "    num_actions = env.gym_env.action_space[0].n\n",
    "    \n",
    "    model.train(False)\n",
    "    returns = []\n",
    "    \n",
    "    obs = env.initial()\n",
    "    action = torch.zeros(bsz).long().to(flags.device)\n",
    "    eps_n_cur = 5\n",
    "\n",
    "    while(len(returns) <= eps_n):\n",
    "        cur_returns = obs['episode_return']    \n",
    "        obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "        new_action, _, _ = n_step_greedy_model(obs['frame'][0], action, model, n, None, temp)        \n",
    "        obs = env.step(new_action)\n",
    "        action = new_action\n",
    "        if torch.any(obs['done']):\n",
    "            returns.extend(cur_returns[obs['done']].numpy())\n",
    "        if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "            eps_n_cur = len(returns) + 10\n",
    "            #print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            #    np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "            \n",
    "    returns = returns[:eps_n]\n",
    "    print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "                np.average(returns), np.std(returns) / np.sqrt(len(returns))))\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa3769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start training models\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args(\"\".split())       \n",
    "\n",
    "flags.env = \"Sokoban-v0\"\n",
    "flags.env_disable_noop = False\n",
    "flags.bsz = 32\n",
    "flags.unroll_len = 3\n",
    "flags.num_actors = 32\n",
    "flags.seq_n = 1000\n",
    "flags.seq_len = 200\n",
    "flags.learning_rate = 0.0002\n",
    "flags.loop_batch_n = 3\n",
    "flags.discounting = 0.97\n",
    "flags.tot_epoch = 10000\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "# Create buffer for actors to write\n",
    "\n",
    "mp.set_sharing_strategy('file_system')\n",
    "ctx = mp.get_context()        \n",
    "\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "buffers = create_buffers_m(flags, obs_shape, num_actions)\n",
    "print(\"Buffer created successfully.\")\n",
    "\n",
    "# Initialize the model and optimizer\n",
    "\n",
    "env = create_env(flags)\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=flags.learning_rate)\n",
    "\n",
    "print(\"model size: \", sum(p.numel() for p in model.parameters()))\n",
    "for k, v in model.named_parameters(): print(k, v.numel())    \n",
    "    \n",
    "tot_step = int(flags.loop_batch_n * flags.seq_n * flags.seq_len / flags.bsz / flags.unroll_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301892ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the preset policy\n",
    "\n",
    "net = BaseNet(observation_shape=(3,80,80), num_actions=5, flags=flags)  \n",
    "checkpoint = torch.load(\"../models/base_1.tar\", map_location=\"cpu\")\n",
    "net.load_state_dict(checkpoint[\"model_state_dict\"])   \n",
    "net.train(False)\n",
    "net.share_memory()\n",
    "\n",
    "# Get the actors to write on the buffer\n",
    "\n",
    "actor_processes = []\n",
    "free_queue = mp.SimpleQueue()\n",
    "loss_stats = [deque(maxlen=400) for _ in range(4)]\n",
    "\n",
    "net.train(False)\n",
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)\n",
    "\n",
    "# Start training loop    \n",
    "\n",
    "model.train(True)\n",
    "for epoch in range(flags.tot_epoch):    \n",
    "    print(\"Batch [%d] starts\" % epoch)\n",
    "    while(not free_queue.empty()): time.sleep(1)\n",
    "    for step in range(tot_step):\n",
    "        if step == 0: \n",
    "            test_n_step_model(1, model, flags, eps_n=100)\n",
    "            model.train(True)\n",
    "        \n",
    "        batch = get_batch_m(flags, buffers)\n",
    "        rs_loss, vs_loss, logits_loss = compute_loss_m(model, batch)\n",
    "        tot_loss = rs_loss + vs_loss + 0.05 * logits_loss\n",
    "        for n, l in enumerate([tot_loss, rs_loss, vs_loss, logits_loss]):\n",
    "            loss_stats[n].append(l.item())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"[%d:%d] F: %d \\t tot_loss %f rs_loss %f vs_loss %f logits_loss %f\" % ((\n",
    "                epoch, step, (step + epoch * tot_step) * flags.bsz * flags.unroll_len,) +\n",
    "                tuple(np.average(l) for l in loss_stats)))\n",
    "        optimizer.zero_grad()\n",
    "        tot_loss.backward()\n",
    "        optimizer.step()    \n",
    "    for m in range(flags.seq_n): free_queue.put(m)\n",
    "        \n",
    "for _ in range(flags.num_actors): free_queue.put(None)        \n",
    "for actor in actor_processes: actor.join(timeout=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(flags.num_actors):\n",
    "    actor = ctx.Process(target=gen_data, args=(flags, i, net, buffers, free_queue, ),)\n",
    "    actor.start()\n",
    "    actor_processes.append(actor)   \n",
    "    \n",
    "for m in range(flags.seq_n): free_queue.put(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab596f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_queue.empty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec2114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the threads\n",
    "\n",
    "for _ in range(flags.num_actors): free_queue.put(None)  \n",
    "for actor in actor_processes: actor.join(timeout=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc08045",
   "metadata": {},
   "source": [
    "<font size=\"5\">Model testing / debug </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\"model_state_dict\": model.state_dict(),},\"../models/model_1.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e7cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model \n",
    "\n",
    "parser = define_parser()        \n",
    "flags = parser.parse_args(\"--env Sokoban-v0\".split())        \n",
    "flags.device = torch.device(\"cuda\")\n",
    "env = create_env(flags)\n",
    "obs_shape, num_actions = env.observation_space.shape, env.action_space.n\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8834194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_returns = {}\n",
    "for n in range(1,4):\n",
    "    t = time.process_time()\n",
    "    all_returns[n] = test_n_step_model(n, model, flags, eps_n=500, temp=20)\n",
    "    print(\"Time required for %d step planning: %f\" %(n, time.process_time()-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test planning algorithm\n",
    "bsz = 1\n",
    "env = gym.vector.SyncVectorEnv([lambda: SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)] * bsz)\n",
    "env = Vec_Environment(env, bsz)\n",
    "obs = env.initial()\n",
    "state = obs['frame'][0].to(flags.device).clone()\n",
    "action = torch.zeros(bsz).long().to(flags.device)\n",
    "encoded = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e67032",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.Tensor([4]).long().to(flags.device)\n",
    "obs = env.step(action)\n",
    "state = obs['frame'][0].to(flags.device).clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c551f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "device = flags.device\n",
    "\n",
    "for _ in range(1):\n",
    "    plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "    plt.show()\n",
    "    ret = np.zeros((5, 5, 5))\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            for k in range(5):\n",
    "                test_action_seq = [i,j,k]\n",
    "                test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "                old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "                rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "                ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "    print(np.max(ret), (np.max(ret) == ret).nonzero())    \n",
    "    new_action = torch.Tensor((np.max(ret) == ret).nonzero()[0]).long().to(flags.device)\n",
    "    #obs = env.step(new_action)\n",
    "    #state = obs['frame'][0].to(flags.device).clone()\n",
    "    #action = new_action            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6a7a264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCTS testing\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"\n",
    "    Core Monte Carlo Tree Search algorithm.\n",
    "    To decide on an action, we run N simulations, always starting at the root of\n",
    "    the search tree and traversing the tree according to the UCB formula until we\n",
    "    reach a leaf node.\n",
    "    \"\"\"\n",
    "    def __init__(self, flags, num_actions):\n",
    "        self.flags = flags\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def run(self, model, obs, add_exploration_noise,):\n",
    "        \"\"\"\n",
    "        At the root of the search tree we use the representation function to obtain a\n",
    "        hidden state given the current observation.\n",
    "        We then run a Monte Carlo Tree Search using only action sequences and the model\n",
    "        learned by the network.\n",
    "        Only supports a batch size of 1.        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            root = Node(0)\n",
    "            _, root_predicted_value, policy_logits, hidden_state = model(\n",
    "                obs[\"frame\"][0], obs[\"last_action\"], one_hot=False)\n",
    "            reward = 0.\n",
    "            root_predicted_value = root_predicted_value[-1].item()\n",
    "            policy_logits = policy_logits[-1]\n",
    "            hidden_state = hidden_state[-1]\n",
    "\n",
    "            root.expand(num_actions, reward, policy_logits, hidden_state,)\n",
    "\n",
    "            if add_exploration_noise:\n",
    "                root.add_exploration_noise(\n",
    "                    dirichlet_alpha=self.flags.root_dirichlet_alpha,\n",
    "                    exploration_fraction=self.flags.root_exploration_fraction,\n",
    "                )\n",
    "\n",
    "            min_max_stats = MinMaxStats()\n",
    "\n",
    "            max_tree_depth = 0\n",
    "            \n",
    "            #print(\"p at root:\", torch.softmax(policy_logits, dim=-1))\n",
    "            for k in range(self.flags.num_simulations): \n",
    "                \n",
    "                #print(\"=======%d iteration======\"%k)\n",
    "                node = root\n",
    "                search_path = [node]\n",
    "                current_tree_depth = 0\n",
    "\n",
    "                while node.expanded():\n",
    "                    current_tree_depth += 1                    \n",
    "                    action, node = self.select_child(node, min_max_stats)                    \n",
    "                    search_path.append(node)\n",
    "                    #print(\"action sel: %d\" % action)\n",
    "                \n",
    "                #np.set_printoptions(precision=5)\n",
    "                #for x in [\"prior_score\", \"value_score\", \"pb_c\", \"prior\", \"visit_count\"]:                    \n",
    "                #    print(x, \"\\t\", np.array([getattr(search_path[0].children[n], x) for n in range(5)]))\n",
    "\n",
    "                # Inside the search tree we use the dynamics function to obtain the next hidden\n",
    "                # state given an action and the previous hidden state\n",
    "                parent = search_path[-2]     \n",
    "                reward, value, policy_logits, hidden_state = model.forward_encoded(\n",
    "                    parent.hidden_state, torch.tensor([[action]]).to(parent.hidden_state.device), one_hot=False)\n",
    "                reward = reward[-1].item()\n",
    "                value = value[-1].item()\n",
    "                #print(\"model final output: %4f\" % value)\n",
    "                policy_logits = policy_logits[-1]\n",
    "                hidden_state = hidden_state[-1]\n",
    "                node.expand(num_actions, reward, policy_logits, hidden_state)\n",
    "                self.backpropagate(search_path, value, min_max_stats)\n",
    "                max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
    "\n",
    "            extra_info = {\n",
    "                \"max_tree_depth\": max_tree_depth,\n",
    "                \"root_predicted_value\": root_predicted_value,\n",
    "            }\n",
    "        return root, extra_info\n",
    "\n",
    "    def select_child(self, node, min_max_stats):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        max_ucb = max(\n",
    "            self.ucb_score(node, child, min_max_stats)\n",
    "            for action, child in node.children.items()\n",
    "        )\n",
    "        action = np.random.choice(\n",
    "            [\n",
    "                action\n",
    "                for action, child in node.children.items()\n",
    "                if self.ucb_score(node, child, min_max_stats) == max_ucb\n",
    "            ]\n",
    "        )\n",
    "        return action, node.children[action]\n",
    "\n",
    "    def ucb_score(self, parent, child, min_max_stats):\n",
    "        \"\"\"\n",
    "        The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
    "        \"\"\"\n",
    "        pb_c = (\n",
    "            math.log(\n",
    "                (parent.visit_count + self.flags.pb_c_base + 1) / self.flags.pb_c_base\n",
    "            )\n",
    "            + self.flags.pb_c_init\n",
    "        )\n",
    "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "        prior_score = pb_c * child.prior\n",
    "\n",
    "        if child.visit_count > 0:\n",
    "            # Mean value Q\n",
    "            value_score = min_max_stats.normalize(\n",
    "                child.reward + self.flags.discounting * child.value())\n",
    "        else:\n",
    "            value_score = 0\n",
    "            \n",
    "        child.pb_c = pb_c\n",
    "        child.prior_score = prior_score\n",
    "        child.value_score = value_score\n",
    "        \n",
    "        return prior_score + value_score\n",
    "\n",
    "    def backpropagate(self, search_path, value, min_max_stats):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        #print(\"bs value: %.4f\" % value)\n",
    "        for n, node in enumerate(reversed(search_path)):\n",
    "            node.value_sum += value\n",
    "            node.visit_count += 1\n",
    "            min_max_stats.update(node.reward + self.flags.discounting * node.value())\n",
    "            value = node.reward + self.flags.discounting * value\n",
    "            #print(\"%d - val: %.4f r: %.4f\" % (n, value, node.reward))\n",
    "            #print(\"node value_sum %.4f\" % node.value_sum)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior):\n",
    "        self.visit_count = 0\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def expand(self, num_actions, reward, policy_logits, hidden_state):\n",
    "        \"\"\"\n",
    "        We expand a node using the value, reward and policy prediction obtained from the\n",
    "        neural network.\n",
    "        \"\"\"\n",
    "        self.reward = reward\n",
    "        self.hidden_state = hidden_state\n",
    "        policy_values = torch.softmax(policy_logits[0], dim=0).tolist()\n",
    "        for a in range(num_actions):\n",
    "            self.children[a] = Node(policy_values[a])\n",
    "\n",
    "    def add_exploration_noise(self, dirichlet_alpha, exploration_fraction):\n",
    "        \"\"\"\n",
    "        At the start of each search, we add dirichlet noise to the prior of the root to\n",
    "        encourage the search to explore new actions.\n",
    "        \"\"\"\n",
    "        actions = list(self.children.keys())\n",
    "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
    "        frac = exploration_fraction\n",
    "        for a, n in zip(actions, noise):\n",
    "            self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "class MinMaxStats:\n",
    "    \"\"\"\n",
    "    A class that holds the min-max values of the tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.maximum = -float(\"inf\")\n",
    "        self.minimum = float(\"inf\")\n",
    "\n",
    "    def update(self, value):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value):\n",
    "        if self.maximum > self.minimum:\n",
    "            # We normalize only when we have set the maximum and minimum values\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value            \n",
    "\n",
    "def select_action(node, temperature):\n",
    "    \"\"\"\n",
    "    Select action according to the visit count distribution and the temperature.\n",
    "    The temperature is changed dynamically with the visit_softmax_temperature function\n",
    "    in the config.\n",
    "    \"\"\"\n",
    "    visit_counts = np.array(\n",
    "        [child.visit_count for child in node.children.values()], dtype=\"int32\"\n",
    "    )\n",
    "    actions = [action for action in node.children.keys()]\n",
    "    if temperature == 0:\n",
    "        action = actions[np.argmax(visit_counts)]\n",
    "    elif temperature == float(\"inf\"):\n",
    "        action = np.random.choice(actions)\n",
    "    else:\n",
    "        # See paper appendix Data Generation\n",
    "        visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "        visit_count_distribution = visit_count_distribution / sum(\n",
    "            visit_count_distribution\n",
    "        )\n",
    "        action = np.random.choice(actions, p=visit_count_distribution)\n",
    "    #print(\"visit_counts\", visit_counts)\n",
    "    #print(\"visit_count_distribution\", visit_count_distribution)\n",
    "    return action\n",
    "    \n",
    "    \n",
    "parser = argparse.ArgumentParser()      \n",
    "flags = parser.parse_args([])   \n",
    "\n",
    "env = SokobanWrapper(gym.make(\"Sokoban-v0\"), noop=True)\n",
    "env = Environment(env)\n",
    "env.initial()\n",
    "obs_shape, num_actions = env.gym_env.observation_space.shape, env.gym_env.action_space.n\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "flags = parser.parse_args([])   \n",
    "flags.discounting = 0.97\n",
    "flags.pb_c_init = 1.25\n",
    "flags.pb_c_base = 19652\n",
    "flags.root_dirichlet_alpha = 0.25\n",
    "flags.root_exploration_fraction = 0.\n",
    "flags.num_simulations = 3\n",
    "flags.temp = 0.5\n",
    "flags.device = torch.device(\"cuda\")\n",
    "\n",
    "eps_n = 10\n",
    "eps_n_cur = 0\n",
    "\n",
    "model = Model(flags, obs_shape, num_actions=num_actions).to(device=flags.device)\n",
    "checkpoint = torch.load(\"../models/model_1.tar\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])  \n",
    "\n",
    "obs = env.initial()\n",
    "returns = []\n",
    "mcts = MCTS(flags, num_actions)\n",
    "obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "root, extra_info = mcts.run(model, obs, add_exploration_noise=True)   \n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "actions = torch.tensor([0, 1, 3, 4, 4, 4, 2, 4, 2, 2, 2, 1, 1, 1, 1]).long().to(flags.device).reshape(-1, 1)\n",
    "reward, value, policy_logits, hidden_state  = model(obs[\"frame\"][0], actions, one_hot=False)\n",
    "print(reward, value)\n",
    "\n",
    "while(len(returns) <= eps_n):\n",
    "    cur_returns = obs['episode_return']    \n",
    "    obs = {k:v.to(flags.device) for k, v in obs.items()}\n",
    "    root, extra_info = mcts.run(model, obs, add_exploration_noise=True)    \n",
    "    new_action = select_action(root, flags.temp)    \n",
    "    #plt.imshow(torch.swapaxes(torch.swapaxes(obs['frame'][0,0].to(\n",
    "    #flags.device).clone().cpu(),0,2),0,1), interpolation='nearest')\n",
    "    #plt.show()\n",
    "    #print(\"action selected\", new_action)\n",
    "    #print(\"===========================================\")\n",
    "    obs = env.step(torch.tensor([new_action]))\n",
    "    if torch.any(obs['done']):\n",
    "        returns.extend(cur_returns[obs['done']].numpy())\n",
    "    if eps_n_cur <= len(returns) and len(returns) > 0: \n",
    "        eps_n_cur = len(returns) + 10\n",
    "print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a9ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, prob, q_ret = n_step_greedy_model(state, action, model, 3, encoded=None, temp=10.)\n",
    "print(\"action: \", action)\n",
    "print(\"prob: \", prob)\n",
    "print(\"q_ret: \", q_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd285be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_action_seq = [2,3,1]\n",
    "test_action_seq = torch.Tensor(test_action_seq).unsqueeze(-1).long().to(device)  \n",
    "old_new_actions = torch.concat([action.unsqueeze(0), test_action_seq], dim=0)\n",
    "rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "ret[i, j, k] = rs[0] + rs[1] * 0.97 + rs[2] * (0.97**2) + vs[-1] * (0.97**3)\n",
    "print(\"rs\", rs)\n",
    "print(\"vs\", vs)\n",
    "print(\"logits\", logits)\n",
    "print(\"ret\", ret[i,j,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ea711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 10.\n",
    "\n",
    "bsz = state.shape[0]\n",
    "device = state.device \n",
    "num_actions = model.num_actions    \n",
    "model.train(False)\n",
    "\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)        \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "\n",
    "for act in range(num_actions):        \n",
    "    new_action = torch.Tensor(np.full(bsz, act)).long().to(device)    \n",
    "    old_new_actions = torch.concat([action.unsqueeze(0), new_action.unsqueeze(0)], dim=0)\n",
    "    rs, vs, logits, encodeds = model(state, old_new_actions)\n",
    "    ret = rs[0] + flags.discounting * vs[1]\n",
    "    rs_act[:, act] = rs[0]\n",
    "    vs_act[:, act] = vs[1]\n",
    "    q_ret[:, act] = ret\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea8e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = flags.device\n",
    "net_state = env.clone_state()\n",
    "\n",
    "bsz = 1\n",
    "temp = 10.\n",
    "q_ret = torch.zeros(bsz, num_actions).to(device)      \n",
    "rs_act = torch.zeros(bsz, num_actions).to(device)        \n",
    "vs_act = torch.zeros(bsz, num_actions).to(device)   \n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "for act in range(num_actions):\n",
    "    obs = env.step(torch.Tensor(np.full(bsz, act)).long())      \n",
    "    obs = {k:v.to(device) for k, v in obs.items()}   \n",
    "    ret = obs['reward'] + flags.discounting * net(obs)[0]['baseline'] * (~obs['done']).float()\n",
    "    rs_act[:, act] = obs['reward']\n",
    "    vs_act[:, act] = net(obs)[0]['baseline']\n",
    "    q_ret[:, act] = ret\n",
    "    env.restore_state(net_state)\n",
    "\n",
    "prob = F.softmax(temp*q_ret, dim=1)\n",
    "action = torch.multinomial(prob, num_samples=1)[:, 0]\n",
    "\n",
    "print(\"rs_act\", rs_act)\n",
    "print(\"vs_act\", vs_act)\n",
    "print(\"q_ret\", q_ret)\n",
    "print(\"prob\", prob)\n",
    "\n",
    "plt.imshow(torch.swapaxes(torch.swapaxes(state[0].cpu(),0,2),0,1), interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd08cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch_m(flags, buffers)\n",
    "print(torch.max(batch[\"reward\"]), (torch.max(batch[\"reward\"]) == batch[\"reward\"]).nonzero())\n",
    "print(batch[\"done\"].nonzero())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e953ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG LOSS\n",
    "\n",
    "#batch = get_batch_m(flags, buffers)\n",
    "\n",
    "model.train(False)\n",
    "\n",
    "rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "logits = logits[:-1]\n",
    "\n",
    "target_rewards = batch['reward'][1:]\n",
    "target_logits = batch['policy_logits'][1:]\n",
    "\n",
    "target_vs = []\n",
    "target_v = model(batch['frame'][-1], batch['action'][[-1]])[1][0]    \n",
    "\n",
    "for t in range(vs.shape[0]-1, 0, -1):\n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))\n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)\n",
    "\n",
    "# if done on step j, r_{j}, v_{j-1}, a_{j-1} has the last valid loss \n",
    "# rs is stored in the form of r_{t+1}, ..., r_{t+k}\n",
    "# vs is stored in the form of v_{t}, ..., v_{t+k-1}\n",
    "# logits is stored in the form of a{t}, ..., a_{t+k-1}\n",
    "\n",
    "done_masks = []\n",
    "done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "for t in range(vs.shape[0]):\n",
    "    done = torch.logical_or(done, batch['done'][t])\n",
    "    done_masks.append(done.unsqueeze(0))\n",
    "\n",
    "done_masks = torch.concat(done_masks[:-1], dim=0)\n",
    "\n",
    "# compute final loss\n",
    "huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "rs_loss = torch.sum(huberloss(rs, target_rewards) * (~done_masks).float())\n",
    "#rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "vs_loss = torch.sum(huberloss(vs[:-1], target_vs) * (~done_masks).float())\n",
    "#vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "logits_loss = compute_cross_entropy_loss(logits, target_logits, done_masks)\n",
    "\n",
    "# debug\n",
    "ind = 21\n",
    "\n",
    "target_vs = []\n",
    "target_v = vs[-1]\n",
    "for t in range(vs.shape[0]-1, 0, -1):        \n",
    "    new_target_v = batch['reward'][t] + flags.discounting * (target_v * (~batch['done'][t]).float() +\n",
    "                       vs[t-1] * (batch['truncated_done'][t]).float())\n",
    "    print(t, \n",
    "          \"reward %2f\" % batch['reward'][t,ind].item(), \n",
    "          \"bootstrap %2f\" % (target_v * (~batch['done'][t]).float())[ind].item(), \n",
    "          \"truncated %2f\" % (vs[t-1] * (batch['truncated_done'][t]).float())[ind].item(),\n",
    "          \"vs[t-1] %2f\" % vs[t-1][ind].item(),\n",
    "          \"new_targ %2f\" % new_target_v[ind].item())\n",
    "    target_vs.append(new_target_v.unsqueeze(0))    \n",
    "    target_v = new_target_v\n",
    "target_vs.reverse()\n",
    "target_vs = torch.concat(target_vs, dim=0)   \n",
    "print(\"done\", batch[\"done\"][:, ind])\n",
    "print(\"done_masks\", done_masks[:, ind])\n",
    "print(\"vs: \", vs[:, ind])\n",
    "print(\"target_vs: \", target_vs[:, ind])\n",
    "print(\"reward: \", rs[:, ind])\n",
    "print(\"target_reward: \", target_rewards[:, ind])\n",
    "print(\"logits: \", logits[:, ind])\n",
    "print(\"target_logits: \", target_logits[:, ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt. version of computing loss by treading terminal state as absorbing state (as in MuZero)\n",
    "\n",
    "def compute_loss_m(model, batch):\n",
    "\n",
    "    rs, vs, logits, _ = model(batch['frame'][0], batch['action'])\n",
    "    logits = logits[:-1]\n",
    "\n",
    "    target_logits = batch['policy_logits'][1:].clone()\n",
    "    target_rewards = batch['reward'][1:].clone()\n",
    "\n",
    "    done_masks = []\n",
    "    done = torch.zeros(vs.shape[1]).bool().to(batch['done'].device)\n",
    "\n",
    "    c_logits = target_logits[0]\n",
    "    c_state = batch['frame'][0]\n",
    "    for t in range(vs.shape[0]-1):\n",
    "        if t > 0: done = torch.logical_or(done, batch['done'][t])\n",
    "        c_logits = torch.where(done.unsqueeze(-1), c_logits, target_logits[t])\n",
    "        target_logits[t] = c_logits\n",
    "        c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][t])  \n",
    "        done_masks.append(done.unsqueeze(0))\n",
    "    done_masks = torch.concat(done_masks, dim=0)\n",
    "    done = torch.logical_or(done, batch['done'][-1])\n",
    "    c_state = torch.where(done.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1), c_state, batch['frame'][-1])\n",
    "    target_rewards = target_rewards * (~done_masks).float()\n",
    "\n",
    "    target_vs = []\n",
    "    target_v = model(c_state, batch['action'][[-1]])[1][0].detach()\n",
    "    \n",
    "    for t in range(vs.shape[0]-1, 0, -1):\n",
    "        new_target_v = batch['reward'][t] + flags.discounting * target_v\n",
    "        target_vs.append(new_target_v.unsqueeze(0))\n",
    "        target_v = new_target_v\n",
    "    target_vs.reverse()\n",
    "    target_vs = torch.concat(target_vs, dim=0)\n",
    "    \n",
    "    # compute final loss\n",
    "    huberloss = torch.nn.HuberLoss(reduction='none', delta=1.0)    \n",
    "    rs_loss = torch.sum(huberloss(rs, target_rewards.detach()))\n",
    "    #rs_loss = torch.sum(((rs - target_rewards) ** 2) * (~r_logit_done_masks).float())\n",
    "    vs_loss = torch.sum(huberloss(vs[:-1], target_vs.detach()))\n",
    "    #vs_loss = torch.sum(((vs[:-1] - target_vs) ** 2) * (~v_done_masks).float())\n",
    "    logits_loss = compute_cross_entropy_loss(logits, target_logits.detach(), None)\n",
    "    \n",
    "    return rs_loss, vs_loss, logits_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
