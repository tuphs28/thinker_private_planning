{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath('thinker/thinker')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from collections import deque\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from thinker.env import Environment\n",
    "from thinker.net import ActorNet, ModelNet\n",
    "from thinker.buffer import ModelBuffer\n",
    "from thinker.gym.asyn_vector_env import AsyncVectorEnv\n",
    "import thinker.util as util\n",
    "import gym\n",
    "import gym_csokoban\n",
    "\n",
    "EnvOut = namedtuple('EnvOut', ['gym_env_out', 'model_out', 'see_mask', 'reward', 'done', \n",
    "    'truncated_done', 'episode_return', 'episode_step', 'cur_t', 'last_action',\n",
    "    'max_rollout_depth'])\n",
    "\n",
    "def gplot(x, ax=None, title=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    if type(x) == np.ndarray: x = torch.tensor(x)        \n",
    "    ax.imshow(torch.swapaxes(torch.swapaxes(x.cpu(),0,2),0,1), interpolation='nearest', aspect=\"auto\")\n",
    "    if title is not None: ax.set_title(title)\n",
    "\n",
    "class TransposeWrap(gym.ObservationWrapper):\n",
    "    \"\"\"Image shape to channels x weight x height\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(TransposeWrap, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.transpose(observation, axes=(2, 0, 1))    \n",
    "\n",
    "class Node:\n",
    "    def __init__(self, parent, action, logit, num_actions, discounting, rec_t, device=None):        \n",
    "        self.device = torch.device(\"cpu\") if device is None else device\n",
    "\n",
    "        self.action = F.one_hot(torch.tensor(action, dtype=torch.long, device=self.device), num_actions) # shape (1, num_actions)        \n",
    "        self.r = torch.tensor([0.], dtype=torch.float32, device=self.device)    \n",
    "        self.v = torch.tensor([0.], dtype=torch.float32, device=self.device)            \n",
    "        self.logit = logit # shape (1,)        \n",
    "        \n",
    "        self.rollout_qs = []  # list of tensors of shape (1,)\n",
    "        self.rollout_n = 0\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.encoded = None \n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "        self.discounting = discounting\n",
    "        self.rec_t = rec_t        \n",
    "        \n",
    "        self.visited = False\n",
    "        \n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def expand(self, r, v, logits, encoded, override=False):\n",
    "        \"\"\"\n",
    "        First time arriving a node and so we expand it\n",
    "        r, v: tensor of shape (1,)\n",
    "        logits: tensor of shape (num_actions,)\n",
    "        \"\"\"\n",
    "        if not override: assert not self.expanded()\n",
    "        if override:\n",
    "            self.rollout_qs = [x - self.r + r for x in self.rollout_qs]\n",
    "            self.rollout_qs[0] = v * self.discounting\n",
    "        self.r = r\n",
    "        self.v = v\n",
    "        self.encoded = encoded\n",
    "        for a in range(self.num_actions):\n",
    "            if not override:\n",
    "                child = self.children.append(Node(self, a, logits[[a]], \n",
    "                   self.num_actions, self.discounting, self.rec_t, device=self.device))\n",
    "            else:\n",
    "                self.children[a].logit = logits[[a]]        \n",
    "            \n",
    "    def visit(self):\n",
    "        self.trail_r = torch.zeros(1, dtype=torch.float32, device=self.device)    \n",
    "        self.trail_discount = 1.\n",
    "        self.propagate(self.r, self.v, not self.visited)        \n",
    "        self.visited = True\n",
    "        \n",
    "    def propagate(self, r, v, new_rollout):\n",
    "        self.trail_r = self.trail_r + self.trail_discount * r\n",
    "        self.trail_discount = self.trail_discount * self.discounting\n",
    "        self.rollout_q = self.trail_r + self.trail_discount * v\n",
    "        if new_rollout:\n",
    "            self.rollout_qs.append(self.rollout_q)\n",
    "            self.rollout_n = self.rollout_n + 1\n",
    "        if self.parent is not None: self.parent.propagate(r, v, new_rollout)\n",
    "            \n",
    "    def stat(self, detailed=False):\n",
    "        assert self.expanded()\n",
    "        self.child_logits = torch.concat([x.logit for x in self.children])        \n",
    "        child_rollout_qs_mean = []\n",
    "        child_rollout_qs_max = []\n",
    "        for x in self.children:\n",
    "            if len(x.rollout_qs) > 0:                \n",
    "                q_mean = torch.mean(torch.cat(x.rollout_qs), dim=-1, keepdim=True)\n",
    "                q_max = torch.max(torch.cat(x.rollout_qs), dim=-1, keepdim=True)[0]\n",
    "            else:\n",
    "                q_mean = torch.tensor([0.], dtype=torch.float32, device=self.device)    \n",
    "                q_max = torch.tensor([0.], dtype=torch.float32, device=self.device)    \n",
    "            child_rollout_qs_mean.append(q_mean)\n",
    "            child_rollout_qs_max.append(q_max)\n",
    "        self.child_rollout_qs_mean = torch.concat(child_rollout_qs_mean)\n",
    "        self.child_rollout_qs_max = torch.concat(child_rollout_qs_max)\n",
    "\n",
    "        if detailed:\n",
    "            self.trail_r_undiscount = self.trail_r / self.discounting\n",
    "            self.rollout_q_undiscount = self.rollout_q / self.discounting\n",
    "            self.max_q = torch.max(torch.concat(self.rollout_qs) - self.r).unsqueeze(-1) / self.discounting            \n",
    "        \n",
    "        self.child_rollout_ns = torch.tensor([x.rollout_n for x in self.children], dtype=torch.long, device=self.device)\n",
    "        self.child_rollout_ns_enc = self.child_rollout_ns / self.rec_t       \n",
    "            \n",
    "        ret_list = [\"action\", \"r\", \"v\", \"child_logits\", \"child_rollout_qs_mean\",\n",
    "                    \"child_rollout_qs_max\", \"child_rollout_ns_enc\"]\n",
    "        if detailed: ret_list.extend([\"trail_r_undiscount\", \"rollout_q_undiscount\", \"max_q\"])\n",
    "        self.ret_dict = {x: getattr(self, x) for x in ret_list}\n",
    "        #for x in ret_list: print(x, getattr(self, x))\n",
    "        out = torch.concat(list(self.ret_dict.values()))        \n",
    "        return out         \n",
    "\n",
    "class VecModelWrapper(gym.Wrapper):\n",
    "    \"\"\"Wrap the gym environment with a model; output for each \n",
    "    step is (out, reward, done, info), where out is a tuple \n",
    "    of (gym_env_out, model_out) that corresponds to underlying \n",
    "    environment frame and output from the model wrapper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, env_n, flags, device=None):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        \n",
    "        self.device = torch.device(\"cpu\") if device is None else device\n",
    "        self.env = env     \n",
    "        self.rec_t = flags.rec_t        \n",
    "        self.flex_t = flags.flex_t \n",
    "        self.flex_t_cost = flags.flex_t_cost         \n",
    "        self.discounting = flags.discounting\n",
    "        self.perfect_model = flags.perfect_model\n",
    "        self.tree_carry = flags.tree_carry\n",
    "        self.thres_carry = flags.thres_carry        \n",
    "        self.thres_discounting = flags.thres_discounting\n",
    "        self.num_actions = env.action_space[0].n\n",
    "        self.env_n = env_n\n",
    "            \n",
    "        if not self.flex_t:\n",
    "            obs_n = 9 + self.num_actions * 10 + self.rec_t\n",
    "        else:\n",
    "            obs_n = 10 + self.num_actions * 11 \n",
    "        \n",
    "        self.observation_space = gym.spaces.Box(\n",
    "          low=-np.inf, high=np.inf, shape=(env_n, obs_n, 1, 1), dtype=float)\n",
    "        \n",
    "        assert self.perfect_model, \"imperfect model not yet supported\"\n",
    "        assert not self.thres_carry, \"thres_carry not yet supported\"\n",
    "        assert not flags.model_rnn, \"model_rnn not yet supported\"\n",
    "        assert flags.reward_type == 1, \"only support reward_type 1\"\n",
    "        \n",
    "    def reset(self, model_net):\n",
    "        with torch.no_grad():\n",
    "            # some init.\n",
    "            self.root_max_q = [None for _ in range(self.env_n)]\n",
    "            self.rollout_depth = torch.zeros(self.env_n, dtype=torch.long, device=self.device)\n",
    "            self.max_rollout_depth = torch.zeros(self.env_n, dtype=torch.long, device=self.device)\n",
    "            self.cur_t = torch.zeros(self.env_n, dtype=torch.long, device=self.device)\n",
    "\n",
    "            # reset obs\n",
    "            obs = self.env.reset()\n",
    "\n",
    "            # obtain output from model\n",
    "            obs_py = torch.tensor(obs, dtype=torch.uint8, device=self.device)\n",
    "            pass_action = torch.zeros(self.env_n, dtype=torch.long, device=self.device)\n",
    "            _, vs, logits, encodeds = model_net(obs_py, \n",
    "                                                pass_action.unsqueeze(0), \n",
    "                                                one_hot=False)  \n",
    "\n",
    "            self._debug = (obs_py, pass_action, logits, obs)\n",
    "            encodeds = self.env.clone_state(inds=np.arange(self.env_n))\n",
    "\n",
    "            # compute and update root node and current node\n",
    "            self.root_nodes = []\n",
    "            self.cur_nodes = []\n",
    "            for n in range(self.env_n):\n",
    "                root_node = Node(parent=None, action=pass_action[n].item(), logit=None, \n",
    "                num_actions=self.num_actions, discounting=self.discounting, rec_t=self.rec_t, device=self.device)\n",
    "                encoded = {\"env_state\": encodeds[n], \"gym_env_out\": obs_py[n]}\n",
    "                root_node.expand(r=torch.zeros(1, dtype=torch.float32, device=self.device), \n",
    "                                 v=vs[-1, n].unsqueeze(-1), \n",
    "                                 logits=logits[-1, n],\n",
    "                                 encoded=encoded)\n",
    "                root_node.visit()\n",
    "                self.root_nodes.append(root_node)\n",
    "                self.cur_nodes.append(root_node)\n",
    "            \n",
    "            # compute model_out\n",
    "            model_out = self.compute_model_out()\n",
    "            gym_env_out = torch.concat([x.encoded[\"gym_env_out\"].unsqueeze(0) for x in self.cur_nodes])\n",
    "\n",
    "            # record initial root_nodes_qmax \n",
    "            self.root_nodes_qmax = torch.tensor([n.max_q for n in self.root_nodes], dtype=torch.float32, device=self.device)\n",
    "            \n",
    "            return model_out, gym_env_out\n",
    "\n",
    "\n",
    "    def step(self, action, model_net):  \n",
    "        # action is tensor of shape (env_n, 3) or (env_n, 4); \n",
    "        # which corresponds to real_action, im_action, reset, term\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if not self.flex_t:\n",
    "                re_action, im_action, reset = action[:, 0], action[:, 1], action[:, 2]\n",
    "                term = None\n",
    "            else:\n",
    "                re_action, im_action, reset, term = (action[:, 0], action[:, 1], \n",
    "                    action[:, 2], action[:, 3])\n",
    "\n",
    "            # compute the mask of real / imagination step\n",
    "            if not self.flex_t:\n",
    "                imagine_b = self.cur_t < self.rec_t - 1\n",
    "            else:\n",
    "                imagine_b = (self.cur_t < self.rec_t - 1) & ~(term.bool())\n",
    "            \n",
    "            self.cur_t += 1\n",
    "            self.cur_t[~imagine_b] = 0\n",
    "            self.rollout_depth += 1\n",
    "            self.rollout_depth[~imagine_b] = 0        \n",
    "            self.max_rollout_depth[~imagine_b] = 0\n",
    "            self.max_rollout_depth = torch.max(self.max_rollout_depth, self.rollout_depth)\n",
    "\n",
    "            # four status: \n",
    "            # 1. real transition; \n",
    "            # 2. imaginary transition and expanded\n",
    "            # 3. imaginary transition and done and unexpanded; \n",
    "            # 4. imagainary transition and not done and unexpanded\n",
    "            # compute the status here\n",
    "            \n",
    "            status = torch.zeros(self.env_n, dtype=torch.long, device=self.device)\n",
    "            status[~imagine_b] = 1             \n",
    "\n",
    "            if torch.any(imagine_b):\n",
    "                sl_cur_nodes = [x for n, x in enumerate(self.cur_nodes) if imagine_b[n]]\n",
    "                sl_next_nodes = [x.children[im_action[n]] for n, x in enumerate(self.cur_nodes) if imagine_b[n]]            \n",
    "                status[imagine_b] = torch.tensor([2 if y.expanded() else (3 if 'done' in x.encoded else 4) for x, y in zip(sl_cur_nodes, sl_next_nodes)],\n",
    "                    dtype=torch.long, device=self.device)\n",
    "            self._status = status\n",
    "            \n",
    "            # use the model; only status 1 and 4 need to use env and model\n",
    "            if torch.any((status == 1) | (status == 4)):\n",
    "                sel_inds = torch.arange(self.env_n, device=self.device)[(status == 1) | (status == 4)]\n",
    "                real_sel_b = status[(status == 1) | (status == 4)] == 1\n",
    "                pass_env_states = [self.root_nodes[n].encoded[\"env_state\"] if s == 1 else \n",
    "                    self.cur_nodes[n].encoded[\"env_state\"] for n, s in enumerate(status) if s in [1, 4]]  \n",
    "                self.env.restore_state(pass_env_states, inds=sel_inds.cpu().numpy())\n",
    "                pass_action = torch.tensor([re_action[n] if s == 1 else \n",
    "                    im_action[n] for n, s in enumerate(status) if s in [1, 4]], dtype=torch.long, device=self.device)\n",
    "                obs, reward, done, _ = self.env.step(pass_action.cpu().numpy(), inds=sel_inds.cpu().numpy()) \n",
    "\n",
    "                self._done = done\n",
    "\n",
    "                # reset if done and the transition is real\n",
    "                reset_needed = torch.zeros(self.env_n, dtype=torch.bool, device=self.device)\n",
    "                reset_needed[sel_inds] = torch.tensor(done, dtype=torch.bool, device=self.device)\n",
    "                reset_needed = reset_needed & (status == 1)\n",
    "                if torch.any(reset_needed):                    \n",
    "                    reset_inds = torch.arange(self.env_n, device=self.device)[reset_needed]\n",
    "                    reset_m = reset_needed[sel_inds].cpu().numpy()                    \n",
    "                    obs_reset = self.env.reset(inds=reset_inds.cpu().numpy()) \n",
    "                    obs[reset_m] = obs_reset\n",
    "                    pass_action[reset_needed[sel_inds]] = 0          \n",
    "\n",
    "                obs_py = torch.tensor(obs, dtype=torch.uint8, device=self.device)\n",
    "                _, vs, logits, encodeds = model_net(obs_py, \n",
    "                                                    pass_action.unsqueeze(0), \n",
    "                                                    one_hot=False)  \n",
    "                encodeds = self.env.clone_state(inds=sel_inds.cpu().numpy())    \n",
    "            else:\n",
    "                reset_needed = torch.zeros(self.env_n, dtype=torch.bool, device=self.device)\n",
    "\n",
    "            m_ind = 0\n",
    "            root_nodes, cur_nodes = [], []\n",
    "\n",
    "            # compute the current and root nodes\n",
    "            for n in range(self.env_n):\n",
    "                if status[n] == 1:\n",
    "                    # real transition\n",
    "                    new_root = (not self.tree_carry or \n",
    "                        not self.root_nodes[n].children[re_action[n]].expanded() or done[m_ind])\n",
    "                    if new_root:\n",
    "                        root_node = Node(parent=None, action=pass_action[m_ind].item(), logit=None, \n",
    "                            num_actions=self.num_actions, discounting=self.discounting, rec_t=self.rec_t,\n",
    "                            device=self.device)\n",
    "                        encoded = {\"env_state\": encodeds[m_ind], \"gym_env_out\": obs_py[m_ind]}\n",
    "                        root_node.expand(r=torch.zeros(1, dtype=torch.float32, device=self.device), \n",
    "                                        v=vs[-1, m_ind].unsqueeze(-1), \n",
    "                                        logits=logits[-1, m_ind],\n",
    "                                        encoded=encoded)\n",
    "                        root_node.visit()\n",
    "                    else:\n",
    "                        root_node = self.root_nodes[n].children[re_action[n]]\n",
    "                        encoded = {\"env_state\": encodeds[m_ind], \"gym_env_out\": obs_py[m_ind]}\n",
    "                        root_node.expand(r=torch.zeros(1, dtype=torch.float32, device=self.device), \n",
    "                                            v=vs[-1, m_ind].unsqueeze(-1), \n",
    "                                            logits=logits[-1, m_ind],\n",
    "                                            encoded=encoded, \n",
    "                                            override=True)\n",
    "                        root_node.parent = None\n",
    "                        root_node.visit()\n",
    "                    \n",
    "                    root_nodes.append(root_node)\n",
    "                    cur_nodes.append(root_node)\n",
    "                    m_ind += 1\n",
    "                \n",
    "                elif status[n] == 2:\n",
    "                    cur_node = self.cur_nodes[n].children[im_action[n]]\n",
    "                    cur_node.visit()\n",
    "                    root_nodes.append(self.root_nodes[n])\n",
    "                    cur_nodes.append(cur_node)                    \n",
    "\n",
    "                elif status[n] == 3:\n",
    "                    par_logits = torch.concat([ch.logit for ch in self.cur_nodes[n].children])  \n",
    "                    cur_node = self.cur_nodes[n].children[im_action[n]]\n",
    "                    cur_node.expand(r=torch.zeros(1, dtype=torch.float32, device=self.device), \n",
    "                                    v=torch.zeros(1, dtype=torch.float32, device=self.device),\n",
    "                                    logits=par_logits, \n",
    "                                    encoded=self.cur_nodes[n].encoded) \n",
    "                    cur_node.visit()\n",
    "                    root_nodes.append(self.root_nodes[n])\n",
    "                    cur_nodes.append(cur_node)\n",
    "                    \n",
    "                elif status[n] == 4:\n",
    "                    encoded = {\"env_state\": encodeds[m_ind], \"gym_env_out\": obs_py[m_ind]}\n",
    "                    if done[m_ind]: encoded[\"done\"] = True\n",
    "                    cur_node = self.cur_nodes[n].children[im_action[n]]\n",
    "                    cur_node.expand(r=torch.tensor([reward[m_ind]], dtype=torch.float32, device=self.device), \n",
    "                                v=vs[-1, m_ind].unsqueeze(0) if not done[m_ind] else torch.zeros(1, dtype=torch.float32, device=self.device),\n",
    "                                logits=logits[-1, m_ind], \n",
    "                                encoded=encoded) \n",
    "                    cur_node.visit()\n",
    "                    root_nodes.append(self.root_nodes[n])\n",
    "                    cur_nodes.append(cur_node)\n",
    "                    \n",
    "                    m_ind += 1\n",
    "\n",
    "            self.root_nodes = root_nodes\n",
    "            self.cur_nodes = cur_nodes\n",
    "\n",
    "        # compute model_out\n",
    "        model_out = self.compute_model_out(action, imagine_b, reset_needed)\n",
    "        gym_env_out = torch.concat([x.encoded[\"gym_env_out\"].unsqueeze(0) for x in self.cur_nodes])        \n",
    "\n",
    "        # compute reward\n",
    "        root_nodes_qmax = torch.tensor([n.max_q for n in self.root_nodes], dtype=torch.float32, device=self.device)\n",
    "        re_reward = torch.zeros(self.env_n,  dtype=torch.float32, device=self.device)\n",
    "        im_reward = torch.zeros(self.env_n,  dtype=torch.float32, device=self.device)\n",
    "        if torch.any(~imagine_b):            \n",
    "            re_reward[~imagine_b] = torch.tensor(reward, dtype=torch.float32, device=self.device)[real_sel_b] # real reward            \n",
    "        if torch.any(imagine_b):\n",
    "            flex_t_cost = 0. if not self.flex_t else self.flex_t_cost\n",
    "            im_reward[imagine_b] = (root_nodes_qmax - self.root_nodes_qmax - flex_t_cost)[imagine_b] # imagine reward\n",
    "        full_reward = torch.concat([re_reward.unsqueeze(-1), im_reward.unsqueeze(-1)], dim=-1)\n",
    "        self.root_nodes_qmax = root_nodes_qmax\n",
    "\n",
    "        # compute done\n",
    "        full_done = torch.zeros(self.env_n, dtype=torch.bool, device=self.device)\n",
    "        if torch.any(~imagine_b):\n",
    "            full_done[~imagine_b] = torch.tensor(done, dtype=torch.bool, device=self.device)[real_sel_b]\n",
    "\n",
    "        # compute reset\n",
    "        self.compute_reset(reset)\n",
    "\n",
    "        # some info\n",
    "        info = {\"cur_t\": self.cur_t,\n",
    "                \"max_rollout_depth\": self.max_rollout_depth}\n",
    "\n",
    "        return (model_out, gym_env_out), full_reward, full_done, info\n",
    "\n",
    "    def compute_model_out(self, action=None, imagine_b=None, reset_needed=None):\n",
    "        nodes_stat = []\n",
    "        for n in range(self.env_n):\n",
    "            root_node_stat = self.root_nodes[n].stat(detailed=True)\n",
    "            cur_node_stat = self.cur_nodes[n].stat()                        \n",
    "            nodes_stat.append(torch.concat([root_node_stat, cur_node_stat]).unsqueeze(0))\n",
    "        nodes_stat = torch.concat(nodes_stat)\n",
    "        if action is None: \n",
    "            reset = torch.ones(self.env_n, 1, dtype=torch.float32, device=self.device)    \n",
    "        else:\n",
    "            reset = action[:, 2].unsqueeze(1)\n",
    "            reset = reset.clone()\n",
    "            reset[~imagine_b] = 1.\n",
    "        depc = (self.discounting ** (self.rollout_depth-1)).unsqueeze(-1)\n",
    "        if not self.flex_t:\n",
    "            time = F.one_hot(self.cur_t, self.rec_t).float()\n",
    "        else:\n",
    "            time = (self.discounting ** (self.cur_t)).unsqueeze(-1)\n",
    "        \n",
    "        if not self.flex_t:\n",
    "            ret_list = [nodes_stat, reset, time, depc]\n",
    "        else:\n",
    "            if action is None: \n",
    "                term = torch.zeros(self.env_n, 1, dtype=torch.float32, device=self.device)    \n",
    "            else:\n",
    "                term = action[:, 3].unsqueeze(1)   \n",
    "                if torch.any(reset_needed):\n",
    "                    term = term.clone()\n",
    "                    term[reset_needed] = 0.\n",
    "            ret_list = [nodes_stat, reset, term, time, depc]\n",
    "        model_out = torch.concat(ret_list, dim=-1)  \n",
    "        return model_out\n",
    "\n",
    "    def compute_reset(self, reset):\n",
    "        reset_b = (reset == 1)\n",
    "        self.rollout_depth[reset_b] = 0\n",
    "        for n in torch.arange(self.env_n, device=self.device)[reset_b]:\n",
    "            self.cur_nodes[n] = self.root_nodes[n]\n",
    "            self.cur_nodes[n].visit()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self.env.seed(seed)\n",
    "\n",
    "def align(model_out, flex_t):    \n",
    "    num_actions = 5\n",
    "    new_model_out = torch.clone(model_out)\n",
    "    s = 5 * num_actions + 2\n",
    "    if flex_t:        \n",
    "        new_model_out[s:2*s] = model_out[s+3:2*s+3]\n",
    "        new_model_out[2*s:2*s+3] = model_out[s:s+3]     \n",
    "        new_model_out[-3] = model_out[-1]  \n",
    "        new_model_out[-2:] = model_out[-3:-1]  \n",
    "    else:\n",
    "        new_model_out[-3:] = model_out[s:s+3]\n",
    "        new_model_out[s:-3] = model_out[s+3:]\n",
    "    return new_model_out\n",
    "\n",
    "\n",
    "class PostVecModelWrapper(gym.Wrapper):\n",
    "    \"\"\"The final wrapper for env.; calculates episode return,\n",
    "       record last action, and returns EnvOut\"\"\"\n",
    "\n",
    "    def __init__(self, env, flags, device=None):\n",
    "        self.device = torch.device(\"cpu\") if device is None else device\n",
    "        self.env = env\n",
    "        self.env_n = env.env_n\n",
    "        self.flags = flags\n",
    "        self.num_actions = env.num_actions\n",
    "        self.actor_see_p = flags.actor_see_p \n",
    "        self.model_out_shape = env.observation_space.shape[1:]\n",
    "        self.gym_env_out_shape = env.env.observation_space.shape[1:]\n",
    "\n",
    "    def initial(self, model_net):\n",
    "        reward_shape = 2 if self.flags.reward_type == 1 else 1\n",
    "        action_shape = 4 if self.flags.flex_t else 3\n",
    "        self.last_action = torch.zeros(self.env_n, action_shape, dtype=torch.long, device=self.device)\n",
    "        self.episode_return = torch.zeros(1, self.env_n, reward_shape, dtype=torch.float32, device=self.device)\n",
    "        self.episode_step = torch.zeros(1, self.env_n, dtype=torch.long, device=self.device)\n",
    "\n",
    "        model_out, gym_env_out = self.env.reset(model_net) \n",
    "        model_out = model_out.unsqueeze(0)\n",
    "        gym_env_out = gym_env_out.unsqueeze(0)\n",
    "\n",
    "        ret = EnvOut(\n",
    "            gym_env_out=gym_env_out,\n",
    "            model_out=model_out,\n",
    "            see_mask=torch.rand(size=(1, self.env_n), device=self.device) > (1 - self.actor_see_p),\n",
    "            reward=torch.zeros(1, self.env_n, reward_shape, dtype=torch.float32, device=self.device),\n",
    "            done=torch.ones(1, self.env_n, dtype=torch.bool, device=self.device),\n",
    "            truncated_done=None,\n",
    "            episode_return=self.episode_return,\n",
    "            episode_step=self.episode_step,\n",
    "            cur_t=torch.zeros(1, self.env_n, dtype=torch.long, device=self.device),\n",
    "            last_action=self.last_action.unsqueeze(0),\n",
    "            max_rollout_depth=torch.zeros(1, self.env_n, dtype=torch.long, device=self.device)\n",
    "        )\n",
    "        return ret, None\n",
    "\n",
    "    def step(self, action, model_net=None, model_state=None):\n",
    "        action_shape = 4 if self.flags.flex_t else 3\n",
    "        assert action.shape == (1, self.env_n, action_shape), (\n",
    "            \"shape of action should be (1, B, %d)\" % action_shape)\n",
    "        out, reward, done, info = self.env.step(action[0], model_net)     \n",
    "        model_out, gym_env_out = out\n",
    "        model_out = model_out.unsqueeze(0)\n",
    "        gym_env_out = gym_env_out.unsqueeze(0)\n",
    "\n",
    "        self.episode_step += 1\n",
    "        self.episode_return = self.episode_return + reward.unsqueeze(0)\n",
    "\n",
    "        episode_step = self.episode_step.clone()\n",
    "        episode_return = self.episode_return.clone()\n",
    "\n",
    "        cur_t = info[\"cur_t\"]\n",
    "        self.episode_step[:, done] = 0.\n",
    "        self.episode_return[:, done] = 0.\n",
    "        self.episode_return[:, cur_t==0, 1] = 0.\n",
    "        self.last_action[cur_t==0] = action[0, cur_t==0]\n",
    "        self.last_action[:, 1:] = action[0, :, 1:]\n",
    "\n",
    "        ret = EnvOut(\n",
    "            gym_env_out=gym_env_out,\n",
    "            model_out=model_out,\n",
    "            see_mask=torch.rand(size=(1, self.env_n), device=self.device) > (1 - self.actor_see_p),\n",
    "            reward=reward.unsqueeze(0),\n",
    "            done=done.unsqueeze(0),\n",
    "            truncated_done=None,\n",
    "            episode_return=episode_return,\n",
    "            episode_step=episode_step,\n",
    "            cur_t=cur_t.unsqueeze(0),\n",
    "            last_action=self.last_action.unsqueeze(0),\n",
    "            max_rollout_depth=info[\"max_rollout_depth\"].unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        return (ret, model_state)\n",
    "\n",
    "    def seed(self, seed):\n",
    "        self.env.seed(seed)\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "flags = util.parse([])\n",
    "flags.rec_t = 5\n",
    "flags.flex_t = True\n",
    "\n",
    "model_net = ModelNet((3,80,80), 5, flags)\n",
    "_ = model_net.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "\n",
      "    s1: 30.775856ms +- 7.364088ms (89.84%) \n",
      "    s2: 3.395637ms +- 0.956232ms (9.91%) \n",
      "    s0: 0.085134ms +- 0.017447ms (0.25%) \n",
      "Total: 34.256627ms\n"
     ]
    }
   ],
   "source": [
    "from thinker.util import Timings\n",
    "import thinker.env\n",
    "\n",
    "\n",
    "def check_diff(env_out, env_out_, n):\n",
    "    check = ['gym_env_out', 'model_out', 'reward', 'done', \n",
    "        'episode_return', 'episode_step', 'cur_t', 'last_action',\n",
    "        'max_rollout_depth']\n",
    "    for c in check:\n",
    "        d = torch.sum(torch.abs(getattr(env_out, c)[:,[0]].cpu().float() - getattr(env_out_, c).cpu().float()))\n",
    "        if d > 1e-6:\n",
    "            if not c == 'gym_env_out':\n",
    "                print(getattr(env_out, c)[[0]], getattr(env_out_, c))\n",
    "            else:\n",
    "                gplot(getattr(env_out, 'gym_env_out')[0,0])\n",
    "                gplot(getattr(env_out_, 'gym_env_out')[0,0])\n",
    "            raise Exception(\"%d check not passed: %s %f\" %(n, c, d))\n",
    "\n",
    "torch.set_printoptions(precision=5)\n",
    "env_n = 16\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model_net_ = ModelNet((3,80,80), 5, flags)\n",
    "model_net_.load_state_dict({k: v.cpu() for k, v in model_net.state_dict().items()})\n",
    "model_net_.train(False)\n",
    "\n",
    "env = AsyncVectorEnv([lambda: TransposeWrap((gym.make(\"cSokoban-v0\"))) for _ in range(env_n)])\n",
    "env = PostVecModelWrapper(VecModelWrapper(env, env_n, flags, device=device), flags, device=device)\n",
    "env.seed(np.arange(env_n))\n",
    "model_net.to(device)\n",
    "env_out, _ = env.initial(model_net)\n",
    "\n",
    "env_ = TransposeWrap((gym.make(\"cSokoban-v0\")))\n",
    "env_ = thinker.env.ModelWrapper(env_, flags)\n",
    "env_ = thinker.env.PostWrapper(env_, True, flags)\n",
    "env_.seed(0)\n",
    "env_out_, _ = env_.initial(model_net_)\n",
    "check_diff(env_out, env_out_, 0)\n",
    "\n",
    "timings = Timings()\n",
    "\n",
    "for n in range(1000):\n",
    "    timings.reset()\n",
    "    action = torch.tensor([[np.random.randint(5),\n",
    "                            np.random.randint(5),\n",
    "                            np.random.randint(2),\n",
    "                            np.random.randint(2)\n",
    "                            ] for _ in range(env_n)])\n",
    "    timings.time(\"s0\")\n",
    "    env_out, _ = env.step(action.unsqueeze(0).to(device), model_net, None)\n",
    "    timings.time(\"s1\")\n",
    "    env_out_, _ = env_.step(action.unsqueeze(0)[:,[0]], model_net_, None)\n",
    "    timings.time(\"s2\")\n",
    "    check_diff(env_out, env_out_, n+1)\n",
    "    if n % 50 == 0: print(n)\n",
    "\n",
    "print(timings.summary())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinker.env\n",
    "torch.set_printoptions(precision=5)\n",
    "env_n = 2\n",
    "\n",
    "env = AsyncVectorEnv([lambda: TransposeWrap((gym.make(\"cSokoban-v0\"))) for _ in range(env_n)])\n",
    "env.seed([0,0])\n",
    "env = VecModelWrapper(env, env_n, flags)\n",
    "model_out, gym_env_out = env.reset(model_net)\n",
    "\n",
    "env_ = TransposeWrap((gym.make(\"cSokoban-v0\")))\n",
    "env_.seed(0)\n",
    "env_ = thinker.env.ModelWrapper(env_, flags)\n",
    "(gym_env_out_, model_out_), _ = env_.reset(model_net)\n",
    "\n",
    "print(\"initial diff \", torch.sum(torch.absolute(model_out_-model_out[0])))\n",
    "\n",
    "for n in range(100000):\n",
    "    action = torch.tensor([[np.random.randint(5),\n",
    "                            np.random.randint(5),\n",
    "                            np.random.randint(2),\n",
    "                            np.random.randint(2)\n",
    "                            ] for _ in range(env_n)])\n",
    "    action_1 = action.clone()\n",
    "    (model_out, gym_env_out), reward, done, info = env.step(action, model_net)\n",
    "    action_2 = action.clone()\n",
    "    (gym_env_out_, model_out_), reward_, done_, info_, _ = env_.step(action[0].numpy(), model_net, None)\n",
    "    action_3 = action.clone()\n",
    "    if done_: (gym_env_out_, model_out_), _ = env_.reset(model_net)\n",
    "\n",
    "    d_model_out = torch.sum(torch.absolute(model_out_- model_out[0]))\n",
    "    d_gym_env_out = torch.sum(torch.absolute(torch.tensor(gym_env_out_) - gym_env_out[0]))\n",
    "    d_reward = torch.sum(torch.absolute(torch.tensor(reward_) - reward[0]))\n",
    "    d_done = torch.sum(torch.absolute(torch.tensor(done_, dtype=torch.float32) - done[0].float()))\n",
    "\n",
    "    if d_model_out > 1e-5 or d_gym_env_out > 1 or d_reward > 1e-7 or d_done > 0.:\n",
    "        raise Exception(\"%d unmatched output; d_model_out: %.6f d_gym_env_out %.6f d_reward %.6f d_done %.6f\" % (\n",
    "            n, d_model_out, d_gym_env_out, d_reward, d_done))\n",
    "    \n",
    "    #if n % 50 == 0: print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actor network size:  660158\n",
      "model network size:  2352882\n"
     ]
    }
   ],
   "source": [
    "from thinker.env import Environment\n",
    "from thinker.net import ModelNet\n",
    "\n",
    "flags = util.parse([])\n",
    "flags.actor_see_p = 0\n",
    "flags.actor_drc = True\n",
    "flags.rec_t = 5\n",
    "\n",
    "env = Environment(flags)\n",
    "model_net = ModelNet((3,80,80), 5, flags)\n",
    "env_out, model_state = env.initial(model_net)\n",
    "actor_net = ActorNet(env.model_out_shape, 5, flags)\n",
    "\n",
    "nc = actor_net\n",
    "print(\"actor network size: \", sum(p.numel() for p in nc.parameters()))\n",
    "nc = model_net\n",
    "print(\"model network size: \", sum(p.numel() for p in nc.parameters()))\n",
    "\n",
    "a = torch.tensor([1,1,1]).long().unsqueeze(0).unsqueeze(0)\n",
    "actor_state = actor_net.initial_state(1)\n",
    "\n",
    "for n in range(5):\n",
    "   env_out, model_state = env.step(a, model_net, model_state)\n",
    "   actor_out, actor_state = actor_net(env_out, actor_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SokobanEnv' object has no attribute 'baseline_max_q'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_248260/1384819550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgym_env_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot_gym_env_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msee_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_max_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mactor_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             )\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             )\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;34m\"attempted to get missing private attribute '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             )\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SokobanEnv' object has no attribute 'baseline_max_q'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.tensor([4,2,0]).long().unsqueeze(0).unsqueeze(0)\n",
    "env_out, model_state = env.step(a, model_net, model_state)\n",
    "cur_t = env_out.cur_t\n",
    "x = env_out.gym_env_out[0]\n",
    "plot_gym_env_out(x)\n",
    "print(env_out.cur_t, env_out.see_mask, env.env.baseline_max_q)\n",
    "device = torch.device(\"cuda\")\n",
    "actor_net.to(device)\n",
    "actor_state = util.tuple_map(actor_state, lambda x: x.to(device))\n",
    "env_out = util.tuple_map(env_out, lambda x: x.to(device))\n",
    "\n",
    "out, actor_state = actor_net(env_out, actor_state)\n",
    "print([x.dtype for x in out[0] if x is not None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 1 1 1]\n",
      "[False False  True False False False]\n",
      "[nan]\n",
      "[6 7 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "t = 4\n",
    "n = 1\n",
    "next_ind = np.array([3, np.nan, np.nan])\n",
    "\n",
    "base_ind = 2 * n\n",
    "base_ind_pri = t * base_ind\n",
    "\n",
    "abs_flat_inds = np.array([6 + base_ind_pri, 5 + base_ind_pri])\n",
    "\n",
    "# compute the correct index to update; if the indexes are across blocks,\n",
    "# we have to carry the overflowing index to the next block\n",
    "\n",
    "flat_inds = abs_flat_inds - base_ind_pri # get the relative index\n",
    "mask = flat_inds > 0 \n",
    "flat_inds = flat_inds[mask] \n",
    "\n",
    "flat_inds = flat_inds[:, np.newaxis] + np.arange(k) # flat_inds now stores uncarried indexes\n",
    "flat_inds_block = flat_inds // (t * n) # block index of flat_inds\n",
    "carry_mask = ~(flat_inds_block[:,[0]] == flat_inds_block).reshape(-1) \n",
    "# if first index block is not the same as the later index block, we need to carry it\n",
    "\n",
    "flat_inds = flat_inds.reshape(-1)\n",
    "flat_inds_block = flat_inds_block.reshape(-1)\n",
    "carry_inds_block = next_ind[flat_inds_block[carry_mask]-1] - base_ind // n  # the correct index block\n",
    "\n",
    "flat_inds = flat_inds.astype(float)\n",
    "flat_inds[carry_mask] = flat_inds[carry_mask] + (-flat_inds_block[carry_mask] + carry_inds_block) * (t * n) \n",
    "mask = ~np.isnan(flat_inds)\n",
    "flat_inds = flat_inds[mask].astype(int)\n",
    "print(flat_inds_block)\n",
    "print(carry_mask)\n",
    "print(carry_inds_block)\n",
    "print(flat_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flags = util.parse([])\n",
    "flags.model_batch_size = 2\n",
    "flags.model_buffer_n = 1000\n",
    "flags.model_warm_up_n = 500\n",
    "flags.model_batch_mode = False\n",
    "flags.model_rnn = True\n",
    "flags.model_unroll_length = 8\n",
    "t = flags.model_unroll_length   \n",
    "flags.model_k_step_return = 5\n",
    "k = flags.model_k_step_return\n",
    "flags.actor_parallel_n = 1\n",
    "n = flags.actor_parallel_n  \n",
    "\n",
    "flags.model_batch_mode = True\n",
    "\n",
    "P = namedtuple(\"P\", [\"x\",\"y\"])\n",
    "model_buffer = ModelBuffer(flags)\n",
    "\n",
    "c = 0\n",
    "for c in range(100):\n",
    "    data = P(torch.full((t+k, n, 1),c), torch.full((t+k, n, 1),c+0.1))    \n",
    "    state = (torch.full((n, 3),c+0.2), torch.full((n, 3),c+0.3))\n",
    "    model_buffer.write(data, state, np.random.randint(10))\n",
    "\n",
    "print(model_buffer.next_inds)\n",
    "data, data_state, weights, abs_flat_inds, inds = model_buffer.read(1)\n",
    "print(data.x[:, :, -1], data_state)\n",
    "\n",
    "state = tuple(torch.zeros_like(x) for x in data_state)\n",
    "priorities = torch.zeros(len(abs_flat_inds))\n",
    "\n",
    "model_buffer.update_priority(abs_flat_inds, priorities, state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network size:  841223\n",
      "network size:  2352882\n"
     ]
    }
   ],
   "source": [
    "from thinker.env import Environment\n",
    "from thinker.net import ModelNet\n",
    "\n",
    "flags = util.parse([])\n",
    "flags.model_rnn = True\n",
    "\n",
    "model_net_1 = ModelNet((3,80,80), 5, flags)\n",
    "nc = model_net_1\n",
    "print(\"network size: \", sum(p.numel() for p in nc.parameters()))\n",
    "\n",
    "flags.model_rnn = False\n",
    "model_net_2 = ModelNet((3,80,80), 5, flags)\n",
    "nc = model_net_2\n",
    "print(\"network size: \", sum(p.numel() for p in nc.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0005,  0.0253,  0.0640,  0.0987, -0.0419],\n",
      "         [ 0.0010,  0.0284,  0.0646,  0.0968, -0.0359],\n",
      "         [ 0.0019,  0.0287,  0.0614,  0.0948, -0.0363],\n",
      "         [ 0.0028,  0.0284,  0.0636,  0.0960, -0.0396]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor([[[ 0.0005,  0.0253,  0.0640,  0.0987, -0.0419],\n",
      "         [ 0.0010,  0.0284,  0.0646,  0.0968, -0.0359],\n",
      "         [ 0.0019,  0.0287,  0.0614,  0.0948, -0.0363],\n",
      "         [ 0.0028,  0.0284,  0.0636,  0.0960, -0.0396]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tran = {\"conv1\":  \"output_rvpi.conv1\",\n",
    "        \"conv2\":  \"output_rvpi.conv2\",\n",
    "        \"policy\": \"output_rvpi.fc_logits\",\n",
    "        \"baseline\": \"output_rvpi.fc_v\",\n",
    "        \"r\": \"output_rvpi.fc_r\",\n",
    "        \"frame_conv.0\":  \"output_rvpi.conv1\",\n",
    "        \"frame_conv.2\":  \"output_rvpi.conv2\",}\n",
    "\n",
    "state_dict = {}\n",
    "for k in model_net_1.state_dict().keys():\n",
    "    #print(k)\n",
    "    if k not in model_net_2.state_dict().keys():\n",
    "        for p, q in tran.items():\n",
    "            if k[:len(p)+1] == p+\".\":\n",
    "                out = q + k[len(p):]\n",
    "                break\n",
    "    else:\n",
    "        out = k\n",
    "    state_dict[k] = model_net_2.state_dict()[out]\n",
    "\n",
    "model_net_1.load_state_dict(state_dict)\n",
    "model_net_1.train(False)\n",
    "model_net_2.train(False)\n",
    "\n",
    "x = torch.rand(4, 3, 80, 80) * 2555\n",
    "actions = torch.zeros(1, 4, dtype=torch.long)\n",
    "done = torch.zeros(1, 4, dtype=torch.bool)\n",
    "\n",
    "state = model_net_1.init_state(4)\n",
    "vs, logits, states = model_net_1(x=x.unsqueeze(0), actions=actions, done=done, state=state)\n",
    "print(logits)\n",
    "\n",
    "rs, vs, logits, encodeds = model_net_2(x=x, actions=actions)\n",
    "print(logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f50a7fa60ad8550b89b217983de73aa91b7ad4da24a2c984b86370b087d0b88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
