{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath('thinker/thinker')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from collections import deque\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from thinker.env import Environment\n",
    "from thinker.net import ActorNet, ModelNet\n",
    "import gym\n",
    "import thinker.util as util\n",
    "from visual import *\n",
    "%matplotlib inline\n",
    "\n",
    "def load_env_model(check_point_path, seed=1):\n",
    "\n",
    "    checkpoint = torch.load(os.path.join(check_point_path,'ckp_model.tar'), torch.device(\"cpu\"))\n",
    "    flags = util.parse(['--load_checkpoint', check_point_path])\n",
    "    flags.actor_see_type = 0\n",
    "    flags_ = copy.deepcopy(flags)\n",
    "\n",
    "    #flags_.actor_see_p = 1\n",
    "    #flags_.actor_see_encode = True\n",
    "    #flags_.actor_see_h = True\n",
    "    #flags_.max_depth = 20\n",
    "    env = Environment(flags_, model_wrap=True)\n",
    "\n",
    "    env.seed([seed])\n",
    "    num_actions = env.num_actions\n",
    "\n",
    "    flags_ = copy.deepcopy(flags)\n",
    "    flags_.perfect_model = True\n",
    "    #flags_.actor_see_p = 1\n",
    "    #flags_.max_depth = 20\n",
    "    perfect_env = Environment(flags_, model_wrap=True)\n",
    "    perfect_env.seed([seed])\n",
    "    \n",
    "\n",
    "    model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "    model_net.train(False)\n",
    "    model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                        checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "\n",
    "    env_out = env.initial(model_net)\n",
    "    perfect_env_out = perfect_env.initial(model_net)\n",
    "\n",
    "    util.decode_model_out(env_out.model_out, num_actions, True)    \n",
    "    #plot_gym_env_out(env_out.gym_env_out.squeeze(0))\n",
    "    #_ = decode(model_net, env_out, visualize=True)\n",
    "\n",
    "    return model_net, env, perfect_env, env_out, perfect_env_out, flags\n",
    "\n",
    "def step_reals(actions, env, perfect_env, model_net, flags):\n",
    "    for a in actions:\n",
    "        while(True):\n",
    "            action = torch.tensor([[[a, 0, 0]]], dtype=torch.long)    \n",
    "            env_out = env.step(action, model_net)            \n",
    "            perfect_env_out = perfect_env.step(action, model_net)                    \n",
    "            sp = torch.sum(torch.abs(env_out.model_encodes) < 1e-4)/(torch.numel(env_out.model_encodes))\n",
    "            if env_out.cur_t[0,0] == 0: break\n",
    "    return env_out, perfect_env_out, sp\n",
    "\n",
    "def tb_out(model_net_out):\n",
    "    d = {}\n",
    "    for k in model_net_out._fields:\n",
    "        if type(getattr(model_net_out,k)) == torch.Tensor:\n",
    "            d[k] = getattr(model_net_out,k).squeeze(0).unsqueeze(1)\n",
    "    return util.construct_tuple(type(model_net_out), **d)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/sokoban_v5/v5_duel_net_100_imgc_done_g/'\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/breakout_v2/v2_duel_net_10_imgc_done'\n",
    "check_point_path = '/media/sc/datadisk/data/thinker/logs/v7/v7_duel_net_1img_type'\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/breakout_v2/v2_duel_net_1img_type_b/'\n",
    "model_net, env, perfect_env, env_out, perfect_env_out, flags = load_env_model(check_point_path, seed=1)\n",
    "\n",
    "#actions = np.random.randint(low=0, high=5, size=(6))\n",
    "#actions = [3,3,3,3,4,4,4,2]\n",
    "actions = [3,3,3,3,3,3,3,3,3,3,3,3]\n",
    "actions = torch.tensor(actions).unsqueeze(-1)\n",
    "#model_net_out = model_net(env_out.gym_env_out, actions=actions, one_hot=False, compute_true_z=False, inference=True)\n",
    "model_net_out = model_net(env_out.gym_env_out[0], actions=actions, one_hot=False)\n",
    "pred_xs = torch.concat([model_net_out.ys], dim=0)\n",
    "#pred_xs = torch.concat([env_out.gym_env_out[[0]].float()/255., model_net_out.pred_xs], dim=0)\n",
    "plot_env_outs = np.clip(pred_xs[:,0].detach().cpu().numpy()*255,0,255)\n",
    "_ = plot_multi_gym_env_out(plot_env_outs, titles=None, col_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the model learns\n",
    "\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/breakout_v2/value_prefix/'\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/breakout_v2/supervise_8'\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/sokoban_v5/v5_l2_loss_sep_zh_d'\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/sokoban_v5/v5_img_loss_0.01'\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/sokoban_v5/v5_cos_loss_sep_zh_b'\n",
    "check_point_path = '/media/sc/datadisk/data/thinker/logs/sokoban_v5/v5_duel_net_100_imgc_done_g'\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/breakout_v2/v2_duel_net_10_imgc_done'\n",
    "check_point_path = '/media/sc/datadisk/data/thinker/logs/v7/v7_duel_net_1img_type'\n",
    "check_point_path = os.path.expanduser(check_point_path)\n",
    "model_net, env, perfect_env, env_out, perfect_env_out, flags = load_env_model(check_point_path)\n",
    "num_actions = env.num_actions\n",
    "\n",
    "actions = [4,2,3,3,2,3,1,4,4,4,4,1,4,4,4,2,2,3,3,3,4,1,2,2,4,4,4,1,1,3,3,3,3,3,3,4,4,4,4,4,4,4,4,2,3]\n",
    "env_out, perfect_env_out, sp = step_reals(actions, env, perfect_env, model_net, flags)\n",
    "\n",
    "#im_actions = [2 for _ in range(20)]\n",
    "#im_actions = [3,2,2,3,2,2,2,2]\n",
    "im_actions = [3,3,3,3,3,2,4,1,2,3,4]\n",
    "model_outs, perfect_model_outs = {}, {}\n",
    "gym_env_outs = []\n",
    "rec_env_outs = []\n",
    "\n",
    "def print_info(n, env_out, perfect_env_out, num_actions):\n",
    "    model_out = util.decode_model_out(env_out.model_out, num_actions, True)\n",
    "    perfect_model_out = util.decode_model_out(perfect_env_out.model_out, num_actions, True)\n",
    "    sp = torch.sum(torch.abs(env_out.model_encodes) < 1e-4)/(torch.numel(env_out.model_encodes))\n",
    "    #z_diff = torch.mean(torch.square(env_out.model_encodes - perfect_env_out.model_encodes))\n",
    "    z_diff = torch.mean(torch.square(model_out[\"raw\"] - perfect_model_out[\"raw\"]))\n",
    "    #z_diff = 0.\n",
    "    print((\"step: %d; z_diff %f, im reward: %f (p: %f) root_v: %f (p: %f) max_v: %f (p: %f)\" +\n",
    "          \"trail_q:%f (p: %f) max_q: %f (p: %f) mean_q: %f (p: %f) sparsity: %f\") % (\n",
    "        n, z_diff,\n",
    "        env_out.reward[0,0,1], perfect_env_out.reward[0,0,1], \n",
    "        model_out[\"root_v\"], perfect_model_out[\"root_v\"],\n",
    "        model_out[\"root_max_v\"], perfect_model_out[\"root_max_v\"],\n",
    "        model_out[\"root_trail_q\"], perfect_model_out[\"root_trail_q\"],\n",
    "        torch.max(model_out[\"root_qs_max\"]), torch.max(perfect_model_out[\"root_qs_max\"]),\n",
    "        torch.mean(model_out[\"root_qs_mean\"]), torch.mean(perfect_model_out[\"root_qs_mean\"]), sp))\n",
    "\n",
    "def decode(model_net, env_out, visualize):\n",
    "    model_encodes = env_out.model_encodes\n",
    "    if model_net.flags.duel_net:\n",
    "        rec_env_out = model_encodes.squeeze(0) \n",
    "    else:\n",
    "        return None\n",
    "    rec_env_out = rec_env_out.squeeze(0).detach()\n",
    "    rec_env_out = (torch.clip(rec_env_out, 0, 1) * 255).int()\n",
    "    if visualize: plot_gym_env_out(rec_env_out.unsqueeze(0))\n",
    "    return rec_env_out\n",
    "\n",
    "print_info(0, env_out, perfect_env_out, num_actions)\n",
    "for n, im_action in enumerate(im_actions):\n",
    "    action = torch.tensor([[[0, im_action, 0]]], dtype=torch.long)\n",
    "    env_out = env.step(action, model_net)    \n",
    "    model_out = util.decode_model_out(env_out.model_out, num_actions, True)        \n",
    "    perfect_env_out = perfect_env.step(action, model_net)          \n",
    "    perfect_model_out = util.decode_model_out(perfect_env_out.model_out, num_actions, True)  \n",
    "\n",
    "    gym_env_outs.append(perfect_env_out.gym_env_out.squeeze(0).squeeze(0))\n",
    "    rec_env_out = decode(model_net, env_out, visualize=False)\n",
    "    if rec_env_out is not None: rec_env_outs.append(rec_env_out)\n",
    "\n",
    "    #pred_encodes = model_net.frameEncoder(rec_env_out.unsqueeze(0), F.one_hot(action[0, :, 1], num_classes=5))[0]\n",
    "    #z_diff = torch.mean(torch.abs(pred_encodes - perfect_env_out.model_encodes))\n",
    "    #print(z_diff)\n",
    "\n",
    "    print_info(n+1, env_out, perfect_env_out, num_actions)\n",
    "    #assert torch.all(model_out[\"root_logits\"] == perfect_model_out[\"root_logits\"])\n",
    "\n",
    "    for outs, out in [[model_outs, model_out], [perfect_model_outs, perfect_model_out]]:\n",
    "        for k, v in out.items():\n",
    "            if k not in outs:\n",
    "                outs[k] = [v]\n",
    "            else:\n",
    "                outs[k].append(v)\n",
    "    \n",
    "titles = []\n",
    "plot_env_outs = []\n",
    "for n in range(len(model_outs[\"cur_r\"])):\n",
    "    titles.append(\"t:%d, r:%.2f,v:%.2f;pred-r:%.2f v:%.2f\" % (\n",
    "        n+1, perfect_model_outs[\"cur_r\"][n],\n",
    "        perfect_model_outs[\"cur_v\"][n], model_outs[\"cur_r\"][n], model_outs[\"cur_v\"][n]))    \n",
    "    plot_env_outs.append(gym_env_outs[n])\n",
    "    if len(rec_env_outs) > 0:\n",
    "        l2_loss = torch.mean(torch.square((rec_env_outs[n]-gym_env_outs[n]).float()/255.))\n",
    "        titles.append(\"t:%d loss:%f\" % (n+1,l2_loss))\n",
    "        plot_env_outs.append(rec_env_outs[n])\n",
    "    \n",
    "\n",
    "fig = plot_multi_gym_env_out(plot_env_outs, titles=titles, col_n=4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = util.parse(['--load_checkpoint', check_point_path])\n",
    "env = Environment(flags, model_wrap=False)\n",
    "env_out = env.initial()\n",
    "xs = [env_out.gym_env_out]\n",
    "actions = [0,1,1,2,2]\n",
    "for a in actions[1:]:\n",
    "    a = torch.tensor(a, dtype=torch.long).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    env_out = env.step(a)\n",
    "    xs.append(env_out.gym_env_out)\n",
    "\n",
    "xs = torch.concat(xs, dim=0)\n",
    "actions = torch.tensor(actions, dtype=torch.long).unsqueeze(-1)\n",
    "model_net_out = model_net(xs, \n",
    "                          actions=actions, \n",
    "                          compute_true_z=True, \n",
    "                          one_hot=False, \n",
    "                          inference=False)\n",
    "print(\"zs L2 error: %f\" % torch.mean(torch.square(model_net_out.true_zs[1:] - model_net_out.pred_zs[1:])))\n",
    "\n",
    "perfect_model_net_out = tb_out(model_net(xs.flatten(0,1).unsqueeze(0), \n",
    "                               actions.flatten(0,1).unsqueeze(0), one_hot=False))\n",
    "     \n",
    "print(\"zs L2 error: %f\" % torch.mean(torch.square(perfect_model_net_out.true_zs[1:] - model_net_out.pred_zs[1:])))\n",
    "print(\"logit error: %f\" % torch.mean(torch.square(perfect_model_net_out.logits - model_net_out.logits)))\n",
    "print(\"v error: %f\" % torch.mean(torch.square(perfect_model_net_out.vs - model_net_out.vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned actor network\n",
    "\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/breakout_v2/supervise_10'\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/breakout_v4/v4_large_tran'\n",
    "check_point_path = '/media/sc/datadisk/data/thinker/logs/sokoban_v5/v5_5_depth_2'\n",
    "#check_point_path = '~/RS/thinker/logs/thinker/latest'\n",
    "\n",
    "check_point_path = os.path.expanduser(check_point_path)\n",
    "max_eps_n = 1\n",
    "visualize = True\n",
    "saveimg = False\n",
    "saveimg_dir = \"/home/sc/RS/thinker/test/data/\"\n",
    "\n",
    "flags = util.parse(['--load_checkpoint', check_point_path])\n",
    "name = \"%s-%s\"%(flags.xpid, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "if saveimg:\n",
    "    saveimg_a = os.path.join(saveimg_dir, name, \"a\")\n",
    "    saveimg_b = os.path.join(saveimg_dir, name, \"b\")\n",
    "    if not (os.path.exists(saveimg_a)): os.makedirs(saveimg_a)\n",
    "    if not (os.path.exists(saveimg_b)): os.makedirs(saveimg_b)\n",
    "    print(\"saving images to %s\" % (os.path.join(saveimg_dir, name)))\n",
    "    savef = open(os.path.join(saveimg_dir, name, \"logs.txt\"), 'a')\n",
    "\n",
    "env = Environment(flags, model_wrap=True)\n",
    "if not flags.perfect_model: \n",
    "    flags_ = copy.deepcopy(flags)\n",
    "    flags_.perfect_model = True\n",
    "    flags_.actor_see_p = 1\n",
    "    perfect_env = Environment(flags_, model_wrap=True)\n",
    "\n",
    "if flags.env == \"cSokoban-v0\" or flags.env == \"Sokoban-v0\":\n",
    "    action_meanings = [\"NOOP\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "else:\n",
    "    action_meanings = gym.make(flags.env).get_action_meanings()\n",
    "num_actions = env.num_actions\n",
    "\n",
    "seed = 3\n",
    "env.seed([seed])\n",
    "if not flags.perfect_model: perfect_env.seed([seed])\n",
    "\n",
    "model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "model_net.train(False)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model.tar'), torch.device(\"cpu\"))\n",
    "#checkpoint = torch.load(os.path.join('/media/sc/datadisk/data/thinker/logs/breakout_v2/supervise_8','ckp_model.tar'), torch.device(\"cpu\"))\n",
    "model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "\n",
    "actor_net = ActorNet(obs_shape=env.model_out_shape, gym_obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_actor.tar'), torch.device(\"cpu\"))         \n",
    "actor_net.set_weights(checkpoint[\"actor_net_state_dict\"])\n",
    "actor_state = actor_net.initial_state(batch_size=1) \n",
    "\n",
    "env_out = env.initial(model_net)\n",
    "if not flags.perfect_model: \n",
    "    perfect_env_out = perfect_env.initial(model_net)\n",
    "    assert torch.all(env_out.gym_env_out == perfect_env_out.gym_env_out)\n",
    "\n",
    "gym_env_out_ = env_out.gym_env_out\n",
    "model_out = util.decode_model_out(env_out.model_out, num_actions, flags.reward_transform)\n",
    "end_gym_env_outs, end_titles = [], []\n",
    "\n",
    "step = 0\n",
    "returns, model_logits, attn_output = [], [], [], \n",
    "im_list = [\"im_policy_logits\", \"reset_policy_logits\", \"im_action\", \"reset_action\"]\n",
    "im_dict = {k: [] for k in im_list}\n",
    "\n",
    "video_stats = {\"real_imgs\": [], \"im_imgs\":[], \"status\":[]}\n",
    "video_stats[\"real_imgs\"].append(env_out.gym_env_out[0,0,-3:].numpy())\n",
    "video_stats[\"im_imgs\"].append(video_stats[\"real_imgs\"][-1])\n",
    "video_stats[\"status\"].append(0) # 0 for real step, 1 for reset, 2 for normal\n",
    "\n",
    "if not visualize: plt.ioff()\n",
    "\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    actor_out, actor_state = actor_net(env_out, actor_state)      \n",
    "\n",
    "    if env_out.cur_t[0,0] == 0: agent_v = actor_out.baseline[0,0,0]\n",
    "    action = [actor_out.action, actor_out.im_action, actor_out.reset_action]\n",
    "    action = torch.cat(action, dim=-1).unsqueeze(0)\n",
    "\n",
    "    # additional stat record    \n",
    "    for k in im_list: \n",
    "        im_dict[k].append(getattr(actor_out, k)[:, 0] if k in actor_out._fields and \n",
    "            getattr(actor_out, k) is not None else None)        \n",
    "    #attn_output.append(torch.cat([x.attn_output_weights.unsqueeze(0).unsqueeze(-2) for x in actor_net.core.layers])[:, :, 0, :])        \n",
    "\n",
    "    model_out_ = util.decode_model_out(env_out.model_out, num_actions, flags.reward_transform)\n",
    "    model_logits.append(model_out_[\"cur_logits\"])\n",
    "    env_out  = env.step(action, model_net)\n",
    "    model_out = util.decode_model_out(env_out.model_out, num_actions, flags.reward_transform)\n",
    "    if len(im_dict['reset_action']) > 0: im_dict['reset_action'][-1] = model_out[\"reset\"]\n",
    "    if not flags.perfect_model: perfect_env_out  = perfect_env.step(action, model_net)    \n",
    "    gym_env_out = env_out.gym_env_out if flags.perfect_model else perfect_env_out.gym_env_out\n",
    "    if not flags.perfect_model:\n",
    "        perfect_model_out = util.decode_model_out(perfect_env_out.model_out, num_actions, flags.reward_transform)\n",
    "\n",
    "    if model_out[\"reset\"] == 1 or env_out.cur_t[0,0] == flags.rec_t-1:   \n",
    "        title = \"pred v: %.2f\" % (model_out[\"cur_v\"].item())\n",
    "        if not flags.perfect_model:\n",
    "            title += \" v: %.2f\" % (perfect_model_out[\"cur_v\"].item())\n",
    "        end_gym_env_outs.append(gym_env_out[0,0].numpy())\n",
    "        end_titles.append(title)\n",
    "\n",
    "    if not flags.perfect_model and env_out.cur_t[0,0] == 0:\n",
    "        assert torch.all(env_out.gym_env_out == perfect_env_out.gym_env_out)    \n",
    "\n",
    "    # record data for generating video\n",
    "    if action[0,0,2] == 1:\n",
    "        video_stats[\"real_imgs\"].append(video_stats[\"real_imgs\"][-1])\n",
    "        video_stats[\"im_imgs\"].append(video_stats[\"real_imgs\"][-1])\n",
    "        video_stats[\"status\"].append(1)\n",
    "\n",
    "    if env_out.cur_t[0,0] == 0:\n",
    "        video_stats[\"real_imgs\"].append(gym_env_out[0,0,-3:].numpy())\n",
    "        video_stats[\"status\"].append(0)\n",
    "    else:\n",
    "        video_stats[\"real_imgs\"].append(video_stats[\"real_imgs\"][-1])\n",
    "        video_stats[\"status\"].append(2)\n",
    "    video_stats[\"im_imgs\"].append(gym_env_out[0,0,-3:].numpy())    \n",
    "\n",
    "    # visualize when a real step is made\n",
    "    if (saveimg or visualize) and env_out.cur_t[0,0] == 0:\n",
    "        fig, axs = plt.subplots(1, 5, figsize=(30,6))      \n",
    "        title = \"%d; v: %.2f (%.2f)\" % (step, model_out_[\"root_v\"][0], agent_v)\n",
    "        title += \" max_q: %.2f mean_q_0: %.2f\" % (env.env.baseline_max_q[0], env.env.baseline_mean_q[0])\n",
    "        for k in im_list: \n",
    "            if im_dict[k][0] is not None:\n",
    "                im_dict[k] = torch.concat(im_dict[k], dim=0)            \n",
    "            else:\n",
    "                im_dict[k] = None        \n",
    "        \n",
    "        if flags.reward_type == 1: title += \" im_return: %.4f\" % env_out.episode_return[..., 1]                        \n",
    "        plot_gym_env_out(gym_env_out_[0], axs[0], title=title)  \n",
    "        plot_base_policies(torch.concat(model_logits), action_meanings=action_meanings, ax=axs[1])  \n",
    "        plot_im_policies(**im_dict, action_meanings=action_meanings, one_hot=False, reset_ind=1, ax=axs[2])   \n",
    "\n",
    "        mean_q_s_a = model_out_[\"root_qs_mean\"][0]\n",
    "        max_q_s_a = model_out_[\"root_qs_max\"][0]\n",
    "        n_s_a = model_out_[\"root_ns\"][0]\n",
    "        plot_qn_sa(mean_q_s_a, n_s_a, action_meanings=action_meanings, max_q_s_a=max_q_s_a, ax=axs[3]) \n",
    "\n",
    "        model_policy_logits = model_out_[\"root_logits\"][0]\n",
    "        agent_policy_logits = actor_out.policy_logits[0,0]\n",
    "        action = torch.nn.functional.one_hot(actor_out.action[0,0], env.num_actions)\n",
    "        plot_policies([model_policy_logits, agent_policy_logits, action], \n",
    "            [\"model policy\", \"agent policy\", \"action\"], action_meanings=action_meanings, ax=axs[4])  \n",
    "        \n",
    "        if saveimg: plt.savefig(os.path.join(saveimg_a,\"%d.png\" %step))\n",
    "        if visualize: plt.show()        \n",
    "        plt.close()\n",
    "\n",
    "        fig = plot_multi_gym_env_out(end_gym_env_outs, end_titles)        \n",
    "        if saveimg: plt.savefig(os.path.join(saveimg_b,\"%d.png\" %step))\n",
    "        if visualize: plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        log_str = \"step:%d; return %.4f(%.4f) done %s real_done %s\" % (\n",
    "            step, env_out.episode_return[0,0,0], env_out.episode_return[0,0,1],\n",
    "            \"True\" if env_out.done[0,0] else \"False\", \n",
    "            \"True\" if env_out.real_done[0,0] else \"False\",)\n",
    "        print(log_str)\n",
    "        im_action_strs = print_im_actions(im_dict, action_meanings, print_stat=visualize)           \n",
    "        if saveimg:\n",
    "            for s in im_action_strs: savef.write(s+\"\\n\")            \n",
    "            savef.write(log_str+\"\\n\") \n",
    "\n",
    "        gym_env_out_ = gym_env_out\n",
    "        im_dict = {k: [] for k in im_list}\n",
    "        model_logits, attn_output, end_gym_env_outs, end_titles = [], [], [], []\n",
    "\n",
    "    if torch.any(env_out.real_done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.real_done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n",
    "        \n",
    "savef.close()        \n",
    "\n",
    "# Generate video\n",
    "import cv2\n",
    "\n",
    "file_path = \"/home/sc/RS/thinker/test/v2_supervise_8-20230301-180711.mp4\"\n",
    "imgs = []\n",
    "hw = video_stats[\"real_imgs\"][0].shape[1]\n",
    "\n",
    "for i in range(len(video_stats[\"real_imgs\"])):\n",
    "    img = np.zeros(shape=(hw,hw*2,3),dtype=np.uint8)\n",
    "    real_img = np.copy(video_stats[\"real_imgs\"][i])\n",
    "    real_img = np.swapaxes(np.swapaxes(real_img, 0, 2),0, 1)\n",
    "    im_img = np.copy(video_stats[\"im_imgs\"][i])\n",
    "    im_img = np.swapaxes(np.swapaxes(im_img, 0, 2),0, 1)\n",
    "    if video_stats[\"status\"][i] == 1: \n",
    "        im_img[:, :, 0] = 255 * 0.3 + im_img[:, :, 0] * 0.7\n",
    "        im_img[:, :, 1] = 255 * 0.3 + im_img[:, :, 1] * 0.7\n",
    "    elif video_stats[\"status\"][i] == 0: \n",
    "        im_img[:, :, 2] = 255 * 0.3 + im_img[:, :, 2] * 0.7\n",
    "\n",
    "    img[:,:hw,:] = real_img\n",
    "    img[:,hw:,:] = im_img\n",
    "    img = np.flip(img, 2)\n",
    "    imgs.append(img)\n",
    "\n",
    "width = hw*2\n",
    "hieght = hw\n",
    "channel = 3\n",
    "fps = 15\n",
    " \n",
    "video = cv2.VideoWriter(file_path, cv2.VideoWriter_fourcc(*'mp4v'), float(fps), (width, hieght)) \n",
    "#video = cv2.VideoWriter('RS/thinker/test.mp4', cv2.VideoWriter_fourcc(*'mp4v'), float(fps), (width, hieght)) \n",
    "for img in imgs: video.write(img) \n",
    "video.release()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug; check if perfect model passed to non-perfect env can match perfect env output\n",
    "\n",
    "import torch.nn as nn\n",
    "from thinker.net import ModelNetOut\n",
    "\n",
    "class PefectModel():\n",
    "    def __init__(self, model_net):\n",
    "        self.model_net = model_net   \n",
    "        self.done_a = False     \n",
    "    \n",
    "    def __call__(self, xs, actions, one_hot=False, dec_enc=False, compute_true_z=False, inference=True):\n",
    "        return self.model_net(xs, actions, one_hot, dec_enc, compute_true_z, inference)\n",
    "    \n",
    "    def forward_zh(self, z, h, r_state, action, one_hot=True, dec_enc=False):        \n",
    "        if self.done_a: action = self.action_a\n",
    "        print(\"forward_zh called\")        \n",
    "        b = action.shape[0]\n",
    "        # gym_env should be set to current node before calling\n",
    "        assert not one_hot        \n",
    "        state = self.gym_env.clone_state(inds=[i for i in range(b)])\n",
    "        obs, reward, done, info = self.gym_env.step(action)\n",
    "        self.gym_env.restore_state(state, inds=[i for i in range(b)])\n",
    "        obs = torch.tensor(obs)\n",
    "        reward = torch.tensor(reward)\n",
    "        done = torch.tensor(done, dtype=bool)\n",
    "        done_mask = (~done).float()\n",
    "        model_net_out = self.model_net.forward(xs=obs, actions=action.unsqueeze(0))\n",
    "        self.model_net_out = ModelNetOut(single_rs=(reward).unsqueeze(0) if not self.done_a else torch.zeros(1, b),\n",
    "                           rs=(reward).unsqueeze(0) if not self.done_a else torch.zeros(1, b), \n",
    "                           r_enc_logits=None, \n",
    "                           vs=model_net_out.vs*done_mask, \n",
    "                           v_enc_logits=None, \n",
    "                           logits=model_net_out.logits, \n",
    "                           pred_zs=torch.zeros(1, b),\n",
    "                           pred_z_logits=None,\n",
    "                           true_zs=None,\n",
    "                           true_z_logits=None,\n",
    "                           hs=torch.zeros(1, b), \n",
    "                           r_state=(torch.zeros(b, 1),)\n",
    "                           )\n",
    "        return self.model_net_out \n",
    "    \n",
    "seed = 1    \n",
    "check_point_path = '/media/sc/datadisk/data/thinker/logs/sokoban_v5/v5_cos_loss_sep_zh_b'\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model.tar'), torch.device(\"cpu\"))\n",
    "flags = util.parse(['--load_checkpoint', check_point_path])\n",
    "\n",
    "flags_ = copy.deepcopy(flags)\n",
    "flags_.actor_see_p = 1\n",
    "flags_.actor_see_encode = True\n",
    "flags_.model_dec_enc = False\n",
    "flags_.tree_carry = True\n",
    "env = Environment(flags_, model_wrap=True)\n",
    "\n",
    "env.seed([seed])\n",
    "num_actions = env.num_actions\n",
    "\n",
    "flags_ = copy.deepcopy(flags)\n",
    "flags_.perfect_model = True\n",
    "flags_.actor_see_p = 1\n",
    "flags_.actor_see_h = False    \n",
    "flags_.tree_carry = True\n",
    "perfect_env = Environment(flags_, model_wrap=True)\n",
    "perfect_env.seed([seed])\n",
    "\n",
    "model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "model_net.train(False)\n",
    "model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "perfect_model_net = PefectModel(model_net)\n",
    "\n",
    "import gym\n",
    "from thinker.gym_add.asyn_vector_env import AsyncVectorEnv\n",
    "from thinker.env import PreWrap\n",
    "gym_env = AsyncVectorEnv([lambda: PreWrap(gym.make(flags.env), flags.env) for _ in range(1)])\n",
    "gym_env.seed([seed])\n",
    "obs = gym_env.reset()\n",
    "perfect_model_net.gym_env = gym_env\n",
    "\n",
    "env_out = env.initial(perfect_model_net)\n",
    "perfect_env_out = perfect_env.initial(model_net)\n",
    "\n",
    "root_state = gym_env.clone_state(inds=[0])\n",
    "done_a = False\n",
    "action_a = None\n",
    "np.random.seed(0)\n",
    "for n in range(100000):\n",
    "    action = torch.tensor([[[np.random.randint(5), np.random.randint(5), np.random.randint(2)]]], dtype=torch.long)\n",
    "\n",
    "    perfect_model_net.done_a = done_a\n",
    "    perfect_model_net.action_a = action_a\n",
    "    env_out = env.step(action, perfect_model_net)\n",
    "    perfect_env_out = perfect_env.step(action, model_net)\n",
    "    diff = torch.sum(torch.square(env_out.model_out - perfect_env_out.model_out))\n",
    "    print(\"step: %d (%d) diff: %f\" % (n+1, env_out.cur_t, diff))\n",
    "    if diff > 1e-6:\n",
    "        raise Exception(\"unmatch\")    \n",
    "\n",
    "    if env_out.cur_t == 0:\n",
    "        gym_env.restore_state(root_state, inds=[0])\n",
    "        obs, reward, done, info = gym_env.step(action[0,:,0])\n",
    "        if done: gym_env.reset(inds=[0])\n",
    "        root_state = gym_env.clone_state(inds=[0])\n",
    "        done_a = False\n",
    "    else:\n",
    "        if util.decode_model_out(env_out.model_out, num_actions, reward_tran=True)[\"reset\"]:\n",
    "            gym_env.restore_state(root_state, inds=[0])\n",
    "            obs = None\n",
    "            done_a = False\n",
    "        else:\n",
    "            if not done_a:\n",
    "                state_ = gym_env.clone_state(inds=[0])\n",
    "                obs, reward, done, info = gym_env.step(action[0,:,1])\n",
    "                if done: gym_env.restore_state(state_, inds=[0])\n",
    "                done_a = done   \n",
    "                action_a = action[0,:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize plain model policy\n",
    "\n",
    "check_point_path = '~/RS/thinker/models/test'\n",
    "check_point_path = os.path.expanduser(check_point_path)\n",
    "max_eps_n = 100\n",
    "visualize = False\n",
    "\n",
    "flags = util.parse([])\n",
    "env = Environment(flags, model_wrap=False)\n",
    "env_out = env.initial()\n",
    "model_net_rnn = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags, rnn=True)\n",
    "model_net_rnn.train(False)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model_rnn.tar'), torch.device(\"cpu\"))                    \n",
    "model_net_rnn.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "state = model_net_rnn.core.init_state(bsz=1)\n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    _, model_rnn_logits, state = model_net_rnn(env_out.gym_env_out, env_out.last_action[:,:,0], env_out.done, state)\n",
    "    action = torch.multinomial(F.softmax(model_rnn_logits[0], dim=1), num_samples=1).unsqueeze(0)\n",
    "    env_out = env.step(action) \n",
    "\n",
    "    if visualize:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(6,3))  \n",
    "        plot_gym_env_out(env_out.gym_env_out[0], ax=axs[0], title=\"Step %d:v(s)=%.4f\" % (step, vs[0]))\n",
    "        plot_policies(model_rnn_logits[0,0], None, ax=axs[1])\n",
    "        plt.show()\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize plain model policy\n",
    "\n",
    "model_path = '/media/sc/datadisk/data/thinker/logs/planner_logs/thinker_model_no_pri/ckp_model.tar'\n",
    "max_eps_n = 10\n",
    "visualize = False\n",
    "\n",
    "flags = util.parse([])\n",
    "env = Environment(flags, model_wrap=False)\n",
    "env_out = env.initial()\n",
    "model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "model_net.train(False)\n",
    "checkpoint = torch.load(model_path, torch.device(\"cpu\"))\n",
    "model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    _, vs, policy_logits, _ = model_net(env_out.gym_env_out[0], env_out.last_action[:,:,0], one_hot=False)                            \n",
    "    action = torch.multinomial(F.softmax(policy_logits[0], dim=1), num_samples=1).unsqueeze(0)\n",
    "    env_out = env.step(action) \n",
    "\n",
    "    if visualize:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(6,3))  \n",
    "        plot_gym_env_out(env_out.gym_env_out[0], ax=axs[0], title=\"Step %d:v(s)=%.4f\" % (step, vs[0]))\n",
    "        plot_policies(policy_logits[0,0], None, ax=axs[1])\n",
    "        plt.show()\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Misc.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "from thinker.self_play import SelfPlayWorker\n",
    "from thinker.learn_actor import ActorLearner\n",
    "from thinker.buffer import *\n",
    "import thinker.util as util\n",
    "from thinker.net import *\n",
    "\n",
    "flags = util.parse([])\n",
    "flags.model_batch_size = 2\n",
    "flags.model_unroll_length = 8\n",
    "flags.model_k_step_return = 5\n",
    "flags.actor_parallel_n = 4\n",
    "flags.model_buffer_n = 1000\n",
    "flags.model_warm_up_n = 500\n",
    "flags.model_batch_mode = True\n",
    "flags.priority_alpha = 5\n",
    "\n",
    "t = flags.model_unroll_length   \n",
    "k = flags.model_k_step_return\n",
    "n = flags.actor_parallel_n  \n",
    "\n",
    "P = namedtuple(\"P\", [\"x\",\"y\"])\n",
    "model_buffer = ModelBuffer(flags)\n",
    "\n",
    "c = 0\n",
    "for c in range(100):\n",
    "    data = P(torch.full((t+k, n, 1),2*c), torch.full((t+k, n, 1),2*c+1))    \n",
    "    model_buffer.write(data)\n",
    "    r = model_buffer.read(1.)    \n",
    "    if r is not None:\n",
    "        data, weights, abs_flat_inds, ps_step = r\n",
    "        #print(data.x[:,:,0]) \n",
    "        #model_buffer.update_priority(abs_flat_inds, np.zeros(flags.model_batch_size))\n",
    "\n",
    "print(\"1 read\", data.x[:,:,0], weights)\n",
    "model_buffer.update_priority(abs_flat_inds, np.array([10, 0]))\n",
    "data, weights, abs_flat_inds, ps_step = model_buffer.read(1.)   \n",
    "print(\"2 read\", data.x[:,:,0], weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f50a7fa60ad8550b89b217983de73aa91b7ad4da24a2c984b86370b087d0b88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
