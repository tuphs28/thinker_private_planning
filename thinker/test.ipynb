{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath('RS/thinker/thinker')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from collections import deque\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from thinker.env import Environment\n",
    "from thinker.net import ActorNet, ModelNet\n",
    "import thinker.util as util\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_gym_env_out(x, ax=None, title=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    ax.imshow(torch.swapaxes(torch.swapaxes(x[0].cpu(),0,2),0,1), interpolation='nearest', aspect=\"auto\")\n",
    "    if title is not None: ax.set_title(title)\n",
    "\n",
    "def plot_multi_gym_env_out(xs):\n",
    "    size_n = 6\n",
    "    col_n = 5\n",
    "    row_n = (len(xs) + (col_n-1))// col_n\n",
    "    \n",
    "    fig, axs = plt.subplots(row_n, col_n, figsize=(col_n * size_n, row_n * size_n))  \n",
    "    if(len(axs.shape) == 1): axs = axs[np.newaxis, :]\n",
    "    m = 0\n",
    "    for y in range(row_n):\n",
    "        for x in range(col_n):\n",
    "            if m >= len(xs): \n",
    "                axs[y][x].set_axis_off()\n",
    "            else:\n",
    "                axs[y][x].imshow(np.transpose(xs[m], axes=(1, 2, 0))/255)\n",
    "                axs[y][x].set_title(\"rollout %d\" % (m+1))\n",
    "            m += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_policies(logits, labels, ax=None, title=\"Real policy prob\"):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    probs = [torch.softmax(logit, dim=-1).detach().cpu().numpy() for logit in logits]\n",
    "    ax.set_title(title)\n",
    "    xs = np.arange(len(probs[0]))    \n",
    "    for n, (prob, label) in enumerate(zip(probs, labels)):\n",
    "        ax.bar(xs + 0.1 * (n-len(logits) // 2), prob, width = 0.1, label=label)\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(probs[0]))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))\n",
    "    ax.set_ylim(0, 1)        \n",
    "    ax.legend()        \n",
    "\n",
    "def plot_base_policies(logits, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    prob = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    rec_t, num_actions = logits.shape\n",
    "    xs = np.arange(rec_t)\n",
    "    labels = ['noop', 'up', 'down', 'left', 'right']\n",
    "    for i in range(num_actions):        \n",
    "        c = ax.bar(xs + 0.8 * (i / num_actions), prob[:,i], width = 0.8 / (num_actions), label=labels[i])  \n",
    "        color = c.patches[0].get_facecolor()\n",
    "        color = color[:3] + (color[3] * 0.5,)\n",
    "        ax.bar(xs + 0.8 * (i / num_actions), prob[:,i], width = 0.8 / (num_actions), color=color)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)   \n",
    "    ax.set_title(\"Model policy prob\")\n",
    "\n",
    "def plot_im_policies(im_policy_logits, reset_policy_logits, term_policy_logits, \n",
    "                     im_action, reset_action, term_action,\n",
    "                     one_hot=True, reset_ind=0, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "        \n",
    "    rec_t, num_actions = im_policy_logits.shape\n",
    "    num_actions += 1\n",
    "    rec_t -= 1\n",
    "        \n",
    "    im_prob = torch.softmax(im_policy_logits, dim=-1).detach().cpu().numpy()\n",
    "    reset_prob = torch.softmax(reset_policy_logits, dim=-1)[:,[reset_ind]].detach().cpu().numpy()\n",
    "    full_prob = np.concatenate([im_prob, reset_prob], axis=-1)\n",
    "    if term_policy_logits is not None:\n",
    "        term_prob = torch.softmax(term_policy_logits, dim=-1)[:,[reset_ind]].detach().cpu().numpy()\n",
    "        full_prob = np.concatenate([full_prob, term_prob], axis=-1)\n",
    "    \n",
    "    if not one_hot: im_action = F.one_hot(im_action, num_actions - 1)\n",
    "    im_action = im_action.detach().cpu().numpy()\n",
    "    reset_action = reset_action.unsqueeze(-1).detach().cpu().numpy()    \n",
    "    full_action = np.concatenate([im_action, reset_action], axis=-1)\n",
    "    if term_action is not None:\n",
    "        term_action = term_action.unsqueeze(-1).detach().cpu().numpy() \n",
    "        full_action = np.concatenate([full_action, term_action], axis=-1)\n",
    "    \n",
    "    xs = np.arange(rec_t+1)\n",
    "    labels = ['noop', 'up', 'down', 'left', 'right', 'reset']   \n",
    "    \n",
    "    if term_action is not None:\n",
    "        labels.append('term')\n",
    "        num_actions += 1\n",
    "        \n",
    "    for i in range(num_actions):        \n",
    "        c = ax.bar(xs + 0.8 * (i / num_actions), full_prob[:,i], width = 0.8 / (num_actions), label=labels[i])  \n",
    "        color = c.patches[0].get_facecolor()\n",
    "        color = color[:3] + (color[3] * 0.5,)\n",
    "        ax.bar(xs + 0.8 * (i / num_actions), full_action[:,i], width = 0.8 / (num_actions), color=color)\n",
    "        \n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)   \n",
    "    ax.set_title(\"Imagainary policy prob\")    \n",
    "\n",
    "def plot_qn_sa(q_s_a, n_s_a, max_q_s_a=None, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    xs = np.arange(len(q_s_a))\n",
    "\n",
    "    ax.bar(xs - 0.3, q_s_a.cpu(), color = 'g', width = 0.3, label=\"q_s_a\")    \n",
    "    ax_n = ax.twinx()\n",
    "    if max_q_s_a is not None:\n",
    "        ax.bar(xs, max_q_s_a.cpu(), color = 'r', width = 0.3, label=\"max_q_s_a\")        \n",
    "    ax_n.bar(xs + (0.3 if max_q_s_a is not None else 0.), \n",
    "             n_s_a.cpu(), bottom=0, color = 'b', width = 0.3, label=\"n_s_a\")\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(q_s_a))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))    \n",
    "    ax.legend(loc=\"upper left\")   \n",
    "    ax_n.legend(loc=\"upper right\") \n",
    "    ax.set_title(\"q_s_a and n_s_a\")    \n",
    "\n",
    "def print_im_actions(im_dict, print_stat=False):\n",
    "\n",
    "    lookup_dict = {0:\"Noop\",\n",
    "                   1:\"Up\",\n",
    "                   2:\"Down\",\n",
    "                   3:\"Left\",\n",
    "                   4:\"Right\"}\n",
    "\n",
    "    print_strs = []\n",
    "    n, s = 1, \"\"\n",
    "    reset = False\n",
    "    for im, reset in zip(im_dict[\"im_action\"][:-1], im_dict[\"reset_action\"][:-1]):\n",
    "        s += lookup_dict[im.item()] + \", \"\n",
    "        if reset:        \n",
    "            s += \"Reset\"\n",
    "            print_strs.append(\"%d: %s\" %(n, s))\n",
    "            s = \"\"\n",
    "            n += 1\n",
    "    if not reset: print_strs.append(\"%d: %s\" %(n, s[:-2]))\n",
    "    if print_stat: \n",
    "        for s in print_strs: print(s) \n",
    "    return print_strs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned actor network\n",
    "\n",
    "#check_point_path = '/media/sc/datadisk/data/thinker/logs/planner_logs/thinker_model_0.5policycost_20rec_t_nopreload'\n",
    "check_point_path = 'D:/data/thinker/logs/planner_logs/thinker_model_0.5policycost_20rec_t_nopreload'\n",
    "check_point_path = os.path.expanduser(check_point_path)\n",
    "max_eps_n = 100\n",
    "visualize = False\n",
    "\n",
    "flags = util.parse(['--load_checkpoint', check_point_path])\n",
    "env = Environment(flags, model_wrap=True)\n",
    "model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "model_net.train(False)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model.tar'), torch.device(\"cpu\"))\n",
    "model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "\n",
    "actor_net = ActorNet(obs_shape=env.model_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_actor.tar'), torch.device(\"cpu\"))         \n",
    "actor_net.set_weights(checkpoint[\"actor_net_state_dict\"])\n",
    "actor_state = actor_net.initial_state(batch_size=1) \n",
    "\n",
    "env.env.debug = True\n",
    "env_out, employ_model_state = env.initial(model_net)\n",
    "last_root_max_q = env.env.root_max_q\n",
    "gym_env_out = env_out.gym_env_out\n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "im_list = [\"im_policy_logits\", \"reset_policy_logits\", \"term_policy_logits\", \"im_action\", \"reset_action\", \"term_action\"]\n",
    "im_dict = {k: [] for k in im_list}\n",
    "model_logits, attn_output = [], []\n",
    "\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    actor_out, actor_state = actor_net(env_out, actor_state)      \n",
    "    action = [actor_out.action, actor_out.im_action, actor_out.reset_action]\n",
    "    if actor_out.term_action is not None:\n",
    "        action.append(actor_out.term_action)\n",
    "    action = torch.cat(action, dim=-1).unsqueeze(0)\n",
    "\n",
    "    # additional stat record\n",
    "    if len(im_dict['reset_action']) > 0:\n",
    "        im_dict['reset_action'][-1] = env.env.ret_dict['reset']\n",
    "    for k in im_list: \n",
    "        im_dict[k].append(getattr(actor_out, k)[0] if k in actor_out._fields and \n",
    "            getattr(actor_out, k) is not None else None)    \n",
    "    model_logits.append(env.env.ret_dict[\"logit\"])\n",
    "    attn_output.append(torch.cat([x.attn_output_weights.unsqueeze(0).unsqueeze(-2) for x in actor_net.core.layers])[:, :, 0, :])        \n",
    "    ret_dict = env.env.ret_dict\n",
    "\n",
    "    env_out, employ_model_state = env.step(action, model_net, employ_model_state)\n",
    "\n",
    "    if visualize and env_out.cur_t[0,0] == 0:\n",
    "        fig, axs = plt.subplots(1, 5, figsize=(30,6))      \n",
    "        title = \"step: %d; values: %.4f\" % (step, ret_dict[\"v0\"][0].cpu())\n",
    "        for k in im_list: \n",
    "            if im_dict[k][0] is not None:\n",
    "                im_dict[k] = torch.concat(im_dict[k], dim=0)            \n",
    "            else:\n",
    "                im_dict[k] = None\n",
    "        #if \"thres\" in ret_dict: title += \" thres: %.4f\" % ret_dict[\"thres\"][0].cpu()\n",
    "        if last_root_max_q is not None:  title += \" max_q_0: %.4f\" % last_root_max_q.cpu()\n",
    "        if flags.reward_type == 1: title += \" im_return: %.4f\" % env_out.episode_return[..., 1]                        \n",
    "        plot_gym_env_out(gym_env_out[0], axs[0], title=title)  \n",
    "        plot_base_policies(torch.concat(model_logits), ax=axs[1])  \n",
    "        plot_im_policies(**im_dict, one_hot=False, reset_ind=1, ax=axs[2])   \n",
    "        max_q_s_a = ret_dict[\"max_q_s_a\"][0] if \"max_q_s_a\" in ret_dict else None    \n",
    "        plot_qn_sa(ret_dict[\"q_s_a\"][0], ret_dict[\"n_s_a\"][0], max_q_s_a, ax=axs[3]) \n",
    "        plot_policies([ret_dict[\"logit0\"][0], actor_out.policy_logits[0,0]], [\"model\", \"agent\"], ax=axs[4])  \n",
    "        plt.show()\n",
    "        plot_multi_gym_env_out(env.env.debug_xs)\n",
    "        print_im_actions(im_dict, print_stat=True)    \n",
    "\n",
    "        gym_env_out = env_out.gym_env_out\n",
    "        last_root_max_q = env.env.root_max_q\n",
    "        im_dict = {k: [] for k in im_list}\n",
    "        model_logits, attn_output = [], []\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned actor network\n",
    "\n",
    "check_point_path = '~/RS/thinker/models/test'\n",
    "check_point_path = os.path.expanduser(check_point_path)\n",
    "max_eps_n = 1\n",
    "visualize = True\n",
    "\n",
    "flags = util.parse(['--load_checkpoint', check_point_path])\n",
    "env = Environment(flags, model_wrap=True)\n",
    "model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "model_net.train(False)\n",
    "\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model.tar'), torch.device(\"cpu\"))\n",
    "model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "\n",
    "\n",
    "actor_net = ActorNet(obs_shape=env.model_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_actor.tar'), torch.device(\"cpu\"))         \n",
    "actor_net.set_weights(checkpoint[\"actor_net_state_dict\"])\n",
    "actor_state = actor_net.initial_state(batch_size=1) \n",
    "\n",
    "model_net_rnn = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags, rnn=True)\n",
    "model_net_rnn.train(False)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model_rnn.tar'), torch.device(\"cpu\"))                    \n",
    "model_net_rnn.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "state = model_net_rnn.core.init_state(bsz=1)\n",
    "\n",
    "env.env.debug = True\n",
    "env_out = env.initial(model_net)\n",
    "gym_env_out = env_out.gym_env_out\n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "im_list = [\"im_policy_logits\", \"reset_policy_logits\", \"term_policy_logits\", \"im_action\", \"reset_action\", \"term_action\"]\n",
    "im_dict = {k: [] for k in im_list}\n",
    "model_logits, attn_output = [], []\n",
    "\n",
    "_, model_rnn_logits, state = model_net_rnn(env_out.gym_env_out, env_out.last_action[:,:,0], env_out.done, state)\n",
    "\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    actor_out, actor_state = actor_net(env_out, actor_state)      \n",
    "    action = [actor_out.action, actor_out.im_action, actor_out.reset_action]\n",
    "    if actor_out.term_action is not None:\n",
    "        action.append(actor_out.term_action)\n",
    "    action = torch.cat(action, dim=-1).unsqueeze(0)\n",
    "\n",
    "    # additional stat record\n",
    "    if len(im_dict['reset_action']) > 0:\n",
    "        im_dict['reset_action'][-1] = env.env.ret_dict['reset']\n",
    "    for k in im_list: \n",
    "        im_dict[k].append(getattr(actor_out, k)[0] if k in actor_out._fields and \n",
    "            getattr(actor_out, k) is not None else None)    \n",
    "    model_logits.append(env.env.ret_dict[\"logit\"])\n",
    "    attn_output.append(torch.cat([x.attn_output_weights.unsqueeze(0).unsqueeze(-2) for x in actor_net.core.layers])[:, :, 0, :])        \n",
    "    ret_dict = env.env.ret_dict\n",
    "\n",
    "    env_out = env.step(action, model_net)\n",
    "\n",
    "    if visualize and env_out.cur_t[0,0] == 0:\n",
    "\n",
    "        fig, axs = plt.subplots(1, 5, figsize=(30,6))      \n",
    "        title = \"step: %d; values: %.4f\" % (step, ret_dict[\"v0\"][0].cpu())\n",
    "        for k in im_list: \n",
    "            if im_dict[k][0] is not None:\n",
    "                im_dict[k] = torch.concat(im_dict[k], dim=0)            \n",
    "            else:\n",
    "                im_dict[k] = None\n",
    "        if \"thres\" in ret_dict: title += \" thres: %.4f\" % ret_dict[\"thres\"][0].cpu()\n",
    "        if flags.reward_type == 1: title += \" im_return: %.4f\" % env_out.episode_return[..., 1]                        \n",
    "        plot_gym_env_out(gym_env_out[0], axs[0], title=title)  \n",
    "        plot_base_policies(torch.concat(model_logits), ax=axs[1])  \n",
    "        plot_im_policies(**im_dict, one_hot=False, reset_ind=1, ax=axs[2])   \n",
    "        max_q_s_a = ret_dict[\"max_q_s_a\"][0] if \"max_q_s_a\" in ret_dict else None    \n",
    "        plot_qn_sa(ret_dict[\"q_s_a\"][0], ret_dict[\"n_s_a\"][0], max_q_s_a, ax=axs[3]) \n",
    "        plot_policies([ret_dict[\"logit0\"][0], actor_out.policy_logits[0,0], model_rnn_logits[0,0]], \n",
    "                      [\"model\", \"agent\", \"model_rnn\"], ax=axs[4])  \n",
    "        plt.show()\n",
    "        plot_multi_gym_env_out(env.env.debug_xs)\n",
    "        print_im_actions(im_dict, print_stat=True)    \n",
    "\n",
    "        gym_env_out = env_out.gym_env_out\n",
    "        im_dict = {k: [] for k in im_list}\n",
    "        model_logits, attn_output = [], []\n",
    "    if env_out.cur_t[0,0] == 0:\n",
    "        _, model_rnn_logits, state = model_net_rnn(env_out.gym_env_out, env_out.last_action[:,:,0], env_out.done, state)\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize plain model policy\n",
    "\n",
    "check_point_path = '~/RS/thinker/models/test'\n",
    "check_point_path = os.path.expanduser(check_point_path)\n",
    "max_eps_n = 100\n",
    "visualize = False\n",
    "\n",
    "flags = util.parse([])\n",
    "env = Environment(flags, model_wrap=False)\n",
    "env_out = env.initial()\n",
    "model_net_rnn = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags, rnn=True)\n",
    "model_net_rnn.train(False)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model_rnn.tar'), torch.device(\"cpu\"))                    \n",
    "model_net_rnn.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "state = model_net_rnn.core.init_state(bsz=1)\n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    _, model_rnn_logits, state = model_net_rnn(env_out.gym_env_out, env_out.last_action[:,:,0], env_out.done, state)\n",
    "    action = torch.multinomial(F.softmax(model_rnn_logits[0], dim=1), num_samples=1).unsqueeze(0)\n",
    "    env_out = env.step(action) \n",
    "\n",
    "    if visualize:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(6,3))  \n",
    "        plot_gym_env_out(env_out.gym_env_out[0], ax=axs[0], title=\"Step %d:v(s)=%.4f\" % (step, vs[0]))\n",
    "        plot_policies(model_rnn_logits[0,0], None, ax=axs[1])\n",
    "        plt.show()\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize plain model policy\n",
    "\n",
    "model_path = '/media/sc/datadisk/data/thinker/logs/planner_logs/thinker_model_no_pri/ckp_model.tar'\n",
    "max_eps_n = 10\n",
    "visualize = False\n",
    "\n",
    "flags = util.parse([])\n",
    "env = Environment(flags, model_wrap=False)\n",
    "env_out = env.initial()\n",
    "model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "model_net.train(False)\n",
    "checkpoint = torch.load(model_path, torch.device(\"cpu\"))\n",
    "model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    _, vs, policy_logits, _ = model_net(env_out.gym_env_out[0], env_out.last_action[:,:,0], one_hot=False)                            \n",
    "    action = torch.multinomial(F.softmax(policy_logits[0], dim=1), num_samples=1).unsqueeze(0)\n",
    "    env_out = env.step(action) \n",
    "\n",
    "    if visualize:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(6,3))  \n",
    "        plot_gym_env_out(env_out.gym_env_out[0], ax=axs[0], title=\"Step %d:v(s)=%.4f\" % (step, vs[0]))\n",
    "        plot_policies(policy_logits[0,0], None, ax=axs[1])\n",
    "        plt.show()\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Misc.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "from thinker.self_play import SelfPlayWorker\n",
    "from thinker.learn_actor import ActorLearner\n",
    "from thinker.buffer import *\n",
    "import thinker.util as util\n",
    "from thinker.net import *\n",
    "\n",
    "flags = util.parse([])\n",
    "flags.model_batch_size = 2\n",
    "flags.model_unroll_length = 8\n",
    "flags.model_k_step_return = 5\n",
    "flags.actor_parallel_n = 4\n",
    "flags.model_buffer_n = 1000\n",
    "flags.model_warm_up_n = 500\n",
    "flags.model_batch_mode = True\n",
    "flags.priority_alpha = 5\n",
    "\n",
    "t = flags.model_unroll_length   \n",
    "k = flags.model_k_step_return\n",
    "n = flags.actor_parallel_n  \n",
    "\n",
    "P = namedtuple(\"P\", [\"x\",\"y\"])\n",
    "model_buffer = ModelBuffer(flags)\n",
    "\n",
    "c = 0\n",
    "for c in range(100):\n",
    "    data = P(torch.full((t+k, n, 1),2*c), torch.full((t+k, n, 1),2*c+1))    \n",
    "    model_buffer.write(data)\n",
    "    r = model_buffer.read(1.)    \n",
    "    if r is not None:\n",
    "        data, weights, abs_flat_inds, ps_step = r\n",
    "        #print(data.x[:,:,0]) \n",
    "        #model_buffer.update_priority(abs_flat_inds, np.zeros(flags.model_batch_size))\n",
    "\n",
    "print(\"1 read\", data.x[:,:,0], weights)\n",
    "model_buffer.update_priority(abs_flat_inds, np.array([10, 0]))\n",
    "data, weights, abs_flat_inds, ps_step = model_buffer.read(1.)   \n",
    "print(\"2 read\", data.x[:,:,0], weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e1c3f15baeac42b350955c7791eba0d4b3a4c7ef635edb2d91a98439423f015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
