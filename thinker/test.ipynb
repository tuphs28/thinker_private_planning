{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath('RS/thinker/thinker')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "from collections import deque\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from thinker.env import Environment\n",
    "from thinker.net import ActorNet, ModelNet\n",
    "import thinker.util as util\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_gym_env_out(x, ax=None, title=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    ax.imshow(torch.swapaxes(torch.swapaxes(x[0].cpu(),0,2),0,1), interpolation='nearest', aspect=\"auto\")\n",
    "    if title is not None: ax.set_title(title)\n",
    "\n",
    "def plot_multi_gym_env_out(xs):\n",
    "    size_n = 6\n",
    "    col_n = 5\n",
    "    row_n = (len(xs) + (col_n-1))// col_n\n",
    "    \n",
    "    fig, axs = plt.subplots(row_n, col_n, figsize=(col_n * size_n, row_n * size_n))  \n",
    "    if(len(axs.shape) == 1): axs = axs[np.newaxis, :]\n",
    "    m = 0\n",
    "    for y in range(row_n):\n",
    "        for x in range(col_n):\n",
    "            if m >= len(xs): \n",
    "                axs[y][x].set_axis_off()\n",
    "            else:\n",
    "                axs[y][x].imshow(np.transpose(xs[m], axes=(1, 2, 0))/255)\n",
    "                axs[y][x].set_title(\"rollout %d\" % (m+1))\n",
    "            m += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_policies(logits, labels, ax=None, title=\"Real policy prob\"):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    probs = [torch.softmax(logit, dim=-1).detach().cpu().numpy() for logit in logits]\n",
    "    ax.set_title(title)\n",
    "    xs = np.arange(len(probs[0]))    \n",
    "    for n, (prob, label) in enumerate(zip(probs, labels)):\n",
    "        ax.bar(xs + 0.1 * (n-len(logits) // 2), prob, width = 0.1, label=label)\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(probs[0]))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))\n",
    "    ax.set_ylim(0, 1)        \n",
    "    ax.legend()        \n",
    "\n",
    "def plot_base_policies(logits, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    prob = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    rec_t, num_actions = logits.shape\n",
    "    xs = np.arange(rec_t)\n",
    "    labels = ['noop', 'up', 'down', 'left', 'right']\n",
    "    for i in range(num_actions):        \n",
    "        c = ax.bar(xs + 0.8 * (i / num_actions), prob[:,i], width = 0.8 / (num_actions), label=labels[i])  \n",
    "        color = c.patches[0].get_facecolor()\n",
    "        color = color[:3] + (color[3] * 0.5,)\n",
    "        ax.bar(xs + 0.8 * (i / num_actions), prob[:,i], width = 0.8 / (num_actions), color=color)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)   \n",
    "    ax.set_title(\"Model policy prob\")\n",
    "\n",
    "def plot_im_policies(im_policy_logits, reset_policy_logits, term_policy_logits, \n",
    "                     im_action, reset_action, term_action,\n",
    "                     one_hot=True, reset_ind=0, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "        \n",
    "    rec_t, num_actions = im_policy_logits.shape\n",
    "    num_actions += 1\n",
    "    rec_t -= 1\n",
    "        \n",
    "    im_prob = torch.softmax(im_policy_logits, dim=-1).detach().cpu().numpy()\n",
    "    reset_prob = torch.softmax(reset_policy_logits, dim=-1)[:,[reset_ind]].detach().cpu().numpy()\n",
    "    full_prob = np.concatenate([im_prob, reset_prob], axis=-1)\n",
    "    if term_policy_logits is not None:\n",
    "        term_prob = torch.softmax(term_policy_logits, dim=-1)[:,[reset_ind]].detach().cpu().numpy()\n",
    "        full_prob = np.concatenate([full_prob, term_prob], axis=-1)\n",
    "    \n",
    "    if not one_hot: im_action = F.one_hot(im_action, num_actions - 1)\n",
    "    im_action = im_action.detach().cpu().numpy()\n",
    "    reset_action = reset_action.unsqueeze(-1).detach().cpu().numpy()    \n",
    "    full_action = np.concatenate([im_action, reset_action], axis=-1)\n",
    "    if term_action is not None:\n",
    "        term_action = term_action.unsqueeze(-1).detach().cpu().numpy() \n",
    "        full_action = np.concatenate([full_action, term_action], axis=-1)\n",
    "    \n",
    "    xs = np.arange(rec_t+1)\n",
    "    labels = ['noop', 'up', 'down', 'left', 'right', 'reset']   \n",
    "    \n",
    "    if term_action is not None:\n",
    "        labels.append('term')\n",
    "        num_actions += 1\n",
    "        \n",
    "    for i in range(num_actions):        \n",
    "        c = ax.bar(xs + 0.8 * (i / num_actions), full_prob[:,i], width = 0.8 / (num_actions), label=labels[i])  \n",
    "        color = c.patches[0].get_facecolor()\n",
    "        color = color[:3] + (color[3] * 0.5,)\n",
    "        ax.bar(xs + 0.8 * (i / num_actions), full_action[:,i], width = 0.8 / (num_actions), color=color)\n",
    "        \n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)   \n",
    "    ax.set_title(\"Imagainary policy prob\")    \n",
    "\n",
    "def plot_qn_sa(q_s_a, n_s_a, max_q_s_a=None, ax=None):\n",
    "    if ax is None: fig, ax = plt.subplots()\n",
    "    xs = np.arange(len(q_s_a))\n",
    "\n",
    "    ax.bar(xs - 0.3, q_s_a.cpu(), color = 'g', width = 0.3, label=\"q_s_a\")    \n",
    "    ax_n = ax.twinx()\n",
    "    if max_q_s_a is not None:\n",
    "        ax.bar(xs, max_q_s_a.cpu(), color = 'r', width = 0.3, label=\"max_q_s_a\")        \n",
    "    ax_n.bar(xs + (0.3 if max_q_s_a is not None else 0.), \n",
    "             n_s_a.cpu(), bottom=0, color = 'b', width = 0.3, label=\"n_s_a\")\n",
    "    ax.xaxis.set_major_locator(mticker.FixedLocator(np.arange(len(q_s_a))))\n",
    "    ax.set_xticklabels(('noop', 'up', 'down', 'left', 'right'))    \n",
    "    ax.legend(loc=\"upper left\")   \n",
    "    ax_n.legend(loc=\"upper right\") \n",
    "    ax.set_title(\"q_s_a and n_s_a\")    \n",
    "\n",
    "def print_im_actions(im_dict, print_stat=False):\n",
    "\n",
    "    lookup_dict = {0:\"Noop\",\n",
    "                   1:\"Up\",\n",
    "                   2:\"Down\",\n",
    "                   3:\"Left\",\n",
    "                   4:\"Right\"}\n",
    "\n",
    "    print_strs = []\n",
    "    n, s = 1, \"\"\n",
    "    reset = False\n",
    "    for im, reset in zip(im_dict[\"im_action\"][:-1], im_dict[\"reset_action\"][:-1]):\n",
    "        s += lookup_dict[im.item()] + \", \"\n",
    "        if reset:        \n",
    "            s += \"Reset\"\n",
    "            print_strs.append(\"%d: %s\" %(n, s))\n",
    "            s = \"\"\n",
    "            n += 1\n",
    "    if not reset: print_strs.append(\"%d: %s\" %(n, s[:-2]))\n",
    "    if print_stat: \n",
    "        for s in print_strs: print(s) \n",
    "    return print_strs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned actor network\n",
    "\n",
    "check_point_path = '~/RS/thinker/models/test'\n",
    "check_point_path = os.path.expanduser(check_point_path)\n",
    "max_eps_n = 1\n",
    "visualize = True\n",
    "\n",
    "flags = util.parse(['--load_checkpoint', check_point_path])\n",
    "env = Environment(flags, model_wrap=True)\n",
    "model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "model_net.train(False)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model.tar'), torch.device(\"cpu\"))\n",
    "model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "\n",
    "actor_net = ActorNet(obs_shape=env.model_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_actor.tar'), torch.device(\"cpu\"))         \n",
    "actor_net.set_weights(checkpoint[\"actor_net_state_dict\"])\n",
    "actor_state = actor_net.initial_state(batch_size=1) \n",
    "\n",
    "env.env.debug = True\n",
    "env_out = env.initial(model_net)\n",
    "gym_env_out = env_out.gym_env_out\n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "im_list = [\"im_policy_logits\", \"reset_policy_logits\", \"term_policy_logits\", \"im_action\", \"reset_action\", \"term_action\"]\n",
    "im_dict = {k: [] for k in im_list}\n",
    "model_logits, attn_output = [], []\n",
    "\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    actor_out, actor_state = actor_net(env_out, actor_state)      \n",
    "    action = [actor_out.action, actor_out.im_action, actor_out.reset_action]\n",
    "    if actor_out.term_action is not None:\n",
    "        action.append(actor_out.term_action)\n",
    "    action = torch.cat(action, dim=-1).unsqueeze(0)\n",
    "\n",
    "    # additional stat record\n",
    "    if len(im_dict['reset_action']) > 0:\n",
    "        im_dict['reset_action'][-1] = env.env.ret_dict['reset']\n",
    "    for k in im_list: \n",
    "        im_dict[k].append(getattr(actor_out, k)[0] if k in actor_out._fields and \n",
    "            getattr(actor_out, k) is not None else None)    \n",
    "    model_logits.append(env.env.ret_dict[\"logit\"])\n",
    "    attn_output.append(torch.cat([x.attn_output_weights.unsqueeze(0).unsqueeze(-2) for x in actor_net.core.layers])[:, :, 0, :])        \n",
    "    ret_dict = env.env.ret_dict\n",
    "\n",
    "    env_out = env.step(action, model_net)\n",
    "\n",
    "    if visualize and env_out.cur_t[0,0] == 0:\n",
    "        fig, axs = plt.subplots(1, 5, figsize=(30,6))      \n",
    "        title = \"step: %d; values: %.4f\" % (step, ret_dict[\"v0\"][0].cpu())\n",
    "        for k in im_list: \n",
    "            if im_dict[k][0] is not None:\n",
    "                im_dict[k] = torch.concat(im_dict[k], dim=0)            \n",
    "            else:\n",
    "                im_dict[k] = None\n",
    "        if \"thres\" in ret_dict: title += \" thres: %.4f\" % ret_dict[\"thres\"][0].cpu()\n",
    "        if flags.reward_type == 1: title += \" im_return: %.4f\" % env_out.episode_return[..., 1]                        \n",
    "        plot_gym_env_out(gym_env_out[0], axs[0], title=title)  \n",
    "        plot_base_policies(torch.concat(model_logits), ax=axs[1])  \n",
    "        plot_im_policies(**im_dict, one_hot=False, reset_ind=1, ax=axs[2])   \n",
    "        max_q_s_a = ret_dict[\"max_q_s_a\"][0] if \"max_q_s_a\" in ret_dict else None    \n",
    "        plot_qn_sa(ret_dict[\"q_s_a\"][0], ret_dict[\"n_s_a\"][0], max_q_s_a, ax=axs[3]) \n",
    "        plot_policies([ret_dict[\"logit0\"][0], actor_out.policy_logits[0,0]], [\"model\", \"agent\"], ax=axs[4])  \n",
    "        plt.show()\n",
    "        plot_multi_gym_env_out(env.env.debug_xs)\n",
    "        print_im_actions(im_dict, print_stat=True)    \n",
    "\n",
    "        gym_env_out = env_out.gym_env_out\n",
    "        im_dict = {k: [] for k in im_list}\n",
    "        model_logits, attn_output = [], []\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned actor network\n",
    "\n",
    "check_point_path = '~/RS/thinker/models/test'\n",
    "check_point_path = os.path.expanduser(check_point_path)\n",
    "max_eps_n = 1\n",
    "visualize = True\n",
    "\n",
    "flags = util.parse(['--load_checkpoint', check_point_path])\n",
    "env = Environment(flags, model_wrap=True)\n",
    "model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "model_net.train(False)\n",
    "\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model.tar'), torch.device(\"cpu\"))\n",
    "model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "\n",
    "\n",
    "actor_net = ActorNet(obs_shape=env.model_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_actor.tar'), torch.device(\"cpu\"))         \n",
    "actor_net.set_weights(checkpoint[\"actor_net_state_dict\"])\n",
    "actor_state = actor_net.initial_state(batch_size=1) \n",
    "\n",
    "model_net_rnn = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags, rnn=True)\n",
    "model_net_rnn.train(False)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model_rnn.tar'), torch.device(\"cpu\"))                    \n",
    "model_net_rnn.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "state = model_net_rnn.core.init_state(bsz=1)\n",
    "\n",
    "env.env.debug = True\n",
    "env_out = env.initial(model_net)\n",
    "gym_env_out = env_out.gym_env_out\n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "im_list = [\"im_policy_logits\", \"reset_policy_logits\", \"term_policy_logits\", \"im_action\", \"reset_action\", \"term_action\"]\n",
    "im_dict = {k: [] for k in im_list}\n",
    "model_logits, attn_output = [], []\n",
    "\n",
    "_, model_rnn_logits, state = model_net_rnn(env_out.gym_env_out, env_out.last_action[:,:,0], env_out.done, state)\n",
    "\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    actor_out, actor_state = actor_net(env_out, actor_state)      \n",
    "    action = [actor_out.action, actor_out.im_action, actor_out.reset_action]\n",
    "    if actor_out.term_action is not None:\n",
    "        action.append(actor_out.term_action)\n",
    "    action = torch.cat(action, dim=-1).unsqueeze(0)\n",
    "\n",
    "    # additional stat record\n",
    "    if len(im_dict['reset_action']) > 0:\n",
    "        im_dict['reset_action'][-1] = env.env.ret_dict['reset']\n",
    "    for k in im_list: \n",
    "        im_dict[k].append(getattr(actor_out, k)[0] if k in actor_out._fields and \n",
    "            getattr(actor_out, k) is not None else None)    \n",
    "    model_logits.append(env.env.ret_dict[\"logit\"])\n",
    "    attn_output.append(torch.cat([x.attn_output_weights.unsqueeze(0).unsqueeze(-2) for x in actor_net.core.layers])[:, :, 0, :])        \n",
    "    ret_dict = env.env.ret_dict\n",
    "\n",
    "    env_out = env.step(action, model_net)\n",
    "\n",
    "    if visualize and env_out.cur_t[0,0] == 0:\n",
    "\n",
    "        fig, axs = plt.subplots(1, 5, figsize=(30,6))      \n",
    "        title = \"step: %d; values: %.4f\" % (step, ret_dict[\"v0\"][0].cpu())\n",
    "        for k in im_list: \n",
    "            if im_dict[k][0] is not None:\n",
    "                im_dict[k] = torch.concat(im_dict[k], dim=0)            \n",
    "            else:\n",
    "                im_dict[k] = None\n",
    "        if \"thres\" in ret_dict: title += \" thres: %.4f\" % ret_dict[\"thres\"][0].cpu()\n",
    "        if flags.reward_type == 1: title += \" im_return: %.4f\" % env_out.episode_return[..., 1]                        \n",
    "        plot_gym_env_out(gym_env_out[0], axs[0], title=title)  \n",
    "        plot_base_policies(torch.concat(model_logits), ax=axs[1])  \n",
    "        plot_im_policies(**im_dict, one_hot=False, reset_ind=1, ax=axs[2])   \n",
    "        max_q_s_a = ret_dict[\"max_q_s_a\"][0] if \"max_q_s_a\" in ret_dict else None    \n",
    "        plot_qn_sa(ret_dict[\"q_s_a\"][0], ret_dict[\"n_s_a\"][0], max_q_s_a, ax=axs[3]) \n",
    "        plot_policies([ret_dict[\"logit0\"][0], actor_out.policy_logits[0,0], model_rnn_logits[0,0]], \n",
    "                      [\"model\", \"agent\", \"model_rnn\"], ax=axs[4])  \n",
    "        plt.show()\n",
    "        plot_multi_gym_env_out(env.env.debug_xs)\n",
    "        print_im_actions(im_dict, print_stat=True)    \n",
    "\n",
    "        gym_env_out = env_out.gym_env_out\n",
    "        im_dict = {k: [] for k in im_list}\n",
    "        model_logits, attn_output = [], []\n",
    "    if env_out.cur_t[0,0] == 0:\n",
    "        _, model_rnn_logits, state = model_net_rnn(env_out.gym_env_out, env_out.last_action[:,:,0], env_out.done, state)\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish 1 episode: avg. return: 0.82 (+-0.00) \n",
      "Finish 2 episode: avg. return: 0.33 (+-0.35) \n",
      "Finish 3 episode: avg. return: 0.16 (+-0.27) \n",
      "Finish 4 episode: avg. return: 0.08 (+-0.21) \n",
      "Finish 5 episode: avg. return: 0.03 (+-0.18) \n",
      "Finish 6 episode: avg. return: -0.01 (+-0.15) \n",
      "Finish 7 episode: avg. return: 0.25 (+-0.27) \n",
      "Finish 8 episode: avg. return: 0.33 (+-0.25) \n",
      "Finish 9 episode: avg. return: 0.27 (+-0.23) \n",
      "Finish 10 episode: avg. return: 0.32 (+-0.21) \n",
      "Finish 11 episode: avg. return: 0.28 (+-0.20) \n",
      "Finish 12 episode: avg. return: 0.24 (+-0.18) \n",
      "Finish 13 episode: avg. return: 0.21 (+-0.17) \n",
      "Finish 14 episode: avg. return: 0.18 (+-0.16) \n",
      "Finish 15 episode: avg. return: 0.15 (+-0.15) \n",
      "Finish 16 episode: avg. return: 0.07 (+-0.17) \n",
      "Finish 17 episode: avg. return: 0.11 (+-0.16) \n",
      "Finish 18 episode: avg. return: 0.15 (+-0.16) \n",
      "Finish 19 episode: avg. return: 0.13 (+-0.15) \n",
      "Finish 20 episode: avg. return: 0.12 (+-0.14) \n",
      "Finish 21 episode: avg. return: 0.05 (+-0.15) \n",
      "Finish 22 episode: avg. return: -0.00 (+-0.15) \n",
      "Finish 23 episode: avg. return: -0.01 (+-0.15) \n",
      "Finish 24 episode: avg. return: 0.07 (+-0.16) \n",
      "Finish 25 episode: avg. return: 0.02 (+-0.16) \n",
      "Finish 26 episode: avg. return: 0.01 (+-0.15) \n",
      "Finish 27 episode: avg. return: -0.04 (+-0.15) \n",
      "Finish 28 episode: avg. return: -0.00 (+-0.15) \n",
      "Finish 29 episode: avg. return: -0.05 (+-0.15) \n",
      "Finish 30 episode: avg. return: -0.02 (+-0.15) \n",
      "Finish 31 episode: avg. return: -0.02 (+-0.15) \n",
      "Finish 32 episode: avg. return: 0.01 (+-0.14) \n",
      "Finish 33 episode: avg. return: -0.03 (+-0.14) \n",
      "Finish 34 episode: avg. return: -0.06 (+-0.14) \n",
      "Finish 35 episode: avg. return: -0.04 (+-0.14) \n",
      "Finish 36 episode: avg. return: -0.04 (+-0.14) \n",
      "Finish 37 episode: avg. return: 0.01 (+-0.14) \n",
      "Finish 38 episode: avg. return: 0.03 (+-0.14) \n",
      "Finish 39 episode: avg. return: 0.38 (+-0.37) \n",
      "Finish 40 episode: avg. return: 0.70 (+-0.48) \n",
      "Finish 41 episode: avg. return: 1.00 (+-0.56) \n",
      "Finish 42 episode: avg. return: 1.30 (+-0.62) \n",
      "Finish 43 episode: avg. return: 1.27 (+-0.60) \n",
      "Finish 44 episode: avg. return: 1.28 (+-0.59) \n",
      "Finish 45 episode: avg. return: 1.29 (+-0.58) \n",
      "Finish 46 episode: avg. return: 1.24 (+-0.57) \n",
      "Finish 47 episode: avg. return: 1.23 (+-0.56) \n",
      "Finish 48 episode: avg. return: 1.48 (+-0.60) \n",
      "Finish 49 episode: avg. return: 1.42 (+-0.59) \n",
      "Finish 50 episode: avg. return: 1.41 (+-0.58) \n",
      "Finish 51 episode: avg. return: 1.38 (+-0.56) \n",
      "Finish 52 episode: avg. return: 1.35 (+-0.55) \n",
      "Finish 53 episode: avg. return: 1.34 (+-0.54) \n",
      "Finish 54 episode: avg. return: 1.29 (+-0.54) \n",
      "Finish 55 episode: avg. return: 1.28 (+-0.53) \n",
      "Finish 56 episode: avg. return: 1.29 (+-0.52) \n",
      "Finish 57 episode: avg. return: 1.27 (+-0.51) \n",
      "Finish 58 episode: avg. return: 1.24 (+-0.50) \n",
      "Finish 59 episode: avg. return: 1.22 (+-0.49) \n",
      "Finish 60 episode: avg. return: 1.19 (+-0.49) \n",
      "Finish 61 episode: avg. return: 1.17 (+-0.48) \n",
      "Finish 62 episode: avg. return: 1.17 (+-0.47) \n",
      "Finish 63 episode: avg. return: 1.16 (+-0.46) \n",
      "Finish 64 episode: avg. return: 1.12 (+-0.46) \n",
      "Finish 65 episode: avg. return: 1.09 (+-0.45) \n",
      "Finish 66 episode: avg. return: 1.07 (+-0.44) \n",
      "Finish 67 episode: avg. return: 1.04 (+-0.44) \n",
      "Finish 68 episode: avg. return: 1.00 (+-0.43) \n",
      "Finish 69 episode: avg. return: 1.00 (+-0.43) \n",
      "Finish 70 episode: avg. return: 1.01 (+-0.42) \n",
      "Finish 71 episode: avg. return: 1.01 (+-0.42) \n",
      "Finish 72 episode: avg. return: 1.02 (+-0.41) \n",
      "Finish 73 episode: avg. return: 1.03 (+-0.40) \n",
      "Finish 74 episode: avg. return: 1.03 (+-0.40) \n",
      "Finish 75 episode: avg. return: 1.01 (+-0.39) \n",
      "Finish 76 episode: avg. return: 1.01 (+-0.39) \n",
      "Finish 77 episode: avg. return: 1.01 (+-0.38) \n",
      "Finish 78 episode: avg. return: 1.01 (+-0.38) \n",
      "Finish 79 episode: avg. return: 0.99 (+-0.37) \n",
      "Finish 80 episode: avg. return: 0.99 (+-0.37) \n",
      "Finish 81 episode: avg. return: 0.99 (+-0.37) \n",
      "Finish 82 episode: avg. return: 1.14 (+-0.39) \n",
      "Finish 83 episode: avg. return: 1.29 (+-0.42) \n",
      "Finish 84 episode: avg. return: 1.29 (+-0.41) \n",
      "Finish 85 episode: avg. return: 1.29 (+-0.41) \n",
      "Finish 86 episode: avg. return: 1.28 (+-0.40) \n",
      "Finish 87 episode: avg. return: 1.27 (+-0.40) \n",
      "Finish 88 episode: avg. return: 1.27 (+-0.39) \n",
      "Finish 89 episode: avg. return: 1.25 (+-0.39) \n",
      "Finish 90 episode: avg. return: 1.25 (+-0.38) \n",
      "Finish 91 episode: avg. return: 1.23 (+-0.38) \n",
      "Finish 92 episode: avg. return: 1.22 (+-0.38) \n",
      "Finish 93 episode: avg. return: 1.35 (+-0.39) \n",
      "Finish 94 episode: avg. return: 1.48 (+-0.41) \n",
      "Finish 95 episode: avg. return: 1.45 (+-0.41) \n",
      "Finish 96 episode: avg. return: 1.44 (+-0.40) \n",
      "Finish 97 episode: avg. return: 1.57 (+-0.42) \n",
      "Finish 98 episode: avg. return: 1.57 (+-0.41) \n",
      "Finish 99 episode: avg. return: 1.69 (+-0.43) \n",
      "Finish 100 episode: avg. return: 1.81 (+-0.44) \n"
     ]
    }
   ],
   "source": [
    "# Visualize plain model policy\n",
    "\n",
    "check_point_path = '~/RS/thinker/models/test'\n",
    "check_point_path = os.path.expanduser(check_point_path)\n",
    "max_eps_n = 100\n",
    "visualize = False\n",
    "\n",
    "flags = util.parse([])\n",
    "env = Environment(flags, model_wrap=False)\n",
    "env_out = env.initial()\n",
    "model_net_rnn = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags, rnn=True)\n",
    "model_net_rnn.train(False)\n",
    "checkpoint = torch.load(os.path.join(check_point_path,'ckp_model_rnn.tar'), torch.device(\"cpu\"))                    \n",
    "model_net_rnn.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "state = model_net_rnn.core.init_state(bsz=1)\n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    _, model_rnn_logits, state = model_net_rnn(env_out.gym_env_out, env_out.last_action[:,:,0], env_out.done, state)\n",
    "    action = torch.multinomial(F.softmax(model_rnn_logits[0], dim=1), num_samples=1).unsqueeze(0)\n",
    "    env_out = env.step(action) \n",
    "\n",
    "    if visualize:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(6,3))  \n",
    "        plot_gym_env_out(env_out.gym_env_out[0], ax=axs[0], title=\"Step %d:v(s)=%.4f\" % (step, vs[0]))\n",
    "        plot_policies(model_rnn_logits[0,0], None, ax=axs[1])\n",
    "        plt.show()\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize plain model policy\n",
    "\n",
    "model_path = '/media/sc/datadisk/data/thinker/logs/planner_logs/thinker_model_no_pri/ckp_model.tar'\n",
    "max_eps_n = 10\n",
    "visualize = False\n",
    "\n",
    "flags = util.parse([])\n",
    "env = Environment(flags, model_wrap=False)\n",
    "env_out = env.initial()\n",
    "model_net = ModelNet(obs_shape=env.gym_env_out_shape, num_actions=env.num_actions, flags=flags)\n",
    "model_net.train(False)\n",
    "checkpoint = torch.load(model_path, torch.device(\"cpu\"))\n",
    "model_net.set_weights(checkpoint[\"model_state_dict\"] if \"model_state_dict\" in \n",
    "                    checkpoint else checkpoint[\"model_net_state_dict\"])  \n",
    "\n",
    "returns = []\n",
    "step = 0\n",
    "while len(returns) < max_eps_n:    \n",
    "    step += 1\n",
    "    _, vs, policy_logits, _ = model_net(env_out.gym_env_out[0], env_out.last_action[:,:,0], one_hot=False)                            \n",
    "    action = torch.multinomial(F.softmax(policy_logits[0], dim=1), num_samples=1).unsqueeze(0)\n",
    "    env_out = env.step(action) \n",
    "\n",
    "    if visualize:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(6,3))  \n",
    "        plot_gym_env_out(env_out.gym_env_out[0], ax=axs[0], title=\"Step %d:v(s)=%.4f\" % (step, vs[0]))\n",
    "        plot_policies(policy_logits[0,0], None, ax=axs[1])\n",
    "        plt.show()\n",
    "\n",
    "    if torch.any(env_out.done):\n",
    "        step = 0\n",
    "        new_rets = env_out.episode_return[env_out.done][:,0].numpy()\n",
    "        returns.extend(new_rets)\n",
    "        print(\"Finish %d episode: avg. return: %.2f (+-%.2f) \" % (len(returns),\n",
    "            np.average(returns), np.std(returns) / np.sqrt(len(returns))))  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Misc.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "from thinker.self_play import SelfPlayWorker\n",
    "from thinker.learn_actor import ActorLearner\n",
    "from thinker.buffer import *\n",
    "import thinker.util as util\n",
    "from thinker.net import *\n",
    "\n",
    "flags = util.parse([])\n",
    "flags.model_batch_size = 2\n",
    "flags.model_unroll_length = 8\n",
    "flags.model_k_step_return = 5\n",
    "flags.actor_parallel_n = 4\n",
    "flags.model_buffer_n = 1000\n",
    "flags.model_warm_up_n = 500\n",
    "flags.model_batch_mode = True\n",
    "flags.priority_alpha = 5\n",
    "\n",
    "t = flags.model_unroll_length   \n",
    "k = flags.model_k_step_return\n",
    "n = flags.actor_parallel_n  \n",
    "\n",
    "P = namedtuple(\"P\", [\"x\",\"y\"])\n",
    "model_buffer = ModelBuffer(flags)\n",
    "\n",
    "c = 0\n",
    "for c in range(100):\n",
    "    data = P(torch.full((t+k, n, 1),2*c), torch.full((t+k, n, 1),2*c+1))    \n",
    "    model_buffer.write(data)\n",
    "    r = model_buffer.read(1.)    \n",
    "    if r is not None:\n",
    "        data, weights, abs_flat_inds, ps_step = r\n",
    "        #print(data.x[:,:,0]) \n",
    "        #model_buffer.update_priority(abs_flat_inds, np.zeros(flags.model_batch_size))\n",
    "\n",
    "print(\"1 read\", data.x[:,:,0], weights)\n",
    "model_buffer.update_priority(abs_flat_inds, np.array([10, 0]))\n",
    "data, weights, abs_flat_inds, ps_step = model_buffer.read(1.)   \n",
    "print(\"2 read\", data.x[:,:,0], weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f50a7fa60ad8550b89b217983de73aa91b7ad4da24a2c984b86370b087d0b88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
