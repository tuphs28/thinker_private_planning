# the setting for default Thinker environment
# training setting
has_model: true # whether the model exists
train_model: true # whether to train the model
model_batch_size: 128 # training batch size of the model
model_learning_rate: 0.0001 # model learning rate
model_grad_norm_clipping: 0 # gradient clipping norm; non-positive for no clipping
min_replay_ratio: 5 # minimum replay ratio (i.e. the number of times that the same transition is used to train a model on average)
max_replay_ratio: 6 # maximum replay ratio (i.e. the number of times that the same transition is used to train a model on average)
total_steps: 50000000 # total number of raw steps to train
# replay buffer setting
priority_alpha: 0.6 # alpha in prioritized sampling from the buffer
priority_beta: 0.4 # beta in prioritized sampling from the buffer
buffer_traj_len: 50 # length of trajectory stored in the replay buffer
model_unroll_len: 5 # model unroll length when training model
model_mem_unroll_len: 0 # model unroll length for computing initial model state (only needed when model_has_memory is true)
model_return_n: 5 # n-step return when training model
model_warm_up_n: 200000 # number of augmented step before model starts to be trained
model_buffer_n: 200000 # capacity of replay buffer in number of augmented step 
# cost setting in training
model_policy_loss_cost: 0.5 # cost for training model's policy
model_vs_loss_cost: 0.25 # cost for training model's values
model_rs_loss_cost: 1.0 # cost for training model's reward
model_done_loss_cost: 1.0 # cost for training model's done signal
model_img_loss_cost: 5.0 # cost for training model's state output (L2 loss) (dual network only)
model_fea_loss_cost: 5.0 # cost for training model's state output (feature loss) (dual network only)
model_sup_loss_cost: 0.0 # cost for SimSiam loss
model_reg_loss_cost: 0.0 # cost for model regularization loss
model_noise_loss_cost: 0.0 # cost for model noise loss
# model setting
dual_net: true # whether to use dual network
dual_pred_f: false # whether to directly predict feature instead of state
model_enc_type: 0 # reward / value encoding type for the model; 0 for no encoding, 1 for scalar encoding, 2 for unbiased vector encoding, 3 for biased vector encoding
model_enc_f_type: 0 # scalar encoding function for reward / value encoding; 0 for MuZero-like encoding, 1 for Dreamer-like encoding
model_size_nn: 1 # model size mulitplier (integer)
model_downscale_c: 2 # model channel size divisor (integer) - state reward network
model_downscale_c_vp: 2 # model channel size divisor (integer) - value policy network
model_disable_bn: true # whether to disable batch norm in model
model_zero_init: true # whether to zero initialize the model's predicted rewards and values
model_has_memory: false # whether to enable memory in model
model_ordinal: false # whether to use ordinal representation in model's policy
# wrapper for env
grayscale: false # whether to apply grayscale for atari
atari: false # whether to use atari wrapper
discrete_k: -1 # bin used to discretize the action space; -1 for no discretizing
obs_norm: false # whether to normalize obs by running avg mean and std
obs_clip: -1 # observation clipping; if positve, the obs is clipped within the absolute value of this values; set to negative number for no clipping
reward_norm: false # whether to normalize reward by running avg mean and std
reward_clip: -1 # reward clipping; if positve, the reward is clipped within the absolute value of this values; set to negative number for no clipping
# wrapper to be used
require_prob: false # whether to use prob to train model instead of sampled action
wrapper_type: 0 # 0: default Thinker; 1: raw env; 2: Thinker w/ perfect model; 3: simple env
rec_t: 20 # stage length
test_rec_t: -1 # stage length for testing; non-positive for being the same as rec_t
max_depth: 5 # maximum search depth before forceful reset
tree_carry: true # whether to carry the tree across stages
reset_mode: 0 # 0 for conventional thinker; 1 for no expansion if reset - return the root node as the current node
return_h: true # whether to return model's hidden state
return_x: false # whether to return the model's predicted state
return_double: false # whether to return both root node and current node's h / raw_s
has_action_seq: true # whether tree rep contains action sequence
se_query_size: 5 # size of query table in wrapper_type 3
se_td_lambda: 0.9 # lambda for discounting td err in wrapper_type 3
se_query_cur: 2 # 0 for no query on current nodes; 1 for query on current nodes; 2 for query on both current & root nodes & enable node carry
se_buffer_n: 10 # buffer size of the keys
se_tree_carry: false # whether to enable tree carry
se_manual_stat: false # manual summing to compute n(s,a) and q(s,a) for discrete only
# sampled-based search
sample_n: -1 # number of sampled action; <0 for disabling sample mode
sample_temp: 4 # whether to use sampled action in imagination
sample_replace: true # whether to have replcaement when sampling action
# reward-related
discounting: 0.97 # discount rate of the raw MDP
im_enable: true # whether to return imagainary reward
stat_mask_type: 0 # masking hint in tree representation: 0: no masking, 1 for masking all auxiliary stat, 2 for masking all auxiliary node stat + v and pi
# curiosity-related
cur_enable: false # whether to return curiosity reward
cur_reward_cost: 0.0 # coefficient for curiosity loss based on reward prediction loss
cur_v_cost: 0.0 # coefficient for curiosity loss based on value prediction loss
cur_pi_cost: 0.0 # coefficient for curiosity loss based on policy prediction loss 
cur_enc_cost: 1.0 # coefficient for curiosity loss based on state prediction loss (in terms of feature loss)
cur_x_cost: 0.0 # coefficient for curiosity loss based on image prediction loss
cur_sup_cost: 0.0 # coefficient for curiosity loss based on SimSiam loss
cur_done_gate: true # whehter to give no curiosity reward when episode ends or is predicted to end
cur_done_elapse: 0 # number of step after episode done that receives no curisoity reward; 0 for disabling
# stochasic model
noise_enable: false # enable vae model
noise_n: 16 # number of catogorical variable
noise_d: 10 # number of category for each variable
noise_alpha: 0.8 # alpha for kl divergence rebalancing
noise_mlp: false # use 2-layer mlp to compress noise
# danger-related env setting
detect_dan_num: 0 # number of dangerous location in Sokoban
# env-specific setting
dm_rgb: false # whether to use rgb for dm control suite
# checkpoint
xpid: '' # name of the run, automatically set if not given
ckp: false # whether to load checkpoint; if set to true, will load checkpoint from savedir/xpid
preload: '' # path for the folder that consists ckp_model.tar, which is used to initialize  actor
# savedir: '../logs/__project__' # base log directory
savedir: 'logs' # base log directory
# reanalyze
reanalyze: false # whether to enable reanalyze; require access to actor
reanalyze_actor_update_freq: 32 # refresh frequence of actor
# misc
profile: false # whether to output all the time statistics of the run
parallel: false # whether to use a parallel ray actor for training the model
float16: false # whether to use mixed precision
base_seed: 1 # base seed for the gym environment
repeat_action_n: 0 # custom frame stacking (only for non-Atari envrionment)
project: ''
# ray resources
ray_mem: -1 # default initialized ray memory
ray_gpu: -1 # number of initial gpu (set to -1 for auto detect)
ray_cpu: -1 # number of initial cpu (set to -1 for auto detect)
gpu_learn: 0.5 # gpu for the model-learning ray actor