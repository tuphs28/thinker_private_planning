{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing v5_sok from ../logs/detect\n",
      "Initializing env 0 with device cuda\n",
      "Model network size: 6637133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../logs/detect/v5_sok/config_c.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model net from /mnt/c/Users/chung/Personal/RS/thinker/logs/detect/v5_sok/ckp_model.tar\n",
      "Process Worker<AsyncVectorEnv>-20:\n",
      "Process Worker<AsyncVectorEnv>-22:\n",
      "Process Worker<AsyncVectorEnv>-18:\n",
      "Process Worker<AsyncVectorEnv>-19:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/scuk/miniconda3/envs/thinker/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/scuk/miniconda3/envs/thinker/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/scuk/miniconda3/envs/thinker/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/scuk/miniconda3/envs/thinker/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/scuk/miniconda3/envs/thinker/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/gym_add/asyn_vector_env.py\", line 616, in _worker_shared_memory\n",
      "    env = env_fn()\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/gym_add/asyn_vector_env.py\", line 616, in _worker_shared_memory\n",
      "    env = env_fn()\n",
      "  File \"/home/scuk/miniconda3/envs/thinker/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/vector/utils/misc.py\", line 22, in __call__\n",
      "    return self.fn()\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/vector/utils/misc.py\", line 22, in __call__\n",
      "    return self.fn()\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/main.py\", line 104, in <lambda>\n",
      "    gym.make(**gym_make_args),\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/gym_add/asyn_vector_env.py\", line 616, in _worker_shared_memory\n",
      "    env = env_fn()\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/main.py\", line 104, in <lambda>\n",
      "    gym.make(**gym_make_args),\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 676, in make\n",
      "    return registry.make(id, **kwargs)\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/vector/utils/misc.py\", line 22, in __call__\n",
      "    return self.fn()\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 676, in make\n",
      "    return registry.make(id, **kwargs)\n",
      "  File \"/home/scuk/miniconda3/envs/thinker/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 520, in make\n",
      "    return spec.make(**kwargs)\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/main.py\", line 104, in <lambda>\n",
      "    gym.make(**gym_make_args),\n",
      "  File \"/home/scuk/miniconda3/envs/thinker/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 676, in make\n",
      "    return registry.make(id, **kwargs)\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 520, in make\n",
      "    return spec.make(**kwargs)\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 140, in make\n",
      "    env = cls(**_kwargs)\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/gym_add/asyn_vector_env.py\", line 616, in _worker_shared_memory\n",
      "    env = env_fn()\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 520, in make\n",
      "    return spec.make(**kwargs)\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 140, in make\n",
      "    env = cls(**_kwargs)\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/sokoban/gym_sokoban/envs/psokoban.py\", line 29, in __init__\n",
      "    self.sokoban = cSokoban(small=small,\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/vector/utils/misc.py\", line 22, in __call__\n",
      "    return self.fn()\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/sokoban/gym_sokoban/envs/psokoban.py\", line 29, in __init__\n",
      "    self.sokoban = cSokoban(small=small,\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 140, in make\n",
      "    env = cls(**_kwargs)\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/main.py\", line 104, in <lambda>\n",
      "    gym.make(**gym_make_args),\n",
      "  File \"gym_sokoban/envs/csokoban.pyx\", line 17, in gym_sokoban.envs.csokoban.cSokoban.__init__\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/sokoban/gym_sokoban/envs/psokoban.py\", line 29, in __init__\n",
      "    self.sokoban = cSokoban(small=small,\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 676, in make\n",
      "    return registry.make(id, **kwargs)\n",
      "  File \"gym_sokoban/envs/csokoban.pyx\", line 17, in gym_sokoban.envs.csokoban.cSokoban.__init__\n",
      "OSError: unable to open /mnt/c/Users/chung/Personal/RS/thinker/sokoban/gym_sokoban/envs/surface//wall_small.bmp: iostream error\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 520, in make\n",
      "    return spec.make(**kwargs)\n",
      "  File \"gym_sokoban/envs/csokoban.pyx\", line 17, in gym_sokoban.envs.csokoban.cSokoban.__init__\n",
      "OSError: unable to open /mnt/c/Users/chung/Personal/RS/thinker/sokoban/gym_sokoban/envs/surface//wall_small.bmp: iostream error\n",
      "OSError: unable to open /mnt/c/Users/chung/Personal/RS/thinker/sokoban/gym_sokoban/envs/surface//player_on_target_small.bmp: iostream error\n",
      "  File \"/home/scuk/.local/lib/python3.8/site-packages/gym/envs/registration.py\", line 140, in make\n",
      "    env = cls(**_kwargs)\n",
      "  File \"/mnt/c/Users/chung/Personal/RS/thinker/sokoban/gym_sokoban/envs/psokoban.py\", line 29, in __init__\n",
      "    self.sokoban = cSokoban(small=small,\n",
      "  File \"gym_sokoban/envs/csokoban.pyx\", line 17, in gym_sokoban.envs.csokoban.cSokoban.__init__\n",
      "OSError: unable to open /mnt/c/Users/chung/Personal/RS/thinker/sokoban/gym_sokoban/envs/surface//dan_small.bmp: iostream error\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 151\u001b[0m\n\u001b[1;32m    148\u001b[0m flags \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcreate_flags(config_path, save_flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    149\u001b[0m disable_thinker \u001b[38;5;241m=\u001b[39m flags\u001b[38;5;241m.\u001b[39mwrapper_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 151\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mEnv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43msavedir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msavedir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxpid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxpid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mckp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m obs_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\n\u001b[1;32m    165\u001b[0m action_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space \n",
      "File \u001b[0;32m/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/main.py:201\u001b[0m, in \u001b[0;36mEnv.__init__\u001b[0;34m(self, name, env_fn, ray_obj, env_n, gpu, load_net, time, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m            \n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# create batched asyn. environments\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mAsyncVectorEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_fn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv_n\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m    202\u001b[0m env\u001b[38;5;241m.\u001b[39mseed([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m*\u001b[39m env_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mbase_seed, \n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m*\u001b[39m env_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mbase_seed \u001b[38;5;241m+\u001b[39m env_n)])       \n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwrapper_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/gym_add/asyn_vector_env.py:166\u001b[0m, in \u001b[0;36mAsyncVectorEnv.__init__\u001b[0;34m(self, env_fns, observation_space, action_space, shared_memory, copy, context, daemon, worker)\u001b[0m\n\u001b[1;32m    163\u001b[0m         child_pipe\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m=\u001b[39m AsyncState\u001b[38;5;241m.\u001b[39mDEFAULT\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_observation_spaces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/gym_add/asyn_vector_env.py:524\u001b[0m, in \u001b[0;36mAsyncVectorEnv._check_observation_spaces\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_is_running()\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent_pipes:\n\u001b[0;32m--> 524\u001b[0m     \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_check_observation_space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_observation_space\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m same_spaces, successes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[pipe\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent_pipes])\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_errors(successes)\n",
      "File \u001b[0;32m~/miniconda3/envs/thinker/lib/python3.8/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/thinker/lib/python3.8/multiprocessing/connection.py:404\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    400\u001b[0m header \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, n)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m16384\u001b[39m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# The payload is large so Nagle's algorithm won't be triggered\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;66;03m# and we'd better avoid the cost of concatenation.\u001b[39;00m\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(buf)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/thinker/lib/python3.8/multiprocessing/connection.py:368\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import time, timeit\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from thinker.actor_net import ActorNet\n",
    "from thinker.main import Env\n",
    "import thinker.util as util\n",
    "from thinker.self_play import init_env_out, create_env_out\n",
    "\n",
    "class DetectBuffer:\n",
    "    def __init__(self, outdir, t, rec_t, logger, delay_n=5):\n",
    "        \"\"\"\n",
    "        Store training data grouped in planning stages and output\n",
    "        whenever the target output is also readydd\n",
    "            Args:\n",
    "                N (int): number of planning stage per training output\n",
    "                delay_n (int): number of planning stage delayed in the output y\n",
    "                rec_t (int): number of step in a planning stage\n",
    "                K (int): number of block to merge into\n",
    "        \"\"\"\n",
    "        self.outdir = outdir\n",
    "        self.t = t # number of time step per file\n",
    "        self.rec_t = rec_t\n",
    "        self.logger = logger        \n",
    "        self.delay_n = delay_n        \n",
    "\n",
    "        self.processed_n, self.xs, self.y, self.done, self.step_status = 0, [], [], [], []\n",
    "        self.file_idx = -1\n",
    "    \n",
    "    def insert(self, xs, y, done, step_status):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xs (dict): dictionary of training input, with each elem having the\n",
    "                shape of (B, *)            \n",
    "            y (tensor): bool tensor of shape (B), being the target output delayed by\n",
    "                delay_n planning stage            \n",
    "            done (tensor): bool tensor of shape (B), being the indicator of episode end\n",
    "            step_status (int): int indicating current step status\n",
    "        Output:\n",
    "            save train_xs in shape (N, rec_t, B, *) and train_y in shape (N, B)\n",
    "        \"\"\"\n",
    "        #print(\"data received! \", y.shape, id, cur_t)\n",
    "        last_step_real = (step_status == 0) | (step_status == 3)\n",
    "        if len(self.step_status) == 0 and not last_step_real: return self.file_idx  # skip until real step\n",
    "                \n",
    "        self.xs.append(util.dict_map(xs, lambda x:x.cpu()))\n",
    "        self.y.append(y.cpu())\n",
    "        self.done.append(done.cpu())\n",
    "        self.step_status.append(step_status)\n",
    "        self.processed_n += int(last_step_real)\n",
    "\n",
    "        if (self.processed_n >= self.t + self.delay_n + 1):               \n",
    "            self.file_idx += 1                     \n",
    "            out = self._extract_data(self.t)\n",
    "            self.processed_n = sum([int(i == 0) for i in self.step_status])\n",
    "            assert self.processed_n == self.delay_n+1, f\"should only have {self.delay_n + 1} data left instead of {self.processed_n}\"\n",
    "            path = f'{self.outdir}/data_{self.file_idx}.pt'\n",
    "            torch.save(out, path)\n",
    "            out_shape = out[0]['env_state'].shape\n",
    "            n = self.file_idx * out_shape[0] * out_shape[2]\n",
    "            self.logger.info(f\"{n}: File saved to {path}; env_state shape {out_shape}\")\n",
    "\n",
    "        return self.file_idx   \n",
    "\n",
    "    def _extract_data(self, t):\n",
    "        # obtain the first N planning stage and the corresponding target_y in data\n",
    "        xs, y, done, step_status = self._collect_data(t)\n",
    "        future_y, future_done = self._collect_data(self.delay_n, y_done_only=True)\n",
    "        y = torch.concat([y, future_y], dim=0)\n",
    "        done = torch.concat([done, future_done], dim=0)                \n",
    "        \n",
    "        last_step_real = (step_status == 0) | (step_status == 3)\n",
    "        assert last_step_real[0], \"cur_t should start with 0\"\n",
    "        assert last_step_real.shape[0] == t*self.rec_t, \\\n",
    "            f\" step_status.shape is {last_step_real.shape}, expected {t*self.rec_t} for the first dimension.\"        \n",
    "        assert y.shape[0] == (t + self.delay_n)*self.rec_t, \\\n",
    "            f\" y.shape is {y.shape}, expected {(t + self.delay_n)*self.rec_t} for the first dimension.\"        \n",
    "        \n",
    "        B = y.shape[1]\n",
    "        y = y.view(t + self.delay_n, self.rec_t, B)[:, 0]\n",
    "        done = done.view(t + self.delay_n, self.rec_t, B)[:, 0]\n",
    "        step_status = step_status.view(t, self.rec_t)\n",
    "        # compute target_y\n",
    "        target_y = self._compute_target_y(y, done, self.delay_n)\n",
    "\n",
    "        for k in xs.keys():\n",
    "            xs[k] = xs[k].view((t, self.rec_t) + xs[k].shape[1:])\n",
    "        \n",
    "        xs[\"done\"] = done[:t]\n",
    "        xs[\"step_status\"] = step_status\n",
    "                \n",
    "        return xs, target_y\n",
    "\n",
    "    def _collect_data(self, t, y_done_only=False):\n",
    "        # collect the first t stage from data\n",
    "        step_status = torch.tensor(self.step_status, dtype=torch.long)\n",
    "        next_step_real = (step_status == 2) | (step_status == 3)        \n",
    "        idx = torch.nonzero(next_step_real, as_tuple=False).squeeze()    \n",
    "        last_idx = idx[t-1] + 1\n",
    "        y = torch.stack(self.y[:last_idx], dim=0)\n",
    "        done = torch.stack(self.done[:last_idx], dim=0)\n",
    "        if not y_done_only:\n",
    "            xs = {}\n",
    "            for k in self.xs[0].keys():\n",
    "                xs[k] = torch.stack([v[k] for v in self.xs[:last_idx]], dim=0)                \n",
    "            step_status = step_status[:last_idx]\n",
    "            self.xs = self.xs[last_idx:]\n",
    "            self.y = self.y[last_idx:]\n",
    "            self.done = self.done[last_idx:]\n",
    "            self.step_status = self.step_status[last_idx:]\n",
    "            return xs, y, done, step_status\n",
    "        else:\n",
    "            return y, done\n",
    "        \n",
    "    def _compute_target_y(self, y, done, delay_n):        \n",
    "        # target_y[i] = (y[i] | (~done[i+1] & y[i+1]) | (~done[i+1] & ~done[i+2] & y[i+2]) | ... | (~done[i+1] & ~done[i+2] & ... & ~done[i+M] & y[i+M]))\n",
    "        t, b = y.shape\n",
    "        t = t - delay_n\n",
    "        not_done_cum = torch.ones(delay_n, t, b, dtype=bool)\n",
    "        target_y = y.clone()[:-delay_n]\n",
    "        not_done_cum[0] = ~done[1:1+t]\n",
    "        target_y = target_y | (not_done_cum[0] & y[1:1+t])\n",
    "        for m in range(1, delay_n):\n",
    "            not_done_cum[m] = not_done_cum[m-1] & ~done[m+1:m+1+t]\n",
    "            target_y = target_y | (not_done_cum[m] & y[m+1:m+1+t])\n",
    "        return target_y\n",
    "\n",
    "total_n = 100000\n",
    "env_n = 128\n",
    "delay_n = 5\n",
    "savedir = \"../logs/detect\"\n",
    "outdir = \"../data/detect\"\n",
    "xpid = \"v5_sok\"\n",
    "\n",
    "_logger = util.logger()\n",
    "_logger.info(f\"Initializing {xpid} from {savedir}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "ckpdir = os.path.join(savedir, xpid)     \n",
    "if os.path.islink(ckpdir): ckpdir = os.readlink(ckpdir)  \n",
    "ckpdir =  os.path.abspath(os.path.expanduser(ckpdir))\n",
    "outdir = os.path.abspath(os.path.expanduser(outdir))\n",
    "\n",
    "config_path = os.path.join(ckpdir, 'config_c.yaml')\n",
    "flags = util.create_flags(config_path, save_flags=False)\n",
    "disable_thinker = flags.wrapper_type == 1\n",
    "\n",
    "env = Env(\n",
    "        name=flags.name,\n",
    "        env_n=env_n,\n",
    "        gpu=True,\n",
    "        train_model=False,\n",
    "        parallel=False,\n",
    "        savedir=savedir,        \n",
    "        xpid=xpid,\n",
    "        ckp=True,\n",
    "        return_x=True,\n",
    "        return_h=True,\n",
    "    )\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space \n",
    "\n",
    "actor_param = {\n",
    "    \"obs_space\": obs_space,\n",
    "    \"action_space\": action_space,\n",
    "    \"flags\": flags,\n",
    "    \"tree_rep_meaning\": env.get_tree_rep_meaning(),\n",
    "}\n",
    "actor_net = ActorNet(**actor_param)\n",
    "\n",
    "path = os.path.join(ckpdir, \"ckp_actor.tar\")\n",
    "checkpoint = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "actor_net.set_weights(checkpoint[\"actor_net_state_dict\"])\n",
    "actor_net.to(device)\n",
    "actor_net.train(False)\n",
    "\n",
    "state = env.reset()\n",
    "env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "\n",
    "# create dir\n",
    "\n",
    "n = 0\n",
    "while True:\n",
    "    name = \"%s-%d-%d\" % (xpid, checkpoint[\"real_step\"], n)\n",
    "    outdir_ = os.path.join(outdir, name)\n",
    "    if not os.path.exists(outdir_):\n",
    "        os.makedirs(outdir_)\n",
    "        print(f\"Outputting to {outdir_}\")\n",
    "        break\n",
    "    n += 1\n",
    "outdir = outdir_\n",
    "\n",
    "detect_buffer = DetectBuffer(outdir=outdir, t=12800//env_n, rec_t=flags.rec_t, logger=_logger, delay_n=delay_n)\n",
    "file_n = total_n // (env_n * detect_buffer.t) + 1\n",
    "_logger.info(f\"Data output directory: {outdir}\")\n",
    "_logger.info(f\"Number of file to be generated: {file_n}\")\n",
    "\n",
    "rescale = \"Sokoban\" in flags.name\n",
    "\n",
    "# save setting\n",
    "\n",
    "env_state_shape = env.observation_space[\"real_states\"].shape[1:]\n",
    "#if rescale: env_state_shape = (3, 40, 40)\n",
    "tree_rep_shape = env.observation_space[\"tree_reps\"].shape[1:]\n",
    "\n",
    "flags_detect = {\n",
    "    \"dim_actions\": actor_net.dim_actions,\n",
    "    \"num_actions\": actor_net.num_actions,\n",
    "    \"tuple_actions\": actor_net.tuple_action,\n",
    "    \"name\": flags.name,\n",
    "    \"env_state_shape\": list(env_state_shape),\n",
    "    \"tree_rep_shape\": list(tree_rep_shape),\n",
    "    \"rescale\": rescale,\n",
    "    \"rec_t\": flags.rec_t,\n",
    "    \"ckpdir\": ckpdir,\n",
    "    \"net_xpid\": xpid,\n",
    "}\n",
    "\n",
    "yaml_file_path = os.path.join(outdir, 'config_detect.yaml')\n",
    "with open(yaml_file_path, 'w') as file:\n",
    "    yaml.dump(flags_detect, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    \n",
    "    while(True):\n",
    "\n",
    "        actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        if not disable_thinker:\n",
    "            primary_action, reset_action = actor_out.action\n",
    "        else:\n",
    "            primary_action, reset_action = actor_out.action, None\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action, \n",
    "            action_prob=actor_out.action_prob[-1])    \n",
    "        env_out = create_env_out(actor_out.action, state, reward, done, info, flags=flags)\n",
    "        \n",
    "        # write to detect buffer\n",
    "        env_state = env_out.xs[0] \n",
    "        if rescale:\n",
    "            #env_state = F.interpolate(env_state , size=(40, 40), mode='bilinear', align_corners=False)\n",
    "            env_state = (env_state * 255).to(torch.uint8)\n",
    "\n",
    "        pri_action = actor_out.action[0]\n",
    "        reset_action = actor_out.action[1]\n",
    "        tree_rep = state[\"tree_reps\"]\n",
    "\n",
    "        xs = {\n",
    "            \"env_state\": env_state,\n",
    "            \"tree_rep\": tree_rep,\n",
    "            \"pri_action\": pri_action,\n",
    "            \"reset_action\": reset_action,\n",
    "        }\n",
    "        y = info['cost']\n",
    "        done = done\n",
    "        step_status = info['step_status'][0].item()\n",
    "\n",
    "        file_idx = detect_buffer.insert(xs, y, done, step_status)\n",
    "        if file_idx >= file_n: \n",
    "            # last file is for validation\n",
    "            os.rename(f'{outdir}/data_{file_idx}.pt', f'{outdir}/val.pt')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from thinker.model_net import BaseNet, FrameEncoder\n",
    "from thinker import util\n",
    "from thinker.core.file_writer import FileWriter\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None, data_n=None, prefix=\"data\"):\n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "        self.data = []        \n",
    "        self.samples_per_file = None   \n",
    "        self.data_n = data_n\n",
    "        self.prefix = prefix\n",
    "        self._preload_data(datadir)  # Preload data        \n",
    "\n",
    "    def _preload_data(self, datadir):\n",
    "        # Preload all .pt files\n",
    "        file_list = [f for f in os.listdir(datadir) if f.endswith('.pt') and f.startswith(self.prefix)]\n",
    "        for file_name in file_list:\n",
    "            print(f\"Starting to preload {file_name}\")\n",
    "            xs, y = torch.load(os.path.join(datadir, file_name))\n",
    "            if self.samples_per_file is None:  # Set samples_per_file based on the first file\n",
    "                self.t = xs['env_state'].shape[0]\n",
    "                self.b = xs['env_state'].shape[2]\n",
    "                self.samples_per_file = self.t * self.b\n",
    "            xs.pop('step_status')\n",
    "            xs.pop('done')\n",
    "            # Flatten data across t and b dimensions for easier indexing\n",
    "            for t_idx in range(self.t):\n",
    "                for b_idx in range(self.b):\n",
    "                    flattened_xs = {k: v[t_idx, :, b_idx] for k, v in xs.items()}\n",
    "                    flattened_y = y[t_idx, b_idx]\n",
    "                    self.data.append((flattened_xs, flattened_y))\n",
    "                    if self.data_n is not None and len(self.data) >= self.data_n: return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xs, y = self.data[idx]\n",
    "        if self.transform:\n",
    "            # Apply transform if necessary. Note: You might need to adjust this part\n",
    "            # based on what your transform expects and can handle\n",
    "            xs = {k: self.transform(v) for k, v in xs.items()}            \n",
    "        return xs, y\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 500):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "        return x\n",
    "\n",
    "class DetectFrameEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,     \n",
    "        dim_rep_actions,   \n",
    "        out_size=128,\n",
    "        stride=2,\n",
    "    ):  \n",
    "        super(DetectFrameEncoder, self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.encoder = FrameEncoder(prefix=\"se\",\n",
    "                                    actions_ch=dim_rep_actions,\n",
    "                                    input_shape=input_shape,                             \n",
    "                                    size_nn=1,             \n",
    "                                    downscale_c=2,    \n",
    "                                    concat_action=False)\n",
    "        \n",
    "        self.conv = []\n",
    "        in_ch =  self.encoder.out_shape[0]\n",
    "        for ch in [64]:\n",
    "            self.conv.append(nn.ReLU())\n",
    "            self.conv.append(nn.Conv2d(in_channels=in_ch,\n",
    "                                       out_channels=ch,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1,))\n",
    "            in_ch = ch\n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "        conv_out_size = in_ch * self.encoder.out_shape[1] * self.encoder.out_shape[2]\n",
    "        self.fc = nn.Sequential(nn.Linear(conv_out_size, self.out_size))       \n",
    "\n",
    "    def forward(self, x, action):\n",
    "        # x in shape of (B, C, H, W)\n",
    "        out, _ = self.encoder(x, done=None, actions=action, state={})\n",
    "        out = self.conv(out)\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out                                \n",
    "        \n",
    "class DetectNet(BaseNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_state_shape,\n",
    "        tree_rep_shape,\n",
    "        dim_actions,\n",
    "        num_actions,\n",
    "        detect_ab=(0,0),\n",
    "        clone=False,\n",
    "        tran_layer_n=3,\n",
    "    ):    \n",
    "        super(DetectNet, self).__init__()\n",
    "        \n",
    "        self.env_state_shape = env_state_shape # in (C, H, W) \n",
    "        self.tree_rep_shape = tree_rep_shape # in (C,) \n",
    "        self.dim_actions = dim_actions\n",
    "        self.num_actions = num_actions\n",
    "        self.dim_rep_actions = self.dim_actions if self.dim_actions > 1 else self.num_actions\n",
    "\n",
    "        self.detect_ab = detect_ab\n",
    "        self.clone = clone\n",
    "\n",
    "        self.enc_out_size = 128        \n",
    "        tran_nhead = 8\n",
    "        reminder = tran_nhead - ((self.enc_out_size + tree_rep_shape[0] + self.dim_rep_actions + 1) % tran_nhead)\n",
    "        self.enc_out_size += reminder\n",
    "        self.true_x_encoder = DetectFrameEncoder(input_shape=env_state_shape, dim_rep_actions=self.dim_rep_actions, out_size=self.enc_out_size)\n",
    "        self.pred_x_encoder = DetectFrameEncoder(input_shape=env_state_shape, dim_rep_actions=self.dim_rep_actions, out_size=self.enc_out_size)\n",
    "\n",
    "        self.embed_size = self.enc_out_size + tree_rep_shape[0] + num_actions + 1\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_size)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.embed_size, \n",
    "                                                   nhead=tran_nhead, \n",
    "                                                   dim_feedforward=512,\n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, tran_layer_n)\n",
    "        self.classifier = nn.Linear(self.embed_size, 1)\n",
    "\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5), requires_grad=False) # portion of negative class\n",
    "\n",
    "    def forward(self, env_state, tree_rep, action, reset):\n",
    "        \"\"\"\n",
    "        Forward pass of detection nn\n",
    "        Args:\n",
    "            env_state: float Tensor in shape of (B, rec_t, C, H, W); true and predicted frame\n",
    "            tree_rep: float Tensor in shape of (B, rec_t, C); model output\n",
    "            action: uint Tensor in shape of (B, rec_t, dim_actions); action (real / imaginary)\n",
    "            reset: bool Tensor in shape of  (B, rec_t); reset action\n",
    "        Return:\n",
    "            logit: float Tensor in shape of (B); logit of classifier output\n",
    "            p: float Tensor in shape of (B); prob of classifier output\n",
    "        \"\"\"\n",
    "        B, rec_t = env_state.shape[:2]\n",
    "        if self.detect_ab[0] in [1, 3] or self.detect_ab[1] in [1, 3]:\n",
    "            if self.clone: env_state = env_state.clone()                \n",
    "            if self.detect_ab[0] in [1, 3]:\n",
    "                env_state[:, 0] = 0.\n",
    "            if self.detect_ab[1] in [1, 3]:\n",
    "                env_state[:, 1:] = 0.\n",
    "        if self.detect_ab[0] in [2, 3] or self.detect_ab[1] in [2, 3]:\n",
    "            if self.clone: tree_rep = tree_rep.clone()\n",
    "            if self.detect_ab[0] in [2, 3]:\n",
    "                tree_rep[:, 0] = 0.\n",
    "            if self.detect_ab[1] in [2, 3]:\n",
    "                tree_rep[:, 1:] = 0.\n",
    "        \n",
    "        action = util.encode_action(action, self.dim_actions, self.num_actions)        \n",
    "        true_proc_x = self.true_x_encoder(env_state[:,0], action[:,0])\n",
    "        pred_proc_x = self.pred_x_encoder(\n",
    "            torch.flatten(env_state[:,1:], 0, 1),\n",
    "            torch.flatten(action[:,1:], 0, 1)\n",
    "                                        )\n",
    "        true_proc_x = true_proc_x.view(B, self.enc_out_size).unsqueeze(1) # (B, 1, C)\n",
    "        pred_proc_x = pred_proc_x.view(B, rec_t - 1, self.enc_out_size)  # (B, rec_t - 1, C)\n",
    "        proc_x = torch.concat([true_proc_x, pred_proc_x], dim=1) # (B, rec_t, C)\n",
    "        \n",
    "        embed = [proc_x, tree_rep, action, reset.unsqueeze(-1)]\n",
    "        embed = torch.concat(embed, dim=2) # (B, rec_t, embed_size)\n",
    "        embed_pos = self.pos_encoder(embed)\n",
    "        out = self.transformer_encoder(embed_pos)\n",
    "        logit = self.classifier(out[:, -1, :]).view(B)\n",
    "        return logit, torch.sigmoid(logit)\n",
    "\n",
    "def transform_data(xs, device):\n",
    "    xs_ = {}\n",
    "\n",
    "    env_state = xs[\"env_state\"]\n",
    "    if flags_data.rescale:\n",
    "        env_state = env_state.float() / 255\n",
    "    xs_[\"env_state\"] = env_state.to(device)\n",
    "\n",
    "    if \"tree_rep\" in xs: xs_[\"tree_rep\"] = xs[\"tree_rep\"].to(device)\n",
    "\n",
    "    action = xs[\"pri_action\"]\n",
    "    if not flags_data.tuple_actions:\n",
    "        action = action.unsqueeze(-1)\n",
    "    xs_[\"action\"] = action.to(device)\n",
    "\n",
    "    if \"reset_action\" in xs: xs_[\"reset\"] = xs[\"reset_action\"].to(device)\n",
    "    return xs_\n",
    "\n",
    "def evaluate_detect(target_y, pred_y):\n",
    "    # Binarize the predictions\n",
    "    pred_y_binarized = (pred_y > 0.5).float()\n",
    "    target_y = target_y.float()\n",
    "\n",
    "    # Compute the accuracy\n",
    "    acc = torch.mean((pred_y_binarized == target_y).float()).item()\n",
    "    \n",
    "    # Compute the recall\n",
    "    true_positives = (pred_y_binarized * target_y).sum().float()\n",
    "    possible_positives = target_y.sum().float()\n",
    "    rec = (true_positives / (possible_positives + 1e-6)).item()\n",
    "    \n",
    "    # Compute the precision\n",
    "    predicted_positives = pred_y_binarized.sum().float()\n",
    "    prec = (true_positives / (predicted_positives + 1e-6)).item()\n",
    "    \n",
    "    # Compute the F1 score\n",
    "    f1 = 2 * (prec * rec) / (prec + rec + 1e-6)   \n",
    "\n",
    "    neg_p = 1 - torch.mean(target_y.float()).item()\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"rec\": rec,\n",
    "        \"prec\": prec,\n",
    "        \"f1\": f1,\n",
    "        \"neg_p\": neg_p,\n",
    "        }\n",
    "\n",
    "def train_epoch(detect_net, dataloader, optimizer, device, flags, train=True):\n",
    "    if train:\n",
    "        detect_net.train()\n",
    "    else:\n",
    "        detect_net.eval()     \n",
    "    running_train_eval = {}   \n",
    "    with torch.set_grad_enabled(train):\n",
    "        step = 0\n",
    "        for xs, target_y in dataloader:\n",
    "            xs = transform_data(xs, device)\n",
    "            target_y = target_y.to(device)\n",
    "            \n",
    "            logit, pred_y = detect_net(**xs)\n",
    "            n_mean_y = torch.mean((~target_y).float()).item()\n",
    "            detect_net.beta.data = 0.99 * detect_net.beta.data + (1 - 0.99) * n_mean_y\n",
    "            detect_net.beta.data.clamp_(0.05, 0.95)\n",
    "            weights = torch.where(target_y == 1, detect_net.beta.data, 1-detect_net.beta.data)\n",
    "            loss = F.binary_cross_entropy_with_logits(logit, target_y.float(), weight=weights)\n",
    "            train_eval = evaluate_detect(target_y, pred_y)\n",
    "            train_eval[\"loss\"] = loss.item()\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "                loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "            \n",
    "            for key in train_eval.keys():\n",
    "                if key not in running_train_eval: \n",
    "                    running_train_eval[key] = train_eval[key]\n",
    "                else:\n",
    "                    running_train_eval[key] += train_eval[key]\n",
    "            step += 1\n",
    "    return {key: val / step for (key, val) in running_train_eval.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = argparse.Namespace()\n",
    "\n",
    "flags.datadir = \"../data/detect/v5_sok-14137536-0/\"\n",
    "flags.xpid = \"test\"\n",
    "flags.batch_size = 128\n",
    "flags.learning_rate = 0.0001\n",
    "flags.num_epochs = 100\n",
    "flags.data_n = 50000\n",
    "flags.ckp = True\n",
    "flags.wandb = False\n",
    "flags.project = \"detect_post\"\n",
    "\n",
    "if not flags.ckp:\n",
    "    flags.datadir = os.path.abspath(os.path.expanduser(flags.datadir))\n",
    "    # create ckp dir\n",
    "    xpid_n = 0\n",
    "    while (True):\n",
    "        xpid_ = flags.xpid if xpid_n == 0 else flags.xpid + f\"_{xpid_n}\"\n",
    "        ckpdir = os.path.join(flags.datadir, xpid_)\n",
    "        xpid_n += 1\n",
    "        if not os.path.exists(ckpdir):\n",
    "            os.mkdir(ckpdir) \n",
    "            flags.xpid = xpid_\n",
    "            break    \n",
    "else:\n",
    "    ckpdir = os.path.join(flags.datadir, flags.xpid)\n",
    "flags.ckpdir = ckpdir\n",
    "flags.ckp_path = os.path.join(ckpdir, \"ckp_detect.tar\")\n",
    "print(f\"Checkpoint path: {flags.ckp_path}\")\n",
    "\n",
    "# load data\n",
    "dataset = CustomDataset(datadir=flags.datadir, transform=None, data_n=flags.data_n)\n",
    "dataloader = DataLoader(dataset, batch_size=flags.batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(datadir=flags.datadir, transform=None, data_n=2000, prefix=\"val\")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=flags.batch_size, shuffle=True)\n",
    "\n",
    "# load setting\n",
    "yaml_file_path = os.path.join(flags.datadir, 'config_detect.yaml')\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    flags_data = yaml.safe_load(file)\n",
    "flags_data = argparse.Namespace(**flags_data)\n",
    "flags = argparse.Namespace(**{**vars(flags), **vars(flags_data)}) # merge the two flags\n",
    "\n",
    "plogger = FileWriter(\n",
    "    xpid=flags.net_xpid + '_' + flags.xpid,\n",
    "    xp_args=flags.__dict__,\n",
    "    rootdir=flags.ckpdir,\n",
    "    overwrite=not flags.ckp,\n",
    ")\n",
    "\n",
    "if flags.wandb: wlogger = util.Wandb(flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize net\n",
    "device = torch.device(\"cuda\")\n",
    "detect_net = DetectNet(\n",
    "    env_state_shape = flags_data.env_state_shape,\n",
    "    tree_rep_shape = flags_data.tree_rep_shape,\n",
    "    dim_actions = flags_data.dim_actions,\n",
    "    num_actions = flags_data.num_actions,\n",
    ")\n",
    "\n",
    "# load optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    detect_net.parameters(), lr=flags.learning_rate, \n",
    ")\n",
    "\n",
    "if flags.ckp:\n",
    "    checkpoint = torch.load(flags.ckp_path, torch.device(\"cpu\"))\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    detect_net.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    del checkpoint\n",
    "else:\n",
    "    epoch = 0\n",
    "\n",
    "detect_net = detect_net.to(device)\n",
    "util.optimizer_to(optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m print_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflags\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m stat\u001b[38;5;241m.\u001b[39mkeys(): \n\u001b[0;32m---> 12\u001b[0m     print_str \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_stat[key]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_stat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mkey]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(print_str)    \n\u001b[1;32m     15\u001b[0m epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m    \n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_acc'"
     ]
    }
   ],
   "source": [
    "while (epoch < flags.num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_train_eval = {}\n",
    "\n",
    "    train_stat = train_epoch(detect_net, dataloader, optimizer, device, flags, train=True)\n",
    "    val_stat = train_epoch(detect_net, val_dataloader, None, device, flags, train=False)\n",
    "    stat = {**train_stat, **{'val_' + key: value for key, value in val_stat.items()}}\n",
    "    stat[\"epoch\"] = epoch\n",
    "    plogger.log(stat)\n",
    "    if flags.use_wandb: wlogger.wandb.log(stat, step=stat['epoch'])\n",
    "\n",
    "    print_str = f'Epoch {epoch+1}/{flags.num_epochs},'\n",
    "    for key in stat.keys(): \n",
    "        print_str += f\" {key}:{stat[key]:.4f} ({stat['val_'+key]:.4f})\"\n",
    "    print(print_str)    \n",
    "\n",
    "    epoch += 1    \n",
    "    if epoch % 5 == 0 or epoch >= flags.num_epochs:\n",
    "        # save checkpoint\n",
    "        d = {\n",
    "            \"epoch\": epoch,\n",
    "            \"flags\": flags,\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"net_state_dict\": detect_net.state_dict(),\n",
    "        }\n",
    "        torch.save(d, flags.ckp_path)\n",
    "        print(f\"Checkpoint saved to {flags.ckp_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2288818359375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deprecated\n",
    "\n",
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from thinker import util\n",
    "\n",
    "datadir = \"../data/detect/v5_sok-5993808-1/\"\n",
    "datadir = os.path.abspath(os.path.expanduser(datadir))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None):\n",
    "        self.datadir = datadir\n",
    "        self.file_list = [f for f in os.listdir(datadir) if f.endswith('.pt')]\n",
    "        self.transform = transform\n",
    "        xs, y = torch.load(os.path.join(datadir, self.file_list[0]))        \n",
    "        self.t = xs['env_state'].shape[0]\n",
    "        self.b = xs['env_state'].shape[2]\n",
    "        self.samples_per_file = self.t * self.b\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) * self.samples_per_file  # Adjust based on your data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx // self.samples_per_file\n",
    "        within_file_idx = idx % self.samples_per_file\n",
    "        t_idx = within_file_idx // self.b\n",
    "        b_idx = within_file_idx % self.b\n",
    "        xs, y = torch.load(os.path.join(self.datadir, self.file_list[file_idx]))\n",
    "        xs.pop('step_status')\n",
    "        xs.pop('done')\n",
    "        xs = util.dict_map(xs, lambda x: x[t_idx, :, b_idx])\n",
    "        y = y[t_idx, b_idx]\n",
    "        return xs, y\n",
    "\n",
    "# To load data and train\n",
    "dataset = CustomDataset(datadir)\n",
    "# print(dataset[100])\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
