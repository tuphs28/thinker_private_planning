{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing v5b_sok_drc from ../logs/detect\n",
      "Initializing env 0 with device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../logs/detect/v5b_sok_drc/config_c.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data output directory: /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0\n",
      "Number of file to be generated: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputting to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import time, timeit\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from thinker.actor_net import ActorNet\n",
    "from thinker.main import Env\n",
    "import thinker.util as util\n",
    "from thinker.self_play import init_env_out, create_env_out\n",
    "\n",
    "class DetectBuffer:\n",
    "    def __init__(self, outdir, t, rec_t, logger, delay_n=5):\n",
    "        \"\"\"\n",
    "        Store training data grouped in planning stages and output\n",
    "        whenever the target output is also readydd\n",
    "            Args:\n",
    "                N (int): number of planning stage per training output\n",
    "                delay_n (int): number of planning stage delayed in the output y\n",
    "                rec_t (int): number of step in a planning stage\n",
    "                K (int): number of block to merge into\n",
    "        \"\"\"\n",
    "        self.outdir = outdir\n",
    "        self.t = t # number of time step per file\n",
    "        self.rec_t = rec_t\n",
    "        self.logger = logger        \n",
    "        self.delay_n = delay_n        \n",
    "\n",
    "        self.processed_n, self.xs, self.y, self.done, self.step_status = 0, [], [], [], []\n",
    "        self.file_idx = -1\n",
    "    \n",
    "    def insert(self, xs, y, done, step_status):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xs (dict): dictionary of training input, with each elem having the\n",
    "                shape of (B, *)            \n",
    "            y (tensor): bool tensor of shape (B), being the target output delayed by\n",
    "                delay_n planning stage            \n",
    "            done (tensor): bool tensor of shape (B), being the indicator of episode end\n",
    "            step_status (int): int indicating current step status\n",
    "        Output:\n",
    "            save train_xs in shape (N, rec_t, B, *) and train_y in shape (N, B)\n",
    "        \"\"\"\n",
    "        #print(\"data received! \", y.shape, id, cur_t)\n",
    "        last_step_real = (step_status == 0) | (step_status == 3)\n",
    "        if len(self.step_status) == 0 and not last_step_real: return self.file_idx  # skip until real step\n",
    "                \n",
    "        self.xs.append(util.dict_map(xs, lambda x:x.cpu()))\n",
    "        self.y.append(y.cpu())\n",
    "        self.done.append(done.cpu())\n",
    "        self.step_status.append(step_status)\n",
    "        self.processed_n += int(last_step_real)\n",
    "\n",
    "        if (self.processed_n >= self.t + self.delay_n + 1):               \n",
    "            self.file_idx += 1                     \n",
    "            out = self._extract_data(self.t)\n",
    "            self.processed_n = sum([int(i == 0) + int(i == 3) for i in self.step_status])\n",
    "            assert self.processed_n == self.delay_n+1, f\"should only have {self.delay_n + 1} data left instead of {self.processed_n}\"\n",
    "            path = f'{self.outdir}/data_{self.file_idx}.pt'\n",
    "            torch.save(out, path)\n",
    "            out_shape = out[0]['env_state'].shape\n",
    "            n = self.file_idx * out_shape[0] * out_shape[2]\n",
    "            self.logger.info(f\"{n}: File saved to {path}; env_state shape {out_shape}\")\n",
    "\n",
    "        return self.file_idx   \n",
    "\n",
    "    def _extract_data(self, t):\n",
    "        # obtain the first N planning stage and the corresponding target_y in data\n",
    "        xs, y, done, step_status = self._collect_data(t)\n",
    "        future_y, future_done = self._collect_data(self.delay_n, y_done_only=True)\n",
    "        y = torch.concat([y, future_y], dim=0)\n",
    "        done = torch.concat([done, future_done], dim=0)                \n",
    "        \n",
    "        last_step_real = (step_status == 0) | (step_status == 3)\n",
    "        assert last_step_real[0], \"cur_t should start with 0\"\n",
    "        assert last_step_real.shape[0] == t*self.rec_t, \\\n",
    "            f\" step_status.shape is {last_step_real.shape}, expected {t*self.rec_t} for the first dimension.\"        \n",
    "        assert y.shape[0] == (t + self.delay_n)*self.rec_t, \\\n",
    "            f\" y.shape is {y.shape}, expected {(t + self.delay_n)*self.rec_t} for the first dimension.\"        \n",
    "        \n",
    "        B = y.shape[1]\n",
    "        y = y.view(t + self.delay_n, self.rec_t, B)[:, 0]\n",
    "        done = done.view(t + self.delay_n, self.rec_t, B)[:, 0]\n",
    "        step_status = step_status.view(t, self.rec_t)\n",
    "        # compute target_y\n",
    "        target_y = self._compute_target_y(y, done, self.delay_n)\n",
    "\n",
    "        for k in xs.keys():\n",
    "            xs[k] = xs[k].view((t, self.rec_t) + xs[k].shape[1:])\n",
    "        \n",
    "        xs[\"done\"] = done[:t]\n",
    "        xs[\"step_status\"] = step_status\n",
    "                \n",
    "        return xs, target_y\n",
    "\n",
    "    def _collect_data(self, t, y_done_only=False):\n",
    "        # collect the first t stage from data\n",
    "        step_status = torch.tensor(self.step_status, dtype=torch.long)\n",
    "        next_step_real = (step_status == 2) | (step_status == 3)        \n",
    "        idx = torch.nonzero(next_step_real, as_tuple=False).squeeze()    \n",
    "        last_idx = idx[t-1] + 1\n",
    "        y = torch.stack(self.y[:last_idx], dim=0)\n",
    "        done = torch.stack(self.done[:last_idx], dim=0)\n",
    "        if not y_done_only:\n",
    "            xs = {}\n",
    "            for k in self.xs[0].keys():\n",
    "                xs[k] = torch.stack([v[k] for v in self.xs[:last_idx]], dim=0)                \n",
    "            step_status = step_status[:last_idx]\n",
    "            self.xs = self.xs[last_idx:]\n",
    "            self.y = self.y[last_idx:]\n",
    "            self.done = self.done[last_idx:]\n",
    "            self.step_status = self.step_status[last_idx:]\n",
    "            return xs, y, done, step_status\n",
    "        else:\n",
    "            return y, done\n",
    "        \n",
    "    def _compute_target_y(self, y, done, delay_n):        \n",
    "        # target_y[i] = (y[i] | (~done[i+1] & y[i+1]) | (~done[i+1] & ~done[i+2] & y[i+2]) | ... | (~done[i+1] & ~done[i+2] & ... & ~done[i+M] & y[i+M]))\n",
    "        t, b = y.shape\n",
    "        t = t - delay_n\n",
    "        not_done_cum = torch.ones(delay_n, t, b, dtype=bool)\n",
    "        target_y = y.clone()[:-delay_n]\n",
    "        not_done_cum[0] = ~done[1:1+t]\n",
    "        target_y = target_y | (not_done_cum[0] & y[1:1+t])\n",
    "        for m in range(1, delay_n):\n",
    "            not_done_cum[m] = not_done_cum[m-1] & ~done[m+1:m+1+t]\n",
    "            target_y = target_y | (not_done_cum[m] & y[m+1:m+1+t])\n",
    "        return target_y\n",
    "\n",
    "# ========================================================\n",
    "    \n",
    "total_n = 100000\n",
    "env_n = 128\n",
    "delay_n = 5\n",
    "savedir = \"../logs/detect\"\n",
    "outdir = \"../data/detect\"\n",
    "xpid = \"v5b_sok_drc\"\n",
    "\n",
    "# ========================================================\n",
    "\n",
    "_logger = util.logger()\n",
    "_logger.info(f\"Initializing {xpid} from {savedir}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "ckpdir = os.path.join(savedir, xpid)     \n",
    "if os.path.islink(ckpdir): ckpdir = os.readlink(ckpdir)  \n",
    "ckpdir =  os.path.abspath(os.path.expanduser(ckpdir))\n",
    "outdir = os.path.abspath(os.path.expanduser(outdir))\n",
    "\n",
    "config_path = os.path.join(ckpdir, 'config_c.yaml')\n",
    "flags = util.create_flags(config_path, save_flags=False)\n",
    "disable_thinker = flags.wrapper_type == 1\n",
    "\n",
    "env = Env(\n",
    "        name=flags.name,\n",
    "        env_n=env_n,\n",
    "        gpu=True,\n",
    "        train_model=False,\n",
    "        parallel=False,\n",
    "        savedir=savedir,        \n",
    "        xpid=xpid,\n",
    "        ckp=True,\n",
    "        return_x=True,\n",
    "        return_h=True,\n",
    "    )\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space \n",
    "\n",
    "actor_param = {\n",
    "    \"obs_space\": obs_space,\n",
    "    \"action_space\": action_space,\n",
    "    \"flags\": flags,\n",
    "    \"tree_rep_meaning\": env.get_tree_rep_meaning() if not disable_thinker else None,\n",
    "}\n",
    "actor_net = ActorNet(**actor_param)\n",
    "\n",
    "path = os.path.join(ckpdir, \"ckp_actor.tar\")\n",
    "checkpoint = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "actor_net.set_weights(checkpoint[\"actor_net_state_dict\"])\n",
    "actor_net.to(device)\n",
    "actor_net.train(False)\n",
    "\n",
    "state = env.reset()\n",
    "env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "\n",
    "# create dir\n",
    "\n",
    "n = 0\n",
    "while True:\n",
    "    name = \"%s-%d-%d\" % (xpid, checkpoint[\"real_step\"], n)\n",
    "    outdir_ = os.path.join(outdir, name)\n",
    "    if not os.path.exists(outdir_):\n",
    "        os.makedirs(outdir_)\n",
    "        print(f\"Outputting to {outdir_}\")\n",
    "        break\n",
    "    n += 1\n",
    "outdir = outdir_\n",
    "\n",
    "detect_buffer = DetectBuffer(outdir=outdir, t=12800//env_n, rec_t=flags.rec_t, logger=_logger, delay_n=delay_n)\n",
    "file_n = total_n // (env_n * detect_buffer.t) + 1\n",
    "_logger.info(f\"Data output directory: {outdir}\")\n",
    "_logger.info(f\"Number of file to be generated: {file_n}\")\n",
    "\n",
    "rescale = \"Sokoban\" in flags.name\n",
    "\n",
    "# save setting\n",
    "\n",
    "env_state_shape = env.observation_space[\"real_states\"].shape[1:]\n",
    "#if rescale: env_state_shape = (3, 40, 40)\n",
    "tree_rep_shape = env.observation_space[\"tree_reps\"].shape[1:] if not disable_thinker else None\n",
    "\n",
    "flags_detect = {\n",
    "    \"dim_actions\": actor_net.dim_actions,\n",
    "    \"num_actions\": actor_net.num_actions,\n",
    "    \"tuple_actions\": actor_net.tuple_action,\n",
    "    \"name\": flags.name,\n",
    "    \"env_state_shape\": list(env_state_shape),\n",
    "    \"tree_rep_shape\": list(tree_rep_shape) if not disable_thinker else None,\n",
    "    \"rescale\": rescale,\n",
    "    \"rec_t\": flags.rec_t,\n",
    "    \"ckpdir\": ckpdir,\n",
    "    \"net_xpid\": xpid,\n",
    "    \"disable_thinker\": disable_thinker,\n",
    "}\n",
    "\n",
    "yaml_file_path = os.path.join(outdir, 'config_detect.yaml')\n",
    "with open(yaml_file_path, 'w') as file:\n",
    "    yaml.dump(flags_detect, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0/data_0.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 246; Return  13.65089437825893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0/data_1.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 522; Return  12.714808594107172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0/data_2.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 801; Return  12.437977707508798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0/data_3.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1113; Return  12.481985800068655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0/data_4.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1408; Return  12.60919760573994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0/data_5.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1702; Return  12.595252807391097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0/data_6.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1989; Return  12.581000670378623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0/data_7.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2284; Return  12.581444997712318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-0/data_8.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False):\n",
    "\n",
    "    rets = []\n",
    "    last_file_idx = None\n",
    "    \n",
    "    while(True):\n",
    "\n",
    "        actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        if not disable_thinker:\n",
    "            primary_action, reset_action = actor_out.action\n",
    "        else:\n",
    "            primary_action, reset_action = actor_out.action, None\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action, \n",
    "            action_prob=actor_out.action_prob[-1])    \n",
    "        env_out = create_env_out(actor_out.action, state, reward, done, info, flags=flags)\n",
    "        if torch.any(done):\n",
    "            rets.extend(info[\"episode_return\"][done].cpu().tolist())\n",
    "        \n",
    "        # write to detect buffer\n",
    "        if not disable_thinker:\n",
    "            env_state = env_out.xs[0] \n",
    "            if rescale:\n",
    "                #env_state = F.interpolate(env_state , size=(40, 40), mode='bilinear', align_corners=False)\n",
    "                env_state = (env_state * 255).to(torch.uint8)\n",
    "        else:\n",
    "            env_state = env_out.real_states\n",
    "        pri_action = primary_action\n",
    "        xs = {\n",
    "            \"env_state\": env_state,\n",
    "            \"pri_action\": pri_action,            \n",
    "            \"cost\": info[\"cost\"],\n",
    "        }\n",
    "        if not disable_thinker:\n",
    "            xs.update({\n",
    "                \"tree_rep\": state[\"tree_reps\"],\n",
    "                \"reset_action\": actor_out.action[1],\n",
    "            })\n",
    "\n",
    "        y = info['cost']\n",
    "        done = done\n",
    "        step_status = info['step_status'][0].item()\n",
    "\n",
    "        file_idx = detect_buffer.insert(xs, y, done, step_status)\n",
    "        \n",
    "        if file_idx >= file_n: \n",
    "            # last file is for validation\n",
    "            os.rename(f'{outdir}/data_{file_idx}.pt', f'{outdir}/val.pt')\n",
    "            break\n",
    "\n",
    "        if last_file_idx is not None and file_idx != last_file_idx:\n",
    "            print(f\"Episode {len(rets)}; Return  {np.mean(np.array(rets))}\")\n",
    "\n",
    "        last_file_idx = file_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 3, 80, 80])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_out.real_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from thinker.model_net import BaseNet, FrameEncoder\n",
    "from thinker.actor_net import ShallowAFrameEncoder\n",
    "from thinker import util\n",
    "from thinker.core.file_writer import FileWriter\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None, data_n=None, prefix=\"data\"):\n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "        self.data = []        \n",
    "        self.samples_per_file = None   \n",
    "        self.data_n = data_n\n",
    "        self.prefix = prefix\n",
    "        self._preload_data(datadir)  # Preload data        \n",
    "\n",
    "    def _preload_data(self, datadir):\n",
    "        # Preload all .pt files\n",
    "        file_list = [f for f in os.listdir(datadir) if f.endswith('.pt') and f.startswith(self.prefix)]\n",
    "        file_list = sorted(file_list, key=lambda x: int(re.search(r'\\d+', x).group()) if re.search(r'\\d+', x) else 0)\n",
    "        for file_name in file_list:\n",
    "            print(f\"Starting to preload {file_name}\")\n",
    "            xs, y = torch.load(os.path.join(datadir, file_name))\n",
    "            if self.samples_per_file is None:  # Set samples_per_file based on the first file\n",
    "                self.t = xs['env_state'].shape[0]\n",
    "                self.b = xs['env_state'].shape[2]\n",
    "                self.samples_per_file = self.t * self.b\n",
    "            xs.pop('step_status')\n",
    "            xs.pop('done')\n",
    "            # Flatten data across t and b dimensions for easier indexing\n",
    "            for t_idx in range(self.t):\n",
    "                for b_idx in range(self.b):\n",
    "                    flattened_xs = {k: v[t_idx, :, b_idx] for k, v in xs.items()}\n",
    "                    flattened_y = y[t_idx, b_idx]\n",
    "                    self.data.append((flattened_xs, flattened_y))\n",
    "                    if self.data_n is not None and len(self.data) >= self.data_n: return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xs, y = self.data[idx]\n",
    "        if self.transform:\n",
    "            # Apply transform if necessary. Note: You might need to adjust this part\n",
    "            # based on what your transform expects and can handle\n",
    "            xs = {k: self.transform(v) for k, v in xs.items()}            \n",
    "        return xs, y\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 500):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "        return x\n",
    "\n",
    "class DetectFrameEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,     \n",
    "        out_size=128,\n",
    "        downscale=True,\n",
    "    ):  \n",
    "        super(DetectFrameEncoder, self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.encoder = FrameEncoder(prefix=\"se\",\n",
    "                                    actions_ch=None,\n",
    "                                    input_shape=input_shape,                             \n",
    "                                    size_nn=1,             \n",
    "                                    downscale_c=2,    \n",
    "                                    downscale=downscale,\n",
    "                                    concat_action=False)\n",
    "        \n",
    "        self.conv = []\n",
    "        in_ch =  self.encoder.out_shape[0]\n",
    "        for ch in [64]:\n",
    "            self.conv.append(nn.ReLU())\n",
    "            self.conv.append(nn.Conv2d(in_channels=in_ch,\n",
    "                                       out_channels=ch,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1,))\n",
    "            in_ch = ch\n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "        conv_out_size = in_ch * self.encoder.out_shape[1] * self.encoder.out_shape[2]\n",
    "        self.fc = nn.Sequential(nn.Linear(conv_out_size, self.out_size))       \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x in shape of (B, C, H, W)\n",
    "        out, _ = self.encoder(x, done=None, actions=None, state={})\n",
    "        out = self.conv(out)\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out                                \n",
    "        \n",
    "class DetectNet(BaseNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_state_shape,\n",
    "        tree_rep_shape,\n",
    "        hidden_state_shape,\n",
    "        dim_actions,\n",
    "        num_actions,\n",
    "        detect_ab=(0,0),\n",
    "        clone=False,\n",
    "        tran_layer_n=3,\n",
    "        tran_ff_n=512,\n",
    "        shallow_encode=False,\n",
    "        disable_thinker=False,\n",
    "    ):    \n",
    "        super(DetectNet, self).__init__()\n",
    "        \n",
    "        self.env_state_shape = env_state_shape # in (C, H, W) \n",
    "        self.tree_rep_shape = tree_rep_shape # in (C,) \n",
    "        self.hidden_state_shape = hidden_state_shape # in (inner_t, C, H, W)\n",
    "        self.dim_actions = dim_actions\n",
    "        self.num_actions = num_actions\n",
    "        self.dim_rep_actions = self.dim_actions if self.dim_actions > 1 else self.num_actions\n",
    "\n",
    "        self.detect_ab = detect_ab\n",
    "        self.clone = clone\n",
    "        self.disable_thinker = disable_thinker\n",
    "\n",
    "        self.enc_out_size = 128\n",
    "        tran_nhead = 8\n",
    "        if not self.disable_thinker:\n",
    "            reminder = tran_nhead - ((self.enc_out_size + tree_rep_shape[0] + self.dim_rep_actions + 1) % tran_nhead)\n",
    "        else:\n",
    "            reminder = tran_nhead - ((self.enc_out_size + self.dim_rep_actions) % tran_nhead)\n",
    "        self.enc_out_size += reminder\n",
    "\n",
    "        FrameEncoder = ShallowAFrameEncoder if shallow_encode else DetectFrameEncoder\n",
    "        self.true_x_encoder = FrameEncoder(input_shape=env_state_shape, out_size=self.enc_out_size)\n",
    "        if not self.disable_thinker:\n",
    "            self.pred_x_encoder = FrameEncoder(input_shape=env_state_shape, out_size=self.enc_out_size)\n",
    "        if hidden_state_shape is not None:\n",
    "            self.h_encoder = FrameEncoder(input_shape=hidden_state_shape[1:], out_size=self.enc_out_size, downscale=False)   \n",
    "\n",
    "        #self.pred_x_encoder = self.true_x_encoder\n",
    "        if not self.disable_thinker:\n",
    "            self.embed_size = self.enc_out_size + tree_rep_shape[0] + self.dim_rep_actions + 1\n",
    "        else:\n",
    "            self.embed_size = self.enc_out_size + self.dim_rep_actions\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_size)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.embed_size, \n",
    "                                                   nhead=tran_nhead, \n",
    "                                                   dim_feedforward=tran_ff_n,\n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, tran_layer_n)\n",
    "        self.classifier = nn.Linear(self.embed_size, 1)\n",
    "\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5), requires_grad=False) # portion of negative class\n",
    "\n",
    "    def forward(self, env_state, tree_rep, hidden_state, action, reset):\n",
    "        \"\"\"\n",
    "        Forward pass of detection nn\n",
    "        Args:\n",
    "            env_state: float Tensor in shape of (B, rec_t, C, H, W); true and predicted frame\n",
    "            tree_rep: float Tensor in shape of (B, rec_t, C); model output\n",
    "            hidden_state: float Tensor in shape of (B, rec_t, inner_t, C, H, W); model output\n",
    "            action: uint Tensor in shape of (B, rec_t, dim_actions); action (real / imaginary)\n",
    "            reset: bool Tensor in shape of  (B, rec_t); reset action\n",
    "        Return:\n",
    "            logit: float Tensor in shape of (B); logit of classifier output\n",
    "            p: float Tensor in shape of (B); prob of classifier output\n",
    "        \"\"\"\n",
    "        B, rec_t = env_state.shape[:2]\n",
    "        if self.detect_ab[0] in [1, 3] or self.detect_ab[1] in [1, 3]:\n",
    "            if self.clone: env_state = env_state.clone()                \n",
    "            if self.detect_ab[0] in [1, 3]:\n",
    "                env_state[:, 0] = 0.\n",
    "            if self.detect_ab[1] in [1, 3]:\n",
    "                env_state[:, 1:] = 0.\n",
    "        if self.detect_ab[0] in [2, 3] or self.detect_ab[1] in [2, 3]:\n",
    "            if self.clone: tree_rep = tree_rep.clone()\n",
    "            if self.detect_ab[0] in [2, 3]:\n",
    "                tree_rep[:, 0] = 0.\n",
    "            if self.detect_ab[1] in [2, 3]:\n",
    "                tree_rep[:, 1:] = 0.\n",
    "        \n",
    "        action = util.encode_action(action, self.dim_actions, self.num_actions)        \n",
    "        true_proc_x = self.true_x_encoder(env_state[:,0])\n",
    "        true_proc_x = true_proc_x.view(B, self.enc_out_size).unsqueeze(1) # (B, 1, C)\n",
    "        if not self.disable_thinker:\n",
    "            pred_proc_x = self.pred_x_encoder(\n",
    "                torch.flatten(env_state[:,1:], 0, 1),\n",
    "                                            )            \n",
    "            pred_proc_x = pred_proc_x.view(B, rec_t - 1, self.enc_out_size)  # (B, rec_t - 1, C)\n",
    "            proc_x = torch.concat([true_proc_x, pred_proc_x], dim=1) # (B, rec_t, C)            \n",
    "            embed = [proc_x, tree_rep, action, reset.unsqueeze(-1)]\n",
    "        else:\n",
    "            proc_h = self.h_encoder(torch.flatten(hidden_state[:,0], 0, 1))\n",
    "            proc_h = proc_h.view(B, -1, self.enc_out_size)  # (B, inner_t, C)\n",
    "            proc_x = torch.concat([true_proc_x, proc_h], dim=1) # (B, 1 + inner_t, C)\n",
    "            embed = [proc_x, torch.broadcast_to(action, (B, proc_x.shape[1], self.dim_rep_actions))]\n",
    "            \n",
    "        embed = torch.concat(embed, dim=2) # (B, rec_t, embed_size)\n",
    "        embed_pos = self.pos_encoder(embed)\n",
    "        out = self.transformer_encoder(embed_pos)\n",
    "        logit = self.classifier(out[:, -1, :]).view(B)\n",
    "        return logit, torch.sigmoid(logit)\n",
    "\n",
    "def transform_data(xs, device, flags):\n",
    "    xs_ = {}\n",
    "\n",
    "    env_state = xs[\"env_state\"]\n",
    "    if flags.rescale:\n",
    "        env_state = env_state.float() / 255\n",
    "    xs_[\"env_state\"] = env_state.to(device)\n",
    "\n",
    "    xs_[\"tree_rep\"] = xs[\"tree_rep\"].to(device) if \"tree_rep\" in xs else None\n",
    "\n",
    "    action = xs[\"pri_action\"]\n",
    "    if not flags.tuple_actions:\n",
    "        action = action.unsqueeze(-1)\n",
    "    xs_[\"action\"] = action.to(device)\n",
    "\n",
    "    xs_[\"reset\"] = xs[\"reset_action\"].to(device) if \"reset_action\" in xs else None\n",
    "    xs_[\"hidden_state\"] = xs[\"hidden_state\"].to(device) if \"hidden_state\" in xs else None\n",
    "    return xs_\n",
    "\n",
    "def evaluate_detect(target_y, pred_y):\n",
    "    # Binarize the predictions\n",
    "    pred_y_binarized = (pred_y > 0.5).float()\n",
    "    target_y = target_y.float()\n",
    "\n",
    "    # Compute the accuracy\n",
    "    acc = torch.mean((pred_y_binarized == target_y).float()).item()\n",
    "    \n",
    "    # Compute the recall\n",
    "    true_positives = (pred_y_binarized * target_y).sum().float()\n",
    "    possible_positives = target_y.sum().float()\n",
    "    rec = (true_positives / (possible_positives + 1e-6)).item()\n",
    "    \n",
    "    # Compute the precision\n",
    "    predicted_positives = pred_y_binarized.sum().float()\n",
    "    prec = (true_positives / (predicted_positives + 1e-6)).item()\n",
    "    \n",
    "    # Compute the F1 score\n",
    "    f1 = 2 * (prec * rec) / (prec + rec + 1e-6)   \n",
    "\n",
    "    neg_p = 1 - torch.mean(target_y.float()).item()\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"rec\": rec,\n",
    "        \"prec\": prec,\n",
    "        \"f1\": f1,\n",
    "        \"neg_p\": neg_p,\n",
    "        }\n",
    "\n",
    "def train_epoch(detect_net, dataloader, optimizer, device, flags, train=True):\n",
    "    if train:\n",
    "        detect_net.train()\n",
    "    else:\n",
    "        detect_net.eval()     \n",
    "    running_train_eval = {}   \n",
    "    with torch.set_grad_enabled(train):\n",
    "        step = 0\n",
    "        for xs, target_y in dataloader:\n",
    "            xs = transform_data(xs, device, flags)\n",
    "            target_y = target_y.to(device)\n",
    "            \n",
    "            logit, pred_y = detect_net(**xs)\n",
    "            n_mean_y = torch.mean((~target_y).float()).item()\n",
    "            detect_net.beta.data = 0.99 * detect_net.beta.data + (1 - 0.99) * n_mean_y\n",
    "            detect_net.beta.data.clamp_(0.05, 0.95)\n",
    "            weights = torch.where(target_y == 1, detect_net.beta.data, 1-detect_net.beta.data)\n",
    "            loss = F.binary_cross_entropy_with_logits(logit, target_y.float(), weight=weights)\n",
    "            train_eval = evaluate_detect(target_y, pred_y)\n",
    "            train_eval[\"loss\"] = loss.item()\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "                loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "            \n",
    "            for key in train_eval.keys():\n",
    "                if key not in running_train_eval: \n",
    "                    running_train_eval[key] = train_eval[key]\n",
    "                else:\n",
    "                    running_train_eval[key] += train_eval[key]\n",
    "            step += 1\n",
    "    return {key: val / step for (key, val) in running_train_eval.items()}\n",
    "\n",
    "def save_ckp(path, epoch, flags, optimizer, detect_net):\n",
    "    # save checkpoint\n",
    "    d = {\n",
    "        \"epoch\": epoch,\n",
    "        \"flags\": flags,\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"net_state_dict\": detect_net.state_dict(),\n",
    "    }\n",
    "    torch.save(d, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "s = 10000\n",
    "data_n = 100000\n",
    "len_list = [s for _ in range(data_n // s)]\n",
    "if data_n % s > 0: len_list.append(data_n % s)\n",
    "print(sum(len_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None, data_n=None, prefix=\"data\", n_chunks=1):\n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "        self.data = []  # Current chunk of data\n",
    "        self.data_n = data_n\n",
    "        self.prefix = prefix\n",
    "        self.file_list = [f for f in os.listdir(datadir) if f.endswith('.pt') and f.startswith(self.prefix)]\n",
    "        self.file_list = sorted(self.file_list, key=lambda x: int(re.search(r'\\d+', x).group()) if re.search(r'\\d+', x) else 0)\n",
    "        \n",
    "        xs, y = torch.load(os.path.join(self.datadir, self.file_list[0]))\n",
    "        self.t = xs['env_state'].shape[0]\n",
    "        self.b = xs['env_state'].shape[2]\n",
    "        self.samples_per_file = self.t * self.b\n",
    "\n",
    "        if data_n is not None:\n",
    "            self.len_list = [self.samples_per_file for _ in range(data_n // self.samples_per_file)]\n",
    "            if data_n % self.samples_per_file > 0: self.len_list.append(data_n % self.samples_per_file)\n",
    "            self.file_list = self.file_list[:len(self.len_list)]\n",
    "        else:\n",
    "            self.len_list = [self.samples_per_file for _ in range(len(self.file_list))]\n",
    "        \n",
    "        self.n_chunks = n_chunks\n",
    "        self.current_chunk = 0  # To track which chunk is currently loaded\n",
    "        self.total_files = len(self.file_list)\n",
    "        self.files_per_chunk = max(1, self.total_files // self.n_chunks)\n",
    "\n",
    "    def _load_chunk(self, chunk_index):\n",
    "        # Determine file range for the current chunk\n",
    "        start_file = chunk_index * self.files_per_chunk\n",
    "        end_file = min(start_file + self.files_per_chunk, self.total_files)\n",
    "        self.data = []  # Clear current data\n",
    "        self.current_chunk = chunk_index\n",
    "        print(\"start_file: \", start_file)\n",
    "        print(\"end_file: \", end_file)\n",
    "        for i in range(start_file, end_file):\n",
    "            data_tmp = []\n",
    "            file_name = self.file_list[i]\n",
    "            print(f\"Loading {file_name}\")\n",
    "            xs, y = torch.load(os.path.join(self.datadir, file_name))\n",
    "            xs.pop('step_status', None)\n",
    "            xs.pop('done', None)\n",
    "            \n",
    "            for t_idx in range(self.t):\n",
    "                for b_idx in range(self.b):\n",
    "                    flattened_xs = {k: v[t_idx, :, b_idx] for k, v in xs.items()}\n",
    "                    flattened_y = y[t_idx, b_idx]\n",
    "                    data_tmp.append((flattened_xs, flattened_y))\n",
    "                    if len(data_tmp) >= self.len_list[i]: \n",
    "                        return  # Stop loading if we reach limit\n",
    "        \n",
    "            assert len(data_tmp) == self.len_list[i], f\"data {i} should have at least {self.len_list[i]} samples instead of {len(data_tmp)}\"\n",
    "            self.data.extend(data_tmp)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.data_n is not None: return self.data_n\n",
    "        return self.samples_per_file * self.total_files\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate which chunk the idx falls into        \n",
    "        chunk_index = idx // (self.__len__() // self.n_chunks)\n",
    "        print(\"chunk_index\", chunk_index)\n",
    "        \n",
    "        # If the requested idx is not in the current chunk, load the correct chunk\n",
    "        if chunk_index != self.current_chunk or not self.data:\n",
    "            self._load_chunk(chunk_index)\n",
    "        \n",
    "        # Adjust idx to the current chunk\n",
    "        idx_within_chunk = idx % (self.__len__() // self.n_chunks)\n",
    "        print(\"idx_within_chunk\", idx_within_chunk)\n",
    "        xs, y = self.data[idx_within_chunk]\n",
    "        if self.transform:\n",
    "            xs = {k: self.transform(v) for k, v in xs.items()}\n",
    "        return xs, y\n",
    "    \n",
    "dataset = CustomDataset(datadir=\"../data/detect/v5b_sok_drc-49622080-1/\", transform=None, data_n=100000, n_chunks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_index 0\n",
      "Loading data_0.pt\n",
      "idx_within_chunk 0\n"
     ]
    }
   ],
   "source": [
    "for n, i in enumerate(dataset):\n",
    "    if n % 100 == 0: print(n)\n",
    "    m = dataset[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_index 0\n",
      "Loading data_0.pt\n",
      "idx_within_chunk 20000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 75\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     73\u001b[0m idx_within_chunk \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m() \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_chunks)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx_within_chunk\u001b[39m\u001b[38;5;124m\"\u001b[39m, idx_within_chunk)\n\u001b[0;32m---> 75\u001b[0m xs, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_within_chunk\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     77\u001b[0m     xs \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m xs\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "dataset[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-23691168-0/test/ckp_detect.tar\n"
     ]
    }
   ],
   "source": [
    "flags = argparse.Namespace()\n",
    "\n",
    "# ========================================================\n",
    "\n",
    "flags.dxpid = \"v5_sok-23691168-0\"\n",
    "flags.dproject = \"detect\"\n",
    "flags.datadir = \"../data/__dproject__/__dxpid__/\"\n",
    "flags.txpid = \"test\"\n",
    "flags.project = \"detect_post\"\n",
    "flags.batch_size = 128\n",
    "flags.learning_rate = 0.0001\n",
    "flags.num_epochs = 50\n",
    "flags.early_stop_n = -1\n",
    "flags.data_n = 50000\n",
    "flags.ckp = False\n",
    "flags.use_wandb = False\n",
    "flags.tran_layer_n = 3\n",
    "flags.tran_ff_n = 512\n",
    "flags.shallow_encode = False\n",
    "\n",
    "flags.datadir = flags.datadir.replace(\"__dproject__\", flags.dproject)\n",
    "flags.datadir = flags.datadir.replace(\"__dxpid__\", flags.dxpid)\n",
    "# ========================================================\n",
    "if not flags.ckp:\n",
    "    flags.datadir = os.path.abspath(os.path.expanduser(flags.datadir))\n",
    "    # create ckp dir\n",
    "    xpid_n = 0\n",
    "    while (True):\n",
    "        xpid_ = flags.txpid if xpid_n == 0 else flags.txpid + f\"_{xpid_n}\"\n",
    "        ckpdir = os.path.join(flags.datadir, xpid_)\n",
    "        xpid_n += 1\n",
    "        if not os.path.exists(ckpdir):\n",
    "            os.mkdir(ckpdir) \n",
    "            flags.txpid = xpid_\n",
    "            break    \n",
    "else:\n",
    "    ckpdir = os.path.join(flags.datadir, flags.txpid)\n",
    "flags.tckpdir = ckpdir\n",
    "flags.tckp_path = os.path.join(ckpdir, \"ckp_detect.tar\")\n",
    "flags.tckp_path_b = os.path.join(ckpdir, \"ckp_detect_best.tar\")\n",
    "print(f\"Checkpoint path: {flags.tckp_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to preload data_0.pt\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(datadir=flags.datadir, transform=None, data_n=10000)\n",
    "dataloader = DataLoader(dataset, batch_size=flags.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, y = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 80, 80])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[\"env_state\"][1,0].float.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(20):\n",
    "    util.plot_raw_state((xs[\"env_state\"][100,s].float()*255).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data\n",
    "dataset = CustomDataset(datadir=flags.datadir, transform=None, data_n=flags.data_n)\n",
    "dataloader = DataLoader(dataset, batch_size=flags.batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(datadir=flags.datadir, transform=None, data_n=5000, prefix=\"val\")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=flags.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load setting\n",
    "yaml_file_path = os.path.join(flags.datadir, 'config_detect.yaml')\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    flags_data = yaml.safe_load(file)\n",
    "flags_data = argparse.Namespace(**flags_data)\n",
    "flags = argparse.Namespace(**{**vars(flags), **vars(flags_data)}) # merge the two flags\n",
    "\n",
    "plogger = FileWriter(\n",
    "    xpid=flags.txpid,\n",
    "    xp_args=flags.__dict__,\n",
    "    rootdir=flags.datadir,\n",
    "    overwrite=not flags.ckp,\n",
    ")\n",
    "flags.full_xpid = flags.dxpid + \"_\" + flags.txpid\n",
    "\n",
    "if flags.use_wandb: wlogger = util.Wandb(flags)\n",
    "\n",
    "# initalize net\n",
    "device = torch.device(\"cuda\")\n",
    "detect_net = DetectNet(\n",
    "    env_state_shape = flags_data.env_state_shape,\n",
    "    tree_rep_shape = getattr(flags_data, \"tree_rep_shape\", None),\n",
    "    hidden_state_shape = getattr(flags_data, \"hidden_state_shape\", None),\n",
    "    dim_actions = flags_data.dim_actions,\n",
    "    num_actions = flags_data.num_actions,\n",
    "    disable_thinker = flags.disable_thinker,\n",
    "    tran_layer_n = flags.tran_layer_n,\n",
    "    tran_ff_n = flags.tran_ff_n,\n",
    "    shallow_encode= flags.shallow_encode,\n",
    ")\n",
    "\n",
    "# load optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    detect_net.parameters(), lr=flags.learning_rate, \n",
    ")\n",
    "\n",
    "if flags.ckp:\n",
    "    checkpoint = torch.load(flags.tckp_path, torch.device(\"cpu\"))\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    detect_net.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    del checkpoint\n",
    "else:\n",
    "    epoch = 0\n",
    "\n",
    "detect_net = detect_net.to(device)\n",
    "util.optimizer_to(optimizer, device)\n",
    "\n",
    "print(\"Detect network size: %d\"\n",
    "        % sum(p.numel() for p in detect_net.parameters())\n",
    "    )\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epoch_since_improve = 0\n",
    "\n",
    "while (epoch < flags.num_epochs):\n",
    "    train_stat = train_epoch(detect_net, dataloader, optimizer, device, flags, train=True)\n",
    "    val_stat = train_epoch(detect_net, val_dataloader, None, device, flags, train=False)\n",
    "    stat = {**train_stat, **{'val/' + key: value for key, value in val_stat.items()}}\n",
    "    stat[\"epoch\"] = epoch\n",
    "    plogger.log(stat)\n",
    "    if flags.use_wandb: wlogger.wandb.log(stat, step=stat['epoch'])\n",
    "    \n",
    "    epoch += 1    \n",
    "    print_str = f'Epoch {epoch}/{flags.num_epochs},'\n",
    "    for key in stat.keys(): \n",
    "        if 'val/' + key in stat.keys():\n",
    "            print_str += f\" {key}:{stat[key]:.4f} ({stat['val/'+key]:.4f})\"\n",
    "    print(print_str)   \n",
    "            \n",
    "    # Early stopping and best model saving logic\n",
    "    if flags.early_stop_n >= 0:  # Check if early stopping is enabled\n",
    "        current_val_loss = val_stat['loss']  # Assuming val_stat contains the validation loss\n",
    "        if current_val_loss < best_val_loss:\n",
    "            best_val_loss = current_val_loss\n",
    "            epoch_since_improve = 0\n",
    "            save_ckp(flags.tckp_path_b, epoch, flags, optimizer, detect_net)\n",
    "            print(f\"New best model saved to {flags.tckp_path_b}\")\n",
    "        else:\n",
    "            epoch_since_improve += 1\n",
    "        \n",
    "        if epoch_since_improve > flags.early_stop_n:\n",
    "            print(f\"Stopping early at epoch {epoch} due to no improvement in validation loss for {flags.early_stop_n} consecutive epochs.\")\n",
    "            break  # Stop the training loop\n",
    "\n",
    "    if epoch % 5 == 0 or epoch >= flags.num_epochs:\n",
    "        save_ckp(flags.tckp_path, epoch, flags, optimizer, detect_net)\n",
    "        print(f\"Checkpoint saved to {flags.tckp_path}\")\n",
    "\n",
    "    if flags.use_wandb and (epoch % 10 == 0 or epoch >= flags.num_epochs):\n",
    "        wlogger.wandb.save(\n",
    "            os.path.join(flags.tckpdir, \"*\"), flags.tckpdir\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2288818359375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deprecated\n",
    "\n",
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from thinker import util\n",
    "\n",
    "datadir = \"../data/detect/v5_sok-5993808-1/\"\n",
    "datadir = os.path.abspath(os.path.expanduser(datadir))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None):\n",
    "        self.datadir = datadir\n",
    "        self.file_list = [f for f in os.listdir(datadir) if f.endswith('.pt')]\n",
    "        self.transform = transform\n",
    "        xs, y = torch.load(os.path.join(datadir, self.file_list[0]))        \n",
    "        self.t = xs['env_state'].shape[0]\n",
    "        self.b = xs['env_state'].shape[2]\n",
    "        self.samples_per_file = self.t * self.b\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) * self.samples_per_file  # Adjust based on your data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx // self.samples_per_file\n",
    "        within_file_idx = idx % self.samples_per_file\n",
    "        t_idx = within_file_idx // self.b\n",
    "        b_idx = within_file_idx % self.b\n",
    "        xs, y = torch.load(os.path.join(self.datadir, self.file_list[file_idx]))\n",
    "        xs.pop('step_status')\n",
    "        xs.pop('done')\n",
    "        xs = util.dict_map(xs, lambda x: x[t_idx, :, b_idx])\n",
    "        y = y[t_idx, b_idx]\n",
    "        return xs, y\n",
    "\n",
    "# To load data and train\n",
    "dataset = CustomDataset(datadir)\n",
    "# print(dataset[100])\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing env 0 with device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symlinked log directory: /mnt/c/Users/chung/Personal/RS/thinker/notebook/logs/latest\n",
      "Wrote config file to /mnt/c/Users/chung/Personal/RS/thinker/notebook/logs/detect-20240204-023020/config_c.yaml\n"
     ]
    }
   ],
   "source": [
    "from thinker.actor_net import DRCNet\n",
    "from thinker.main import Env\n",
    "from thinker.self_play import init_env_out, create_env_out\n",
    "from thinker import util\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "env_n = 16\n",
    "flags = util.create_setting(args=[], drc=True, save_flags=False, wrapper_type=1)\n",
    "env = Env(\n",
    "        name=\"Sokoban-v0\",\n",
    "        env_n=env_n,\n",
    "        gpu=True,\n",
    "        train_model=False,\n",
    "        parallel=False,\n",
    "        return_x=True,\n",
    "        return_h=True,\n",
    "        flags=flags,\n",
    "        wrapper_type=1,\n",
    "    )\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space \n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "actor_net = DRCNet(obs_space=obs_space, action_space=action_space, flags=flags, tree_rep_meaning=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"../logs/detect/v1a_drc/ckp_actor.tar\")\n",
    "actor_net.load_state_dict(checkpoint[\"actor_net_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_ = torch.load(\"../logs/detect/v5b_sok_drc/ckp_actor.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_net.to(device)\n",
    "state = env.reset()\n",
    "env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "rets = []\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    \n",
    "    while(True):\n",
    "        actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        primary_action, reset_action = actor_out.action, None\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action)    \n",
    "        if torch.any(done):\n",
    "            rets.extend(info[\"episode_return\"][done].cpu().tolist())            \n",
    "            print(f\"Episode {len(rets)}; Return  {np.mean(np.array(rets))}\")\n",
    "        env_out = create_env_out(primary_action, state, reward, done, info, flags=flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ['step', 'real_step', 'actor_net_optimizer_state_dict', 'actor_net_scheduler_state_dict', 'actor_net_state_dict']\n",
    "for c in cs: checkpoint_[c] = checkpoint[c]\n",
    "torch.save(checkpoint_, \"../logs/detect/v5b_sok_drc/ckp_actor.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1; Return  13.789999961853027\n",
      "Episode 2; Return  13.78000020980835\n",
      "Episode 3; Return  13.773333549499512\n",
      "Episode 4; Return  13.760000228881836\n",
      "Episode 5; Return  13.744000244140626\n",
      "Episode 6; Return  13.730000178019205\n",
      "Episode 8; Return  13.710000038146973\n",
      "Episode 9; Return  13.700000021192762\n",
      "Episode 11; Return  13.683636491948908\n",
      "Episode 12; Return  13.674166758855185\n",
      "Episode 13; Return  13.680000085097094\n",
      "Episode 15; Return  13.670000076293945\n",
      "Episode 16; Return  13.658750057220459\n",
      "Episode 17; Return  13.662353010738597\n",
      "Episode 18; Return  13.651666747199165\n",
      "Episode 19; Return  13.654210592571058\n",
      "Episode 20; Return  13.655500078201294\n",
      "Episode 21; Return  13.652857235499791\n",
      "Episode 23; Return  13.656087046084197\n",
      "Episode 24; Return  13.651250084241232\n",
      "Episode 25; Return  13.649200096130372\n",
      "Episode 26; Return  13.650769343742958\n",
      "Episode 27; Return  13.653703795539009\n",
      "Episode 28; Return  13.652857235499791\n",
      "Episode 30; Return  13.649000072479248\n",
      "Episode 32; Return  13.656562596559525\n",
      "Episode 33; Return  13.659090995788574\n",
      "Episode 34; Return  13.651764785542207\n",
      "Episode 35; Return  13.286000108718872\n",
      "Episode 36; Return  13.28666678402159\n",
      "Episode 37; Return  13.294594706715765\n",
      "Episode 38; Return  13.30078960092444\n",
      "Episode 39; Return  13.313333456332867\n",
      "Episode 40; Return  13.319000118970871\n",
      "Episode 42; Return  13.33547630196526\n",
      "Episode 43; Return  13.344651261041331\n",
      "Episode 44; Return  13.349772816354578\n",
      "Episode 45; Return  13.35822229915195\n",
      "Episode 46; Return  13.34826095726179\n",
      "Episode 47; Return  13.350000092323791\n",
      "Episode 48; Return  13.352708434065184\n",
      "Episode 49; Return  13.358367447950402\n",
      "Episode 51; Return  13.099411870626842\n",
      "Episode 52; Return  13.110961638677578\n",
      "Episode 53; Return  13.120566133901757\n",
      "Episode 55; Return  13.142181916399435\n",
      "Episode 56; Return  12.921964398186121\n",
      "Episode 57; Return  12.9357895848521\n",
      "Episode 58; Return  12.951034586748172\n",
      "Episode 59; Return  12.964745868818234\n",
      "Episode 60; Return  12.97716677164038\n",
      "Episode 61; Return  12.985573869992475\n",
      "Episode 63; Return  13.005238207323211\n",
      "Episode 65; Return  13.022923197883825\n",
      "Episode 66; Return  13.03060619013779\n",
      "Episode 68; Return  13.04661778119557\n",
      "Episode 69; Return  12.854927674583767\n",
      "Episode 70; Return  12.867428701264517\n",
      "Episode 71; Return  12.87859167515392\n",
      "Episode 72; Return  12.888472348451614\n",
      "Episode 74; Return  12.909864995930645\n",
      "Episode 75; Return  12.919200137456258\n",
      "Episode 76; Return  12.929210665978884\n",
      "Episode 77; Return  12.940389747743483\n",
      "Episode 78; Return  12.94833347430596\n",
      "Episode 79; Return  12.95734190940857\n",
      "Episode 80; Return  12.965125134587288\n",
      "Episode 81; Return  12.96679026109201\n",
      "Episode 82; Return  12.974634289741516\n",
      "Episode 83; Return  12.978674845523145\n",
      "Episode 84; Return  12.986904907794226\n",
      "Episode 85; Return  12.992941320643705\n",
      "Episode 86; Return  12.99790712012801\n",
      "Episode 88; Return  12.863750154321844\n",
      "Episode 90; Return  12.882555707295735\n",
      "Episode 91; Return  12.888901249393003\n",
      "Episode 92; Return  12.89728275589321\n",
      "Episode 93; Return  12.904301222934517\n",
      "Episode 94; Return  12.913829935357926\n",
      "Episode 95; Return  12.923789616634972\n",
      "Episode 96; Return  12.92927098274231\n",
      "Episode 97; Return  12.936288813954777\n",
      "Episode 98; Return  12.822959347647064\n",
      "Episode 99; Return  12.830808246978606\n",
      "Episode 101; Return  12.847524919132194\n",
      "Episode 102; Return  12.854902129547268\n",
      "Episode 103; Return  12.738252603891985\n",
      "Episode 104; Return  12.746538640214848\n",
      "Episode 106; Return  12.763019041070399\n",
      "Episode 109; Return  12.787156132383084\n",
      "Episode 111; Return  12.801261430388099\n",
      "Episode 112; Return  12.809375167957374\n",
      "Episode 113; Return  12.816283353662069\n",
      "Episode 114; Return  12.82526332558247\n",
      "Episode 115; Return  12.83173929401066\n",
      "Episode 116; Return  12.83663809813302\n",
      "Episode 117; Return  12.844786490130629\n",
      "Episode 118; Return  12.85025439929154\n",
      "Episode 119; Return  12.85739511702241\n",
      "Episode 121; Return  12.868429917934513\n",
      "Episode 123; Return  12.786585537398734\n",
      "Episode 125; Return  12.799680176734924\n",
      "Episode 126; Return  12.805635097480955\n",
      "Episode 127; Return  12.812519862895876\n",
      "Episode 128; Return  12.819609553553164\n",
      "Episode 131; Return  12.739542171245313\n",
      "Episode 132; Return  12.747803214824561\n",
      "Episode 133; Return  12.754812215503893\n",
      "Episode 134; Return  12.673433028050322\n",
      "Episode 135; Return  12.68170389422664\n",
      "Episode 136; Return  12.687279604813632\n",
      "Episode 137; Return  12.694452746941225\n",
      "Episode 139; Return  12.70913688227427\n",
      "Episode 140; Return  12.715285902363913\n",
      "Episode 141; Return  12.722766147437671\n",
      "Episode 143; Return  12.651958240495695\n",
      "Episode 144; Return  12.65944463842445\n",
      "Episode 145; Return  12.58455192467262\n",
      "Episode 147; Return  12.597415160159676\n",
      "Episode 148; Return  12.605067757335869\n",
      "Episode 149; Return  12.612349185367558\n",
      "Episode 150; Return  12.617533526420594\n",
      "Episode 151; Return  12.62576177893885\n",
      "Episode 152; Return  12.632500185778266\n",
      "Episode 153; Return  12.639804106132656\n",
      "Episode 154; Return  12.646363817252121\n",
      "Episode 155; Return  12.653225986419185\n",
      "Episode 156; Return  12.659743769046587\n",
      "Episode 157; Return  12.665923745768845\n",
      "Episode 158; Return  12.672658402708512\n",
      "Episode 159; Return  12.680000174720332\n",
      "Episode 160; Return  12.6868751719594\n",
      "Episode 161; Return  12.69248464388877\n",
      "Episode 162; Return  12.69839523015199\n",
      "Episode 164; Return  12.638719686647741\n",
      "Episode 166; Return  12.646385721413486\n",
      "Episode 167; Return  12.65251514868822\n",
      "Episode 168; Return  12.587916851043701\n",
      "Episode 170; Return  12.60117665459128\n",
      "Episode 171; Return  12.60649141233567\n",
      "Episode 172; Return  12.613023436346719\n",
      "Episode 173; Return  12.616531972940258\n",
      "Episode 174; Return  12.623218569262274\n",
      "Episode 175; Return  12.62977160862514\n",
      "Episode 176; Return  12.636136542667042\n",
      "Episode 177; Return  12.575141427207129\n",
      "Episode 178; Return  12.580730518598235\n",
      "Episode 180; Return  12.593500179714626\n",
      "Episode 181; Return  12.52834272582228\n",
      "Episode 182; Return  12.534176008386927\n",
      "Episode 184; Return  12.546956704362579\n",
      "Episode 185; Return  12.553459641095754\n",
      "Episode 186; Return  12.558709861770753\n",
      "Episode 187; Return  12.565401250028355\n",
      "Episode 190; Return  12.583263336357318\n",
      "Episode 191; Return  12.58858656696\n",
      "Episode 192; Return  12.593802262718478\n",
      "Episode 193; Return  12.600414685634751\n",
      "Episode 194; Return  12.605515641649975\n",
      "Episode 195; Return  12.61097453618661\n",
      "Episode 196; Return  12.615612423541595\n",
      "Episode 197; Return  12.620913882546013\n",
      "Episode 198; Return  12.62661633768467\n",
      "Episode 199; Return  12.632412234143397\n",
      "Episode 200; Return  12.578350178599358\n",
      "Episode 201; Return  12.58343301542956\n",
      "Episode 202; Return  12.587425922993386\n",
      "Episode 203; Return  12.534532204637387\n",
      "Episode 204; Return  12.539657046397528\n",
      "Episode 205; Return  12.545317254996881\n",
      "Episode 206; Return  12.493398244519836\n",
      "Episode 207; Return  12.499662020932073\n",
      "Episode 209; Return  12.510526501390922\n",
      "Episode 211; Return  12.52000018601169\n",
      "Episode 212; Return  12.524905842992494\n",
      "Episode 213; Return  12.530141028440054\n",
      "Episode 214; Return  12.535327286920815\n",
      "Episode 215; Return  12.48548856058786\n",
      "Episode 216; Return  12.490833522544968\n",
      "Episode 217; Return  12.495484059307433\n",
      "Episode 218; Return  12.50073413400475\n",
      "Episode 219; Return  12.5063015592697\n",
      "Episode 220; Return  12.511136550794948\n",
      "Episode 221; Return  12.51619928185217\n",
      "Episode 222; Return  12.521621808812425\n",
      "Episode 223; Return  12.526906015092482\n",
      "Episode 224; Return  12.531785898442779\n",
      "Episode 225; Return  12.535644630326166\n",
      "Episode 226; Return  12.540309920247678\n",
      "Episode 227; Return  12.544185207278717\n",
      "Episode 228; Return  12.549868606161652\n",
      "Episode 229; Return  12.554235992993851\n",
      "Episode 231; Return  12.562857328039227\n",
      "Episode 232; Return  12.567629495057567\n",
      "Episode 233; Return  12.572918639674207\n",
      "Episode 234; Return  12.577051465837364\n",
      "Episode 235; Return  12.581021460573725\n",
      "Episode 237; Return  12.589620434785191\n",
      "Episode 239; Return  12.598535746710072\n",
      "Episode 240; Return  12.601583514114221\n",
      "Episode 241; Return  12.606224247528804\n",
      "Episode 242; Return  12.611198525290844\n",
      "Episode 243; Return  12.615926104317968\n",
      "Episode 246; Return  12.626422937807998\n",
      "Episode 247; Return  12.631012319553236\n",
      "Episode 248; Return  12.634798558969651\n",
      "Episode 249; Return  12.63911663719928\n",
      "Episode 250; Return  12.642520169734954\n",
      "Episode 252; Return  12.651349375645319\n",
      "Episode 253; Return  12.655652341635331\n",
      "Episode 255; Return  12.663137422355952\n",
      "Episode 256; Return  12.667500165756792\n",
      "Episode 257; Return  12.669688882994745\n",
      "Episode 259; Return  12.677451904676136\n",
      "Episode 260; Return  12.681615552076927\n",
      "Episode 261; Return  12.68574729413365\n",
      "Episode 262; Return  12.689809327362148\n",
      "Episode 264; Return  12.69814410489617\n",
      "Episode 266; Return  12.706015200991379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m#actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m#primary_action, reset_action = actor_out.action, None\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m         primary_action, actor_state \u001b[38;5;241m=\u001b[39m \u001b[43mactor_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactor_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         primary_action \u001b[38;5;241m=\u001b[39m primary_action[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m         reset_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/actor_net.py:974\u001b[0m, in \u001b[0;36mDRCNet_.forward\u001b[0;34m(self, obs, core_state, greedy)\u001b[0m\n\u001b[1;32m    972\u001b[0m             nd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(nd)\n\u001b[1;32m    973\u001b[0m         nd \u001b[38;5;241m=\u001b[39m nd\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 974\u001b[0m         output, core_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mnd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mnd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m     core_output_list\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    976\u001b[0m core_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(core_output_list)\n",
      "File \u001b[0;32m/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/core/rnn.py:339\u001b[0m, in \u001b[0;36mConvAttnLSTM.forward_single\u001b[0;34m(self, x, core_state, reset, reset_attn)\u001b[0m\n\u001b[1;32m    336\u001b[0m concat_k_cur \u001b[38;5;241m=\u001b[39m core_state[n \u001b[38;5;241m*\u001b[39m layer_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    337\u001b[0m concat_v_cur \u001b[38;5;241m=\u001b[39m core_state[n \u001b[38;5;241m*\u001b[39m layer_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m h_next, c_next, concat_k, concat_v \u001b[38;5;241m=\u001b[39m \u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_k_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_v_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask_reshape\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_scale \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m h_next\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    343\u001b[0m     h_next\u001b[38;5;241m.\u001b[39mregister_hook(\u001b[38;5;28;01mlambda\u001b[39;00m grad: grad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_scale)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/core/rnn.py:108\u001b[0m, in \u001b[0;36mConvAttnLSTMCell.forward\u001b[0;34m(self, input, h_cur, c_cur, concat_k, concat_v, attn_mask)\u001b[0m\n\u001b[1;32m    106\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28minput\u001b[39m, h_cur], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# concatenate along channel axis\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_inject:\n\u001b[0;32m--> 108\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_max_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_cur\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# concatenate along channel axis\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear:\n\u001b[1;32m    113\u001b[0m     combined_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain(combined[:, :, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "actor_net = actor_net.to(device)\n",
    "state = env.reset()\n",
    "#env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "env_out = init_env_out(state, flags=flags, dim_actions=1, tuple_action=False)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "rets = []\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    \n",
    "    while(True):\n",
    "        #actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        #primary_action, reset_action = actor_out.action, None\n",
    "        primary_action, actor_state = actor_net(obs=env_out, core_state=actor_state, greedy=False)\n",
    "        primary_action = primary_action[0]\n",
    "        reset_action = None\n",
    "\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action)    \n",
    "        if torch.any(done):\n",
    "            rets.extend(info[\"episode_return\"][done].cpu().tolist())            \n",
    "            print(f\"Episode {len(rets)}; Return  {np.mean(np.array(rets))}\")\n",
    "        env_out = create_env_out(primary_action, state, reward, done, info, flags=flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
