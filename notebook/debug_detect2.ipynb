{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing v5_sok from ../logs/detect\n",
      "Initializing env 0 with device cuda\n",
      "Model network size: 6637133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../logs/detect/v5_sok/config_c.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded model net from /mnt/c/Users/chung/Personal/RS/thinker/logs/detect/v5_sok/ckp_model.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree rep shape:  105\n",
      "Tree rep meaning:  {'root_td': slice(0, 1, None), 'root_action': slice(1, 6, None), 'root_r': slice(6, 7, None), 'root_v': slice(7, 8, None), 'root_logits': slice(8, 13, None), 'cur_td': slice(13, 14, None), 'cur_action': slice(14, 19, None), 'cur_r': slice(19, 20, None), 'cur_v': slice(20, 21, None), 'cur_logits': slice(21, 26, None), 'cur_reset': slice(26, 27, None), 'one_hot_k': slice(27, 47, None), 'rollout_return': slice(47, 48, None), 'max_rollout_return': slice(48, 49, None), 'rollout_done': slice(49, 50, None), 'action_seq': slice(50, 75, None), 'root_action_table': slice(75, 100, None), 'root_td_table': slice(100, 105, None)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data output directory: /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0\n",
      "Number of file to be generated: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputting to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import time, timeit\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "from thinker.actor_net import ActorNet\n",
    "from thinker.main import Env\n",
    "import thinker.util as util\n",
    "from thinker.self_play import init_env_out, create_env_out\n",
    "\n",
    "class DetectBuffer:\n",
    "    def __init__(self, outdir, t, rec_t, logger, delay_n=5):\n",
    "        \"\"\"\n",
    "        Store training data grouped in planning stages and output\n",
    "        whenever the target output is also readydd\n",
    "            Args:\n",
    "                N (int): number of planning stage per training output\n",
    "                delay_n (int): number of planning stage delayed in the output y\n",
    "                rec_t (int): number of step in a planning stage\n",
    "                K (int): number of block to merge into\n",
    "        \"\"\"\n",
    "        self.outdir = outdir\n",
    "        self.t = t # number of time step per file\n",
    "        self.rec_t = rec_t\n",
    "        self.logger = logger        \n",
    "        self.delay_n = delay_n        \n",
    "\n",
    "        self.processed_n, self.xs, self.y, self.done, self.step_status = 0, [], [], [], []\n",
    "        self.file_idx = -1\n",
    "    \n",
    "    def insert(self, xs, y, done, step_status):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xs (dict): dictionary of training input, with each elem having the\n",
    "                shape of (B, *)            \n",
    "            y (tensor): bool tensor of shape (B), being the target output delayed by\n",
    "                delay_n planning stage            \n",
    "            done (tensor): bool tensor of shape (B), being the indicator of episode end\n",
    "            step_status (int): int indicating current step status\n",
    "        Output:\n",
    "            save train_xs in shape (N, rec_t, B, *) and train_y in shape (N, B)\n",
    "        \"\"\"\n",
    "        #print(\"data received! \", y.shape, id, cur_t)\n",
    "        last_step_real = (step_status == 0) | (step_status == 3)\n",
    "        if len(self.step_status) == 0 and not last_step_real: return self.file_idx  # skip until real step\n",
    "                \n",
    "        self.xs.append(util.dict_map(xs, lambda x:x.cpu()))\n",
    "        self.y.append(y.cpu())\n",
    "        self.done.append(done.cpu())\n",
    "        self.step_status.append(step_status)\n",
    "        self.processed_n += int(last_step_real)\n",
    "\n",
    "        if (self.processed_n >= self.t + self.delay_n + 1):               \n",
    "            self.file_idx += 1                     \n",
    "            out = self._extract_data(self.t)\n",
    "            self.processed_n = sum([int(i == 0) for i in self.step_status])\n",
    "            assert self.processed_n == self.delay_n+1, f\"should only have {self.delay_n + 1} data left instead of {self.processed_n}\"\n",
    "            path = f'{self.outdir}/data_{self.file_idx}.pt'\n",
    "            torch.save(out, path)\n",
    "            out_shape = out[0]['env_state'].shape\n",
    "            n = self.file_idx * out_shape[0] * out_shape[2]\n",
    "            self.logger.info(f\"{n}: File saved to {path}; env_state shape {out_shape}\")\n",
    "\n",
    "        return self.file_idx   \n",
    "\n",
    "    def _extract_data(self, t):\n",
    "        # obtain the first N planning stage and the corresponding target_y in data\n",
    "        xs, y, done, step_status = self._collect_data(t)\n",
    "        future_y, future_done = self._collect_data(self.delay_n, y_done_only=True)\n",
    "        y = torch.concat([y, future_y], dim=0)\n",
    "        done = torch.concat([done, future_done], dim=0)                \n",
    "        \n",
    "        last_step_real = (step_status == 0) | (step_status == 3)\n",
    "        assert last_step_real[0], \"cur_t should start with 0\"\n",
    "        assert last_step_real.shape[0] == t*self.rec_t, \\\n",
    "            f\" step_status.shape is {last_step_real.shape}, expected {t*self.rec_t} for the first dimension.\"        \n",
    "        assert y.shape[0] == (t + self.delay_n)*self.rec_t, \\\n",
    "            f\" y.shape is {y.shape}, expected {(t + self.delay_n)*self.rec_t} for the first dimension.\"        \n",
    "        \n",
    "        B = y.shape[1]\n",
    "        y = y.view(t + self.delay_n, self.rec_t, B)[:, 0]\n",
    "        done = done.view(t + self.delay_n, self.rec_t, B)[:, 0]\n",
    "        step_status = step_status.view(t, self.rec_t)\n",
    "        # compute target_y\n",
    "        target_y = self._compute_target_y(y, done, self.delay_n)\n",
    "\n",
    "        for k in xs.keys():\n",
    "            xs[k] = xs[k].view((t, self.rec_t) + xs[k].shape[1:])\n",
    "        \n",
    "        xs[\"done\"] = done[:t]\n",
    "        xs[\"step_status\"] = step_status\n",
    "                \n",
    "        return xs, target_y\n",
    "\n",
    "    def _collect_data(self, t, y_done_only=False):\n",
    "        # collect the first t stage from data\n",
    "        step_status = torch.tensor(self.step_status, dtype=torch.long)\n",
    "        next_step_real = (step_status == 2) | (step_status == 3)        \n",
    "        idx = torch.nonzero(next_step_real, as_tuple=False).squeeze()    \n",
    "        last_idx = idx[t-1] + 1\n",
    "        y = torch.stack(self.y[:last_idx], dim=0)\n",
    "        done = torch.stack(self.done[:last_idx], dim=0)\n",
    "        if not y_done_only:\n",
    "            xs = {}\n",
    "            for k in self.xs[0].keys():\n",
    "                xs[k] = torch.stack([v[k] for v in self.xs[:last_idx]], dim=0)                \n",
    "            step_status = step_status[:last_idx]\n",
    "            self.xs = self.xs[last_idx:]\n",
    "            self.y = self.y[last_idx:]\n",
    "            self.done = self.done[last_idx:]\n",
    "            self.step_status = self.step_status[last_idx:]\n",
    "            return xs, y, done, step_status\n",
    "        else:\n",
    "            return y, done\n",
    "        \n",
    "    def _compute_target_y(self, y, done, delay_n):        \n",
    "        # target_y[i] = (y[i] | (~done[i+1] & y[i+1]) | (~done[i+1] & ~done[i+2] & y[i+2]) | ... | (~done[i+1] & ~done[i+2] & ... & ~done[i+M] & y[i+M]))\n",
    "        t, b = y.shape\n",
    "        t = t - delay_n\n",
    "        not_done_cum = torch.ones(delay_n, t, b, dtype=bool)\n",
    "        target_y = y.clone()[:-delay_n]\n",
    "        not_done_cum[0] = ~done[1:1+t]\n",
    "        target_y = target_y | (not_done_cum[0] & y[1:1+t])\n",
    "        for m in range(1, delay_n):\n",
    "            not_done_cum[m] = not_done_cum[m-1] & ~done[m+1:m+1+t]\n",
    "            target_y = target_y | (not_done_cum[m] & y[m+1:m+1+t])\n",
    "        return target_y\n",
    "\n",
    "total_n = 100000\n",
    "env_n = 128\n",
    "delay_n = 5\n",
    "savedir = \"../logs/detect\"\n",
    "outdir = \"../data/detect\"\n",
    "xpid = \"v5_sok\"\n",
    "\n",
    "_logger = util.logger()\n",
    "_logger.info(f\"Initializing {xpid} from {savedir}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "ckpdir = os.path.join(savedir, xpid)     \n",
    "if os.path.islink(ckpdir): ckpdir = os.readlink(ckpdir)  \n",
    "ckpdir =  os.path.abspath(os.path.expanduser(ckpdir))\n",
    "outdir = os.path.abspath(os.path.expanduser(outdir))\n",
    "\n",
    "config_path = os.path.join(ckpdir, 'config_c.yaml')\n",
    "flags = util.create_flags(config_path, save_flags=False)\n",
    "disable_thinker = flags.wrapper_type == 1\n",
    "\n",
    "env = Env(\n",
    "        name=flags.name,\n",
    "        env_n=env_n,\n",
    "        gpu=True,\n",
    "        train_model=False,\n",
    "        parallel=False,\n",
    "        savedir=savedir,        \n",
    "        xpid=xpid,\n",
    "        ckp=True,\n",
    "        return_x=True,\n",
    "        return_h=True,\n",
    "    )\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space \n",
    "\n",
    "actor_param = {\n",
    "    \"obs_space\": obs_space,\n",
    "    \"action_space\": action_space,\n",
    "    \"flags\": flags,\n",
    "    \"tree_rep_meaning\": env.get_tree_rep_meaning(),\n",
    "}\n",
    "actor_net = ActorNet(**actor_param)\n",
    "\n",
    "path = os.path.join(ckpdir, \"ckp_actor.tar\")\n",
    "checkpoint = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "actor_net.set_weights(checkpoint[\"actor_net_state_dict\"])\n",
    "actor_net.to(device)\n",
    "actor_net.train(False)\n",
    "\n",
    "state = env.reset()\n",
    "env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "\n",
    "# create dir\n",
    "\n",
    "n = 0\n",
    "while True:\n",
    "    name = \"%s-%d-%d\" % (xpid, checkpoint[\"real_step\"], n)\n",
    "    outdir_ = os.path.join(outdir, name)\n",
    "    if not os.path.exists(outdir_):\n",
    "        os.makedirs(outdir_)\n",
    "        print(f\"Outputting to {outdir_}\")\n",
    "        break\n",
    "    n += 1\n",
    "outdir = outdir_\n",
    "\n",
    "detect_buffer = DetectBuffer(outdir=outdir, t=12800//env_n, rec_t=flags.rec_t, logger=_logger, delay_n=delay_n)\n",
    "file_n = total_n // (env_n * detect_buffer.t) + 1\n",
    "_logger.info(f\"Data output directory: {outdir}\")\n",
    "_logger.info(f\"Number of file to be generated: {file_n}\")\n",
    "\n",
    "rescale = \"Sokoban\" in flags.name\n",
    "\n",
    "# save setting\n",
    "\n",
    "env_state_shape = env.observation_space[\"real_states\"].shape[1:]\n",
    "#if rescale: env_state_shape = (3, 40, 40)\n",
    "tree_rep_shape = env.observation_space[\"tree_reps\"].shape[1:]\n",
    "\n",
    "flags_detect = {\n",
    "    \"dim_actions\": actor_net.dim_actions,\n",
    "    \"num_actions\": actor_net.num_actions,\n",
    "    \"tuple_actions\": actor_net.tuple_action,\n",
    "    \"name\": flags.name,\n",
    "    \"env_state_shape\": list(env_state_shape),\n",
    "    \"tree_rep_shape\": list(tree_rep_shape),\n",
    "    \"rescale\": rescale,\n",
    "    \"rec_t\": flags.rec_t,\n",
    "    \"ckpdir\": ckpdir,\n",
    "}\n",
    "\n",
    "yaml_file_path = os.path.join(outdir, 'config_detect.yaml')\n",
    "with open(yaml_file_path, 'w') as file:\n",
    "    yaml.dump(flags_detect, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    \n",
    "    while(True):\n",
    "\n",
    "        actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        if not disable_thinker:\n",
    "            primary_action, reset_action = actor_out.action\n",
    "        else:\n",
    "            primary_action, reset_action = actor_out.action, None\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action, \n",
    "            action_prob=actor_out.action_prob[-1])    \n",
    "        env_out = create_env_out(actor_out.action, state, reward, done, info, flags=flags)\n",
    "        \n",
    "        # write to detect buffer\n",
    "        env_state = env_out.xs[0] \n",
    "        if rescale:\n",
    "            #env_state = F.interpolate(env_state , size=(40, 40), mode='bilinear', align_corners=False)\n",
    "            env_state = (env_state * 255).to(torch.uint8)\n",
    "\n",
    "        pri_action = actor_out.action[0]\n",
    "        reset_action = actor_out.action[1]\n",
    "        tree_rep = state[\"tree_reps\"]\n",
    "\n",
    "        xs = {\n",
    "            \"env_state\": env_state,\n",
    "            \"tree_rep\": tree_rep,\n",
    "            \"pri_action\": pri_action,\n",
    "            \"reset_action\": reset_action,\n",
    "        }\n",
    "        y = info['cost']\n",
    "        done = done\n",
    "        step_status = info['step_status'][0].item()\n",
    "\n",
    "        file_idx = detect_buffer.insert(xs, y, done, step_status)\n",
    "        if file_idx >= file_n: \n",
    "            # last file is for validation\n",
    "            os.rename(f'{outdir}/data_{file_idx}.pt', f'{outdir}/val.pt')\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from thinker.model_net import BaseNet, FrameEncoder\n",
    "from thinker import util\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None, data_n=None, prefix=\"data\"):\n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "        self.data = []        \n",
    "        self.samples_per_file = None   \n",
    "        self.data_n = data_n\n",
    "        self.prefix = prefix\n",
    "        self._preload_data(datadir)  # Preload data        \n",
    "\n",
    "    def _preload_data(self, datadir):\n",
    "        # Preload all .pt files\n",
    "        file_list = [f for f in os.listdir(datadir) if f.endswith('.pt') and f.startswith(self.prefix)]\n",
    "        for file_name in file_list:\n",
    "            print(f\"Starting to preload {file_name}\")\n",
    "            xs, y = torch.load(os.path.join(datadir, file_name))\n",
    "            if self.samples_per_file is None:  # Set samples_per_file based on the first file\n",
    "                self.t = xs['env_state'].shape[0]\n",
    "                self.b = xs['env_state'].shape[2]\n",
    "                self.samples_per_file = self.t * self.b\n",
    "            xs.pop('step_status')\n",
    "            xs.pop('done')\n",
    "            # Flatten data across t and b dimensions for easier indexing\n",
    "            for t_idx in range(self.t):\n",
    "                for b_idx in range(self.b):\n",
    "                    flattened_xs = {k: v[t_idx, :, b_idx] for k, v in xs.items()}\n",
    "                    flattened_y = y[t_idx, b_idx]\n",
    "                    self.data.append((flattened_xs, flattened_y))\n",
    "                    if self.data_n is not None and len(self.data) >= self.data_n: return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xs, y = self.data[idx]\n",
    "        if self.transform:\n",
    "            # Apply transform if necessary. Note: You might need to adjust this part\n",
    "            # based on what your transform expects and can handle\n",
    "            xs = {k: self.transform(v) for k, v in xs.items()}            \n",
    "        return xs, y\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 500):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "        return x\n",
    "\n",
    "class DetectFrameEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,     \n",
    "        dim_rep_actions,   \n",
    "        out_size=128,\n",
    "        stride=2,\n",
    "    ):  \n",
    "        super(DetectFrameEncoder, self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.encoder = FrameEncoder(prefix=\"se\",\n",
    "                                    actions_ch=dim_rep_actions,\n",
    "                                    input_shape=input_shape,                             \n",
    "                                    size_nn=1,             \n",
    "                                    downscale_c=2,    \n",
    "                                    concat_action=False)\n",
    "        \n",
    "        self.conv = []\n",
    "        in_ch =  self.encoder.out_shape[0]\n",
    "        for ch in [64]:\n",
    "            self.conv.append(nn.ReLU())\n",
    "            self.conv.append(nn.Conv2d(in_channels=in_ch,\n",
    "                                       out_channels=ch,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1,))\n",
    "            in_ch = ch\n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "        conv_out_size = in_ch * self.encoder.out_shape[1] * self.encoder.out_shape[2]\n",
    "        self.fc = nn.Sequential(nn.Linear(conv_out_size, self.out_size))       \n",
    "\n",
    "    def forward(self, x, action):\n",
    "        # x in shape of (B, C, H, W)\n",
    "        out, _ = self.encoder(x, done=None, actions=action, state={})\n",
    "        out = self.conv(out)\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out                                \n",
    "        \n",
    "class DetectNet(BaseNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_state_shape,\n",
    "        tree_rep_shape,\n",
    "        dim_actions,\n",
    "        num_actions,\n",
    "        detect_ab=(0,0),\n",
    "        clone=False,\n",
    "        tran_layer_n=3,\n",
    "    ):    \n",
    "        super(DetectNet, self).__init__()\n",
    "        \n",
    "        self.env_state_shape = env_state_shape # in (C, H, W) \n",
    "        self.tree_rep_shape = tree_rep_shape # in (C,) \n",
    "        self.dim_actions = dim_actions\n",
    "        self.num_actions = num_actions\n",
    "        self.dim_rep_actions = self.dim_actions if self.dim_actions > 1 else self.num_actions\n",
    "\n",
    "        self.detect_ab = detect_ab\n",
    "        self.clone = clone\n",
    "\n",
    "        self.enc_out_size = 128        \n",
    "        tran_nhead = 8\n",
    "        reminder = tran_nhead - ((self.enc_out_size + tree_rep_shape[0] + self.dim_rep_actions + 1) % tran_nhead)\n",
    "        self.enc_out_size += reminder\n",
    "        self.true_x_encoder = DetectFrameEncoder(input_shape=env_state_shape, dim_rep_actions=self.dim_rep_actions, out_size=self.enc_out_size)\n",
    "        self.pred_x_encoder = DetectFrameEncoder(input_shape=env_state_shape, dim_rep_actions=self.dim_rep_actions, out_size=self.enc_out_size)\n",
    "\n",
    "        self.embed_size = self.enc_out_size + tree_rep_shape[0] + num_actions + 1\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_size)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.embed_size, \n",
    "                                                   nhead=tran_nhead, \n",
    "                                                   dim_feedforward=512,\n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, tran_layer_n)\n",
    "        self.classifier = nn.Linear(self.embed_size, 1)\n",
    "\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5), requires_grad=False) # portion of negative class\n",
    "\n",
    "    def forward(self, env_state, tree_rep, action, reset):\n",
    "        \"\"\"\n",
    "        Forward pass of detection nn\n",
    "        Args:\n",
    "            env_state: float Tensor in shape of (B, rec_t, C, H, W); true and predicted frame\n",
    "            tree_rep: float Tensor in shape of (B, rec_t, C); model output\n",
    "            action: uint Tensor in shape of (B, rec_t, dim_actions); action (real / imaginary)\n",
    "            reset: bool Tensor in shape of  (B, rec_t); reset action\n",
    "        Return:\n",
    "            logit: float Tensor in shape of (B); logit of classifier output\n",
    "            p: float Tensor in shape of (B); prob of classifier output\n",
    "        \"\"\"\n",
    "        B, rec_t = env_state.shape[:2]\n",
    "        if self.detect_ab[0] in [1, 3] or self.detect_ab[1] in [1, 3]:\n",
    "            if self.clone: env_state = env_state.clone()                \n",
    "            if self.detect_ab[0] in [1, 3]:\n",
    "                env_state[:, 0] = 0.\n",
    "            if self.detect_ab[1] in [1, 3]:\n",
    "                env_state[:, 1:] = 0.\n",
    "        if self.detect_ab[0] in [2, 3] or self.detect_ab[1] in [2, 3]:\n",
    "            if self.clone: tree_rep = tree_rep.clone()\n",
    "            if self.detect_ab[0] in [2, 3]:\n",
    "                tree_rep[:, 0] = 0.\n",
    "            if self.detect_ab[1] in [2, 3]:\n",
    "                tree_rep[:, 1:] = 0.\n",
    "        \n",
    "        action = util.encode_action(action, self.dim_actions, self.num_actions)        \n",
    "        true_proc_x = self.true_x_encoder(env_state[:,0], action[:,0])\n",
    "        pred_proc_x = self.pred_x_encoder(\n",
    "            torch.flatten(env_state[:,1:], 0, 1),\n",
    "            torch.flatten(action[:,1:], 0, 1)\n",
    "                                        )\n",
    "        true_proc_x = true_proc_x.view(B, self.enc_out_size).unsqueeze(1) # (B, 1, C)\n",
    "        pred_proc_x = pred_proc_x.view(B, rec_t - 1, self.enc_out_size)  # (B, rec_t - 1, C)\n",
    "        proc_x = torch.concat([true_proc_x, pred_proc_x], dim=1) # (B, rec_t, C)\n",
    "        \n",
    "        embed = [proc_x, tree_rep, action, reset.unsqueeze(-1)]\n",
    "        embed = torch.concat(embed, dim=2) # (B, rec_t, embed_size)\n",
    "        embed_pos = self.pos_encoder(embed)\n",
    "        out = self.transformer_encoder(embed_pos)\n",
    "        logit = self.classifier(out[:, -1, :]).view(B)\n",
    "        return logit, torch.sigmoid(logit)\n",
    "\n",
    "def transform_data(xs, device):\n",
    "    xs_ = {}\n",
    "\n",
    "    env_state = xs[\"env_state\"]\n",
    "    if flags_data.rescale:\n",
    "        env_state = env_state.float() / 255\n",
    "    xs_[\"env_state\"] = env_state.to(device)\n",
    "\n",
    "    if \"tree_rep\" in xs: xs_[\"tree_rep\"] = xs[\"tree_rep\"].to(device)\n",
    "\n",
    "    action = xs[\"pri_action\"]\n",
    "    if not flags_data.tuple_actions:\n",
    "        action = action.unsqueeze(-1)\n",
    "    xs_[\"action\"] = action.to(device)\n",
    "\n",
    "    if \"reset_action\" in xs: xs_[\"reset\"] = xs[\"reset_action\"].to(device)\n",
    "    return xs_\n",
    "\n",
    "def evaluate_detect(target_y, pred_y):\n",
    "    # Binarize the predictions\n",
    "    pred_y_binarized = (pred_y > 0.5).float()\n",
    "    target_y = target_y.float()\n",
    "\n",
    "    # Compute the accuracy\n",
    "    acc = torch.mean((pred_y_binarized == target_y).float()).item()\n",
    "    \n",
    "    # Compute the recall\n",
    "    true_positives = (pred_y_binarized * target_y).sum().float()\n",
    "    possible_positives = target_y.sum().float()\n",
    "    rec = (true_positives / (possible_positives + 1e-6)).item()\n",
    "    \n",
    "    # Compute the precision\n",
    "    predicted_positives = pred_y_binarized.sum().float()\n",
    "    prec = (true_positives / (predicted_positives + 1e-6)).item()\n",
    "    \n",
    "    # Compute the F1 score\n",
    "    f1 = 2 * (prec * rec) / (prec + rec + 1e-6)   \n",
    "\n",
    "    neg_p = 1 - torch.mean(target_y.float()).item()\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"rec\": rec,\n",
    "        \"prec\": prec,\n",
    "        \"f1\": f1,\n",
    "        \"neg_p\": neg_p,\n",
    "        }\n",
    "\n",
    "def train_epoch(detect_net, dataloader, optimizer, device, flags, train=True):\n",
    "    if train:\n",
    "        detect_net.train()\n",
    "    else:\n",
    "        detect_net.eval()     \n",
    "    running_train_eval = {}   \n",
    "    with torch.set_grad_enabled(train):\n",
    "        step = 0\n",
    "        for xs, target_y in dataloader:\n",
    "            xs = transform_data(xs, device)\n",
    "            target_y = target_y.to(device)\n",
    "            \n",
    "            logit, pred_y = detect_net(**xs)\n",
    "            n_mean_y = torch.mean((~target_y).float()).item()\n",
    "            detect_net.beta.data = 0.99 * detect_net.beta.data + (1 - 0.99) * n_mean_y\n",
    "            detect_net.beta.data.clamp_(0.05, 0.95)\n",
    "            weights = torch.where(target_y == 1, detect_net.beta.data, 1-detect_net.beta.data)\n",
    "            loss = F.binary_cross_entropy_with_logits(logit, target_y.float(), weight=weights)\n",
    "            train_eval = evaluate_detect(target_y, pred_y)\n",
    "            train_eval[\"loss\"] = loss.item()\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "                loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "            \n",
    "            for key in train_eval.keys():\n",
    "                if key not in running_train_eval: \n",
    "                    running_train_eval[key] = train_eval[key]\n",
    "                else:\n",
    "                    running_train_eval[key] += train_eval[key]\n",
    "            step += 1\n",
    "    return {key: val / step for (key, val) in running_train_eval.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Starting to preload data_0.pt\n",
      "Starting to preload data_1.pt\n",
      "Starting to preload data_2.pt\n",
      "Starting to preload data_3.pt\n",
      "Starting to preload val.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flags = argparse.Namespace()\n",
    "\n",
    "flags.datadir = \"../data/detect/v5_sok-14137536-0/\"\n",
    "flags.xpid = \"test\"\n",
    "flags.batch_size = 128\n",
    "flags.learning_rate = 0.0001\n",
    "flags.num_epochs = 100\n",
    "flags.data_n = 50000\n",
    "flags.ckp = False\n",
    "\n",
    "if not flags.ckp:\n",
    "    flags.datadir = os.path.abspath(os.path.expanduser(flags.datadir))\n",
    "    # create ckp dir\n",
    "    xpid_n = 0\n",
    "    while (True):\n",
    "        xpid_ = flags.xpid if xpid_n == 0 else flags.xpid + f\"_{xpid_n}\"\n",
    "        ckpdir = os.path.join(flags.datadir, xpid_)\n",
    "        xpid_n += 1\n",
    "        if not os.path.exists(ckpdir):\n",
    "            os.mkdir(ckpdir) \n",
    "            flags.xpid = xpid_\n",
    "            break    \n",
    "else:\n",
    "    ckpdir = os.path.join(flags.datadir, flags.xpid)\n",
    "flags.ckpdir = ckpdir\n",
    "flags.ckp_path = os.path.join(ckpdir, \"ckp_detect.tar\")\n",
    "print(f\"Checkpoint path: {flags.ckp_path}\")\n",
    "\n",
    "# load data\n",
    "dataset = CustomDataset(datadir=flags.datadir, transform=None, data_n=flags.data_n)\n",
    "dataloader = DataLoader(dataset, batch_size=flags.batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(datadir=flags.datadir, transform=None, data_n=2000, prefix=\"val\")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=flags.batch_size, shuffle=True)\n",
    "\n",
    "# load setting\n",
    "yaml_file_path = os.path.join(flags.datadir, 'config_detect.yaml')\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    flags_data = yaml.safe_load(file)\n",
    "flags_data = argparse.Namespace(**flags_data)\n",
    "flags = argparse.Namespace(**{**vars(flags), **vars(flags_data)}) # merge the two flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initalize net\n",
    "device = torch.device(\"cuda\")\n",
    "detect_net = DetectNet(\n",
    "    env_state_shape = flags_data.env_state_shape,\n",
    "    tree_rep_shape = flags_data.tree_rep_shape,\n",
    "    dim_actions = flags_data.dim_actions,\n",
    "    num_actions = flags_data.num_actions,\n",
    ")\n",
    "\n",
    "# load optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    detect_net.parameters(), lr=flags.learning_rate, \n",
    ")\n",
    "\n",
    "if flags.ckp:\n",
    "    checkpoint = torch.load(flags.ckp_path, torch.device(\"cpu\"))\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    detect_net.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    del checkpoint\n",
    "else:\n",
    "    epoch = 0\n",
    "\n",
    "detect_net = detect_net.to(device)\n",
    "util.optimizer_to(optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, acc:0.8860 (0.8019) rec:0.6524 (0.9215) prec:0.4468 (0.3401) f1:0.5092 (0.4894) neg_p:0.8755 (0.8934) loss:0.0991 (0.0748)\n",
      "Epoch 2/100, acc:0.9169 (0.9297) rec:0.9284 (0.7902) prec:0.6207 (0.6574) f1:0.7336 (0.7077) neg_p:0.8755 (0.8925) loss:0.0425 (0.0711)\n",
      "Epoch 3/100, acc:0.9340 (0.9204) rec:0.9470 (0.8364) prec:0.6729 (0.5887) f1:0.7789 (0.6869) neg_p:0.8755 (0.8937) loss:0.0336 (0.0759)\n",
      "Epoch 4/100, acc:0.9429 (0.8788) rec:0.9612 (0.8676) prec:0.7017 (0.4622) f1:0.8042 (0.5999) neg_p:0.8755 (0.8939) loss:0.0281 (0.0607)\n",
      "Epoch 5/100, acc:0.9514 (0.8806) rec:0.9708 (0.8798) prec:0.7376 (0.4695) f1:0.8313 (0.6008) neg_p:0.8755 (0.8939) loss:0.0240 (0.0758)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Epoch 6/100, acc:0.9595 (0.8639) rec:0.9761 (0.8884) prec:0.7695 (0.4316) f1:0.8548 (0.5732) neg_p:0.8755 (0.8931) loss:0.0200 (0.0827)\n",
      "Epoch 7/100, acc:0.9651 (0.9388) rec:0.9816 (0.6830) prec:0.7943 (0.6932) f1:0.8729 (0.6826) neg_p:0.8755 (0.8937) loss:0.0172 (0.1100)\n",
      "Epoch 8/100, acc:0.9709 (0.9230) rec:0.9833 (0.8175) prec:0.8208 (0.6003) f1:0.8907 (0.6880) neg_p:0.8755 (0.8939) loss:0.0146 (0.0910)\n",
      "Epoch 9/100, acc:0.9730 (0.8339) rec:0.9840 (0.8927) prec:0.8333 (0.3799) f1:0.8980 (0.5291) neg_p:0.8755 (0.8945) loss:0.0137 (0.0970)\n",
      "Epoch 10/100, acc:0.9766 (0.9418) rec:0.9892 (0.6649) prec:0.8495 (0.7617) f1:0.9108 (0.6995) neg_p:0.8754 (0.8942) loss:0.0120 (0.1531)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Epoch 11/100, acc:0.9782 (0.9058) rec:0.9898 (0.8284) prec:0.8602 (0.5358) f1:0.9165 (0.6455) neg_p:0.8755 (0.8934) loss:0.0112 (0.0975)\n",
      "Epoch 12/100, acc:0.9785 (0.9223) rec:0.9880 (0.7772) prec:0.8638 (0.6146) f1:0.9187 (0.6813) neg_p:0.8755 (0.8925) loss:0.0112 (0.1141)\n",
      "Epoch 13/100, acc:0.9799 (0.9149) rec:0.9901 (0.8670) prec:0.8685 (0.5708) f1:0.9223 (0.6842) neg_p:0.8754 (0.8939) loss:0.0100 (0.0811)\n",
      "Epoch 14/100, acc:0.9824 (0.9174) rec:0.9920 (0.7625) prec:0.8825 (0.5845) f1:0.9315 (0.6588) neg_p:0.8755 (0.8931) loss:0.0086 (0.1595)\n",
      "Epoch 15/100, acc:0.9835 (0.9348) rec:0.9916 (0.7248) prec:0.8894 (0.6869) f1:0.9354 (0.7005) neg_p:0.8755 (0.8928) loss:0.0086 (0.1467)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Epoch 16/100, acc:0.9847 (0.9372) rec:0.9926 (0.7638) prec:0.8973 (0.6942) f1:0.9403 (0.7217) neg_p:0.8754 (0.8939) loss:0.0078 (0.1526)\n",
      "Epoch 17/100, acc:0.9843 (0.9022) rec:0.9915 (0.8740) prec:0.8957 (0.5145) f1:0.9388 (0.6416) neg_p:0.8755 (0.8942) loss:0.0080 (0.0917)\n",
      "Epoch 18/100, acc:0.9872 (0.9433) rec:0.9938 (0.7094) prec:0.9105 (0.7357) f1:0.9485 (0.7163) neg_p:0.8755 (0.8939) loss:0.0066 (0.1844)\n",
      "Epoch 19/100, acc:0.9865 (0.9268) rec:0.9928 (0.7771) prec:0.9040 (0.6092) f1:0.9442 (0.6755) neg_p:0.8755 (0.8948) loss:0.0067 (0.1268)\n",
      "Epoch 20/100, acc:0.9871 (0.9020) rec:0.9936 (0.8178) prec:0.9101 (0.5171) f1:0.9483 (0.6269) neg_p:0.8755 (0.8922) loss:0.0064 (0.1328)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Epoch 21/100, acc:0.9881 (0.9380) rec:0.9944 (0.7218) prec:0.9168 (0.6989) f1:0.9521 (0.7054) neg_p:0.8755 (0.8931) loss:0.0061 (0.1559)\n",
      "Epoch 22/100, acc:0.9870 (0.9358) rec:0.9923 (0.6811) prec:0.9115 (0.7076) f1:0.9484 (0.6860) neg_p:0.8755 (0.8939) loss:0.0068 (0.2012)\n",
      "Epoch 23/100, acc:0.9895 (0.9294) rec:0.9964 (0.6290) prec:0.9237 (0.6735) f1:0.9569 (0.6451) neg_p:0.8754 (0.8934) loss:0.0053 (0.2304)\n",
      "Epoch 24/100, acc:0.9886 (0.9447) rec:0.9941 (0.7076) prec:0.9217 (0.7527) f1:0.9550 (0.7190) neg_p:0.8755 (0.8931) loss:0.0057 (0.1795)\n",
      "Epoch 25/100, acc:0.9880 (0.9077) rec:0.9934 (0.7830) prec:0.9170 (0.5430) f1:0.9522 (0.6335) neg_p:0.8755 (0.8919) loss:0.0059 (0.1385)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Epoch 26/100, acc:0.9869 (0.8860) rec:0.9924 (0.8311) prec:0.9091 (0.4739) f1:0.9467 (0.5998) neg_p:0.8754 (0.8934) loss:0.0067 (0.1107)\n",
      "Epoch 27/100, acc:0.9898 (0.9290) rec:0.9941 (0.7407) prec:0.9310 (0.6597) f1:0.9601 (0.6910) neg_p:0.8754 (0.8922) loss:0.0051 (0.1497)\n",
      "Epoch 28/100, acc:0.9904 (0.8862) rec:0.9954 (0.8669) prec:0.9331 (0.4757) f1:0.9619 (0.6100) neg_p:0.8754 (0.8931) loss:0.0046 (0.1205)\n",
      "Epoch 29/100, acc:0.9888 (0.9183) rec:0.9955 (0.8657) prec:0.9212 (0.5802) f1:0.9552 (0.6869) neg_p:0.8755 (0.8942) loss:0.0057 (0.1187)\n",
      "Epoch 30/100, acc:0.9906 (0.9045) rec:0.9957 (0.8281) prec:0.9338 (0.5317) f1:0.9623 (0.6444) neg_p:0.8754 (0.8934) loss:0.0047 (0.1240)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Epoch 31/100, acc:0.9904 (0.9382) rec:0.9949 (0.4831) prec:0.9340 (0.8478) f1:0.9621 (0.6023) neg_p:0.8755 (0.8934) loss:0.0048 (0.2982)\n",
      "Epoch 32/100, acc:0.9897 (0.8993) rec:0.9945 (0.7894) prec:0.9254 (0.5178) f1:0.9571 (0.6221) neg_p:0.8755 (0.8942) loss:0.0052 (0.1439)\n",
      "Epoch 33/100, acc:0.9898 (0.9406) rec:0.9947 (0.7825) prec:0.9305 (0.6963) f1:0.9597 (0.7285) neg_p:0.8755 (0.8942) loss:0.0048 (0.1737)\n",
      "Epoch 34/100, acc:0.9900 (0.9451) rec:0.9955 (0.7687) prec:0.9299 (0.7274) f1:0.9600 (0.7416) neg_p:0.8755 (0.8937) loss:0.0048 (0.1414)\n",
      "Epoch 35/100, acc:0.9904 (0.8896) rec:0.9953 (0.8326) prec:0.9320 (0.4883) f1:0.9612 (0.6105) neg_p:0.8755 (0.8934) loss:0.0048 (0.1095)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Epoch 36/100, acc:0.9906 (0.9432) rec:0.9945 (0.6447) prec:0.9345 (0.7802) f1:0.9621 (0.6994) neg_p:0.8755 (0.8942) loss:0.0046 (0.2339)\n",
      "Epoch 37/100, acc:0.9911 (0.9345) rec:0.9949 (0.6129) prec:0.9374 (0.7331) f1:0.9641 (0.6607) neg_p:0.8755 (0.8934) loss:0.0042 (0.2026)\n",
      "Epoch 38/100, acc:0.9916 (0.7965) rec:0.9955 (0.9012) prec:0.9397 (0.3258) f1:0.9656 (0.4738) neg_p:0.8754 (0.8945) loss:0.0041 (0.1339)\n",
      "Epoch 39/100, acc:0.9917 (0.9460) rec:0.9964 (0.7386) prec:0.9415 (0.7531) f1:0.9670 (0.7393) neg_p:0.8755 (0.8939) loss:0.0040 (0.1718)\n",
      "Epoch 40/100, acc:0.9923 (0.9399) rec:0.9957 (0.5505) prec:0.9444 (0.8453) f1:0.9682 (0.6490) neg_p:0.8755 (0.8928) loss:0.0038 (0.3084)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Epoch 41/100, acc:0.9903 (0.9445) rec:0.9960 (0.6900) prec:0.9318 (0.7690) f1:0.9613 (0.7217) neg_p:0.8754 (0.8939) loss:0.0046 (0.1969)\n",
      "Epoch 42/100, acc:0.9912 (0.9380) rec:0.9967 (0.7246) prec:0.9376 (0.6899) f1:0.9650 (0.7025) neg_p:0.8755 (0.8939) loss:0.0044 (0.1717)\n",
      "Epoch 43/100, acc:0.9917 (0.9352) rec:0.9965 (0.4866) prec:0.9414 (0.7841) f1:0.9670 (0.5910) neg_p:0.8754 (0.8937) loss:0.0041 (0.3576)\n",
      "Epoch 44/100, acc:0.9916 (0.9381) rec:0.9949 (0.8023) prec:0.9411 (0.6689) f1:0.9661 (0.7208) neg_p:0.8755 (0.8951) loss:0.0043 (0.1342)\n",
      "Epoch 45/100, acc:0.9938 (0.9108) rec:0.9975 (0.7942) prec:0.9547 (0.5675) f1:0.9747 (0.6569) neg_p:0.8755 (0.8937) loss:0.0029 (0.1658)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n",
      "Epoch 46/100, acc:0.9927 (0.9324) rec:0.9957 (0.8063) prec:0.9498 (0.6472) f1:0.9708 (0.7122) neg_p:0.8755 (0.8934) loss:0.0038 (0.1680)\n",
      "Epoch 47/100, acc:0.9909 (0.8621) rec:0.9946 (0.8779) prec:0.9362 (0.4377) f1:0.9631 (0.5775) neg_p:0.8755 (0.8928) loss:0.0047 (0.1173)\n",
      "Epoch 48/100, acc:0.9913 (0.9291) rec:0.9957 (0.7990) prec:0.9374 (0.6199) f1:0.9641 (0.6881) neg_p:0.8755 (0.8942) loss:0.0043 (0.1307)\n",
      "Epoch 49/100, acc:0.9924 (0.8996) rec:0.9948 (0.8394) prec:0.9470 (0.5213) f1:0.9693 (0.6382) neg_p:0.8755 (0.8922) loss:0.0042 (0.1263)\n",
      "Epoch 50/100, acc:0.9926 (0.8829) rec:0.9961 (0.8474) prec:0.9467 (0.4701) f1:0.9698 (0.6000) neg_p:0.8755 (0.8931) loss:0.0035 (0.1350)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test/ckp_detect.tar\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      3\u001b[0m running_train_eval \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m----> 5\u001b[0m train_stat \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetect_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m val_stat \u001b[38;5;241m=\u001b[39m train_epoch(detect_net, val_dataloader, \u001b[38;5;28;01mNone\u001b[39;00m, device, flags, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m print_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflags\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[1], line 255\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(detect_net, dataloader, optimizer, device, flags, train)\u001b[0m\n\u001b[1;32m    253\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xs, target_y \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m--> 255\u001b[0m     xs \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     target_y \u001b[38;5;241m=\u001b[39m target_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    258\u001b[0m     logit, pred_y \u001b[38;5;241m=\u001b[39m detect_net(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mxs)\n",
      "Cell \u001b[0;32mIn[1], line 204\u001b[0m, in \u001b[0;36mtransform_data\u001b[0;34m(xs, device)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flags_data\u001b[38;5;241m.\u001b[39mrescale:\n\u001b[1;32m    203\u001b[0m     env_state \u001b[38;5;241m=\u001b[39m env_state\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[0;32m--> 204\u001b[0m xs_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_state\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43menv_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_rep\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m xs: xs_[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_rep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m xs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree_rep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    208\u001b[0m action \u001b[38;5;241m=\u001b[39m xs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpri_action\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while (epoch < flags.num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_train_eval = {}\n",
    "\n",
    "    train_stat = train_epoch(detect_net, dataloader, optimizer, device, flags, train=True)\n",
    "    val_stat = train_epoch(detect_net, val_dataloader, None, device, flags, train=False)\n",
    "\n",
    "    print_str = f'Epoch {epoch+1}/{flags.num_epochs},'\n",
    "    for key in train_stat.keys(): \n",
    "        print_str += f\" {key}:{train_stat[key]:.4f} ({val_stat[key]:.4f})\"\n",
    "    print(print_str)\n",
    "\n",
    "    epoch += 1    \n",
    "    if epoch % 5 == 0 or epoch >= flags.num_epochs:\n",
    "        # save checkpoint\n",
    "        d = {\n",
    "            \"epoch\": epoch,\n",
    "            \"flags\": flags,\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"net_state_dict\": detect_net.state_dict(),\n",
    "        }\n",
    "        torch.save(d, flags.ckp_path)\n",
    "        print(f\"Checkpoint saved to {flags.ckp_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2288818359375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deprecated\n",
    "\n",
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from thinker import util\n",
    "\n",
    "datadir = \"../data/detect/v5_sok-5993808-1/\"\n",
    "datadir = os.path.abspath(os.path.expanduser(datadir))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None):\n",
    "        self.datadir = datadir\n",
    "        self.file_list = [f for f in os.listdir(datadir) if f.endswith('.pt')]\n",
    "        self.transform = transform\n",
    "        xs, y = torch.load(os.path.join(datadir, self.file_list[0]))        \n",
    "        self.t = xs['env_state'].shape[0]\n",
    "        self.b = xs['env_state'].shape[2]\n",
    "        self.samples_per_file = self.t * self.b\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) * self.samples_per_file  # Adjust based on your data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx // self.samples_per_file\n",
    "        within_file_idx = idx % self.samples_per_file\n",
    "        t_idx = within_file_idx // self.b\n",
    "        b_idx = within_file_idx % self.b\n",
    "        xs, y = torch.load(os.path.join(self.datadir, self.file_list[file_idx]))\n",
    "        xs.pop('step_status')\n",
    "        xs.pop('done')\n",
    "        xs = util.dict_map(xs, lambda x: x[t_idx, :, b_idx])\n",
    "        y = y[t_idx, b_idx]\n",
    "        return xs, y\n",
    "\n",
    "# To load data and train\n",
    "dataset = CustomDataset(datadir)\n",
    "# print(dataset[100])\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
