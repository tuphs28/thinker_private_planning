{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing v5b_sok_drc from ../logs/detect\n",
      "Initializing env 0 with device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../logs/detect/v5b_sok_drc/config_c.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data output directory: /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7\n",
      "Number of file to be generated: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputting to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "import time, timeit\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from thinker.actor_net import ActorNet\n",
    "from thinker.main import Env\n",
    "import thinker.util as util\n",
    "from thinker.self_play import init_env_out, create_env_out\n",
    "\n",
    "class DetectBuffer:\n",
    "    def __init__(self, outdir, t, rec_t, logger, delay_n=5):\n",
    "        \"\"\"\n",
    "        Store training data grouped in planning stages and output\n",
    "        whenever the target output is also readydd\n",
    "            Args:\n",
    "                N (int): number of planning stage per training output\n",
    "                delay_n (int): number of planning stage delayed in the output y\n",
    "                rec_t (int): number of step in a planning stage\n",
    "                K (int): number of block to merge into\n",
    "        \"\"\"\n",
    "        self.outdir = outdir\n",
    "        self.t = t # number of time step per file\n",
    "        self.rec_t = rec_t\n",
    "        self.logger = logger        \n",
    "        self.delay_n = delay_n        \n",
    "\n",
    "        self.processed_n, self.xs, self.y, self.done, self.step_status = 0, [], [], [], []\n",
    "        self.file_idx = -1\n",
    "    \n",
    "    def insert(self, xs, y, done, step_status):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            xs (dict): dictionary of training input, with each elem having the\n",
    "                shape of (B, *)            \n",
    "            y (tensor): bool tensor of shape (B), being the target output delayed by\n",
    "                delay_n planning stage            \n",
    "            done (tensor): bool tensor of shape (B), being the indicator of episode end\n",
    "            step_status (int): int indicating current step status\n",
    "        Output:\n",
    "            save train_xs in shape (N, rec_t, B, *) and train_y in shape (N, B)\n",
    "        \"\"\"\n",
    "        #print(\"data received! \", y.shape, id, cur_t)\n",
    "        last_step_real = (step_status == 0) | (step_status == 3)\n",
    "        if len(self.step_status) == 0 and not last_step_real: return self.file_idx  # skip until real step\n",
    "                \n",
    "        self.xs.append(util.dict_map(xs, lambda x:x.cpu()))\n",
    "        self.y.append(y.cpu())\n",
    "        self.done.append(done.cpu())\n",
    "        self.step_status.append(step_status)\n",
    "        self.processed_n += int(last_step_real)\n",
    "\n",
    "        if (self.processed_n >= self.t + self.delay_n + 1):               \n",
    "            self.file_idx += 1                     \n",
    "            out = self._extract_data(self.t)\n",
    "            self.processed_n = sum([int(i == 0) + int(i == 3) for i in self.step_status])\n",
    "            assert self.processed_n == self.delay_n+1, f\"should only have {self.delay_n + 1} data left instead of {self.processed_n}\"\n",
    "            path = f'{self.outdir}/data_{self.file_idx}.pt'\n",
    "            torch.save(out, path)\n",
    "            out_shape = out[0]['env_state'].shape\n",
    "            n = self.file_idx * out_shape[0] * out_shape[2]\n",
    "            self.logger.info(f\"{n}: File saved to {path}; env_state shape {out_shape}\")\n",
    "\n",
    "        return self.file_idx   \n",
    "\n",
    "    def _extract_data(self, t):\n",
    "        # obtain the first N planning stage and the corresponding target_y in data\n",
    "        xs, y, done, step_status = self._collect_data(t)\n",
    "        future_y, future_done = self._collect_data(self.delay_n, y_done_only=True)\n",
    "        y = torch.concat([y, future_y], dim=0)\n",
    "        done = torch.concat([done, future_done], dim=0)                \n",
    "        \n",
    "        last_step_real = (step_status == 0) | (step_status == 3)\n",
    "        assert last_step_real[0], \"cur_t should start with 0\"\n",
    "        assert last_step_real.shape[0] == t*self.rec_t, \\\n",
    "            f\" step_status.shape is {last_step_real.shape}, expected {t*self.rec_t} for the first dimension.\"        \n",
    "        assert y.shape[0] == (t + self.delay_n)*self.rec_t, \\\n",
    "            f\" y.shape is {y.shape}, expected {(t + self.delay_n)*self.rec_t} for the first dimension.\"        \n",
    "        \n",
    "        B = y.shape[1]\n",
    "        y = y.view(t + self.delay_n, self.rec_t, B)[:, 0]\n",
    "        done = done.view(t + self.delay_n, self.rec_t, B)[:, 0]\n",
    "        step_status = step_status.view(t, self.rec_t)\n",
    "        # compute target_y\n",
    "        target_y = self._compute_target_y(y, done, self.delay_n)\n",
    "\n",
    "        for k in xs.keys():\n",
    "            xs[k] = xs[k].view((t, self.rec_t) + xs[k].shape[1:])\n",
    "        \n",
    "        xs[\"done\"] = done[:t]\n",
    "        xs[\"step_status\"] = step_status\n",
    "                \n",
    "        return xs, target_y\n",
    "\n",
    "    def _collect_data(self, t, y_done_only=False):\n",
    "        # collect the first t stage from data\n",
    "        step_status = torch.tensor(self.step_status, dtype=torch.long)\n",
    "        next_step_real = (step_status == 2) | (step_status == 3)        \n",
    "        idx = torch.nonzero(next_step_real, as_tuple=False).squeeze()    \n",
    "        last_idx = idx[t-1] + 1\n",
    "        y = torch.stack(self.y[:last_idx], dim=0)\n",
    "        done = torch.stack(self.done[:last_idx], dim=0)\n",
    "        if not y_done_only:\n",
    "            xs = {}\n",
    "            for k in self.xs[0].keys():\n",
    "                xs[k] = torch.stack([v[k] for v in self.xs[:last_idx]], dim=0)                \n",
    "            step_status = step_status[:last_idx]\n",
    "            self.xs = self.xs[last_idx:]\n",
    "            self.y = self.y[last_idx:]\n",
    "            self.done = self.done[last_idx:]\n",
    "            self.step_status = self.step_status[last_idx:]\n",
    "            return xs, y, done, step_status\n",
    "        else:\n",
    "            return y, done\n",
    "        \n",
    "    def _compute_target_y(self, y, done, delay_n):        \n",
    "        # target_y[i] = (y[i] | (~done[i+1] & y[i+1]) | (~done[i+1] & ~done[i+2] & y[i+2]) | ... | (~done[i+1] & ~done[i+2] & ... & ~done[i+M] & y[i+M]))\n",
    "        t, b = y.shape\n",
    "        t = t - delay_n\n",
    "        not_done_cum = torch.ones(delay_n, t, b, dtype=bool)\n",
    "        target_y = y.clone()[:-delay_n]\n",
    "        not_done_cum[0] = ~done[1:1+t]\n",
    "        target_y = target_y | (not_done_cum[0] & y[1:1+t])\n",
    "        for m in range(1, delay_n):\n",
    "            not_done_cum[m] = not_done_cum[m-1] & ~done[m+1:m+1+t]\n",
    "            target_y = target_y | (not_done_cum[m] & y[m+1:m+1+t])\n",
    "        return target_y\n",
    "\n",
    "# ========================================================\n",
    "    \n",
    "total_n = 100000\n",
    "env_n = 128\n",
    "delay_n = 5\n",
    "savedir = \"../logs/detect\"\n",
    "outdir = \"../data/detect\"\n",
    "xpid = \"v5b_sok_drc\"\n",
    "\n",
    "# ========================================================\n",
    "\n",
    "_logger = util.logger()\n",
    "_logger.info(f\"Initializing {xpid} from {savedir}\")\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "ckpdir = os.path.join(savedir, xpid)     \n",
    "if os.path.islink(ckpdir): ckpdir = os.readlink(ckpdir)  \n",
    "ckpdir =  os.path.abspath(os.path.expanduser(ckpdir))\n",
    "outdir = os.path.abspath(os.path.expanduser(outdir))\n",
    "\n",
    "config_path = os.path.join(ckpdir, 'config_c.yaml')\n",
    "flags = util.create_flags(config_path, save_flags=False)\n",
    "disable_thinker = flags.wrapper_type == 1\n",
    "\n",
    "env = Env(\n",
    "        name=flags.name,\n",
    "        env_n=env_n,\n",
    "        gpu=True,\n",
    "        train_model=False,\n",
    "        parallel=False,\n",
    "        savedir=savedir,        \n",
    "        xpid=xpid,\n",
    "        ckp=True,\n",
    "        return_x=True,\n",
    "        return_h=True,\n",
    "    )\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space \n",
    "\n",
    "actor_param = {\n",
    "    \"obs_space\": obs_space,\n",
    "    \"action_space\": action_space,\n",
    "    \"flags\": flags,\n",
    "    \"tree_rep_meaning\": env.get_tree_rep_meaning() if not disable_thinker else None,\n",
    "}\n",
    "actor_net = ActorNet(**actor_param)\n",
    "\n",
    "path = os.path.join(ckpdir, \"ckp_actor.tar\")\n",
    "checkpoint = torch.load(path, map_location=torch.device(\"cpu\"))\n",
    "actor_net.set_weights(checkpoint[\"actor_net_state_dict\"])\n",
    "actor_net.to(device)\n",
    "actor_net.train(False)\n",
    "\n",
    "state = env.reset()\n",
    "env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "\n",
    "# create dir\n",
    "\n",
    "n = 0\n",
    "while True:\n",
    "    name = \"%s-%d-%d\" % (xpid, checkpoint[\"real_step\"], n)\n",
    "    outdir_ = os.path.join(outdir, name)\n",
    "    if not os.path.exists(outdir_):\n",
    "        os.makedirs(outdir_)\n",
    "        print(f\"Outputting to {outdir_}\")\n",
    "        break\n",
    "    n += 1\n",
    "outdir = outdir_\n",
    "\n",
    "detect_buffer = DetectBuffer(outdir=outdir, t=12800//env_n, rec_t=flags.rec_t, logger=_logger, delay_n=delay_n)\n",
    "file_n = total_n // (env_n * detect_buffer.t) + 1\n",
    "_logger.info(f\"Data output directory: {outdir}\")\n",
    "_logger.info(f\"Number of file to be generated: {file_n}\")\n",
    "\n",
    "rescale = \"Sokoban\" in flags.name\n",
    "\n",
    "# save setting\n",
    "\n",
    "env_state_shape = env.observation_space[\"real_states\"].shape[1:]\n",
    "#if rescale: env_state_shape = (3, 40, 40)\n",
    "tree_rep_shape = env.observation_space[\"tree_reps\"].shape[1:] if not disable_thinker else None\n",
    "\n",
    "flags_detect = {\n",
    "    \"dim_actions\": actor_net.dim_actions,\n",
    "    \"num_actions\": actor_net.num_actions,\n",
    "    \"tuple_actions\": actor_net.tuple_action,\n",
    "    \"name\": flags.name,\n",
    "    \"env_state_shape\": list(env_state_shape),\n",
    "    \"tree_rep_shape\": list(tree_rep_shape) if not disable_thinker else None,\n",
    "    \"rescale\": rescale,\n",
    "    \"rec_t\": flags.rec_t,\n",
    "    \"ckpdir\": ckpdir,\n",
    "    \"net_xpid\": xpid,\n",
    "    \"disable_thinker\": disable_thinker,\n",
    "}\n",
    "\n",
    "yaml_file_path = os.path.join(outdir, 'config_detect.yaml')\n",
    "with open(yaml_file_path, 'w') as file:\n",
    "    yaml.dump(flags_detect, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7/data_0.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 253; Return  13.651897272102447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7/data_1.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 531; Return  12.74655379930234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7/data_2.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 821; Return  12.566577492129788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7/data_3.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1108; Return  12.576778128312814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7/data_4.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1408; Return  12.592699009597048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7/data_5.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1713; Return  12.5870462672288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7/data_6.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2011; Return  12.596807713201246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7/data_7.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2303; Return  12.614637580550188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800: File saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5b_sok_drc-49622080-7/data_8.pt; env_state shape torch.Size([100, 1, 1, 128, 3, 80, 80])\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False):\n",
    "\n",
    "    rets = []\n",
    "    last_file_idx = None\n",
    "    \n",
    "    while(True):\n",
    "\n",
    "        actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        if not disable_thinker:\n",
    "            primary_action, reset_action = actor_out.action\n",
    "        else:\n",
    "            primary_action, reset_action = actor_out.action, None\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action, \n",
    "            action_prob=actor_out.action_prob[-1])    \n",
    "        env_out = create_env_out(actor_out.action, state, reward, done, info, flags=flags)\n",
    "        if torch.any(done):\n",
    "            rets.extend(info[\"episode_return\"][done].cpu().tolist())\n",
    "        \n",
    "        # write to detect buffer\n",
    "        if not disable_thinker:\n",
    "            env_state = env_out.xs[0] \n",
    "            if rescale:\n",
    "                #env_state = F.interpolate(env_state , size=(40, 40), mode='bilinear', align_corners=False)\n",
    "                env_state = (env_state * 255).to(torch.uint8)\n",
    "        else:\n",
    "            env_state = env_out.real_states\n",
    "        pri_action = primary_action\n",
    "        xs = {\n",
    "            \"env_state\": env_state,\n",
    "            \"pri_action\": pri_action,            \n",
    "            \"cost\": info[\"cost\"],\n",
    "        }\n",
    "        if not disable_thinker:\n",
    "            xs.update({\n",
    "                \"tree_rep\": state[\"tree_reps\"],\n",
    "                \"reset_action\": actor_out.action[1],\n",
    "            })\n",
    "\n",
    "        y = info['cost']\n",
    "        done = done\n",
    "        step_status = info['step_status'][0].item()\n",
    "\n",
    "        file_idx = detect_buffer.insert(xs, y, done, step_status)\n",
    "        \n",
    "        if file_idx >= file_n: \n",
    "            # last file is for validation\n",
    "            os.rename(f'{outdir}/data_{file_idx}.pt', f'{outdir}/val.pt')\n",
    "            break\n",
    "\n",
    "        if last_file_idx is not None and file_idx != last_file_idx:\n",
    "            print(f\"Episode {len(rets)}; Return  {np.mean(np.array(rets))}\")\n",
    "\n",
    "        last_file_idx = file_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['real_done', 'truncated_done', 'step_status', 'episode_step', 'episode_return', 'model_status'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from thinker.model_net import BaseNet, FrameEncoder\n",
    "from thinker.actor_net import ShallowAFrameEncoder\n",
    "from thinker import util\n",
    "from thinker.core.file_writer import FileWriter\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None, data_n=None, prefix=\"data\"):\n",
    "        self.datadir = datadir\n",
    "        self.transform = transform\n",
    "        self.data = []        \n",
    "        self.samples_per_file = None   \n",
    "        self.data_n = data_n\n",
    "        self.prefix = prefix\n",
    "        self._preload_data(datadir)  # Preload data        \n",
    "\n",
    "    def _preload_data(self, datadir):\n",
    "        # Preload all .pt files\n",
    "        file_list = [f for f in os.listdir(datadir) if f.endswith('.pt') and f.startswith(self.prefix)]\n",
    "        for file_name in file_list:\n",
    "            print(f\"Starting to preload {file_name}\")\n",
    "            xs, y = torch.load(os.path.join(datadir, file_name))\n",
    "            if self.samples_per_file is None:  # Set samples_per_file based on the first file\n",
    "                self.t = xs['env_state'].shape[0]\n",
    "                self.b = xs['env_state'].shape[2]\n",
    "                self.samples_per_file = self.t * self.b\n",
    "            xs.pop('step_status')\n",
    "            xs.pop('done')\n",
    "            # Flatten data across t and b dimensions for easier indexing\n",
    "            for t_idx in range(self.t):\n",
    "                for b_idx in range(self.b):\n",
    "                    flattened_xs = {k: v[t_idx, :, b_idx] for k, v in xs.items()}\n",
    "                    flattened_y = y[t_idx, b_idx]\n",
    "                    self.data.append((flattened_xs, flattened_y))\n",
    "                    if self.data_n is not None and len(self.data) >= self.data_n: return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        xs, y = self.data[idx]\n",
    "        if self.transform:\n",
    "            # Apply transform if necessary. Note: You might need to adjust this part\n",
    "            # based on what your transform expects and can handle\n",
    "            xs = {k: self.transform(v) for k, v in xs.items()}            \n",
    "        return xs, y\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 500):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "        return x\n",
    "\n",
    "class DetectFrameEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,     \n",
    "        dim_rep_actions,   \n",
    "        out_size=128,\n",
    "        stride=2,\n",
    "    ):  \n",
    "        super(DetectFrameEncoder, self).__init__()\n",
    "        self.out_size = out_size\n",
    "        self.encoder = FrameEncoder(prefix=\"se\",\n",
    "                                    actions_ch=dim_rep_actions,\n",
    "                                    input_shape=input_shape,                             \n",
    "                                    size_nn=1,             \n",
    "                                    downscale_c=2,    \n",
    "                                    concat_action=False)\n",
    "        \n",
    "        self.conv = []\n",
    "        in_ch =  self.encoder.out_shape[0]\n",
    "        for ch in [64]:\n",
    "            self.conv.append(nn.ReLU())\n",
    "            self.conv.append(nn.Conv2d(in_channels=in_ch,\n",
    "                                       out_channels=ch,\n",
    "                                       kernel_size=3,\n",
    "                                       stride=1,\n",
    "                                       padding=1,))\n",
    "            in_ch = ch\n",
    "        self.conv = nn.Sequential(*self.conv)\n",
    "        conv_out_size = in_ch * self.encoder.out_shape[1] * self.encoder.out_shape[2]\n",
    "        self.fc = nn.Sequential(nn.Linear(conv_out_size, self.out_size))       \n",
    "\n",
    "    def forward(self, x, action):\n",
    "        # x in shape of (B, C, H, W)\n",
    "        out, _ = self.encoder(x, done=None, actions=action, state={})\n",
    "        out = self.conv(out)\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.fc(out)\n",
    "        return out                                \n",
    "        \n",
    "class DetectNet(BaseNet):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_state_shape,\n",
    "        tree_rep_shape,\n",
    "        dim_actions,\n",
    "        num_actions,\n",
    "        detect_ab=(0,0),\n",
    "        clone=False,\n",
    "        tran_layer_n=3,\n",
    "    ):    \n",
    "        super(DetectNet, self).__init__()\n",
    "        \n",
    "        self.env_state_shape = env_state_shape # in (C, H, W) \n",
    "        self.tree_rep_shape = tree_rep_shape # in (C,) \n",
    "        self.dim_actions = dim_actions\n",
    "        self.num_actions = num_actions\n",
    "        self.dim_rep_actions = self.dim_actions if self.dim_actions > 1 else self.num_actions\n",
    "\n",
    "        self.detect_ab = detect_ab\n",
    "        self.clone = clone\n",
    "\n",
    "        self.enc_out_size = 128\n",
    "        tran_nhead = 8\n",
    "        reminder = tran_nhead - ((self.enc_out_size + tree_rep_shape[0] + self.dim_rep_actions + 1) % tran_nhead)\n",
    "        self.enc_out_size += reminder\n",
    "        #self.true_x_encoder = ShallowAFrameEncoder(input_shape=env_state_shape, out_size=self.enc_out_size)\n",
    "        #self.pred_x_encoder = ShallowAFrameEncoder(input_shape=env_state_shape, out_size=self.enc_out_size)\n",
    "        self.true_x_encoder = DetectFrameEncoder(input_shape=env_state_shape, dim_rep_actions=self.dim_rep_actions, out_size=self.enc_out_size)\n",
    "        self.pred_x_encoder = DetectFrameEncoder(input_shape=env_state_shape, dim_rep_actions=self.dim_rep_actions, out_size=self.enc_out_size)\n",
    "        #self.pred_x_encoder = self.true_x_encoder\n",
    "\n",
    "        self.embed_size = self.enc_out_size + tree_rep_shape[0] + num_actions + 1\n",
    "        self.pos_encoder = PositionalEncoding(self.embed_size)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.embed_size, \n",
    "                                                   nhead=tran_nhead, \n",
    "                                                   dim_feedforward=512,\n",
    "                                                   batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, tran_layer_n)\n",
    "        self.classifier = nn.Linear(self.embed_size, 1)\n",
    "\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5), requires_grad=False) # portion of negative class\n",
    "\n",
    "    def forward(self, env_state, tree_rep, action, reset):\n",
    "        \"\"\"\n",
    "        Forward pass of detection nn\n",
    "        Args:\n",
    "            env_state: float Tensor in shape of (B, rec_t, C, H, W); true and predicted frame\n",
    "            tree_rep: float Tensor in shape of (B, rec_t, C); model output\n",
    "            action: uint Tensor in shape of (B, rec_t, dim_actions); action (real / imaginary)\n",
    "            reset: bool Tensor in shape of  (B, rec_t); reset action\n",
    "        Return:\n",
    "            logit: float Tensor in shape of (B); logit of classifier output\n",
    "            p: float Tensor in shape of (B); prob of classifier output\n",
    "        \"\"\"\n",
    "        B, rec_t = env_state.shape[:2]\n",
    "        if self.detect_ab[0] in [1, 3] or self.detect_ab[1] in [1, 3]:\n",
    "            if self.clone: env_state = env_state.clone()                \n",
    "            if self.detect_ab[0] in [1, 3]:\n",
    "                env_state[:, 0] = 0.\n",
    "            if self.detect_ab[1] in [1, 3]:\n",
    "                env_state[:, 1:] = 0.\n",
    "        if self.detect_ab[0] in [2, 3] or self.detect_ab[1] in [2, 3]:\n",
    "            if self.clone: tree_rep = tree_rep.clone()\n",
    "            if self.detect_ab[0] in [2, 3]:\n",
    "                tree_rep[:, 0] = 0.\n",
    "            if self.detect_ab[1] in [2, 3]:\n",
    "                tree_rep[:, 1:] = 0.\n",
    "        \n",
    "        action = util.encode_action(action, self.dim_actions, self.num_actions)        \n",
    "        true_proc_x = self.true_x_encoder(env_state[:,0], action[:,0])\n",
    "        pred_proc_x = self.pred_x_encoder(\n",
    "            torch.flatten(env_state[:,1:], 0, 1),\n",
    "            torch.flatten(action[:,1:], 0, 1)\n",
    "                                        )\n",
    "        true_proc_x = true_proc_x.view(B, self.enc_out_size).unsqueeze(1) # (B, 1, C)\n",
    "        pred_proc_x = pred_proc_x.view(B, rec_t - 1, self.enc_out_size)  # (B, rec_t - 1, C)\n",
    "        proc_x = torch.concat([true_proc_x, pred_proc_x], dim=1) # (B, rec_t, C)\n",
    "        \n",
    "        embed = [proc_x, tree_rep, action, reset.unsqueeze(-1)]\n",
    "        embed = torch.concat(embed, dim=2) # (B, rec_t, embed_size)\n",
    "        embed_pos = self.pos_encoder(embed)\n",
    "        out = self.transformer_encoder(embed_pos)\n",
    "        logit = self.classifier(out[:, -1, :]).view(B)\n",
    "        return logit, torch.sigmoid(logit)\n",
    "\n",
    "def transform_data(xs, device, flags):\n",
    "    xs_ = {}\n",
    "\n",
    "    env_state = xs[\"env_state\"]\n",
    "    if flags.rescale:\n",
    "        env_state = env_state.float() / 255\n",
    "    xs_[\"env_state\"] = env_state.to(device)\n",
    "\n",
    "    if \"tree_rep\" in xs: xs_[\"tree_rep\"] = xs[\"tree_rep\"].to(device)\n",
    "\n",
    "    action = xs[\"pri_action\"]\n",
    "    if not flags.tuple_actions:\n",
    "        action = action.unsqueeze(-1)\n",
    "    xs_[\"action\"] = action.to(device)\n",
    "\n",
    "    if \"reset_action\" in xs: xs_[\"reset\"] = xs[\"reset_action\"].to(device)\n",
    "    return xs_\n",
    "\n",
    "def evaluate_detect(target_y, pred_y):\n",
    "    # Binarize the predictions\n",
    "    pred_y_binarized = (pred_y > 0.5).float()\n",
    "    target_y = target_y.float()\n",
    "\n",
    "    # Compute the accuracy\n",
    "    acc = torch.mean((pred_y_binarized == target_y).float()).item()\n",
    "    \n",
    "    # Compute the recall\n",
    "    true_positives = (pred_y_binarized * target_y).sum().float()\n",
    "    possible_positives = target_y.sum().float()\n",
    "    rec = (true_positives / (possible_positives + 1e-6)).item()\n",
    "    \n",
    "    # Compute the precision\n",
    "    predicted_positives = pred_y_binarized.sum().float()\n",
    "    prec = (true_positives / (predicted_positives + 1e-6)).item()\n",
    "    \n",
    "    # Compute the F1 score\n",
    "    f1 = 2 * (prec * rec) / (prec + rec + 1e-6)   \n",
    "\n",
    "    neg_p = 1 - torch.mean(target_y.float()).item()\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"rec\": rec,\n",
    "        \"prec\": prec,\n",
    "        \"f1\": f1,\n",
    "        \"neg_p\": neg_p,\n",
    "        }\n",
    "\n",
    "def train_epoch(detect_net, dataloader, optimizer, device, flags, train=True):\n",
    "    if train:\n",
    "        detect_net.train()\n",
    "    else:\n",
    "        detect_net.eval()     \n",
    "    running_train_eval = {}   \n",
    "    with torch.set_grad_enabled(train):\n",
    "        step = 0\n",
    "        for xs, target_y in dataloader:\n",
    "            xs = transform_data(xs, device, flags)\n",
    "            target_y = target_y.to(device)\n",
    "            \n",
    "            logit, pred_y = detect_net(**xs)\n",
    "            n_mean_y = torch.mean((~target_y).float()).item()\n",
    "            detect_net.beta.data = 0.99 * detect_net.beta.data + (1 - 0.99) * n_mean_y\n",
    "            detect_net.beta.data.clamp_(0.05, 0.95)\n",
    "            weights = torch.where(target_y == 1, detect_net.beta.data, 1-detect_net.beta.data)\n",
    "            loss = F.binary_cross_entropy_with_logits(logit, target_y.float(), weight=weights)\n",
    "            train_eval = evaluate_detect(target_y, pred_y)\n",
    "            train_eval[\"loss\"] = loss.item()\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()  # Zero the gradients\n",
    "                loss.backward()  # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "                optimizer.step()  # Perform a single optimization step (parameter update)\n",
    "            \n",
    "            for key in train_eval.keys():\n",
    "                if key not in running_train_eval: \n",
    "                    running_train_eval[key] = train_eval[key]\n",
    "                else:\n",
    "                    running_train_eval[key] += train_eval[key]\n",
    "            step += 1\n",
    "    return {key: val / step for (key, val) in running_train_eval.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint path: /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Starting to preload data_0.pt\n",
      "Starting to preload data_1.pt\n",
      "Starting to preload data_2.pt\n",
      "Starting to preload data_3.pt\n",
      "Starting to preload val.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating log directory: /mnt/c/Users/chung/Personal/RS/thinker/logs/detect/v5_sok/test_senc_1\n",
      "Symlinked log directory: /mnt/c/Users/chung/Personal/RS/thinker/logs/detect/v5_sok/latest\n",
      "Saving arguments to /mnt/c/Users/chung/Personal/RS/thinker/logs/detect/v5_sok/test_senc_1/meta.json\n",
      "Saving messages to /mnt/c/Users/chung/Personal/RS/thinker/logs/detect/v5_sok/test_senc_1/out.log\n",
      "Saving logs data to /mnt/c/Users/chung/Personal/RS/thinker/logs/detect/v5_sok/test_senc_1/logs.csv\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstephen-chung-mh\u001b[0m (\u001b[33mstephen-chung\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/chung/Personal/RS/thinker/notebook/wandb/run-20240204_044114-v5_sok_test_senc_1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stephen-chung/detect_post/runs/v5_sok_test_senc_1' target=\"_blank\">v5_sok_test_senc_1</a></strong> to <a href='https://wandb.ai/stephen-chung/detect_post' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stephen-chung/detect_post' target=\"_blank\">https://wandb.ai/stephen-chung/detect_post</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stephen-chung/detect_post/runs/v5_sok_test_senc_1' target=\"_blank\">https://wandb.ai/stephen-chung/detect_post/runs/v5_sok_test_senc_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detect network size: 2457539\n",
      "Epoch 1/50, acc:0.8717 (0.8424) rec:0.6097 (0.8952) prec:0.3867 (0.3946) f1:0.4521 (0.5443) neg_p:0.8755 (0.8937) loss:0.1100 (0.0684)\n",
      "Epoch 2/50, acc:0.9191 (0.9382) rec:0.9236 (0.7576) prec:0.6258 (0.6877) f1:0.7360 (0.7106) neg_p:0.8756 (0.8937) loss:0.0432 (0.0806)\n",
      "Epoch 3/50, acc:0.9324 (0.9157) rec:0.9456 (0.7616) prec:0.6687 (0.5836) f1:0.7736 (0.6564) neg_p:0.8755 (0.8931) loss:0.0347 (0.0756)\n",
      "Epoch 4/50, acc:0.9450 (0.8632) rec:0.9611 (0.9001) prec:0.7073 (0.4326) f1:0.8088 (0.5764) neg_p:0.8755 (0.8928) loss:0.0273 (0.0684)\n",
      "Epoch 5/50, acc:0.9547 (0.8173) rec:0.9734 (0.9226) prec:0.7477 (0.3620) f1:0.8397 (0.5164) neg_p:0.8755 (0.8937) loss:0.0221 (0.0801)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Epoch 6/50, acc:0.9613 (0.9129) rec:0.9807 (0.8062) prec:0.7721 (0.5593) f1:0.8592 (0.6509) neg_p:0.8754 (0.8916) loss:0.0192 (0.0858)\n",
      "Epoch 7/50, acc:0.9665 (0.8362) rec:0.9833 (0.8858) prec:0.7982 (0.3829) f1:0.8764 (0.5285) neg_p:0.8755 (0.8942) loss:0.0165 (0.0896)\n",
      "Epoch 8/50, acc:0.9704 (0.9140) rec:0.9870 (0.7761) prec:0.8184 (0.5693) f1:0.8909 (0.6502) neg_p:0.8755 (0.8939) loss:0.0143 (0.1292)\n",
      "Epoch 9/50, acc:0.9738 (0.4822) rec:0.9870 (0.9600) prec:0.8336 (0.1670) f1:0.9007 (0.2810) neg_p:0.8755 (0.8937) loss:0.0130 (0.1648)\n",
      "Epoch 10/50, acc:0.9750 (0.9215) rec:0.9877 (0.7862) prec:0.8399 (0.6035) f1:0.9037 (0.6715) neg_p:0.8755 (0.8934) loss:0.0128 (0.1092)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Epoch 11/50, acc:0.9768 (0.9249) rec:0.9874 (0.7609) prec:0.8512 (0.6209) f1:0.9105 (0.6755) neg_p:0.8754 (0.8939) loss:0.0119 (0.1102)\n",
      "Epoch 12/50, acc:0.9799 (0.9253) rec:0.9918 (0.7568) prec:0.8671 (0.6215) f1:0.9225 (0.6766) neg_p:0.8754 (0.8939) loss:0.0102 (0.1465)\n",
      "Epoch 13/50, acc:0.9824 (0.8961) rec:0.9904 (0.8244) prec:0.8825 (0.5091) f1:0.9314 (0.6246) neg_p:0.8755 (0.8934) loss:0.0092 (0.1169)\n",
      "Epoch 14/50, acc:0.9823 (0.9077) rec:0.9911 (0.8196) prec:0.8831 (0.5437) f1:0.9316 (0.6437) neg_p:0.8754 (0.8945) loss:0.0091 (0.1098)\n",
      "Epoch 15/50, acc:0.9824 (0.9158) rec:0.9907 (0.7341) prec:0.8822 (0.5662) f1:0.9304 (0.6350) neg_p:0.8755 (0.8925) loss:0.0089 (0.1220)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Epoch 16/50, acc:0.9855 (0.8971) rec:0.9930 (0.7854) prec:0.9008 (0.5070) f1:0.9428 (0.6106) neg_p:0.8755 (0.8937) loss:0.0077 (0.1221)\n",
      "Epoch 17/50, acc:0.9866 (0.8806) rec:0.9932 (0.8559) prec:0.9091 (0.4624) f1:0.9473 (0.5912) neg_p:0.8755 (0.8945) loss:0.0067 (0.1253)\n",
      "Epoch 18/50, acc:0.9844 (0.9348) rec:0.9902 (0.6960) prec:0.8958 (0.6662) f1:0.9379 (0.6739) neg_p:0.8755 (0.8945) loss:0.0082 (0.1580)\n",
      "Epoch 19/50, acc:0.9862 (0.7724) rec:0.9928 (0.9059) prec:0.9056 (0.3104) f1:0.9452 (0.4596) neg_p:0.8755 (0.8922) loss:0.0078 (0.1415)\n",
      "Epoch 20/50, acc:0.9865 (0.9371) rec:0.9924 (0.6021) prec:0.9084 (0.7636) f1:0.9468 (0.6677) neg_p:0.8755 (0.8928) loss:0.0070 (0.2302)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Epoch 21/50, acc:0.9872 (0.9128) rec:0.9935 (0.7973) prec:0.9123 (0.5662) f1:0.9494 (0.6578) neg_p:0.8755 (0.8939) loss:0.0066 (0.1356)\n",
      "Epoch 22/50, acc:0.9869 (0.9292) rec:0.9933 (0.7234) prec:0.9107 (0.6532) f1:0.9481 (0.6805) neg_p:0.8755 (0.8928) loss:0.0069 (0.1643)\n",
      "Epoch 23/50, acc:0.9888 (0.9263) rec:0.9941 (0.7524) prec:0.9230 (0.6370) f1:0.9557 (0.6837) neg_p:0.8754 (0.8919) loss:0.0058 (0.1679)\n",
      "Epoch 24/50, acc:0.9872 (0.9037) rec:0.9935 (0.8114) prec:0.9123 (0.5324) f1:0.9492 (0.6349) neg_p:0.8755 (0.8937) loss:0.0063 (0.1275)\n",
      "Epoch 25/50, acc:0.9887 (0.9034) rec:0.9948 (0.7909) prec:0.9218 (0.5435) f1:0.9554 (0.6345) neg_p:0.8755 (0.8928) loss:0.0057 (0.1443)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Epoch 26/50, acc:0.9883 (0.9398) rec:0.9936 (0.7023) prec:0.9199 (0.7336) f1:0.9536 (0.7106) neg_p:0.8755 (0.8942) loss:0.0064 (0.1982)\n",
      "Epoch 27/50, acc:0.9890 (0.9227) rec:0.9939 (0.7754) prec:0.9244 (0.6130) f1:0.9560 (0.6766) neg_p:0.8755 (0.8937) loss:0.0055 (0.1751)\n",
      "Epoch 28/50, acc:0.9879 (0.8488) rec:0.9925 (0.8589) prec:0.9200 (0.4030) f1:0.9528 (0.5398) neg_p:0.8755 (0.8922) loss:0.0063 (0.1403)\n",
      "Epoch 29/50, acc:0.9875 (0.8604) rec:0.9934 (0.8396) prec:0.9151 (0.4246) f1:0.9507 (0.5613) neg_p:0.8755 (0.8925) loss:0.0062 (0.0976)\n",
      "Epoch 30/50, acc:0.9891 (0.9243) rec:0.9938 (0.7851) prec:0.9256 (0.6136) f1:0.9571 (0.6855) neg_p:0.8755 (0.8939) loss:0.0056 (0.1350)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Epoch 31/50, acc:0.9904 (0.8275) rec:0.9956 (0.8910) prec:0.9318 (0.3772) f1:0.9612 (0.5215) neg_p:0.8755 (0.8931) loss:0.0049 (0.1336)\n",
      "Epoch 32/50, acc:0.9896 (0.9230) rec:0.9954 (0.7496) prec:0.9267 (0.6045) f1:0.9585 (0.6638) neg_p:0.8755 (0.8942) loss:0.0053 (0.1573)\n",
      "Epoch 33/50, acc:0.9891 (0.9229) rec:0.9935 (0.7632) prec:0.9256 (0.6077) f1:0.9567 (0.6684) neg_p:0.8755 (0.8934) loss:0.0055 (0.1434)\n",
      "Epoch 34/50, acc:0.9900 (0.9377) rec:0.9951 (0.6679) prec:0.9293 (0.7249) f1:0.9594 (0.6891) neg_p:0.8755 (0.8931) loss:0.0050 (0.1991)\n",
      "Epoch 35/50, acc:0.9903 (0.9373) rec:0.9945 (0.6321) prec:0.9317 (0.7571) f1:0.9605 (0.6811) neg_p:0.8755 (0.8937) loss:0.0047 (0.2336)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Epoch 36/50, acc:0.9897 (0.8096) rec:0.9952 (0.8618) prec:0.9264 (0.3416) f1:0.9581 (0.4839) neg_p:0.8755 (0.8934) loss:0.0050 (0.1462)\n",
      "Epoch 37/50, acc:0.9906 (0.8148) rec:0.9965 (0.9224) prec:0.9330 (0.3515) f1:0.9623 (0.5015) neg_p:0.8755 (0.8931) loss:0.0045 (0.1411)\n",
      "Epoch 38/50, acc:0.9907 (0.9127) rec:0.9965 (0.8152) prec:0.9334 (0.5554) f1:0.9626 (0.6563) neg_p:0.8755 (0.8948) loss:0.0045 (0.1478)\n",
      "Epoch 39/50, acc:0.9916 (0.9302) rec:0.9971 (0.7704) prec:0.9390 (0.6564) f1:0.9661 (0.7005) neg_p:0.8755 (0.8928) loss:0.0037 (0.1752)\n",
      "Epoch 40/50, acc:0.9909 (0.9415) rec:0.9950 (0.5777) prec:0.9372 (0.8250) f1:0.9641 (0.6700) neg_p:0.8754 (0.8942) loss:0.0045 (0.2702)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Epoch 41/50, acc:0.9909 (0.9207) rec:0.9950 (0.7414) prec:0.9355 (0.6001) f1:0.9629 (0.6564) neg_p:0.8755 (0.8951) loss:0.0044 (0.1535)\n",
      "Epoch 42/50, acc:0.9924 (0.9254) rec:0.9947 (0.7574) prec:0.9475 (0.6216) f1:0.9696 (0.6746) neg_p:0.8755 (0.8928) loss:0.0038 (0.1835)\n",
      "Epoch 43/50, acc:0.9910 (0.9287) rec:0.9952 (0.6302) prec:0.9368 (0.6659) f1:0.9639 (0.6383) neg_p:0.8755 (0.8939) loss:0.0043 (0.2293)\n",
      "Epoch 44/50, acc:0.9919 (0.9227) rec:0.9963 (0.7336) prec:0.9410 (0.6167) f1:0.9666 (0.6621) neg_p:0.8755 (0.8942) loss:0.0039 (0.2105)\n",
      "Epoch 45/50, acc:0.9923 (0.9334) rec:0.9960 (0.6935) prec:0.9458 (0.6945) f1:0.9692 (0.6879) neg_p:0.8755 (0.8934) loss:0.0039 (0.2250)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n",
      "Epoch 46/50, acc:0.9920 (0.9206) rec:0.9967 (0.7528) prec:0.9425 (0.5955) f1:0.9675 (0.6610) neg_p:0.8755 (0.8942) loss:0.0040 (0.1477)\n",
      "Epoch 47/50, acc:0.9916 (0.9303) rec:0.9965 (0.6680) prec:0.9398 (0.6824) f1:0.9661 (0.6688) neg_p:0.8755 (0.8937) loss:0.0040 (0.2284)\n",
      "Epoch 48/50, acc:0.9925 (0.6131) rec:0.9958 (0.9767) prec:0.9468 (0.2130) f1:0.9697 (0.3472) neg_p:0.8755 (0.8942) loss:0.0033 (0.2264)\n",
      "Epoch 49/50, acc:0.9905 (0.9008) rec:0.9939 (0.8308) prec:0.9326 (0.5188) f1:0.9608 (0.6328) neg_p:0.8754 (0.8928) loss:0.0045 (0.1408)\n",
      "Epoch 50/50, acc:0.9913 (0.9073) rec:0.9957 (0.7979) prec:0.9381 (0.5505) f1:0.9648 (0.6453) neg_p:0.8755 (0.8925) loss:0.0040 (0.1503)\n",
      "Checkpoint saved to /mnt/c/Users/chung/Personal/RS/thinker/data/detect/v5_sok-14137536-0/test_senc_1/ckp_detect.tar\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "flags = argparse.Namespace()\n",
    "\n",
    "# ========================================================\n",
    "\n",
    "flags.ndata = \"v5_sok-14137536-0\"\n",
    "flags.nproject = \"detect\"\n",
    "flags.datadir = \"../data/__nproject__/__ndata__/\"\n",
    "flags.xpid = \"test_senc\"\n",
    "flags.project = \"detect_post\"\n",
    "flags.batch_size = 128\n",
    "flags.learning_rate = 0.0001\n",
    "flags.num_epochs = 50\n",
    "flags.data_n = 50000\n",
    "flags.ckp = False\n",
    "flags.use_wandb = True\n",
    "\n",
    "flags.datadir = flags.datadir.replace(\"__nproject__\", flags.nproject)\n",
    "flags.datadir = flags.datadir.replace(\"__ndata__\", flags.nxpid)\n",
    "# ========================================================\n",
    "\n",
    "\n",
    "if not flags.ckp:\n",
    "    flags.datadir = os.path.abspath(os.path.expanduser(flags.datadir))\n",
    "    # create ckp dir\n",
    "    xpid_n = 0\n",
    "    while (True):\n",
    "        xpid_ = flags.xpid if xpid_n == 0 else flags.xpid + f\"_{xpid_n}\"\n",
    "        ckpdir = os.path.join(flags.datadir, xpid_)\n",
    "        xpid_n += 1\n",
    "        if not os.path.exists(ckpdir):\n",
    "            os.mkdir(ckpdir) \n",
    "            flags.xpid = xpid_\n",
    "            break    \n",
    "else:\n",
    "    ckpdir = os.path.join(flags.datadir, flags.xpid)\n",
    "flags.ckpdir = ckpdir\n",
    "flags.ckp_path = os.path.join(ckpdir, \"ckp_detect.tar\")\n",
    "print(f\"Checkpoint path: {flags.ckp_path}\")\n",
    "\n",
    "# load data\n",
    "dataset = CustomDataset(datadir=flags.datadir, transform=None, data_n=flags.data_n)\n",
    "dataloader = DataLoader(dataset, batch_size=flags.batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(datadir=flags.datadir, transform=None, data_n=2000, prefix=\"val\")\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=flags.batch_size, shuffle=True)\n",
    "\n",
    "# load setting\n",
    "yaml_file_path = os.path.join(flags.datadir, 'config_detect.yaml')\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    flags_data = yaml.safe_load(file)\n",
    "flags_data = argparse.Namespace(**flags_data)\n",
    "flags = argparse.Namespace(**{**vars(flags), **vars(flags_data)}) # merge the two flags\n",
    "flags.full_xpid = flags.net_xpid + \"_\" + flags.xpid\n",
    "\n",
    "plogger = FileWriter(\n",
    "    xpid=flags.xpid,\n",
    "    xp_args=flags.__dict__,\n",
    "    rootdir=flags.ckpdir,\n",
    "    overwrite=not flags.ckp,\n",
    ")\n",
    "\n",
    "if flags.use_wandb: wlogger = util.Wandb(flags)\n",
    "\n",
    "# initalize net\n",
    "device = torch.device(\"cuda\")\n",
    "detect_net = DetectNet(\n",
    "    env_state_shape = flags_data.env_state_shape,\n",
    "    tree_rep_shape = flags_data.tree_rep_shape,\n",
    "    dim_actions = flags_data.dim_actions,\n",
    "    num_actions = flags_data.num_actions,\n",
    ")\n",
    "\n",
    "# load optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    detect_net.parameters(), lr=flags.learning_rate, \n",
    ")\n",
    "\n",
    "if flags.ckp:\n",
    "    checkpoint = torch.load(flags.ckp_path, torch.device(\"cpu\"))\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    detect_net.load_state_dict(checkpoint[\"net_state_dict\"])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    del checkpoint\n",
    "else:\n",
    "    epoch = 0\n",
    "\n",
    "detect_net = detect_net.to(device)\n",
    "util.optimizer_to(optimizer, device)\n",
    "\n",
    "print(\"Detect network size: %d\"\n",
    "        % sum(p.numel() for p in detect_net.parameters())\n",
    "     )\n",
    "\n",
    "while (epoch < flags.num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_train_eval = {}\n",
    "\n",
    "    train_stat = train_epoch(detect_net, dataloader, optimizer, device, flags, train=True)\n",
    "    val_stat = train_epoch(detect_net, val_dataloader, None, device, flags, train=False)\n",
    "    stat = {**train_stat, **{'val/' + key: value for key, value in val_stat.items()}}\n",
    "    stat[\"epoch\"] = epoch\n",
    "    plogger.log(stat)\n",
    "    if flags.use_wandb: wlogger.wandb.log(stat, step=stat['epoch'])\n",
    "\n",
    "    print_str = f'Epoch {epoch+1}/{flags.num_epochs},'\n",
    "    for key in stat.keys(): \n",
    "        if 'val/' + key in stat.keys():\n",
    "            print_str += f\" {key}:{stat[key]:.4f} ({stat['val/'+key]:.4f})\"\n",
    "    print(print_str)    \n",
    "\n",
    "    epoch += 1    \n",
    "    if epoch % 5 == 0 or epoch >= flags.num_epochs:\n",
    "        # save checkpoint\n",
    "        d = {\n",
    "            \"epoch\": epoch,\n",
    "            \"flags\": flags,\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"net_state_dict\": detect_net.state_dict(),\n",
    "        }\n",
    "        torch.save(d, flags.ckp_path)\n",
    "        print(f\"Checkpoint saved to {flags.ckp_path}\")\n",
    "\n",
    "    if epoch % 10 == 0 or epoch >= flags.num_epochs:\n",
    "        wlogger.wandb.save(\n",
    "            os.path.join(flags.ckpdir, \"*\"), flags.ckpdir\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2288818359375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deprecated\n",
    "\n",
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from thinker import util\n",
    "\n",
    "datadir = \"../data/detect/v5_sok-5993808-1/\"\n",
    "datadir = os.path.abspath(os.path.expanduser(datadir))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None):\n",
    "        self.datadir = datadir\n",
    "        self.file_list = [f for f in os.listdir(datadir) if f.endswith('.pt')]\n",
    "        self.transform = transform\n",
    "        xs, y = torch.load(os.path.join(datadir, self.file_list[0]))        \n",
    "        self.t = xs['env_state'].shape[0]\n",
    "        self.b = xs['env_state'].shape[2]\n",
    "        self.samples_per_file = self.t * self.b\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) * self.samples_per_file  # Adjust based on your data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx // self.samples_per_file\n",
    "        within_file_idx = idx % self.samples_per_file\n",
    "        t_idx = within_file_idx // self.b\n",
    "        b_idx = within_file_idx % self.b\n",
    "        xs, y = torch.load(os.path.join(self.datadir, self.file_list[file_idx]))\n",
    "        xs.pop('step_status')\n",
    "        xs.pop('done')\n",
    "        xs = util.dict_map(xs, lambda x: x[t_idx, :, b_idx])\n",
    "        y = y[t_idx, b_idx]\n",
    "        return xs, y\n",
    "\n",
    "# To load data and train\n",
    "dataset = CustomDataset(datadir)\n",
    "# print(dataset[100])\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing env 0 with device cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symlinked log directory: /mnt/c/Users/chung/Personal/RS/thinker/notebook/logs/latest\n",
      "Wrote config file to /mnt/c/Users/chung/Personal/RS/thinker/notebook/logs/detect-20240204-023020/config_c.yaml\n"
     ]
    }
   ],
   "source": [
    "from thinker.actor_net import DRCNet\n",
    "from thinker.main import Env\n",
    "from thinker.self_play import init_env_out, create_env_out\n",
    "from thinker import util\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "env_n = 16\n",
    "flags = util.create_setting(args=[], drc=True, save_flags=False, wrapper_type=1)\n",
    "env = Env(\n",
    "        name=\"Sokoban-v0\",\n",
    "        env_n=env_n,\n",
    "        gpu=True,\n",
    "        train_model=False,\n",
    "        parallel=False,\n",
    "        return_x=True,\n",
    "        return_h=True,\n",
    "        flags=flags,\n",
    "        wrapper_type=1,\n",
    "    )\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space \n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "actor_net = DRCNet(obs_space=obs_space, action_space=action_space, flags=flags, tree_rep_meaning=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"../logs/detect/v1a_drc/ckp_actor.tar\")\n",
    "actor_net.load_state_dict(checkpoint[\"actor_net_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_ = torch.load(\"../logs/detect/v5b_sok_drc/ckp_actor.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_net.to(device)\n",
    "state = env.reset()\n",
    "env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "rets = []\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    \n",
    "    while(True):\n",
    "        actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        primary_action, reset_action = actor_out.action, None\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action)    \n",
    "        if torch.any(done):\n",
    "            rets.extend(info[\"episode_return\"][done].cpu().tolist())            \n",
    "            print(f\"Episode {len(rets)}; Return  {np.mean(np.array(rets))}\")\n",
    "        env_out = create_env_out(primary_action, state, reward, done, info, flags=flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ['step', 'real_step', 'actor_net_optimizer_state_dict', 'actor_net_scheduler_state_dict', 'actor_net_state_dict']\n",
    "for c in cs: checkpoint_[c] = checkpoint[c]\n",
    "torch.save(checkpoint_, \"../logs/detect/v5b_sok_drc/ckp_actor.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1; Return  13.789999961853027\n",
      "Episode 2; Return  13.78000020980835\n",
      "Episode 3; Return  13.773333549499512\n",
      "Episode 4; Return  13.760000228881836\n",
      "Episode 5; Return  13.744000244140626\n",
      "Episode 6; Return  13.730000178019205\n",
      "Episode 8; Return  13.710000038146973\n",
      "Episode 9; Return  13.700000021192762\n",
      "Episode 11; Return  13.683636491948908\n",
      "Episode 12; Return  13.674166758855185\n",
      "Episode 13; Return  13.680000085097094\n",
      "Episode 15; Return  13.670000076293945\n",
      "Episode 16; Return  13.658750057220459\n",
      "Episode 17; Return  13.662353010738597\n",
      "Episode 18; Return  13.651666747199165\n",
      "Episode 19; Return  13.654210592571058\n",
      "Episode 20; Return  13.655500078201294\n",
      "Episode 21; Return  13.652857235499791\n",
      "Episode 23; Return  13.656087046084197\n",
      "Episode 24; Return  13.651250084241232\n",
      "Episode 25; Return  13.649200096130372\n",
      "Episode 26; Return  13.650769343742958\n",
      "Episode 27; Return  13.653703795539009\n",
      "Episode 28; Return  13.652857235499791\n",
      "Episode 30; Return  13.649000072479248\n",
      "Episode 32; Return  13.656562596559525\n",
      "Episode 33; Return  13.659090995788574\n",
      "Episode 34; Return  13.651764785542207\n",
      "Episode 35; Return  13.286000108718872\n",
      "Episode 36; Return  13.28666678402159\n",
      "Episode 37; Return  13.294594706715765\n",
      "Episode 38; Return  13.30078960092444\n",
      "Episode 39; Return  13.313333456332867\n",
      "Episode 40; Return  13.319000118970871\n",
      "Episode 42; Return  13.33547630196526\n",
      "Episode 43; Return  13.344651261041331\n",
      "Episode 44; Return  13.349772816354578\n",
      "Episode 45; Return  13.35822229915195\n",
      "Episode 46; Return  13.34826095726179\n",
      "Episode 47; Return  13.350000092323791\n",
      "Episode 48; Return  13.352708434065184\n",
      "Episode 49; Return  13.358367447950402\n",
      "Episode 51; Return  13.099411870626842\n",
      "Episode 52; Return  13.110961638677578\n",
      "Episode 53; Return  13.120566133901757\n",
      "Episode 55; Return  13.142181916399435\n",
      "Episode 56; Return  12.921964398186121\n",
      "Episode 57; Return  12.9357895848521\n",
      "Episode 58; Return  12.951034586748172\n",
      "Episode 59; Return  12.964745868818234\n",
      "Episode 60; Return  12.97716677164038\n",
      "Episode 61; Return  12.985573869992475\n",
      "Episode 63; Return  13.005238207323211\n",
      "Episode 65; Return  13.022923197883825\n",
      "Episode 66; Return  13.03060619013779\n",
      "Episode 68; Return  13.04661778119557\n",
      "Episode 69; Return  12.854927674583767\n",
      "Episode 70; Return  12.867428701264517\n",
      "Episode 71; Return  12.87859167515392\n",
      "Episode 72; Return  12.888472348451614\n",
      "Episode 74; Return  12.909864995930645\n",
      "Episode 75; Return  12.919200137456258\n",
      "Episode 76; Return  12.929210665978884\n",
      "Episode 77; Return  12.940389747743483\n",
      "Episode 78; Return  12.94833347430596\n",
      "Episode 79; Return  12.95734190940857\n",
      "Episode 80; Return  12.965125134587288\n",
      "Episode 81; Return  12.96679026109201\n",
      "Episode 82; Return  12.974634289741516\n",
      "Episode 83; Return  12.978674845523145\n",
      "Episode 84; Return  12.986904907794226\n",
      "Episode 85; Return  12.992941320643705\n",
      "Episode 86; Return  12.99790712012801\n",
      "Episode 88; Return  12.863750154321844\n",
      "Episode 90; Return  12.882555707295735\n",
      "Episode 91; Return  12.888901249393003\n",
      "Episode 92; Return  12.89728275589321\n",
      "Episode 93; Return  12.904301222934517\n",
      "Episode 94; Return  12.913829935357926\n",
      "Episode 95; Return  12.923789616634972\n",
      "Episode 96; Return  12.92927098274231\n",
      "Episode 97; Return  12.936288813954777\n",
      "Episode 98; Return  12.822959347647064\n",
      "Episode 99; Return  12.830808246978606\n",
      "Episode 101; Return  12.847524919132194\n",
      "Episode 102; Return  12.854902129547268\n",
      "Episode 103; Return  12.738252603891985\n",
      "Episode 104; Return  12.746538640214848\n",
      "Episode 106; Return  12.763019041070399\n",
      "Episode 109; Return  12.787156132383084\n",
      "Episode 111; Return  12.801261430388099\n",
      "Episode 112; Return  12.809375167957374\n",
      "Episode 113; Return  12.816283353662069\n",
      "Episode 114; Return  12.82526332558247\n",
      "Episode 115; Return  12.83173929401066\n",
      "Episode 116; Return  12.83663809813302\n",
      "Episode 117; Return  12.844786490130629\n",
      "Episode 118; Return  12.85025439929154\n",
      "Episode 119; Return  12.85739511702241\n",
      "Episode 121; Return  12.868429917934513\n",
      "Episode 123; Return  12.786585537398734\n",
      "Episode 125; Return  12.799680176734924\n",
      "Episode 126; Return  12.805635097480955\n",
      "Episode 127; Return  12.812519862895876\n",
      "Episode 128; Return  12.819609553553164\n",
      "Episode 131; Return  12.739542171245313\n",
      "Episode 132; Return  12.747803214824561\n",
      "Episode 133; Return  12.754812215503893\n",
      "Episode 134; Return  12.673433028050322\n",
      "Episode 135; Return  12.68170389422664\n",
      "Episode 136; Return  12.687279604813632\n",
      "Episode 137; Return  12.694452746941225\n",
      "Episode 139; Return  12.70913688227427\n",
      "Episode 140; Return  12.715285902363913\n",
      "Episode 141; Return  12.722766147437671\n",
      "Episode 143; Return  12.651958240495695\n",
      "Episode 144; Return  12.65944463842445\n",
      "Episode 145; Return  12.58455192467262\n",
      "Episode 147; Return  12.597415160159676\n",
      "Episode 148; Return  12.605067757335869\n",
      "Episode 149; Return  12.612349185367558\n",
      "Episode 150; Return  12.617533526420594\n",
      "Episode 151; Return  12.62576177893885\n",
      "Episode 152; Return  12.632500185778266\n",
      "Episode 153; Return  12.639804106132656\n",
      "Episode 154; Return  12.646363817252121\n",
      "Episode 155; Return  12.653225986419185\n",
      "Episode 156; Return  12.659743769046587\n",
      "Episode 157; Return  12.665923745768845\n",
      "Episode 158; Return  12.672658402708512\n",
      "Episode 159; Return  12.680000174720332\n",
      "Episode 160; Return  12.6868751719594\n",
      "Episode 161; Return  12.69248464388877\n",
      "Episode 162; Return  12.69839523015199\n",
      "Episode 164; Return  12.638719686647741\n",
      "Episode 166; Return  12.646385721413486\n",
      "Episode 167; Return  12.65251514868822\n",
      "Episode 168; Return  12.587916851043701\n",
      "Episode 170; Return  12.60117665459128\n",
      "Episode 171; Return  12.60649141233567\n",
      "Episode 172; Return  12.613023436346719\n",
      "Episode 173; Return  12.616531972940258\n",
      "Episode 174; Return  12.623218569262274\n",
      "Episode 175; Return  12.62977160862514\n",
      "Episode 176; Return  12.636136542667042\n",
      "Episode 177; Return  12.575141427207129\n",
      "Episode 178; Return  12.580730518598235\n",
      "Episode 180; Return  12.593500179714626\n",
      "Episode 181; Return  12.52834272582228\n",
      "Episode 182; Return  12.534176008386927\n",
      "Episode 184; Return  12.546956704362579\n",
      "Episode 185; Return  12.553459641095754\n",
      "Episode 186; Return  12.558709861770753\n",
      "Episode 187; Return  12.565401250028355\n",
      "Episode 190; Return  12.583263336357318\n",
      "Episode 191; Return  12.58858656696\n",
      "Episode 192; Return  12.593802262718478\n",
      "Episode 193; Return  12.600414685634751\n",
      "Episode 194; Return  12.605515641649975\n",
      "Episode 195; Return  12.61097453618661\n",
      "Episode 196; Return  12.615612423541595\n",
      "Episode 197; Return  12.620913882546013\n",
      "Episode 198; Return  12.62661633768467\n",
      "Episode 199; Return  12.632412234143397\n",
      "Episode 200; Return  12.578350178599358\n",
      "Episode 201; Return  12.58343301542956\n",
      "Episode 202; Return  12.587425922993386\n",
      "Episode 203; Return  12.534532204637387\n",
      "Episode 204; Return  12.539657046397528\n",
      "Episode 205; Return  12.545317254996881\n",
      "Episode 206; Return  12.493398244519836\n",
      "Episode 207; Return  12.499662020932073\n",
      "Episode 209; Return  12.510526501390922\n",
      "Episode 211; Return  12.52000018601169\n",
      "Episode 212; Return  12.524905842992494\n",
      "Episode 213; Return  12.530141028440054\n",
      "Episode 214; Return  12.535327286920815\n",
      "Episode 215; Return  12.48548856058786\n",
      "Episode 216; Return  12.490833522544968\n",
      "Episode 217; Return  12.495484059307433\n",
      "Episode 218; Return  12.50073413400475\n",
      "Episode 219; Return  12.5063015592697\n",
      "Episode 220; Return  12.511136550794948\n",
      "Episode 221; Return  12.51619928185217\n",
      "Episode 222; Return  12.521621808812425\n",
      "Episode 223; Return  12.526906015092482\n",
      "Episode 224; Return  12.531785898442779\n",
      "Episode 225; Return  12.535644630326166\n",
      "Episode 226; Return  12.540309920247678\n",
      "Episode 227; Return  12.544185207278717\n",
      "Episode 228; Return  12.549868606161652\n",
      "Episode 229; Return  12.554235992993851\n",
      "Episode 231; Return  12.562857328039227\n",
      "Episode 232; Return  12.567629495057567\n",
      "Episode 233; Return  12.572918639674207\n",
      "Episode 234; Return  12.577051465837364\n",
      "Episode 235; Return  12.581021460573725\n",
      "Episode 237; Return  12.589620434785191\n",
      "Episode 239; Return  12.598535746710072\n",
      "Episode 240; Return  12.601583514114221\n",
      "Episode 241; Return  12.606224247528804\n",
      "Episode 242; Return  12.611198525290844\n",
      "Episode 243; Return  12.615926104317968\n",
      "Episode 246; Return  12.626422937807998\n",
      "Episode 247; Return  12.631012319553236\n",
      "Episode 248; Return  12.634798558969651\n",
      "Episode 249; Return  12.63911663719928\n",
      "Episode 250; Return  12.642520169734954\n",
      "Episode 252; Return  12.651349375645319\n",
      "Episode 253; Return  12.655652341635331\n",
      "Episode 255; Return  12.663137422355952\n",
      "Episode 256; Return  12.667500165756792\n",
      "Episode 257; Return  12.669688882994745\n",
      "Episode 259; Return  12.677451904676136\n",
      "Episode 260; Return  12.681615552076927\n",
      "Episode 261; Return  12.68574729413365\n",
      "Episode 262; Return  12.689809327362148\n",
      "Episode 264; Return  12.69814410489617\n",
      "Episode 266; Return  12.706015200991379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m#actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m#primary_action, reset_action = actor_out.action, None\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m         primary_action, actor_state \u001b[38;5;241m=\u001b[39m \u001b[43mactor_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactor_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         primary_action \u001b[38;5;241m=\u001b[39m primary_action[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     17\u001b[0m         reset_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/actor_net.py:974\u001b[0m, in \u001b[0;36mDRCNet_.forward\u001b[0;34m(self, obs, core_state, greedy)\u001b[0m\n\u001b[1;32m    972\u001b[0m             nd \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(nd)\n\u001b[1;32m    973\u001b[0m         nd \u001b[38;5;241m=\u001b[39m nd\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 974\u001b[0m         output, core_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcore_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mnd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mnd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m     core_output_list\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    976\u001b[0m core_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(core_output_list)\n",
      "File \u001b[0;32m/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/core/rnn.py:339\u001b[0m, in \u001b[0;36mConvAttnLSTM.forward_single\u001b[0;34m(self, x, core_state, reset, reset_attn)\u001b[0m\n\u001b[1;32m    336\u001b[0m concat_k_cur \u001b[38;5;241m=\u001b[39m core_state[n \u001b[38;5;241m*\u001b[39m layer_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    337\u001b[0m concat_v_cur \u001b[38;5;241m=\u001b[39m core_state[n \u001b[38;5;241m*\u001b[39m layer_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m h_next, c_next, concat_k, concat_v \u001b[38;5;241m=\u001b[39m \u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcell_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_k_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_v_cur\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask_reshape\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_scale \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m h_next\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[1;32m    343\u001b[0m     h_next\u001b[38;5;241m.\u001b[39mregister_hook(\u001b[38;5;28;01mlambda\u001b[39;00m grad: grad \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_scale)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/chung/Personal/RS/thinker/thinker/thinker/core/rnn.py:108\u001b[0m, in \u001b[0;36mConvAttnLSTMCell.forward\u001b[0;34m(self, input, h_cur, c_cur, concat_k, concat_v, attn_mask)\u001b[0m\n\u001b[1;32m    106\u001b[0m combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28minput\u001b[39m, h_cur], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# concatenate along channel axis\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool_inject:\n\u001b[0;32m--> 108\u001b[0m     combined \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj_max_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_cur\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# concatenate along channel axis\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear:\n\u001b[1;32m    113\u001b[0m     combined_conv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain(combined[:, :, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "actor_net = actor_net.to(device)\n",
    "state = env.reset()\n",
    "#env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "env_out = init_env_out(state, flags=flags, dim_actions=1, tuple_action=False)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "rets = []\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    \n",
    "    while(True):\n",
    "        #actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        #primary_action, reset_action = actor_out.action, None\n",
    "        primary_action, actor_state = actor_net(obs=env_out, core_state=actor_state, greedy=False)\n",
    "        primary_action = primary_action[0]\n",
    "        reset_action = None\n",
    "\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action)    \n",
    "        if torch.any(done):\n",
    "            rets.extend(info[\"episode_return\"][done].cpu().tolist())            \n",
    "            print(f\"Episode {len(rets)}; Return  {np.mean(np.array(rets))}\")\n",
    "        env_out = create_env_out(primary_action, state, reward, done, info, flags=flags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
