{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting DRC\n",
    "\n",
    "This notebook will give a brief guide for the project of interpreting planning agents. Note that the majority of the code in this repo is used for another planning algorithm called the Thinker, so only a few Python files in this repo are necessary for the project. First, follow the readme to install Sokoban and the Thinker repo. If Sokoban is installed successfully, you should be able to run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import thinker\n",
    "import thinker.viz_utils as viz\n",
    "import thinker.util as util\n",
    "import gym\n",
    "import gym_sokoban\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment shape + type: (8, 8, 7) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test if Sokoban is installed sucessfully\n",
    "mini = True\n",
    "env = gym.make(\"Sokoban-v0\", mini=mini)\n",
    "state = env.reset()\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "print(\"Environment shape + type:\", state.shape, type(state))\n",
    "if mini:\n",
    "    viz.plot_mini_sokoban(state, True)\n",
    "else:  \n",
    "    plt.imshow(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sokoban\n",
    "The goal is to push all four boxes to the target space (the red-bordered location). There are five actions: no-op, left, up, down, right. Boxes can only be pushed but not pulled. A reward of +1 is given if a box is pushed onto the target, and an additional +10 reward is given if all four boxes are on the target, which will also end the episode. A reward of -1 is given if a box on the target is pushed away from it. A reward of -0.01 is added at every time step to encourage solving the task as quickly as possible. The maximum length of each episode is around 120 steps, after which the episode is forced to terminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing env 0 with device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Init. environment with obs space \u001b[91mBox(0, 1, (7, 8, 8), uint8)\u001b[0m and action space \u001b[91mDiscrete(5)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 7, 8, 8]) <class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test if Thinker is installed sucessfully\n",
    "# the thinker.make environment return a batched environment; here we use a batch size of 16\n",
    "\n",
    "env_n = 4 # batch size of the environment; can be increased to like 128 if using GPU\n",
    "gpu = False # change to True for using GPU instead of CPU\n",
    "mini_sokoban = True # if True, use mini-sokoban board (i.e. board is 8x8x7 array)\n",
    "\n",
    "env = thinker.make(\n",
    "    \"Sokoban-v0\", \n",
    "    env_n=env_n, \n",
    "    gpu=gpu,\n",
    "    wrapper_type=1, # wrapper_type 1 means default environment without Thinker-augmentation\n",
    "    has_model=False, # the following arg are mainly for Thinker-augmentation only\n",
    "    train_model=False, \n",
    "    parallel=False, \n",
    "    save_flags=False,\n",
    "    mini=mini_sokoban     \n",
    "    ) \n",
    "state = env.reset()\n",
    "for _ in range(10):\n",
    "    action = torch.tensor(env.action_space.sample())\n",
    "    state, reward, done, info = env.step(action)\n",
    "print(state[\"real_states\"].shape, type(state[\"real_states\"]))\n",
    "if mini_sokoban:\n",
    "    viz.plot_mini_sokoban(state[\"real_states\"][0])\n",
    "else:\n",
    "    util.plot_raw_state(state[\"real_states\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return: -0.51\n"
     ]
    }
   ],
   "source": [
    "# now let's define a DRC agent\n",
    "from thinker.actor_net import DRCNet\n",
    "\n",
    "flags = util.create_setting(args=[], save_flags=False, wrapper_type=1) # the default flags; almost all of them won't be used in DRC\n",
    "flags.mini = mini\n",
    "drc_net = DRCNet(\n",
    "    obs_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    flags=flags,\n",
    "    record_state=True,\n",
    "    )\n",
    "drc_net.to(env.device)\n",
    "\n",
    "# define initial RNN-state of DRC\n",
    "rnn_state = drc_net.initial_state(batch_size=env_n, device=env.device)\n",
    "\n",
    "state = env.reset() \n",
    "env_out = util.init_env_out(state, flags, dim_actions=1, tuple_action=False) # this converts the state to EnvOut object that can be processed by actor\n",
    "actor_out, rnn_state = drc_net(env_out, rnn_state) # actor_out contains both the critic and actor output of DRC\n",
    "\n",
    "# should take less than a few minutes to complete 100 episodes; average return should be around -1.00\n",
    "episode_returns = []\n",
    "while(len(episode_returns) < 10):\n",
    "    state, reward, done, info = env.step(actor_out.action)\n",
    "    env_out = util.create_env_out(actor_out.action, state, reward, done, info, flags)\n",
    "    actor_out, rnn_state = drc_net(env_out, rnn_state)\n",
    "    # record done episode\n",
    "    if torch.any(done):\n",
    "        episode_returns.extend(info[\"episode_return\"][done].tolist())\n",
    "    \n",
    "print(\"Average return: %.2f\" % torch.mean(torch.tensor(episode_returns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average return: 12.25\n"
     ]
    }
   ],
   "source": [
    "# now let load a trained DRC agent\n",
    "import os\n",
    "ckp_path = \"../drc_mini\"\n",
    "ckp_path = os.path.join(util.full_path(ckp_path), \"ckp_actor_realstep49500192.tar\")\n",
    "ckp = torch.load(ckp_path, env.device)\n",
    "drc_net.load_state_dict(ckp[\"actor_net_state_dict\"], strict=False)\n",
    "\n",
    "# create list to store agent+env states\n",
    "agent_env_list = []\n",
    "\n",
    "# define initial RNN-state of DRC\n",
    "rnn_state = drc_net.initial_state(batch_size=env_n, device=env.device)\n",
    "\n",
    "# run the trained drc again\n",
    "state = env.reset() \n",
    "env_out = util.init_env_out(state, flags, dim_actions=1, tuple_action=False) # this converts the state to EnvOut object that can be processed by actor\n",
    "actor_out, rnn_state = drc_net(env_out, rnn_state) # actor_out contains both the critic and actor output of DRC\n",
    "\n",
    "# create and activate logit lens\n",
    "from thinker.logitlens import DRCTickLogitLens\n",
    "drc_lens = DRCTickLogitLens(drc_net=drc_net)\n",
    "drc_lens.activate()\n",
    "logit_list = []\n",
    "\n",
    "episode_returns = []\n",
    "while(len(episode_returns) < 10):\n",
    "    state, reward, done, info = env.step(actor_out.action)\n",
    "    env_out = util.create_env_out(actor_out.action, state, reward, done, info, flags)\n",
    "    actor_out, rnn_state = drc_net(env_out, rnn_state)\n",
    "    logit_list.append(drc_lens.get_logits(env_out))\n",
    "    agent_env_list.append((drc_net.hidden_state, state[\"real_states\"]))\n",
    "    # record done episode\n",
    "    if torch.any(done):\n",
    "        episode_returns.extend(info[\"episode_return\"][done].tolist())\n",
    "\n",
    "drc_lens.deactivate()\n",
    "    \n",
    "print(\"Average return: %.2f\" % torch.mean(torch.tensor(episode_returns))) # should have a return of around 13, i.e. over 90% solving rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinker.viz_utils import create_gif_multi_env_single_channel, create_gif_single_env_multi_channels\n",
    "for layer in [0,1,2]:\n",
    "    for channel in range(64):\n",
    "        create_gif_multi_env_single_channel(agent_env_list=agent_env_list,\n",
    "                                            envs=[0,1,2,3], layer=layer,\n",
    "                                            channel=channel, \n",
    "                                            mini=True, \n",
    "                                            gif_name=f\"layer{layer}_channel{channel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_detector(feature_idx, mode):\n",
    "    \"\"\"Create feature detector functions to extract features from mini-sokoban boards. Boards must be (7,8,8) arrays\n",
    "    Args:\n",
    "        feature_idx (int): index of feature of interest (see sokoban.cpp)\n",
    "        mode (str): either \"adj\" (to count number of adjacent features) or \"num\" (to count total number of features on board)\n",
    "    \"\"\"\n",
    "    assert mode in [\"adj\", \"num\"], \"Please enter a valid mode - either ADJ or NUM\"\n",
    "    if mode == \"adj\":\n",
    "        def feature_detector(board):\n",
    "            h, w = board.shape[1:]\n",
    "            x, y = ((board[4,:,:]==1) + (board[5,:,:]==1)).nonzero()[0,:]\n",
    "            adj_coords = [(xp, yp) for xp, yp in [(x+1,y), (x-1,y), (x,y+1), (x,y-1)] if xp>-1 and xp<h and yp>-1 and yp<w]\n",
    "            n_hits = 0\n",
    "            for (xp,yp) in adj_coords:\n",
    "                if board[feature_idx, xp, yp] == 1:\n",
    "                    n_hits += 1\n",
    "            return n_hits\n",
    "    else:\n",
    "        def feature_detector(board):\n",
    "            return torch.sum((board[feature_idx,:,:]==1).int()).item()\n",
    "    return feature_detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_wall_detector = make_feature_detector(feature_idx=0, mode=\"adj\")\n",
    "adj_boxnotontar_detector = make_feature_detector(feature_idx=2, mode=\"adj\")\n",
    "adj_boxontar_detector = make_feature_detector(feature_idx=3, mode=\"adj\")\n",
    "adj_tar_detector = make_feature_detector(feature_idx=6, mode=\"adj\")\n",
    "num_boxnotontar_detector = make_feature_detector(feature_idx=2, mode=\"num\")\n",
    "feature_fncs = [\n",
    "    (\"adj_walls\", adj_wall_detector),\n",
    "    (\"adj_boxnotontar\", adj_boxnotontar_detector),\n",
    "    (\"adj_boxontar\", adj_boxontar_detector),\n",
    "    (\"adj_tar_detector\", adj_tar_detector),\n",
    "    (\"num_boxnotontar_detector\", num_boxnotontar_detector)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tom/mlmi/dissertation/thinker_private_planning/logs/thinker/drc_base/ckp_actor.tar'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "util.full_path(ckp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing env 0 with device cpu\n",
      "Init. environment with obs space \u001b[91mBox(0, 1, (7, 8, 8), uint8)\u001b[0m and action space \u001b[91mDiscrete(5)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = thinker.make(\n",
    "    \"Sokoban-v0\", \n",
    "    env_n=1, \n",
    "    gpu=gpu,\n",
    "    wrapper_type=1, # wrapper_type 1 means default environment without Thinker-augmentation\n",
    "    has_model=False, # the following arg are mainly for Thinker-augmentation only\n",
    "    train_model=False, \n",
    "    parallel=False, \n",
    "    save_flags=False,\n",
    "    mini=mini_sokoban \n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class ProbingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "probing_data = []\n",
    "episode_entry = []\n",
    "\n",
    "rnn_state = drc_net.initial_state(batch_size=1, device=env.device)\n",
    "\n",
    "# run the trained drc again\n",
    "state = env.reset() \n",
    "env_out = util.init_env_out(state, flags, dim_actions=1, tuple_action=False) # this converts the state to EnvOut object that can be processed by actor\n",
    "actor_out, rnn_state = drc_net(env_out, rnn_state) # actor_out contains both the critic and actor output of DRC\n",
    "episode_length = 0\n",
    "board_num = 0\n",
    "\n",
    "episode_returns = []\n",
    "while(len(episode_returns) < 10):\n",
    "\n",
    "    if episode_length > 0:\n",
    "        step_entry[\"reward\"] = reward.item()\n",
    "        episode_entry.append(step_entry)\n",
    "\n",
    "    state, reward, done, info = env.step(actor_out.action)\n",
    "    env_out = util.create_env_out(actor_out.action, state, reward, done, info, flags)\n",
    "    actor_out, rnn_state = drc_net(env_out, rnn_state)\n",
    "\n",
    "    step_entry = {feature:fnc(state[\"real_states\"][0]) for feature, fnc in feature_fncs}\n",
    "    step_entry[\"action\"] = actor_out.action.item()\n",
    "    step_entry[\"board_state\"] = state[\"real_states\"][0]\n",
    "    step_entry[\"hidden_states\"] = drc_net.hidden_state\n",
    "    step_entry[\"board_num\"] = board_num\n",
    "    episode_length += 1\n",
    "\n",
    "    if done:\n",
    "        for step, step_entry in enumerate(episode_entry):\n",
    "            step_entry[\"episode_length\"] = episode_length\n",
    "            step_entry[\"steps_remaining\"] = episode_length - step\n",
    "            step_entry[\"action_plus1\"] = episode_entry[step+1][\"action\"] if step < episode_length-2 else 9\n",
    "            step_entry[\"action_plus2\"] = episode_entry[step+2][\"action\"] if step < episode_length-3 else 9\n",
    "            step_entry[\"action_plus3\"] = episode_entry[step+3][\"action\"] if step < episode_length-4 else 9\n",
    "            step_entry[\"action_plus4\"] = episode_entry[step+4][\"action\"] if step < episode_length-5 else 9\n",
    "            step_entry[\"action_plus5\"] = episode_entry[step+5][\"action\"] if step < episode_length-6 else 9\n",
    "            step_entry[\"reward_plus1\"] = episode_entry[step+1][\"reward\"] if step < episode_length-2 else 9\n",
    "            step_entry[\"reward_plus2\"] = episode_entry[step+2][\"reward\"] if step < episode_length-3 else 9\n",
    "            step_entry[\"reward_plus3\"] = episode_entry[step+3][\"reward\"] if step < episode_length-4 else 9\n",
    "            step_entry[\"reward_plus4\"] = episode_entry[step+4][\"reward\"] if step < episode_length-5 else 9\n",
    "            step_entry[\"reward_plus5\"] = episode_entry[step+5][\"reward\"] if step < episode_length-6 else 9\n",
    "        probing_data += episode_entry\n",
    "        episode_returns.extend(info[\"episode_return\"][done].tolist())\n",
    "        episode_length = 0\n",
    "        board_num += 1\n",
    "\n",
    "probing_dataset = ProbingDataset(probing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(probing_dataset, './probe.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.load(\"./probe.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(episode_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(5)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, step_entry in enumerate(episode_entry):\n",
    "    step_entry[\"episode_length\"] = episode_length\n",
    "    step_entry[\"steps_remaining\"] = episode_length - step\n",
    "    step_entry[\"action_plus1\"] = episode_entry[step+1] if step < episode_length-1 else 9\n",
    "    step_entry[\"action_plus2\"] = episode_entry[step+2] if step < episode_length-2 else 9\n",
    "    step_entry[\"action_plus3\"] = episode_entry[step+3] if step < episode_length-3 else 9\n",
    "    step_entry[\"action_plus4\"] = episode_entry[step+4] if step < episode_length-4 else 9\n",
    "    step_entry[\"action_plus5\"] = episode_entry[step+5] if step < episode_length-5 else 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_dataset = ProbingDataset(probing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adj_walls': 1,\n",
       " 'adj_boxnotontar': 0,\n",
       " 'adj_boxontar': 0,\n",
       " 'adj_tar_detector': 0,\n",
       " 'num_boxnotontar_detector': 4,\n",
       " 'action': 2,\n",
       " 'board_state': tensor([[[1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 0, 0, 0, 1],\n",
       "          [1, 1, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 1, 0, 1, 1, 1],\n",
       "          [0, 0, 1, 0, 1, 1, 0, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 0, 0],\n",
       "          [0, 0, 0, 0, 1, 1, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 0, 0, 1],\n",
       "          [0, 0, 1, 0, 1, 0, 1, 1],\n",
       "          [0, 0, 0, 0, 0, 1, 1, 1]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 1, 0, 0, 1, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 1, 0, 1, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 1, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0, 0, 1, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 1, 1, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.uint8),\n",
       " 'board_num': 0,\n",
       " 'reward': -0.009999999776482582,\n",
       " 'episode_length': 26,\n",
       " 'steps_remaining': 26}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probing_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'real_done': tensor([True]),\n",
       " 'truncated_done': tensor([False]),\n",
       " 'cost': tensor([False]),\n",
       " 'episode_return': tensor([13.0100], dtype=torch.float64),\n",
       " 'episode_step': tensor([99]),\n",
       " 'step_status': tensor([3]),\n",
       " 'model_status': {'processed_n': 0,\n",
       "  'warm_up_n': 0,\n",
       "  'running': False,\n",
       "  'finish': True}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 99},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 98},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 97},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 96},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 95},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 94},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 93},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 2,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 92},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 91},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 2,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 90},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 2,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 89},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 88},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 2,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 87},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 86},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 2,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 85},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 84},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 83},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 82},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 81},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 80},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 79},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 2,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 78},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 77},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 2,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 76},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 75},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 74},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 73},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 72},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 71},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 70},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 69},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 68},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 67},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 2,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 66},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 65},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 64},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 63},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 62},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 61},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 2,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 60},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 59},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 2,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 58},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 57},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 56},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 55},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 54},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 53},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 4,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 52},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 1,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': 0.9900000095367432,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 51},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 1,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 50},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 1,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 49},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 48},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 1,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 47},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 46},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 1,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 45},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 1,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 44},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 43},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 42},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 41},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 40},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 39},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 38},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 3,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 37},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 1,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': 0.9900000095367432,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 36},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 35},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 1,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 34},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 33},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 32},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 31},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 30},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 2,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 29},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 28},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 27},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 26},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 25},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 1,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 24},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 23},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 22},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 21},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 20},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 19},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 18},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 2,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 17},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 1,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': 0.9900000095367432,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 16},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 15},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 14},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 13},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 12},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 11},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 10},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 9},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 8},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 7},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 6},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 5},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 4},\n",
       " {'adj_walls': 2,\n",
       "  'adj_boxnotontar': 0,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 3},\n",
       " {'adj_walls': 1,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 2},\n",
       " {'adj_walls': 0,\n",
       "  'adj_boxnotontar': 1,\n",
       "  'adj_boxontar': 0,\n",
       "  'adj_tar_detector': 0,\n",
       "  'num_boxnotontar_detector': 1,\n",
       "  'reward': -0.009999999776482582,\n",
       "  'episode_length': 99,\n",
       "  'steps_remaining': 1}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 192, 8, 8])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drc_net.hidden_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc\n",
    "\n",
    "You can directly access the stacked hidden state by `.hidden_state` after each call to drc_net:\n",
    "\n",
    "`print(drc_net.hidden_state.shape) # should be [16, 4, 192, 10, 10]`\n",
    "\n",
    "16 is the batch size; 4 is the four hidden state for the three inner ticks (h_0, h_1, h_2, h_3); 192 is the stacked channel (each RNN-layer has 64 channel, and there are 3 RNN-layers); 10 are the width and height \n",
    "\n",
    "## Relevant Files\n",
    "\n",
    "- `thinker/actor_net.py` contains the DRC network\n",
    "- `thinker/core/rnn.py` contains the DRC-block used for building DRC\n",
    "- `learn_actor.py` contains the code for training the agent; not necessary for this project unless you need to train a new agent\n",
    "\n",
    "In case you want to train a new DRC agent, run (take around a day for a 3090):\n",
    "\n",
    "`python train.py --xpid drc --drc true --actor_unroll_len 20 --reg_cost 1 --actor_learning_rate 4e-4 --entropy_cost 1e-2 --v_trace_lamb 0.97 --actor_adam_eps 1e-4 --has_model false`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
