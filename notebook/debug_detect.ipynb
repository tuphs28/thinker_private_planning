{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handcraft network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detect_train import *\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "#datadir = \"/home/scuk/RS/thinker/data/detect/v5_sok-32052928-0\"\n",
    "datadir = \"/home/scuk/RS/thinker/data/detect/v5c_sp0-49956736-0\"\n",
    "dataset = CustomDataset(datadir=datadir, transform=None, chunk_n=1, data_n=10000)\n",
    "sampler = ChunkSampler(dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=2048, sampler=ChunkSampler(dataset))\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# load setting\n",
    "yaml_file_path = os.path.join(datadir, 'config_detect.yaml')\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    flags_data = yaml.safe_load(file)\n",
    "flags_data = argparse.Namespace(**flags_data)\n",
    "num_actions = flags_data.num_actions\n",
    "rec_t = flags_data.rec_t\n",
    "\n",
    "# Path to your BMP file\n",
    "image_path = '/home/scuk/RS/thinker/data/player_on_dan_small.bmp'\n",
    "# Load the image\n",
    "image = Image.open(image_path)\n",
    "# Convert the image to a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to Tensor, scales to [0, 1] range\n",
    "])\n",
    "search_image = transform(image).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.870221757888794, 'rec': 0.6387614607810974, 'prec': 0.6094553828239441, 'f1': 0.6229297759159571, 'neg_p': 0.8314314842224121}\n"
     ]
    }
   ],
   "source": [
    "def find_max_similarity_single_function(x, search_image):\n",
    "    B, C, H, W = x.shape  # Batch size, Channels, Height, Width\n",
    "    block_size = 8\n",
    "    num_blocks_h = H // block_size  # Number of horizontal blocks\n",
    "    num_blocks_w = W // block_size  # Number of vertical blocks\n",
    "\n",
    "    x_reshaped = x.view(B, C, num_blocks_h, block_size, num_blocks_w, block_size)\n",
    "    # Permute to group blocks together while keeping the batch and channel dimensions intact\n",
    "    x_permuted = x_reshaped.permute(0, 2, 4, 1, 3, 5)\n",
    "    # Flatten the block grid dimensions to list all blocks sequentially\n",
    "    x_blocks = x_permuted.reshape(B, num_blocks_h * num_blocks_w, C * block_size * block_size)\n",
    "    # Normalize the blocks and the search_image\n",
    "    x_blocks_norm = F.normalize(x_blocks+1e-6, p=2, dim=-1)  # Normalize over channel dimension\n",
    "    search_image_norm = F.normalize(torch.flatten(search_image), p=2, dim=-1)\n",
    "\n",
    "    similarity = torch.sum(x_blocks_norm * search_image_norm, dim=-1)\n",
    "\n",
    "    # Find the maximum similarity for each image in the batch\n",
    "    max_similarity, _ = similarity.view(B, -1).max(dim=1)\n",
    "    return max_similarity\n",
    "\n",
    "def mask_top_rank(x, rank):\n",
    "    # args: x (tensor) of shape (B, N); rank (int)\n",
    "    # return a mask that equals 1 if the element of each row is the rank largest element\n",
    "    B, N = x.shape\n",
    "    sorted_values, _ = x.sort(dim=1, descending=True)\n",
    "    ties = (sorted_values[:, 1:] - sorted_values[:, :-1]) != 0\n",
    "    cum_ties = torch.cumsum(ties, dim=-1)\n",
    "    cum_ties = torch.concat([torch.zeros(B, 1, device=x.device), cum_ties], dim=-1)\n",
    "    idx = torch.argmax((cum_ties == rank).float(), dim=1)\n",
    "    not_found = torch.all(~(cum_ties == rank), dim=-1)\n",
    "    rank_values = sorted_values[torch.arange(B, device=x.device), idx]\n",
    "    mask = x == rank_values.unsqueeze(-1)\n",
    "    mask[not_found] = False\n",
    "    return mask\n",
    "\n",
    "#B = 2048\n",
    "#env_state = torch.stack([dataset[idx][0][\"env_state\"] for idx in range(B)]).to(device)\n",
    "#tree_rep = torch.stack([dataset[idx][0][\"tree_rep\"] for idx in range(B)]).to(device)\n",
    "#target_y = torch.stack([dataset[idx][1] for idx in range(B)]).to(device)\n",
    "\n",
    "eval_results = {}\n",
    "search_rank = 0\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "\n",
    "    for xs, target_y in dataloader:\n",
    "\n",
    "        env_state = xs[\"env_state\"].to(device)\n",
    "        tree_rep = xs[\"tree_rep\"].to(device)\n",
    "        target_y = target_y.to(device)\n",
    "\n",
    "        B, rec_t = env_state.shape[:2]\n",
    "\n",
    "        # for sokoban\n",
    "        # max_sim = find_max_similarity_single_function(torch.flatten(env_state, 0, 1), search_image)\n",
    "        # max_sim = max_sim.view(B, rec_t)\n",
    "\n",
    "        # for point goal\n",
    "        mask = torch.zeros(240, dtype=torch.bool)\n",
    "        for i in range(3, 4):\n",
    "            mask[60*i+22:60*i+22+16] = 1\n",
    "            #mask[60*i+41:60*i+41+16] = 1\n",
    "        #max_sim = torch.sum((env_state[:, :, mask] > 0.95).float(), dim=-1) >= 0.5\n",
    "        max_sim = torch.sum((env_state[:, :, mask] > 0.9).float(), dim=-1) >= 1\n",
    "\n",
    "        # compute last rollout return\n",
    "\n",
    "        idx_reset = num_actions * 4 + 6\n",
    "        idx_rr = idx_reset + flags_data.rec_t + 1\n",
    "        reset = tree_rep[:, :, idx_reset].bool()\n",
    "        rollout_return = tree_rep[:, :, idx_rr]\n",
    "\n",
    "        last_rollout_return = rollout_return.clone()\n",
    "        r = last_rollout_return[:, -1].clone()\n",
    "        for n in range(flags_data.rec_t-1, -1, -1):\n",
    "            r[reset[:, n]] = last_rollout_return[reset[:, n], n]\n",
    "            last_rollout_return[:, n] = r  \n",
    "\n",
    "        search_mask = torch.zeros(B, rec_t, dtype=torch.bool, device=device)\n",
    "        search_mask[:, 0] = 1\n",
    "\n",
    "        for m in range(search_rank+1):\n",
    "            search_mask = search_mask | mask_top_rank(last_rollout_return, m)\n",
    "\n",
    "        max_sim[~search_mask] = 0\n",
    "        max_sim = torch.max(max_sim, dim=-1)[0]\n",
    "        pred_y = max_sim > 0.95\n",
    "        \n",
    "        result = evaluate_detect(target_y, pred_y)\n",
    "        for k, v in result.items():\n",
    "            if k not in eval_results: \n",
    "                eval_results[k] = [v]\n",
    "            else:\n",
    "                eval_results[k].append(v)\n",
    "\n",
    "\n",
    "for k in eval_results:\n",
    "    eval_results[k] = np.mean(np.array(eval_results[k]))\n",
    "\n",
    "print(eval_results)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sim = find_max_similarity_single_function(torch.flatten(env_state, 0, 1), search_image)\n",
    "env_state_ = env_state.flatten(0, 1)\n",
    "\n",
    "midx = torch.nonzero(max_sim > 0.95).squeeze(-1)\n",
    "idx = torch.randperm(midx.shape[0])\n",
    "midx = midx[idx][:20]\n",
    "d_env_state = env_state_[midx]\n",
    "\n",
    "midx = torch.nonzero(max_sim < 0.95).squeeze(-1)\n",
    "idx = torch.randperm(midx.shape[0])\n",
    "midx = midx[idx][:20]\n",
    "s_env_state = env_state_[midx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "\n",
    "border_size = 0  # Size of the black border\n",
    "scale_factor = 6\n",
    "\n",
    "# Calculate the new size including the border\n",
    "if border_size > 0:\n",
    "    new_height = d_env_state.size(2) + 2*border_size\n",
    "    new_width = d_env_state.size(3) + 2*border_size\n",
    "\n",
    "    d_env_state_ = torch.zeros((d_env_state.size(0), 3, new_height, new_width), device=d_env_state.device)\n",
    "    d_env_state_[:, :, border_size:-border_size, border_size:-border_size] = d_env_state\n",
    "\n",
    "    s_env_state_ = torch.zeros((s_env_state.size(0), 3, new_height, new_width), device=s_env_state.device)\n",
    "    s_env_state_[:, :, border_size:-border_size, border_size:-border_size] = s_env_state\n",
    "else:\n",
    "    d_env_state_ = d_env_state\n",
    "    s_env_state_ = s_env_state\n",
    "\n",
    "d_env_state_ = F.interpolate(d_env_state_, scale_factor=scale_factor, mode='nearest')\n",
    "s_env_state_ = F.interpolate(s_env_state_, scale_factor=scale_factor, mode='nearest')\n",
    "\n",
    "for i in range(d_env_state.shape[0]):\n",
    "    image_filename = f\"../data/sample/d_{i}.png\"\n",
    "    img = d_env_state_[i]\n",
    "    save_image(img, image_filename)\n",
    "for i in range(s_env_state.shape[0]):\n",
    "    image_filename = f\"../data/sample/s_{i}.png\"\n",
    "    img = s_env_state_[i]\n",
    "    save_image(img, image_filename)    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRC hidden state visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRC hidden state visualization\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "import argparse\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Sampler\n",
    "from detect_train import *\n",
    "\n",
    "datadir = \"../data/detect/v18_drc_2-1/\"\n",
    "dataset = CustomDataset(datadir=datadir, transform=None, data_n=3200, prefix=\"test\", chunk_n=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "for n in range(20):\n",
    "    data = dataset[n]\n",
    "    env_state = data[0][\"env_state\"][0]\n",
    "    hidden_state = data[0][\"hidden_state\"][0]\n",
    "\n",
    "    # Calculate the correct number of rows for all channels\n",
    "    num_channels_per_part = 64  # This should be 64 given 192 total channels\n",
    "\n",
    "    # Create a figure with a custom layout, making the first row larger\n",
    "    fig = plt.figure(figsize=(16, 3 * num_channels_per_part + 4))  # Adjusted figure height based on number of channels\n",
    "    gs = gridspec.GridSpec(nrows=num_channels_per_part * 2 + 1, ncols=12, height_ratios=[2] + [1] * 2 * num_channels_per_part)\n",
    "\n",
    "    # Plotting env_state in the first row, spanning all columns\n",
    "    env_ax = plt.subplot(gs[0, :])\n",
    "    env_ax.imshow(torch.clamp(env_state, 0., 1.).float().permute(1, 2, 0).numpy())\n",
    "    env_ax.set_title('Environment State')\n",
    "    env_ax.axis('off')\n",
    "\n",
    "    # Plotting hidden state images\n",
    "    hidden_state_parts = torch.chunk(hidden_state, 3, dim=1)\n",
    "    for part_idx, part in enumerate(hidden_state_parts):\n",
    "        for channel_idx in range(num_channels_per_part):  # Adjusted to iterate over all channels\n",
    "            h_min = torch.min(part[:, channel_idx]).item()\n",
    "            h_max = torch.max(part[:, channel_idx]).item()\n",
    "            for t in range(4):\n",
    "                row = channel_idx * 2 + 1  # Start from the second row of the grid\n",
    "                col = t + part_idx * 4\n",
    "                ax = plt.subplot(gs[row, col])\n",
    "                img_to_plot = part[t, channel_idx].detach().numpy()\n",
    "                ax.imshow(img_to_plot, cmap='gray', vmin=h_min, vmax=h_max)                \n",
    "                ax.axis('off')\n",
    "                if channel_idx == 0:  # Labeling the first row of hidden state images\n",
    "                    ax.set_title(f'Layer {part_idx + 1}: t={t}')\n",
    "                if col == 0 and channel_idx > 0:\n",
    "                    ax.set_title(f\"Channel {channel_idx + 1}\")\n",
    "                if t > 0:\n",
    "                    img_to_plot = (part[t, channel_idx] - part[t - 1, channel_idx]).detach().numpy()\n",
    "                    ax = plt.subplot(gs[row + 1, col])\n",
    "                    ax.imshow(img_to_plot, cmap='gray')\n",
    "                    ax.axis('off')\n",
    "                    \n",
    "\n",
    "    # plt.show()\n",
    "    plt.savefig(f'drc_{n}.png', bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "import argparse\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from detect_train import *\n",
    "\n",
    "xs, ys = torch.load(os.path.abspath(\"../data/detect/v18_drc_8mds-4/data_0.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_actions = torch.transpose(xs[\"pri_action\"], 1, 2)\n",
    "im_action = im_actions[..., 1:]\n",
    "real_action = ys['last_real_actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4341)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.all(im_action[..., :1] == real_action[..., :1], dim=-1).float())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deprecated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2288818359375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deprecated\n",
    "\n",
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from thinker import util\n",
    "\n",
    "datadir = \"../data/detect/v5_sok-5993808-1/\"\n",
    "datadir = os.path.abspath(os.path.expanduser(datadir))\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, datadir, transform=None):\n",
    "        self.datadir = datadir\n",
    "        self.file_list = [f for f in os.listdir(datadir) if f.endswith('.pt')]\n",
    "        self.transform = transform\n",
    "        xs, y = torch.load(os.path.join(datadir, self.file_list[0]))        \n",
    "        self.t = xs['env_state'].shape[0]\n",
    "        self.b = xs['env_state'].shape[2]\n",
    "        self.samples_per_file = self.t * self.b\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list) * self.samples_per_file  # Adjust based on your data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_idx = idx // self.samples_per_file\n",
    "        within_file_idx = idx % self.samples_per_file\n",
    "        t_idx = within_file_idx // self.b\n",
    "        b_idx = within_file_idx % self.b\n",
    "        xs, y = torch.load(os.path.join(self.datadir, self.file_list[file_idx]))\n",
    "        xs.pop('step_status')\n",
    "        xs.pop('done')\n",
    "        xs = util.dict_map(xs, lambda x: x[t_idx, :, b_idx])\n",
    "        y = y[t_idx, b_idx]\n",
    "        return xs, y\n",
    "\n",
    "# To load data and train\n",
    "dataset = CustomDataset(datadir)\n",
    "# print(dataset[100])\n",
    "train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing env 0 with device cuda\n",
      "Model network size: 6637133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symlinked log directory: /home/stephen/RS/thinker/notebook/logs/latest\n",
      "Wrote config file to /home/stephen/RS/thinker/notebook/logs/detect-20240205-143653/config_c.yaml\n"
     ]
    }
   ],
   "source": [
    "from thinker.actor_net import DRCNet, ActorNetBase\n",
    "from thinker.main import Env\n",
    "from thinker.self_play import init_env_out, create_env_out\n",
    "from thinker import util\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "env_n = 16\n",
    "flags = util.create_setting(args=[], drc=False, save_flags=False, see_h=True, legacy=True, wrapper_type=0, has_action_seq=False)\n",
    "env = Env(\n",
    "        name=\"Sokoban-v0\",\n",
    "        env_n=env_n,\n",
    "        gpu=True,\n",
    "        train_model=False,\n",
    "        parallel=False,\n",
    "        return_x=True,\n",
    "        return_h=True,\n",
    "        flags=flags,\n",
    "    )\n",
    "\n",
    "obs_space = env.observation_space\n",
    "action_space = env.action_space \n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "# actor_net = DRCNet(obs_space=obs_space, action_space=action_space, flags=flags, tree_rep_meaning=None)\n",
    "actor_net = ActorNetBase(obs_space=obs_space, action_space=action_space, flags=flags, tree_rep_meaning=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-inf, inf, (16, 111), float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_space[\"tree_reps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../logs/detect/v1a_base_dirloss/ckp_actor.tar\")[\"actor_net_state_dict\"]\n",
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    key = key.replace(\"actor_encoder\", \"h_encoder\")\n",
    "    key = key.replace(\"core\", \"tree_rep_encoder.rnn\")    \n",
    "    key = key.replace(\"initial_enc\", \"tree_rep_encoder.rnn_in_fc\")    \n",
    "    key = key.replace(\"model_stat_fc\", \"tree_rep_encoder.rnn_out_fc\")    \n",
    "    new_state_dict[key] = value\n",
    "actor_net.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in actor_net.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in new_state_dict.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../logs/detect/v1a_base_dirloss/ckp_actor.tar\")[\"actor_net_state_dict\"]\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_net.to(device)\n",
    "state = env.reset()\n",
    "env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "rets = []\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    \n",
    "    while(True):\n",
    "        actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        primary_action, reset_action = actor_out.action, None\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action)    \n",
    "        if torch.any(done):\n",
    "            rets.extend(info[\"episode_return\"][done].cpu().tolist())            \n",
    "            print(f\"Episode {len(rets)}; Return  {np.mean(np.array(rets))}\")\n",
    "        env_out = create_env_out(primary_action, state, reward, done, info, flags=flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ['step', 'real_step', 'actor_net_optimizer_state_dict', 'actor_net_scheduler_state_dict', 'actor_net_state_dict']\n",
    "for c in cs: checkpoint_[c] = checkpoint[c]\n",
    "torch.save(checkpoint_, \"../logs/detect/v5b_sok_drc/ckp_actor.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "actor_net = actor_net.to(device)\n",
    "state = env.reset()\n",
    "#env_out = init_env_out(state, flags=flags, dim_actions=actor_net.dim_actions, tuple_action=actor_net.tuple_action)  \n",
    "env_out = init_env_out(state, flags=flags, dim_actions=1, tuple_action=False)  \n",
    "actor_state = actor_net.initial_state(batch_size=env_n, device=device)\n",
    "rets = []\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    \n",
    "    while(True):\n",
    "        #actor_out, actor_state = actor_net(env_out=env_out, core_state=actor_state, greedy=False)\n",
    "        #primary_action, reset_action = actor_out.action, None\n",
    "        primary_action, actor_state = actor_net(obs=env_out, core_state=actor_state, greedy=False)\n",
    "        primary_action = primary_action[0]\n",
    "        reset_action = None\n",
    "\n",
    "        state, reward, done, info = env.step(\n",
    "            primary_action=primary_action, \n",
    "            reset_action=reset_action)    \n",
    "        if torch.any(done):\n",
    "            rets.extend(info[\"episode_return\"][done].cpu().tolist())            \n",
    "            print(f\"Episode {len(rets)}; Return  {np.mean(np.array(rets))}\")\n",
    "        env_out = create_env_out(primary_action, state, reward, done, info, flags=flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinker.wrapper import DMSuiteEnv\n",
    "# Example usage\n",
    "env = DMSuiteEnv(domain_name=\"acrobot\", task_name=\"swingup\", rgb=False)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(obs, reward, done)\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thinker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
